{"meta":{"title":"XNERV SURVEYS","subtitle":"God's in his heaven.<br/>All's right with the world.","description":"Xnerv Wang (xnervwang) 的技术博客，主要涉及C/C++、数据库引擎开发、文件系统、TCP/IP与网络、分布式系统和Linux等领域，也会偶尔刷一刷LeetCode等题库。毕业于南京大学软件学院，曾经在腾讯、百度、微软上海等大型互联网企业工作，目前奋斗在微软西雅图，从事Azure上的MySQL/PostgreSQL开源数据库云服务开发。","author":"Xnerv Wang (xnervwang)","url":"https://xnerv.wang","root":"/"},"pages":[{"title":"于是，开始写博客吧！","date":"2017-05-28T19:06:00.000Z","updated":"2018-03-14T12:46:31.000Z","comments":true,"path":"about/index.html","permalink":"https://xnerv.wang/about/index.html","excerpt":"记忆中从小的时候，大概还是在初中吧，就一直希望有自己的网站。当时还是2001年还是2002年的时候，博客的概念还没有兴起，很多人唯一接触电脑的机会就是在那些破旧小巷里的网吧。现在想来我走上IT之路的星星之火就是从这时开始闪烁出微弱的光芒的。那时候还不懂编程，唯一接触过跟编程相关的可能就是那种类似小霸王学习机上自带的被阉割过的QBasic语言吧。那时候也不像现在这样是一个知识爆炸的年代，有什么不懂都可以百度一下谷歌一下。身边的人大多还不明白电脑是什么，仅有的接触过电脑的人也只是在电脑上玩过CS、红警、以及后来的传奇。于是小霸王说明书的“CTRL+C”就这么困扰了我两三年，一直不明白“+”到底代表什么。直到后来突然有一天如醍醐灌顶般顿悟，同时按下CTRL和C两个键，Bingo！","text":"记忆中从小的时候，大概还是在初中吧，就一直希望有自己的网站。当时还是2001年还是2002年的时候，博客的概念还没有兴起，很多人唯一接触电脑的机会就是在那些破旧小巷里的网吧。现在想来我走上IT之路的星星之火就是从这时开始闪烁出微弱的光芒的。那时候还不懂编程，唯一接触过跟编程相关的可能就是那种类似小霸王学习机上自带的被阉割过的QBasic语言吧。那时候也不像现在这样是一个知识爆炸的年代，有什么不懂都可以百度一下谷歌一下。身边的人大多还不明白电脑是什么，仅有的接触过电脑的人也只是在电脑上玩过CS、红警、以及后来的传奇。于是小霸王说明书的“CTRL+C”就这么困扰了我两三年，一直不明白“+”到底代表什么。直到后来突然有一天如醍醐灌顶般顿悟，同时按下CTRL和C两个键，Bingo！ 上大学选择了软件工程专业，有机会也有能力搭建自己的网站了。此时个人博客的概念开始火起来，我也在CSDN、博客园等技术网站上申请了博客，虽然不那么积极更新就是。网页收藏夹中的技术文章链接常常达到几百篇，不断地消化又不断地堆积。从大一到现在已十年，现在收藏夹中仍有近百篇文章等待去啃。关于技术的总结和笔记也很多，也经历了记事本，Word，印象笔记，为知笔记，Cmd Markdown，Atom+OneDrive这样一条不断演化的笔记书写和保存之路。关于Atom我后面应该会再另开一篇文章来总结，这也是一个挺不错的编辑工具，无论是编辑博客还是编辑代码。 走出校园走上工作之后，经常会想重拾博客这条曾经还没走多远就放弃了的路。虽然现在博客的时代已经过去，就连微博的光芒也已暗淡，朋友圈看似已经占领了碎片化阅读的整个世界。不过对于搞IT技术的人而言，技术博客仍然是我们平时学习的重要一环。也很想将自己这些年来学习到的知识通过博客的形式分享给大家，只是这几年来从大学到腾讯到百度到微软，经历过很多产品和技术，零零碎碎的东西收集了一大堆，却一直没有好的机会拾掇拾掇，用更简洁的直观的方式展示给大家。分享的过程，也是总结和复习的过程。无论是工作还是生活，走得更远，有时也会更加迷茫。偶尔停下匆忙的脚步，回想和记录下自己曾经经历过的一切，提醒自己真正想要追寻的一切。 因此，这段时间也一直在摸索如何在自己的精力范围内搭建自己的博客。最终还是选择用Hexo再加上NexT这样的简洁主题，Markdown肯定是工程师的不二选择，没有哪个工程师会愿意深陷在类似word这种富文本编辑的泥坑中：“为什么第二段的行间间距比第一段大？”，“如何让列表重新从1开始编号？”……Hexo的优势之一是可以用Atom等编辑器来编辑和管理文章，然后直接用Hexo就可以发布到GitHub Pages等可以托管静态web内容的地方，也可以绑定一级域名。但是Hexo还是有很多的细节耗费了不少的精力，例如访问计数、评论系统等，尤其是最近多说评论关闭，不得不让我担忧，即使使用其它的免费评论系统，最终哪天也不得不面对系统关闭的命运，到时备份和导出评论的工作可能会坑爹。最终还是决定暂时不开放评论，等到博客有一定人气的时候再考虑这个问题，现在评论系统对于我而言只是一个伪需求。 之所以没有采用WordPress，主要是因为对于我们工程师而言，WP的一大缺点可能就是Markdown的显示效果实在是不咋地，各种调整后还是不甚满意。而且由于我可能会时常修改已经发布的文章，例如纠错，或者补充内容等，所以还是希望能用Atom等编辑器管理文章内容（用vim也行啊哈哈），而不是用WP的web编辑框，然后用一个脚本批量更新发布。我也正在开发一个通过直接管理WP的数据库来发布和更新文章的脚本，这样以后就能像Hexo批量发布一样批量更新WP的文章列表了。WP的数据库结构不是很复杂，如果只是要批量更新文章的话，脚本逻辑应该还比较简单，到时候我也会开源道GitHub上供大家参考。 之前有写过一些相关的系列文章，如LeetCode刷题题解等，除了在本博客上再发布一次之外，我还会用GitBook单独发布出来。GitBook有点类似Hexo，但文章是以类似电子书而不是博客的形式进行组织，方便查找和阅读整个系列主题，而不是去一堆文章中遍历。但是GitBook刚面世也不久，bug不少feature不多，很多功能都不齐全，plugins也缺胳膊断腿很多都长期不更新或者不兼容了。后续我可能也会写一篇关于GitBook使用上的文章来说明我在使用中遇到的一些问题和解决的方法。 OK，说干就干，一颗博客界的新星冉冉升起！"},{"title":"404页面","date":"2017-05-16T00:53:23.000Z","updated":"2017-05-15T10:11:03.000Z","comments":true,"path":"/404.html","permalink":"https://xnerv.wang/404.html","excerpt":"","text":""},{"title":"分类","date":"2014-12-22T20:39:04.000Z","updated":"2019-12-16T05:24:12.000Z","comments":true,"path":"categories/index.html","permalink":"https://xnerv.wang/categories/index.html","excerpt":"","text":""},{"title":"schedule","date":"2019-11-18T06:28:31.000Z","updated":"2019-11-18T06:33:06.000Z","comments":true,"path":"schedule/index.html","permalink":"https://xnerv.wang/schedule/index.html","excerpt":"","text":""},{"title":"标签","date":"2015-10-01T19:39:04.000Z","updated":"2019-12-16T05:24:44.000Z","comments":true,"path":"tags/index.html","permalink":"https://xnerv.wang/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"用VSCode代替Atom作为Markdown编辑器","slug":"use-vscode-instead-of-atom-as-markdown","date":"2020-06-01T05:06:00.000Z","updated":"2023-08-21T02:24:21.381Z","comments":true,"path":"use-vscode-instead-of-atom-as-markdown/","link":"","permalink":"https://xnerv.wang/use-vscode-instead-of-atom-as-markdown/","excerpt":"","text":"从大学开始我就有做编程笔记的习惯。最开始是写在txt文件中直接放本地硬盘，也因此由于不小心格式化硬盘而丢了一些原始的笔记。后来在云笔记开始兴起之后，使用过几年的为知笔记。再加上为知笔记有直接保存网页的插件，因此我也将浏览器收藏夹中收集的一些技术网页直接保存到了为知笔记中，避免了网页失效而造成的知识丢失（其实现在的技术博客会被套娃式地转载好多次，就算删除原始文章也基本能找到了。。。）。随着积累的没看过的网页越来越多，消耗的速度完全跟不上增长的速度，也慢慢开始总结和消化收集的网页上的知识，记录下要点并标注来源URL，因此开始转向了Markdown。我喜欢上Markdown的另一个原因是我有点很多码农都有的编程洁癖，对于富文本总是想调整好每一处的格式、颜色和字体，因此支持富文本的Word、网络云笔记啥的简直就是我的噩梦，而Markdown则拯救了我这样的强迫症。 几年前，为了找了一款可以在Windows平台上运行的满意的Markdown编辑器，我曾经试用和对比了能找到的几款Markdown编辑器。 作业部落的Cmd Markdown是当时我接触的完成度最高的Markdown编辑器，插入图片非常方便，各种辅助功能都已经事先做好，不需要自己去做一大堆的配置和安装插件，并且最大的特点是笔记会云同步备份在服务器上。我用过一段时间的Cmd Markdown，但最终还是迁走了。最大的原因是笔记是保存在编辑器的数据文件中，而不是以明文的md文件直接保存在本地。这让我有点不安，哪一天如果这个编辑器出了bug打不开程序，或者这编辑器背后的公司破产，我的数据可能就此付之一炬，而我没有任何办法导出我的文件。 马克飞翔是印象笔记Evernote的一个Markdown插件。我对马克飞翔的印象是UI做得很好，各种功能也很齐全。然而缺点跟Cmd Markdown是一样的，由于印象笔记的文件也是保存在数据文件中的，无法通过其它文本工具直接编辑。而我更希望将文件直接放在我的硬盘里，可以通过各种文本工具进行编辑。 我还用过一个叫做Typora的比较有特色的的Markdown编辑器。大多数的Markdown编辑器是将界面一分为二，左边写Markdown源码，右边是最终效果Preview，当然一般左边的编辑栏也会有一些基本的语法高亮。但Typora则是将两个界面整合在了一起，同时可以在源码视图和结果视图中进行切换。老实说这样的做法有利有弊，纯粹看个人喜好。Typora至今应该仍不支持File Tree，也就是说一次只能编辑一个文档，如果要同时编辑多个Markdown文档就需要同时打开多个Typora进程。我至今仍保留安装着Typora，并希望将来有机会能拿出来用用，虽然至今我仍未再次用过。 再后来，我开始使用github开源的Atom编辑器。有一个叫做Markdown Preview Enhanced的Atom插件，将一些常用的Markdown功能都整合到了一起，我非常喜欢这个插件。Atom应该是我使用时间最长且至今最为满意的Markdown编辑器了，配合各种Atom插件非常顺手，各种Markdown效果也基本满足我的需求。然后我再用Seafile搭建了个人云存储，将Markdown文件放在个人云盘里。 不知道是不是由于微软收购了github和微软大力推广VSCode的缘故，感觉Atom最近几年的更新已经很少了，而且经常会出现插件加载失败的问题。例如Atom官网论坛上的18年的帖子Is atom dead?和19年的帖子Is atom dead, again?都在讨论这个问题，相信不只是我的个人猜测。 而最后让我下定决心离开Atom的则是Markdown Preview Enhanced not working in Atom 1.47.0 #1380。我最爱的Markdown Preview Enhanced由于Atom的bug而彻底不能使用，但目前看来Atom方面的修复还遥遥无期。 很久以前我曾尝试过用VSCode写Markdown。当时Markdown Preview Enhanced还不支持VSCode，因此我在整合了一堆Markdown的插件后仍觉得效果不佳，因此放弃。不知道是不是Markdown Preview Enhanced的作者也察觉到了Atom正在走向死亡，因此也开始支持VSCode平台。在试用了最新的VSCode + Markdown Preview Enhanced后，我发现原本在Atom上常用的功能，在VSCode上都能找到，例如： Atom上可以用Tree View浏览目录结构，而VSCode自带File Explorer左侧边栏。 Atom上有Project Manager插件可以在左侧边栏上同时打开多个目录，这样我就可以将多个Markdown目录同时加入进来。而VSCode也支持Workspace的概念，可以将多个目录加入进来。 VSCode有和Atom类似的file-icons插件，可以在左侧边栏上针对不同类型的文件显示不同的icon便于区分。 VSCode自带minimap/autosave功能，而Atom上则是通过安装插件实现。 VSCode可以登录微软账号或github账号同步插件和配置。Atom则是通过安装插件实现，配置保存在指定Gist。但根据我使用了几年的体验，如果有多台电脑同步配置的情况下，经常会出现一些问题，因此我从去年开始屏蔽了Atom上的同步插件。 但遗憾的是，像Cmd Markdown等编辑器所支持的TOC大纲（就是有一个单独的窗口或者下拉菜单显示各级标题，类似Word的左侧边栏的大纲视图），在VSCode和Atom中都没有相应的插件可以支持，而只是可以在文章中插入TOC链接。我所希望的是有一个插件可以在VSCode的左侧边栏上可以显示TOC大纲，但目前暂未发现有这样的插件。 本文地址：http://xnerv.wang/use-vscode-instead-of-atom-as-markdown/","categories":[{"name":"程序员生活","slug":"程序员生活","permalink":"https://xnerv.wang/categories/%E7%A8%8B%E5%BA%8F%E5%91%98%E7%94%9F%E6%B4%BB/"}],"tags":[{"name":"程序员生活","slug":"程序员生活","permalink":"https://xnerv.wang/tags/%E7%A8%8B%E5%BA%8F%E5%91%98%E7%94%9F%E6%B4%BB/"},{"name":"VSCode","slug":"VSCode","permalink":"https://xnerv.wang/tags/VSCode/"},{"name":"Atom","slug":"Atom","permalink":"https://xnerv.wang/tags/Atom/"},{"name":"Markdown","slug":"Markdown","permalink":"https://xnerv.wang/tags/Markdown/"}]},{"title":"关于Linux X-Window的一些名词深究","slug":"linux-xwindow-investigation","date":"2020-05-06T06:25:00.000Z","updated":"2023-08-21T02:24:19.625Z","comments":true,"path":"linux-xwindow-investigation/","link":"","permalink":"https://xnerv.wang/linux-xwindow-investigation/","excerpt":"一直以来对X-Window、Xrdp、KDE、VNC等词半懂不懂，因此大致地调查了下。这篇文章包括了一些我自己的总结，因此可能有一些地方有不准确之处，敬请谅解。参考了http://cn.linux.vbird.org/linux_basic/0590xwindow_1.php。 X-Window/X Protocol：在XWindow 简介中有比较好的解释，这其实是一套图形接口（协议）。不同于Windows已经将图形接口与操作系统完全融为一体的做法，Linux的图形接口是可选的。而X-Window就是这样的一种图形接口。这个图形接口是属于CS架构的（client/server）。X Server负责画面的绘制和显示，以及接收用户的输入并传到给X Client。X Client负责处理传递过来的用户输入并决定呈现数据，然后由X Server来进行绘制。这与通常的对于CS架构的理解是相反的，与用户直接沟通的其实是X Server。 X-Window是一种协议，因此还需要具体的实现，例如Xfree86、Xorg，Xming和Xnest。 X11R6：X Protocol version 11 Release 6（X协议第11版第六次发行）。","text":"一直以来对X-Window、Xrdp、KDE、VNC等词半懂不懂，因此大致地调查了下。这篇文章包括了一些我自己的总结，因此可能有一些地方有不准确之处，敬请谅解。参考了http://cn.linux.vbird.org/linux_basic/0590xwindow_1.php。 X-Window/X Protocol：在XWindow 简介中有比较好的解释，这其实是一套图形接口（协议）。不同于Windows已经将图形接口与操作系统完全融为一体的做法，Linux的图形接口是可选的。而X-Window就是这样的一种图形接口。这个图形接口是属于CS架构的（client/server）。X Server负责画面的绘制和显示，以及接收用户的输入并传到给X Client。X Client负责处理传递过来的用户输入并决定呈现数据，然后由X Server来进行绘制。这与通常的对于CS架构的理解是相反的，与用户直接沟通的其实是X Server。 X-Window是一种协议，因此还需要具体的实现，例如Xfree86、Xorg，Xming和Xnest。 X11R6：X Protocol version 11 Release 6（X协议第11版第六次发行）。 Window Manager（WM）：个人看法，每一个窗口程序可能就对应一个（或多个？）X Client，而WM就是管理这些窗口移动、窗口大小和重叠显示的管理程序，常见的WM有GNOME、KDE、XFCE。 远程桌面：当你从另一台电脑上（主要是Windows）上想要通过图形化界面操作远程Linux时需要用到。常见的图形化远程桌面连接协议是RDP和VNC。Windows远程桌面用的就是RDP。RDP和VNC的区别可以参考VNC与RDP的区别。VNC主要传图像，适用于瘦客户端。RDP主要传指令，适用于低速网络。此外微软还有一项针对RDP的增强技术RemoteFX。 常见的VNC服务器软件有vnc4server、TightVNC，RealVNC等。 常见的VNC客户端有RealVNC Viewer、Ultra VNC等。 而如果你想用Windows自带的远程桌面连接Linux机器时，就必须用RDP协议了。需要在Linux上装兼容RDP的服务器，例如Xrdp。Xrdp使用Xvnc，X11rdp或xorgxrdp作为后端（XRDP与VNC的关系）。如果在Windows Hyper-V中安装Ubuntu等，在登录的时候就需要从几个选项中选择一个后端。根据What is x11rdp?和Xvnc中所提到的，X11rdp和Xvnc都属于X Server，用于显示“虚拟屏幕”，而不是物理屏幕。而Xorg中证实了Xorg是X-Window的一种实现，那感觉Xorg和Xvnc等并不是同一个层面上的概念，Xorg包括了X Server和X Client，而Xvnc只是X Server的一种实现。同时Xvnc对于用户而言又是VNC Server。 本文地址：http://xnerv.wang/linux-xwindow-investigation/","categories":[{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/categories/Linux/"}],"tags":[{"name":"原创","slug":"原创","permalink":"https://xnerv.wang/tags/%E5%8E%9F%E5%88%9B/"},{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/tags/Linux/"},{"name":"图形界面","slug":"图形界面","permalink":"https://xnerv.wang/tags/%E5%9B%BE%E5%BD%A2%E7%95%8C%E9%9D%A2/"},{"name":"X-Window","slug":"X-Window","permalink":"https://xnerv.wang/tags/X-Window/"},{"name":"X协议","slug":"X协议","permalink":"https://xnerv.wang/tags/X%E5%8D%8F%E8%AE%AE/"},{"name":"远程桌面","slug":"远程桌面","permalink":"https://xnerv.wang/tags/%E8%BF%9C%E7%A8%8B%E6%A1%8C%E9%9D%A2/"}]},{"title":"在美国第一次购买和使用树莓派时遇到的一些问题","slug":"the-first-time-to-buy-and-use-raspberry-in-usa","date":"2020-04-29T00:43:00.000Z","updated":"2023-08-21T02:24:21.274Z","comments":true,"path":"the-first-time-to-buy-and-use-raspberry-in-usa/","link":"","permalink":"https://xnerv.wang/the-first-time-to-buy-and-use-raspberry-in-usa/","excerpt":"新买了一个Raspberry Pi 4 Model B，第一次使用树莓派，遇到一些问题，记录下来希望对自己和他人所有帮助。 买什么套装 虽然在我买的时候2GB款的树莓派4B税前价格是35刀，但是亚马逊上是不直接卖板子的，一些要捆绑一些配件像Micro SD卡、电源、散热片、Micro HDMI线啥的。众所周知美国这边的各种线卖的巨贵无比，国内淘宝9块9包邮的线，在这边可能就是20刀不包税。而像CanaKit等可以直接卖板子的网站，则一般需要6到7刀左右的邮费。最后我还是在亚马逊上买了一个包括电源和三块散热片的套装，税前45刀左右。 树莓派4B的电源线是5V3A Type-C接口，我看了一下现在手机的充电线大多是5V2A这个样子，据说只要别接太多额外负载，手机充电线也是可以的。散热片不确定是否是必须的，我在不接散热片的情况下尽跑翻墙代理程序的时候在45摄氏度左右，但西雅图这边气温比较低，夏天最高温度也就32到34摄氏度左右。如果是在国内温度比较高的地区，建议还是贴散热片比较保险。 盒子的话淘宝20块就能买个很好的了，美国这边差不多20刀不包税。估计盒子会对散热造成一定负面影响，因此还是建议至少贴散热片。另外我一直在找有没有能装下一个2.5英寸或者3.5英寸硬盘的树莓派盒子，但是淘宝和亚马逊上都没有找到，移动硬盘只能单独放盒子外面，通过USB和树莓派连接在一起了。 如果是把树莓派作为翻墙代理等服务器用途的话，Micro HDMI不是必须的，我下面会讲到怎么远程初始化树莓派。","text":"新买了一个Raspberry Pi 4 Model B，第一次使用树莓派，遇到一些问题，记录下来希望对自己和他人所有帮助。 买什么套装 虽然在我买的时候2GB款的树莓派4B税前价格是35刀，但是亚马逊上是不直接卖板子的，一些要捆绑一些配件像Micro SD卡、电源、散热片、Micro HDMI线啥的。众所周知美国这边的各种线卖的巨贵无比，国内淘宝9块9包邮的线，在这边可能就是20刀不包税。而像CanaKit等可以直接卖板子的网站，则一般需要6到7刀左右的邮费。最后我还是在亚马逊上买了一个包括电源和三块散热片的套装，税前45刀左右。 树莓派4B的电源线是5V3A Type-C接口，我看了一下现在手机的充电线大多是5V2A这个样子，据说只要别接太多额外负载，手机充电线也是可以的。散热片不确定是否是必须的，我在不接散热片的情况下尽跑翻墙代理程序的时候在45摄氏度左右，但西雅图这边气温比较低，夏天最高温度也就32到34摄氏度左右。如果是在国内温度比较高的地区，建议还是贴散热片比较保险。 盒子的话淘宝20块就能买个很好的了，美国这边差不多20刀不包税。估计盒子会对散热造成一定负面影响，因此还是建议至少贴散热片。另外我一直在找有没有能装下一个2.5英寸或者3.5英寸硬盘的树莓派盒子，但是淘宝和亚马逊上都没有找到，移动硬盘只能单独放盒子外面，通过USB和树莓派连接在一起了。 如果是把树莓派作为翻墙代理等服务器用途的话，Micro HDMI不是必须的，我下面会讲到怎么远程初始化树莓派。 没有Micro HDMI线 第一次知道有Micro HDMI接口，所以很显然我没有这种线。电脑大多使用的都是普通大小的HDMI线。在美国买线相当划不来，一个这样的线得一、二十刀。所以我通过网线将树莓派接在路由器上，然后通过远程SSH的方式连接到树莓派上进行配置。 但是需要注意的是，虽然安装树莓派系统有两种方式（如何给树莓派安装操作系统）： 方式一：将NOOBS写入Micro SD卡 方式二：直接将操作系统镜像写入Micro SD卡 但方式一应该是需要将树莓派连接显示器才能操作安装程序的，所以我用的第二种方式，直接将操作系统Raspbian镜像写入Micro SD卡。树莓派上有一个Micro SD卡读卡器，需要将安装了Raspbian的Micro SD卡插入这里。 此外，树莓派4B的SSH默认是关闭的，在写入镜像完成后，还需要通过电脑在Micro SD里创建一个文件名为SSH的空文件。然后将Micro SD卡插入树莓派卡槽，通电后SSH Server才会启动，这样才能通过SSH Client连接上去。 然后问题来了，SSH连接的时候如何得知树莓派的IP？如果是自己家，你有路由器的管理密码的话，登录进去找找看所有连接上来的设备，看有没有类似Raspberry名字的设备连接上来。如果没有路由器的管理密码或者想偷懒的话，可以参考如何这没有显示器的情况下获取树莓派IP？这篇文章，或者用ipscan等局域网内IP扫描工具直接扫描所有IP。 启用WIFI 按照从网上找到的一些教程（例如树莓派4B，3B+和3B，如何配置WiFi和蓝牙），我在执行sudo iwlist wlan0 scan的时候遇到了类似interface doesnt support scanning的错误信息。在执行sudo ifconfig wlan0 up的时候，则遇到了SIOCSIFFLAGS: Operation not possible due to RF-kill的错误信息。据查rfkill是管理WIFI和蓝牙功能的一个软开关，跟控制耗电相关。根据SIOCSIFFLAGS: Operation not possible due to RF-kill关闭了相关的设置后解决问题。 通过蓝牙SSH 参考了一下文章但目前暂时未能解决这个问题，主要的问题是我的笔记本检测不到树莓派的蓝牙信号，虽然我的手机能够检测到树莓派的蓝牙。通过蓝牙SSH并不是刚需，等以后如果我能解决这个问题的时候再补完这一部分。 本文地址：http://xnerv.wang/the-first-time-to-buy-and-use-raspberry-in-usa/","categories":[{"name":"树莓派","slug":"树莓派","permalink":"https://xnerv.wang/categories/%E6%A0%91%E8%8E%93%E6%B4%BE/"}],"tags":[{"name":"原创","slug":"原创","permalink":"https://xnerv.wang/tags/%E5%8E%9F%E5%88%9B/"},{"name":"树莓派","slug":"树莓派","permalink":"https://xnerv.wang/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"},{"name":"Raspberry","slug":"Raspberry","permalink":"https://xnerv.wang/tags/Raspberry/"}]},{"title":"为什么PostgreSQL要使用OS缓存？","slug":"why-pg-uses-os-cache","date":"2020-01-02T02:47:00.000Z","updated":"2023-08-21T02:24:18.644Z","comments":true,"path":"why-pg-uses-os-cache/","link":"","permalink":"https://xnerv.wang/why-pg-uses-os-cache/","excerpt":"与MySQL等开源数据库不同的是，PostgreSQL（PG）并不使用O_DIRECT来写data文件，而是依赖于OS缓存，并且强调在设置shared buffer在大小时不能过大，否则会造成过于频繁的swap而导致IO性能下降。这与MySQL等数据库的buffer pool size越大性能越好的指导原则是相反的。并且PG依赖于OS缓存的这一特性也给提供PostgreSQL云服务造成了很多问题。例如云服务要求multiple tenants资源隔离，也就是说跑在同一个VM上的多个PG servers相互之间不能互相影响，但共用OS缓存显然会造成资源竞争。不知道Docker是否可以进行OS缓存的资源隔离，但现阶段还还依赖于Service Fabric架构的Azure PG显然得自己解决这个问题，也因此造成了架构设计上不得不考虑OS缓存的隔离。 从The Internals of PostgreSQL: Chapter 2 Process and Memory Architecture这篇文章看来，与MySQL使用多线程或线程池和架构不同的是，PG使用的是多进程架构。多进程模型在Windows平台上会造成很大的性能问题，这暂且不提。但多进程之间是共用的shared buffer。既然如此，那就应该不是寄希望于OS缓存来便于多进程之间共享shared buffer吧。","text":"与MySQL等开源数据库不同的是，PostgreSQL（PG）并不使用O_DIRECT来写data文件，而是依赖于OS缓存，并且强调在设置shared buffer在大小时不能过大，否则会造成过于频繁的swap而导致IO性能下降。这与MySQL等数据库的buffer pool size越大性能越好的指导原则是相反的。并且PG依赖于OS缓存的这一特性也给提供PostgreSQL云服务造成了很多问题。例如云服务要求multiple tenants资源隔离，也就是说跑在同一个VM上的多个PG servers相互之间不能互相影响，但共用OS缓存显然会造成资源竞争。不知道Docker是否可以进行OS缓存的资源隔离，但现阶段还还依赖于Service Fabric架构的Azure PG显然得自己解决这个问题，也因此造成了架构设计上不得不考虑OS缓存的隔离。 从The Internals of PostgreSQL: Chapter 2 Process and Memory Architecture这篇文章看来，与MySQL使用多线程或线程池和架构不同的是，PG使用的是多进程架构。多进程模型在Windows平台上会造成很大的性能问题，这暂且不提。但多进程之间是共用的shared buffer。既然如此，那就应该不是寄希望于OS缓存来便于多进程之间共享shared buffer吧。 PgSQL和MySQL的bufferpool探讨这篇文章提出了一种猜想，认为由于PostgreSQL是诞生在实验室中，主要为了研究数据库内核原理，那么使用buffer io能够减少IO栈的代码开发，进而能够减少额外的debug。这种说法是有可能的、虽然PG的代码比MySQL的干净太多，MySQL的代码中经常有类似xxx_function，xxx_function2，another_xxx_function2这种奇怪的函数名字，而且毫无注释完全不明所以。但是PG很明显在工程化和成熟度上不如MySQL。文章中提到PG中有这么一段代码： 也就是说PG依赖于OS缓存来减少日志归档和流复制中的文件IO读取次数。但这应该仅限于WAL xlog文件。对于shared buffer pool，是什么原因使得还需要保留这种double buffering的设计呢？ The Internals of PostgreSQL: Chapter 8 Buffer Manager提到可以参考Why we are going to have to go DirectIO这篇讨论和Thread summary: the Linux kernel and PostgreSQL这篇文章。目前看来，应该还是PG的storage layer实现上过于简陋，效率上存在很大的问题，不足以脱离OS缓存而独自运行。这也印证了PgSQL和MySQL的bufferpool探讨这篇文章的说法。Understanding caching in Postgres - An in-depth guide这里也提到，PG依赖于OS缓存来调度写请求，这恐怕也是PG storage layer本身缺乏相应调度机制的一种表现。 PostgreSQL pain points中也有提到flow fsync和double buffering等问题，看起来PG的developers依赖于Linux kernel提供相应的解决方案。我不知道这是否是一种正确的方向，毕竟这种与特定OS紧耦合的方案也就限制了PG在其它OS平台上运行的能力。但是考虑到现在PG本来就在Windows上跑得很差像个demo，如果PG本身的开发力量不够的话，也许这也算是一种解决方案吧。 本文地址：http://xnerv.wang/why-pg-uses-os-cache/","categories":[{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/categories/DBMS/"}],"tags":[{"name":"原创","slug":"原创","permalink":"https://xnerv.wang/tags/%E5%8E%9F%E5%88%9B/"},{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/tags/DBMS/"},{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://xnerv.wang/tags/PostgreSQL/"},{"name":"PG","slug":"PG","permalink":"https://xnerv.wang/tags/PG/"}]},{"title":"如果阿里月饼黑客事件发生在谷歌会怎样？前Google人亲述他抢了50件T-shirt的故事（转载）","slug":"google-50-tshirt","date":"2020-01-02T01:47:00.000Z","updated":"2023-08-21T02:24:21.448Z","comments":true,"path":"google-50-tshirt/","link":"","permalink":"https://xnerv.wang/google-50-tshirt/","excerpt":"本文作者吴卓浩，前Google中国用户体验团队负责人。微信公众号、知乎号：uxbang 2016年中秋节，被阿里的月饼门刷屏了。知乎上有人发起了讨论，“如果这样的事发生在Google会怎样”——如果也和阿里一样，我看着柜子里的差不多50件Google T-shirt，一头汗。 月饼事件的热闹，在于各种人都能从各种角度做各种分析评论。但是无论真相如何，上半场已经结束，希望那几位兄弟们吸取教训、但不要被吓怕，接下来的发展一定会有自己的精彩；而下半场才刚刚开始，对这几位兄弟来说是，对阿里，对整个行业，甚至对更大的体系，都是。 不论对于一个普通员工，还是对于一个公司的老大，公司 VS 个人，文化 VS 制度，永远是无法回避的问题，而且是没有标准解答的问题。比如，为了提高工作效率： 在一个初创公司中经常是没有标准流程的效率更高，而在一个大公司中是有标准流程效率更高； 在一个内容创作型的公司中，追随热点、快速响应的效率更高，而在一个产品研发型的公司中，做好计划、理清流程的效率更高； 在一个业务稳定成熟的公司中，以KPI推动工作效率更高，而在一个业务快速创新的公司中，以OKR推动工作效率更高；","text":"本文作者吴卓浩，前Google中国用户体验团队负责人。微信公众号、知乎号：uxbang 2016年中秋节，被阿里的月饼门刷屏了。知乎上有人发起了讨论，“如果这样的事发生在Google会怎样”——如果也和阿里一样，我看着柜子里的差不多50件Google T-shirt，一头汗。 月饼事件的热闹，在于各种人都能从各种角度做各种分析评论。但是无论真相如何，上半场已经结束，希望那几位兄弟们吸取教训、但不要被吓怕，接下来的发展一定会有自己的精彩；而下半场才刚刚开始，对这几位兄弟来说是，对阿里，对整个行业，甚至对更大的体系，都是。 不论对于一个普通员工，还是对于一个公司的老大，公司 VS 个人，文化 VS 制度，永远是无法回避的问题，而且是没有标准解答的问题。比如，为了提高工作效率： 在一个初创公司中经常是没有标准流程的效率更高，而在一个大公司中是有标准流程效率更高； 在一个内容创作型的公司中，追随热点、快速响应的效率更高，而在一个产品研发型的公司中，做好计划、理清流程的效率更高； 在一个业务稳定成熟的公司中，以KPI推动工作效率更高，而在一个业务快速创新的公司中，以OKR推动工作效率更高； 创始人能力强，开始时亲力亲为效率高，可是如果不能接下来把中层建立培养起来，公司发展效率就不会高； 个别员工能力强，单兵作战效率高，可是如果不能把个体能力变为团队能力，团队战斗效率就不会高； 事事靠老大拿主意，眼前完成任务效率高，可是如果不能让员工形成独立思考能力，老大累死也不会得到真正的效率； 用制度管人，能马上见效的效率高，可是如果不能让员工形成文化上的认同和判断力，压力有多大反弹就会有多强； 公司本身也是一个产品，一个由创始人、员工、利益相关方、用户共同设计、不断迭代的产品，一直在路上、不断更成熟、永远不完美的产品。对我来说，如果在公司 中的人做的事情出了问题，无论是出于善意、恶意或者无意，首先需要反思的是公司的组织、制度、流程本身，最重要的是以此为契机改进公司本身。 过去的十多年，在我所服务或者创立的公司中，我也有不少请员工离开的经历，但每次都会深深自责反思，因为没有不合适的员工，只有不合适的工作，而把TA带进 来、放在不合适的位置上的人，是我。甚至当员工在工作上出现失误，造成公司不小的损失，我只会严肃的让员工明白事情的严重性、如何改进公司的系统来避免问 题再次发生，而损失由自己去承担，因为我能承担，而员工的经济状况无法承担。 在Google的面试中，除了对于工作能力的各项判断，最后还 有特别的一项：这个人是否Googley？Googley是Google自创的一个词，这个问题的含义是，这个人像不像Google人？对Googley 这个词，Google没有官方的定义，它具体的含义没人知道、却又人人明白；就像判例法，没有人去自上而下规定Google人应该怎么做，但是大家都互相 耳濡目染的学会要怎么做。 所以，当新来的刚毕业的大学生沉醉于零食间，没有人会去鄙视；当有人带朋友来蹭饭，公司提醒如果人多要提前和厨房 说、否则会影响到其他同事吃不上饭（接待旅游团，以此挣钱，显然是另一回事，这已经是靠出售公司资源谋私利，在任何文化下都不会被允许）。因为 Google人相信留在Google的同伴总归会变得Googley。 当员工为了抢每周补充的免费T-shirt，写程序、装监控摄像头， 是开放出一个邮件列表，让以此为乐的同事都能参与；当员工班车出现问题的时候，员工会自发组织起来，跨部门协作，以工程研发的方式做数据采集、写算法程 序，达成真正满足大多数人的解决方案。因为在Google，为全世界用户设计和开发产品，也正是这么做的呀！ 开头所说的Google T-shirt，是我2006-2010年间收藏的，其实还不止这些，有的已经被亲朋好友强行索要走。Google有印制T-shirt庆祝产品上线或者 活动发布的文化，用户体验部门几乎是跨产品跨部门协作最多的，自然就获得了更多沾光的机会。每次整理这些T-shirt，都忍不住回想起背后的一个个人和 故事，心里暖暖的。这也是Google文化的强大之处，无论在不在Google，无论当时是因为什么原因离开Google，（曾经的）Google人都被 心中的Google文化仅仅团结在一起。 谢谢Google不会因为我抢了这么多T-shirt开除我！：） 本文地址：http://xnerv.wang/google-50-tshirt/ 转载自：如果阿里月饼黑客事件发生在谷歌会怎样？前Google人亲述他抢了50件T-shirt的故事","categories":[{"name":"程序员生活","slug":"程序员生活","permalink":"https://xnerv.wang/categories/%E7%A8%8B%E5%BA%8F%E5%91%98%E7%94%9F%E6%B4%BB/"}],"tags":[{"name":"程序员生活","slug":"程序员生活","permalink":"https://xnerv.wang/tags/%E7%A8%8B%E5%BA%8F%E5%91%98%E7%94%9F%E6%B4%BB/"},{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"阿里月饼","slug":"阿里月饼","permalink":"https://xnerv.wang/tags/%E9%98%BF%E9%87%8C%E6%9C%88%E9%A5%BC/"}]},{"title":"让 CPU 告诉你硬盘和网络到底有多慢（转载）","slug":"how-slow-is-disk-and-network","date":"2019-12-28T08:29:00.000Z","updated":"2023-08-21T02:24:21.879Z","comments":true,"path":"how-slow-is-disk-and-network/","link":"","permalink":"https://xnerv.wang/how-slow-is-disk-and-network/","excerpt":"简介 经常听到有人说磁盘很慢、网络很卡，这都是站在人类的感知维度去表述的，比如拷贝一个文件到硬盘需要几分钟到几十分钟，够我去吃个饭啦；而从网络下载一部电影，有时候需要几个小时，我都可以睡一觉了。 最为我们熟知的关于计算机不同组件速度差异的图表，是下面这种金字塔形式：越往上速度越快，容量越小，而价格越高。这张图只是给了我们一个直观地感觉，并没有对各个速度和性能做出量化的说明和解释。而实际上，不同层级之间的差异要比这张图大的多。这篇文章就让你站在 CPU 的角度看这个世界，说说到底它们有多慢。 希望你看到看完这篇文章能明白两件事情：磁盘和网络真的很慢，性能优化是个复杂的系统性的活。 注：所有的数据都是来自这个地址。所有的数据会因为机器配置不同，或者硬件的更新而有出入，但是不影响我们直觉的感受。如果对这些数据比较感兴趣，这个网址给出了不同年份一些指标的数值。","text":"简介 经常听到有人说磁盘很慢、网络很卡，这都是站在人类的感知维度去表述的，比如拷贝一个文件到硬盘需要几分钟到几十分钟，够我去吃个饭啦；而从网络下载一部电影，有时候需要几个小时，我都可以睡一觉了。 最为我们熟知的关于计算机不同组件速度差异的图表，是下面这种金字塔形式：越往上速度越快，容量越小，而价格越高。这张图只是给了我们一个直观地感觉，并没有对各个速度和性能做出量化的说明和解释。而实际上，不同层级之间的差异要比这张图大的多。这篇文章就让你站在 CPU 的角度看这个世界，说说到底它们有多慢。 希望你看到看完这篇文章能明白两件事情：磁盘和网络真的很慢，性能优化是个复杂的系统性的活。 注：所有的数据都是来自这个地址。所有的数据会因为机器配置不同，或者硬件的更新而有出入，但是不影响我们直觉的感受。如果对这些数据比较感兴趣，这个网址给出了不同年份一些指标的数值。 数据 先来看看 CPU 的速度，就拿我的电脑来说，主频是 2.6G，也就是说每秒可以执行 2.6*10^9 个指令，每个指令只需要 0.38ns（现在很多个人计算机的主频要比这个高，配置比较高的能达到 3.0G+）。我们把这个时间当做基本单位 1s，因为 1s 大概是人类能感知的最小时间单位。 一级缓存读取时间为 0.5ns，换算成人类时间大约是 1.3s，大约一次或者两次心跳的时间。这里能看出缓存的重要性，因为它的速度可以赶上 CPU，程序本身的 locality 特性加上指令层级上的优化，cache 访问的命中率很高，这最终能极大提高效率。 分支预测错误需要耗时 5ns，换算成人类时间大约是 13s，这个就有点久了，所以你会看到很多文章分析如何优化代码来降低分支预测的几率，比如这个得分非常高的 stackoverflow 问题。 二级缓存时间就比较久了，大约在 7ns，换算成人类时间大约是 18.2s，可以看到的是如果一级缓存没有命中，然后去二级缓存读取数据，时间差了一个数量级。 小知识：为什么需要多层的 CPU 缓存呢？这篇文章通过一个通俗易懂的例子给出了讲解。 我们继续，互斥锁的加锁和解锁时间需要 25ns，换算成人类时间大约是 65s，首次达到了一分钟。并发编程中，我们经常听说锁是一个很耗时的东西，因为在微波炉里加热一个东西需要一分钟的话，你要在那傻傻地等蛮久了。 然后就到了内存，每次内存寻址需要 100ns，换算成人类时间是 260s，也就是4分多钟，如果读一些不需要太多思考的文章，这么久能读完2-3千字（这个快阅读的时代，很少人在手机上能静心多这么字了）。看起来还不算坏，不多要从内存中读取一段数据需要的时间会更多。到了内存之后，时间就变了一个量级，CPU 和内存之间的速度瓶颈被称为冯诺依曼瓶颈。 一次 CPU 上下文切换（系统调用）需要大约 1500ns，也就是 1.5us（这个数字参考了这篇文章，采用的是单核 CPU 线程平均时间），换算成人类时间大约是 65分钟，嗯，也就是一个小时。我们也知道上下文切换是很耗时的行为，毕竟每次浪费一个小时，也很让人有罪恶感的。上下文切换更恐怖的事情在于，这段时间里 CPU 没有做任何有用的计算，只是切换了两个不同进程的寄存器和内存状态；而且这个过程还破坏了缓存，让后续的计算更加耗时。 在 1Gbps 的网络上传输 2K 的数据需要 20us，换算成人类时间是 14.4小时，这么久都能把《星球大战》六部曲看完了（甚至还加上吃饭撒尿的时间）！可以看到网络上非常少数据传输对于 CPU 来说，已经很漫长。而且这里的时间还是理论最大值，实际过程还要更慢一些。 SSD 随机读取耗时为 150us，换算成人类时间大约是 4.5天。换句话说，SSD 读点数据，CPU 都能休假，报团参加周边游了。虽然我们知道 SSD 要比机械硬盘快很多，但是这个速度对于 CPU 来说也是像乌龟一样。I/O 设备 从硬盘开始速度开始变得漫长，这个时候我们就想起内存的好处了。尽量减少 IO 设备的读写，把最常用的数据放到内存中作为缓存是所有程序的通识。像 memcached 和 redis 这样的高速缓存系统近几年的异军突起，就是解决了这里的问题。 从内存中读取 1MB 的连续数据，耗时大约为 250us，换算成人类时间是 7.5天，这次假期升级到国庆七天国外游了。 同一个数据中心网络上跑一个来回需要 0.5ms，换算成人类时间大约是 15天，也就是半个月的时间。如果你的程序有段代码需要和数据中心的其他服务器交互，在这段时间里 CPU 都已经狂做了半个月的运算。减少不同服务组件的网络请求，是性能优化的一大课题。 从 SSD 读取 1MB 的顺序数据，大约需要 1ms，换算成人类时间是 1个月。也就是说 SSD 读一个普通的文件，如果要等你做完，CPU 一个月时间就荒废了。尽管如此，SSD 已经很快啦，不信你看下面机械磁盘的表现。 磁盘寻址时间为 10ms，换算成人类时间是 10个月，刚好够人类创造一个新的生命了。如果 CPU 需要让磁盘泡杯咖啡，在它眼里，磁盘去生了个孩子，回来告诉它你让我泡的咖啡好了。机械硬盘使用 RPM(Revolutions Per Minute/每分钟转速) 来评估磁盘的性能：RPM 越大，平均寻址时间更短，磁盘性能越好。寻址只是把磁头移动到正确的磁道上，然后才能读取指定扇区的内容。换句话说，寻址虽然很浪费时间，但其实它并没有办任何的正事（读取磁盘内容）。 从磁盘读取 1MB 连续数据需要 20ms，换算成人类时间是 20个月。IO 设备是计算机系统的瓶颈，希望读到这里你能更深切地理解这句话！如果还不理解，不妨想想你在网上买的东西，快递送了将近两年，你的心情是怎么样的。 而从世界上不同城市网络上走一个来回，平均需要 150ms（参考世界各地 ping 报文的时间），换算成人类时间是 12.5年。不难理解，所有的程序和架构都会尽量避免不同城市甚至是跨国家的网络访问，CDN 就是这个问题的一个解决方案：让用户和最接近自己的服务器交互，从而减少网络上报文的传输时间。 虚拟机重启一次大约要 4s 时间，换算成人类的时间是 3百多年。对于此，我想到了乔布斯要死命优化 Mac 系统开机启动时间的故事。如果机器能少重启而且每次启动能快一点，不仅能救人命，也能救 CPU 的命。 物理服务器重启一次需要 5min，换算成人类时间是 2万5千年，快赶上人类的文明史了。5 分钟人类都要等一会了，更别提 CPU 了，所以没事不要乱重启服务器啊，分分钟终结一个文明的节奏。 参考资料 What Every Programmer Should Know About Memory Getting Physical With Memory 本文地址：http://xnerv.wang/how-slow-is-disk-and-network/ 转载自：让 CPU 告诉你硬盘和网络到底有多慢","categories":[{"name":"计算机硬件","slug":"计算机硬件","permalink":"https://xnerv.wang/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A1%AC%E4%BB%B6/"}],"tags":[{"name":"性能瓶颈","slug":"性能瓶颈","permalink":"https://xnerv.wang/tags/%E6%80%A7%E8%83%BD%E7%93%B6%E9%A2%88/"},{"name":"CPU","slug":"CPU","permalink":"https://xnerv.wang/tags/CPU/"},{"name":"内存访问","slug":"内存访问","permalink":"https://xnerv.wang/tags/%E5%86%85%E5%AD%98%E8%AE%BF%E9%97%AE/"},{"name":"网络传输","slug":"网络传输","permalink":"https://xnerv.wang/tags/%E7%BD%91%E7%BB%9C%E4%BC%A0%E8%BE%93/"},{"name":"分支预测","slug":"分支预测","permalink":"https://xnerv.wang/tags/%E5%88%86%E6%94%AF%E9%A2%84%E6%B5%8B/"},{"name":"硬盘","slug":"硬盘","permalink":"https://xnerv.wang/tags/%E7%A1%AC%E7%9B%98/"},{"name":"SSD","slug":"SSD","permalink":"https://xnerv.wang/tags/SSD/"}]},{"title":"为什么Docker能运行不同的Linux发行版？","slug":"why-docker-has-ability-to-run-different-linux-distribution","date":"2019-12-26T07:58:00.000Z","updated":"2023-08-21T02:24:19.609Z","comments":true,"path":"why-docker-has-ability-to-run-different-linux-distribution/","link":"","permalink":"https://xnerv.wang/why-docker-has-ability-to-run-different-linux-distribution/","excerpt":"","text":"结合What is the relationship between the docker host OS and the container base image OS?、How can Docker run distros with different kernels?和[Why docker has ability to run different linux distribution?(https://stackoverflow.com/questions/25444099/why-docker-has-ability-to-run-different-linux-distribution)这几篇文章看来，Docker实例和Host OS之间通讯的唯一桥梁就是Host OS的内核。挡在Fedora上跑一个Ubuntu 16.04的Docker实例时，Docker实例用的内核仍然是Fedora的内核，而不是Ubuntu 16.04所对应的的内核，因此有可能和原生的Ubuntu 16.04有一些内核上的区别。 本文地址：http://xnerv.wang/why-docker-has-ability-to-run-different-linux-distribution/","categories":[{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/categories/Linux/"}],"tags":[{"name":"原创","slug":"原创","permalink":"https://xnerv.wang/tags/%E5%8E%9F%E5%88%9B/"},{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/tags/Linux/"},{"name":"Docker","slug":"Docker","permalink":"https://xnerv.wang/tags/Docker/"}]},{"title":"关于MinGW和Cygwin的一些个人总结和推测","slug":"mingw-cygwin-summary-and-thinking","date":"2019-12-25T23:40:00.000Z","updated":"2023-08-21T02:24:21.818Z","comments":true,"path":"mingw-cygwin-summary-and-thinking/","link":"","permalink":"https://xnerv.wang/mingw-cygwin-summary-and-thinking/","excerpt":"本文主要是对MinGW和Cygwin相关的一些名词的研究和推测，以求澄清一些似乎而非的概念，并记录当前已经弄清楚的一些问题，以及还需要进一步调研的一些细节。所有调研都基于Windows平台。 关于MinGW和Cygwin的关系 网上大部分博文复制粘贴的文章都是讨论MinGW和Cygwin的区别和优劣。而我主要是分析两者的联系，以及一些需要同时用到MinGW和Cygwin的交叉编译场景。 从MinGW的维基百科上看，Cygwin是提供一个模拟的POSIX层（cygwin1.dll）。我推测Cygwin也提供了一系列基于Cygwin的编译工具，在将需要移植的Linux代码在Cygwin上重新编译后，可以获得可以在Windows上直接运行的exe，而这个exe调用的还是POSIX风格的API，只不过这些API由cygwin1.dll提供模拟实现。而MinGW也提供了一系列编译工具，但MinGW-GCC是在编译时将代码中的POSIX API调用直接修改为对应的Windows API调用，从而不需要一个额外的dll转换层。需要额外提到的是，gcc的这种在编译时直接修改调用的API的行为不仅不少见，而且非常常见，在64位Linux上编译C++程序时，例如调用open这个函数，实际上在gcc编译后，调用的是libc中的open64函数，这个可以通过objdump导出外部依赖符号表来确认。另外就是，MinGW并不提供某些难以用Windows API实现的POSIX API，例如fork()，mmap()和ioctl()。","text":"本文主要是对MinGW和Cygwin相关的一些名词的研究和推测，以求澄清一些似乎而非的概念，并记录当前已经弄清楚的一些问题，以及还需要进一步调研的一些细节。所有调研都基于Windows平台。 关于MinGW和Cygwin的关系 网上大部分博文复制粘贴的文章都是讨论MinGW和Cygwin的区别和优劣。而我主要是分析两者的联系，以及一些需要同时用到MinGW和Cygwin的交叉编译场景。 从MinGW的维基百科上看，Cygwin是提供一个模拟的POSIX层（cygwin1.dll）。我推测Cygwin也提供了一系列基于Cygwin的编译工具，在将需要移植的Linux代码在Cygwin上重新编译后，可以获得可以在Windows上直接运行的exe，而这个exe调用的还是POSIX风格的API，只不过这些API由cygwin1.dll提供模拟实现。而MinGW也提供了一系列编译工具，但MinGW-GCC是在编译时将代码中的POSIX API调用直接修改为对应的Windows API调用，从而不需要一个额外的dll转换层。需要额外提到的是，gcc的这种在编译时直接修改调用的API的行为不仅不少见，而且非常常见，在64位Linux上编译C++程序时，例如调用open这个函数，实际上在gcc编译后，调用的是libc中的open64函数，这个可以通过objdump导出外部依赖符号表来确认。另外就是，MinGW并不提供某些难以用Windows API实现的POSIX API，例如fork()，mmap()和ioctl()。 MinGW和MinGW-w64 MinGW（mingw32）据说更新太慢代码太老，因此另一帮人就新搞了一个MinGW-w64，据说老的MinGW不支持编译64位程序。不知道是不是这就意味着可以完全放弃掉MinGW而直接采用MinGW-x64？ 在安装MinGW时需要选择线程模型：posix或win32。从mingw-w64 threads: posix vs win32看来，win32是在C++11之前MinGW-GCC搞的一套基于win32 threads模型的多线程库，而posix则是基于libwinpthreads，支持C++11的一些新的头文件。有另外一个单独的GitHub项目mingw-std-threads可以让win32模型也支持这些C++11头文件。 MinGW-w64可以安装在Windows上，可以安装在Linux上，甚至可以安装在Cygwin里（而CygWin看起来只能安装在Windows中）。MinGW（mingw32）好像有另外一个相关项目MinGW cross compiling environment提供Linux安装，但感觉项目不是很活跃。并且从这篇更新日志来看，似乎作者已经放弃更新并转向MinGW-w64。 MSYS 根据MinGW官网对于MSYS的描述，MSYS是对MinGW的补充，提供了bash，make， gawk和grep等GNU工具来辅助编译。从网上可以找到的一些MinGW编译入门文章来看，完全可以在Windows cmd环境中调用MinGW的gcc命令行去编译基于MinGW的Windows程序，并不是一定需要在bash环境中进行，但是像bash脚本这种应该还是需要bash环境的。另外据说MSYS是从Cygwin派生出来的分支，本来我推测像MSYS中的gcc应该是运行在cygwin1.dll或者类似名字的dll模拟层上。但是检查了gcc的dll依赖关系： 发现MinGW上的gcc最终似乎是直接依赖于Windows的dlls，并没有类似cygwin1.dll的东西。这种有点奇怪了，难道这个gcc是通过Cygwin上的MinGW-GCC用自举（bootstrapping）的方式创建出来的？找到关于MinGW-x64有关自举编译的一篇文章Creating a native Win64 compiler，我怀疑MinGW是不是也是用的类似的自举编译gcc。 但是再看bash的话，bash.exe却还是依赖于msys1.0.dll以及其它一些msys开头的dlls。猜测这些dlls应该就是类似于cygwin1.dll的模拟层。MinGW和MSYS是通过同一个安装程序来安装，推测由于gcc.exe属于MinGW，而bash.exe属于MSYS，而只有MSYS的工具才需要依赖msys相关的dlls。检查安装路径后果然发现，gcc.exe是在C:\\MinGW\\bin下面，而bash.exe是在C:\\MinGW\\msys\\1.0\\bin下面，印证了我的想法。 此外结合MinGW的中文维基百科和我自己的MinGW-w64安装经历，选择i686工具链时可以从DWARF和SJLJ这两种异常实现机制中二选一，而选择x86_64时需要从SEH和SJLJ中二选一。 MSYS2 回过头来再看看MSYS2。MSYS2似乎是配套MinGW-w64出现的，提供三种配置的模式：msys2，mingw64（使用mingw-w64 x86_64 toolchain工具链）和mingw32（使用mingw-w64 i686 toolchain工具链）。个人推测msys2模式编译出来的程序需要依赖msys2.0.dll，就像Cygwin下编译出来的程序一样。据说MSYS2相对于Cygwin的最大区别是移植了包管理工具Pacman。据说三种模式的主要区别是在$PATH中的搜索优先顺序不同，msys2只使用/usr/local/bin和/usr/bin下的工具，mingw64优先使用/mingw64/bin下的工具，mingw32优先使用/mingw32/bin下的工具。 类似于MinGW和MSYS，安装MSYS2的时候也会自动安装MinGW-w64。而且看起来在x64平台上是mingw64和mingw32会同时被安装。 我首先单独安装了MinGW-w64，看起来其中的gcc等工具也是直接依赖于Windows的dlls，并没有一个中间层dll。并且在安装的时候需要选择是用x86_64工具链还是i686工具链，以及异常的处理方式。这么看来安装MSYS2的话就可以同时拥有两种工具链了。安装MSYS2时并没有要求选择异常处理机制和所使用的线程库，根据What’s the difference between Mingw-builds and Mingw packages in Msys2这个帖子中的讨论，看起来 MSYS2 only provides posix thread model, dwarf for i686, seh for x86_64 使用Pacman直接安装的gcc依赖于msys2系列的dlls。我估计这就类似于在Cygwin上直接安装gcc，会依赖于cygwin1.dll。如果要安装只依赖于Windows native dlls的gcc，应该安装mingw版本的gcc。 安装mingw-w64-i686-toolchain和mingw-w64-x86_64-toolchain则分别会在mingw32\\bin和mingw64\\bin目录下产生gcc.exe，并且只依赖于Windows dlls和libwinpthread-1.dll，看来这个就是MinGW-w64版本的gcc，可以生成不依赖于任何dll的Windows程序。 本文地址：http://xnerv.wang/mingw-cygwin-summary-and-thinking/","categories":[{"name":"编译器","slug":"编译器","permalink":"https://xnerv.wang/categories/%E7%BC%96%E8%AF%91%E5%99%A8/"}],"tags":[{"name":"原创","slug":"原创","permalink":"https://xnerv.wang/tags/%E5%8E%9F%E5%88%9B/"},{"name":"MinGW","slug":"MinGW","permalink":"https://xnerv.wang/tags/MinGW/"},{"name":"编译器","slug":"编译器","permalink":"https://xnerv.wang/tags/%E7%BC%96%E8%AF%91%E5%99%A8/"},{"name":"Cygwin","slug":"Cygwin","permalink":"https://xnerv.wang/tags/Cygwin/"}]},{"title":"编译Windows版的Seafile客户端","slug":"compile-windows-seafile-client","date":"2019-12-23T20:20:00.000Z","updated":"2023-08-21T02:24:20.475Z","comments":true,"path":"compile-windows-seafile-client/","link":"","permalink":"https://xnerv.wang/compile-windows-seafile-client/","excerpt":"Seafile是国内少有的做的还不错的开源产品之一。相信很多朋友都经历过几年前的各大云盘厂商大战然后纷纷衰落的这个过程。金山快盘应该是我用过的个人觉得最好的一款云盘了，和Windows GUI的集成也非常完美。可惜现如今各大云盘不是停止运营了就是今非昔比了。因此与其每天提心吊胆地担心自己宝贵的“资源”是不是会被某度网盘封杀，作为码农的我们何不自己动手来搭建一个私有云盘。Seafile的UI是基于Qt，考虑到可移植性和跨平台兼容性，虽然不能像金山快盘那样拥有一些Windows上的特效，例如在托盘上查看当前在上传/下载哪些文件，以及上传/下载的进度。但优势是多平台，在Linux、安卓，iOS上都有相应的客户端。 Seafile官网只提供了Linux版本客户端的编译方法，并没有提供Windows版本的步骤。搜索了官方论坛，虽然有不少人提问问过，但是官方并没有给出回答。只能说国内的开源的文档还需要进一步完善，遮遮掩掩不算是真正的开源。搭建seafile windows客户端的交叉编译环境这篇文章给出了一种基于MinGW在Fedora上交叉编译Windows版本Seafile的方法。我曾尝试过在Ubuntu 18.04上重复这个步骤，但是因为一些packages的原因没能成功。不同的Linux发行版之间的package有一些区别。我也曾尝试过在Windows上的MinGW上交叉编译，但有一些packages死活找不到在Windows MinGW上对应的版本，而且编译时也有一些错误（Windows上的MinGW好像是跑在Cygwin上面的，也就是说Cygwin上的packages可能不全或者跟Fedora上有一些不同）。最终由于时间关系，我放弃了去弄清楚其中的原因，而是采用在Ubuntu上用docker安装Fedora镜像这种类似作弊的方法来重现这篇文章中的步骤。","text":"Seafile是国内少有的做的还不错的开源产品之一。相信很多朋友都经历过几年前的各大云盘厂商大战然后纷纷衰落的这个过程。金山快盘应该是我用过的个人觉得最好的一款云盘了，和Windows GUI的集成也非常完美。可惜现如今各大云盘不是停止运营了就是今非昔比了。因此与其每天提心吊胆地担心自己宝贵的“资源”是不是会被某度网盘封杀，作为码农的我们何不自己动手来搭建一个私有云盘。Seafile的UI是基于Qt，考虑到可移植性和跨平台兼容性，虽然不能像金山快盘那样拥有一些Windows上的特效，例如在托盘上查看当前在上传/下载哪些文件，以及上传/下载的进度。但优势是多平台，在Linux、安卓，iOS上都有相应的客户端。 Seafile官网只提供了Linux版本客户端的编译方法，并没有提供Windows版本的步骤。搜索了官方论坛，虽然有不少人提问问过，但是官方并没有给出回答。只能说国内的开源的文档还需要进一步完善，遮遮掩掩不算是真正的开源。搭建seafile windows客户端的交叉编译环境这篇文章给出了一种基于MinGW在Fedora上交叉编译Windows版本Seafile的方法。我曾尝试过在Ubuntu 18.04上重复这个步骤，但是因为一些packages的原因没能成功。不同的Linux发行版之间的package有一些区别。我也曾尝试过在Windows上的MinGW上交叉编译，但有一些packages死活找不到在Windows MinGW上对应的版本，而且编译时也有一些错误（Windows上的MinGW好像是跑在Cygwin上面的，也就是说Cygwin上的packages可能不全或者跟Fedora上有一些不同）。最终由于时间关系，我放弃了去弄清楚其中的原因，而是采用在Ubuntu上用docker安装Fedora镜像这种类似作弊的方法来重现这篇文章中的步骤。 基于搭建seafile windows客户端的交叉编译环境这篇文章，我写了一些脚本来自动化整个编译流程以及打包流程，这些脚本我都放到了GitHub上：xnervwang/SeafileClientBuildTools。脚本分成两种，一种是在Host环境（例如我的Ubuntu）上执行的，一种是在docker实例中执行的。先介绍一下各个脚本的作用： InitDockerVerification.sh：这个脚本是在Host环境中执行的。会根据Dockerfile创建相应的Fedora docker image，并且在当前的Host目录创建三个文件夹：build, ms-build, ms-build64，分别用于存放编译出来的Linux版本的Seafile客户端，32位Windows客户端，64位Windows客户端。build, ms-build, ms-build64这三个目录会通过docker volume映射到docker实例内，从而便于docker实例和Host共享目录。 Dockerfile：InitDockerVerification.sh在创建docker image时会使用该文件。这个Dockerfile指定docker镜像在创建时，会git clone我的git repo xnervwang/SeafileClientBuildTools，然后运行其中的InstallDevPackagesFedora.sh安装编译所需要的一些packages。然后基于该docker镜像创建的docker实例会自动执行DockerEntry.sh。 RunDockerVerification.sh：创建docker实例并启动。 DockerEntry.sh：这个脚本被docker实例在启动时执行，会先更新本地的git repo，然后执行OneKey.sh。OneKey.sh会编译Linux版本的Seafile客户端，32位Windows客户端，64位Windows客户端。然后调用ResolveDllDeps.py这个脚本分别对三种客户端进行打包，打包后的产物分别位于build, ms-build, ms-build64这三个目录内。 ResolveDllDeps.py：这是我写的一个比较有意思的脚本。可以自动递归分析binary所依赖的所有.so动态链接库文件，然后将这些.so文件都复制到打包目录中，从而便于发布。需要注意的是编译出来的Seafile客户端还依赖于qt5/plugins/imageformats和qt5/plugins/platforms这两个插件目录，但是我的这个脚本却无法从Seafile客户端的可执行文件推导出这样的依赖关系。我推测可能这两个插件目录中的插件是通过dlopen的方式动态加载的，因此不能通过静态分析获得依赖关系。所以我将这连个目录中的.so也加入到了第一级的binary列表中。 Makefile：这个是在docker实例中，在DockerEntry.sh执行编译时所使用的的Makefile。 Toolchain-cross-linux.i686.cmake/Toolchain-cross-linux.x86_64.cmake：MinGW编译工具所使用的配置文件，分别用于32位/64位Windows客户端。 DropDockerVerification.sh：删除docker实例和相关的生成目录。 因此，基本的使用方法就是，首先基于docker官方文档安装docker环境，然后git clone我的git repo，然后先执行InitDockerVerification.sh环境，然后以后就可以每次通过执行RunDockerVerification.sh来获取编译后的客户端，分别位于Host机器的build, ms-build, ms-build64这三个目录内。 本文地址：http://xnerv.wang/compile-windows-seafile-client/","categories":[{"name":"分布式及存储","slug":"分布式及存储","permalink":"https://xnerv.wang/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8F%8A%E5%AD%98%E5%82%A8/"}],"tags":[{"name":"原创","slug":"原创","permalink":"https://xnerv.wang/tags/%E5%8E%9F%E5%88%9B/"},{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/tags/Windows/"},{"name":"分布式及存储","slug":"分布式及存储","permalink":"https://xnerv.wang/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8F%8A%E5%AD%98%E5%82%A8/"},{"name":"Seafile","slug":"Seafile","permalink":"https://xnerv.wang/tags/Seafile/"},{"name":"MinGW","slug":"MinGW","permalink":"https://xnerv.wang/tags/MinGW/"},{"name":"Docker","slug":"Docker","permalink":"https://xnerv.wang/tags/Docker/"}]},{"title":"TCP模型知识点总结","slug":"tcp-model-knowledge-summary","date":"2019-11-17T01:02:00.000Z","updated":"2023-08-21T02:24:21.837Z","comments":true,"path":"tcp-model-knowledge-summary/","link":"","permalink":"https://xnerv.wang/tcp-model-knowledge-summary/","excerpt":"协议格式 IPv4 IP首部中的校验和只覆盖IP的首部，不覆盖IP数据报中的任何数据。 IP层会丢弃传输中损坏的数据报，但是不产生错误消息，由上层去检测和重传。但是如果发生了分片，IP层应该能保证原子性。 在IP层下面的每一种数据链路层都有自己的帧格式，其中包括帧格式中的数据字段的最大长度，即最大传送单元 MTU (Maximum Transfer Unit)。当一个数据报封装成链路层的帧时，此数据报的总长度（即首部加上数据部分）最好不能超过下面的数据链路层的MTU值，否则要分片。 增加首部的可变部分是为了增加IP数据报的功能，但这同时也使得IP数据报的首部长度成为可变的。这就增加了每一个路由器处理数据报的开销，实际上这些选项很少被使用。新的IP版本IPv6就将IP数据报的首部长度做成固定的。 IP包中只有首部检验和，由TCP和UDP报文各自包含自身的数据校验和。","text":"协议格式 IPv4 IP首部中的校验和只覆盖IP的首部，不覆盖IP数据报中的任何数据。 IP层会丢弃传输中损坏的数据报，但是不产生错误消息，由上层去检测和重传。但是如果发生了分片，IP层应该能保证原子性。 在IP层下面的每一种数据链路层都有自己的帧格式，其中包括帧格式中的数据字段的最大长度，即最大传送单元 MTU (Maximum Transfer Unit)。当一个数据报封装成链路层的帧时，此数据报的总长度（即首部加上数据部分）最好不能超过下面的数据链路层的MTU值，否则要分片。 增加首部的可变部分是为了增加IP数据报的功能，但这同时也使得IP数据报的首部长度成为可变的。这就增加了每一个路由器处理数据报的开销，实际上这些选项很少被使用。新的IP版本IPv6就将IP数据报的首部长度做成固定的。 IP包中只有首部检验和，由TCP和UDP报文各自包含自身的数据校验和。 IPv6 IPv6的区别 首部长度 首部长度可变，IPv4首部的选项字段允许IP首部被扩展，由此导致数据报首部长度可变，故不能预先确定数据字段从何开始，同时也使路由器处理一个IP数据报所需时间差异很大(有的要处理选项，有的不需要)。基于此，IPv6采用固定40字节长度的报头长度(称基本报头)。IPv6如何实现IPv4选项字段类似的功能，答案是扩展报头，并由IPv6基本报头的下一个首部指向扩展报头(如果有的话)。路由器不处理扩展报头，提升了路由器处理效率。 分片/重组 IPv6，分片与重组只能在源与目的地上执行，不允许在中间路由器进行。分片与重组是个耗时的操作，将该功能从路由器转移到端系统，大大加快了网络中的IP转发速率。那，如果路由器收到IPv6数据报太大而不能转发到出链路上怎么办？该路由器丢弃该包，并向发送发发回一个&quot;分组太大&quot;的ICMP差错报文，于是发送发使用较小长度的IP数据报重发数据。 首部检查和 IPv4中由于TTL的递减，所以每经过一个路由器都需要重新计算校验和，导致路由器处理速度的低下。加之，传输层和链路层协议执行了检验操作，网络传输可靠性提升，所以IPv6不进行首部检查和，从而更快速处理IP分组。（但在网络传输的过程中，链路层packet是可能损坏的，考虑到厂商设备的多样性和高负载。所以TCP校验应该是关键，如果发现checksum不对，TCP可以要求对方重传丢失的内容。） TCP头 校验和是针对header和data计算出来的。TCP和UDP计算校验和时，都要加上一个12字节的伪首部。伪首部共有12字节，包含如下信息：源IP地址、目的IP地址、保留字节(置0)、传输层协议号(TCP是6)、TCP报文长度(报头+数据)。伪首部是为了增加TCP校验和的检错能力：如检查TCP报文是否收错了(目的IP地址)、传输层协议是否选对了(传输层协议号)等。 TCP和UDP的报文都没有一个字段可以表明自身长度，这个长度是由IP包中的总长度来记录的。 TCP报文段首部格式详解 TCP首部长度：由于TCP首部包含一个长度可变的选项部分，所以需要这么一个值来指定这个TCP报文段到底有多长。或者可以这么理解：就是表示TCP报文段中数据部分在整个TCP报文段中的位置。该字段的单位是32位字，即：4个字节。 选项部分：其最大长度可根据TCP首部长度进行推算。TCP首部长度用4位表示，那么选项部分最长为：(2^4-1)*4-20=40字节（但要全零填充为4字节的整数倍）。 选项部分的应用： MSS最大报文段长度(Maxium Segment Size)：指明数据字段的最大长度，数据字段的长度加上TCP首部的长度才等于整个TCP报文段的长度。MSS值指示自己期望对方发送TCP报文段时那个数据字段的长度。通信双方可以有不同的MSS值。如果未填写，默认采用536字节。MSS只出现在SYN报文中。即：MSS出现在SYN=1的报文段中。 窗口扩大选项(Windows Scaling)：由于TCP首部的窗口大小字段长度是16位，所以其表示的最大数是65535。但是随着时延和带宽比较大的通信产生（如卫星通信），需要更大的窗口来满足性能和吞吐率，所以产生了这个窗口扩大选项。 SACK选择确认项(Selective Acknowledgements)：用来确保只重传缺少的报文段，而不是重传所有报文段。比如主机A发送报文段1、2、3，而主机B仅收到报文段1、3。那么此时就需要使用SACK选项来告诉发送方只发送丢失的数据。那么又如何指明丢失了哪些报文段呢？使用SACK需要两个功能字节。一个表示要使用SACK选项，另一个指明这个选项占用多少字节。描述丢失的报文段2，是通过描述它的左右边界报文段1、3来完成的。而这个1、3实际上是表示序列号，所以描述一个丢失的报文段需要64位即8个字节的空间。那么可以推算整个选项字段最多描述(40-2)/8=4个丢失的报文段。 时间戳选项（Timestamps）：可以用来计算RTT(往返时间)，发送方发送TCP报文时，把当前的时间值放入时间戳字段，接收方收到后发送确认报文时，把这个时间戳字段的值复制到确认报文中，当发送方收到确认报文后即可计算出RTT。也可以用来防止回绕序号PAWS，也可以说可以用来区分相同序列号的不同报文。因为序列号用32为表示，每2^32个序列号就会产生回绕，那么使用时间戳字段就很容易区分相同序列号的不同报文。 NOP(NO-Operation)：它要求选项部分中的每种选项长度必须是4字节的倍数，不足的则用NOP填充。同时也可以用来分割不同的选项字段。如窗口扩大选项和SACK之间用NOP隔开。 UDP linux网络编程之：UDP数据包格式 UDP数据报格式有首部和数据两个部分。首部很简单，共8字节。包括： 源端口（Source Port）：2字节，源端口号。 目的端口（Destination Port ）：2字节，目的端口号。 长度（Length）：2字节，UDP用户数据报的总长度，以字节为单位。 检验和（Checksum）：2字节，用于校验UDP数据报的数字段和包含UDP数据报首部的“伪首部”。其校验方法同IP分组首部中的首部校验和。 伪首部，又称为伪包头（Pseudo Header）：是指在TCP的分段或UDP的数据报格式中，在数据报首部前面增加源IP地址、目的IP地址、IP分组的协议字段、TCP或UDP数据报的总长度等共12字节，所构成的扩展首部结构。此伪首部是一个临时的结构，它既不向上也不向下传递，仅仅只是为了保证可以校验套接字的正确性。 UDP的校验和是可选的，如果校验码为 0 ,意味着发送者末产生校验码。这表示对于数据段不使用校验,因为 IP 只是对 IP 首部进行校验。 RST 产生复位的一种常见情况是当连接请求到达时，目的端口没有进程正在监听。对于UDP，当一个数据报到达目的端口时，该端口没在使用，它将产生一个ICMP端口不可达的信息。而TCP则使用复位/重置连接。 RST报文段不会导致另一端产生任何响应，另一端根本不进行确认。收到RST的一方将终止该连接，并通知应用层连接复位。 带外数据SO_OOBINLINE 其实就是一个指针指向正常数据中的一个字节的后一个字节位置。 别用TCP的紧急数据提到带外数据已经不建议使用。同时提到带外数据可以用于控制意图，这样就不用像FTP一样得单独开一个控制连接了。 TCP的紧急指针，一般都不建议使用，而且不同的TCP/IP实现，也不同，一般说如果你有紧急数据宁愿再建立一个新的TCP/IP连接发送数据，让对方紧急处理。但是，虽然sendUrgentData的参数data是int类型，但只有这个int类型的低字节被发送，其它的三个字节被忽略。 接收端如何处理这个数据存在一些模糊。有的平台和API把它和平常数据分开处理，然后大多数解决方案是把它放到普通数据队列，让应用自己去从队列中获取处理。 一些TCP参数 tcp_max_syn_backlog Linux TCP队列相关参数的总结 建立连接涉及两个队列： 半连接队列，保存SYN_RECV状态的连接。队列长度由net.ipv4.tcp_max_syn_backlog设置。 accept队列，保存ESTABLISHED状态的连接。队列长度为min(net.core.somaxconn, backlog)。其中backlog是我们创建ServerSocket(intport, int backlog)时指定的参数，最终会传递给listen方法。 TCP SOCKET中backlog参数的用途是什么？ 在linux 2.2以前，backlog大小包括了半连接状态和全连接状态两种队列大小。linux 2.2以后，分离为两个backlog来分别限制半连接SYN_RCVD状态的未完成连接队列大小跟全连接ESTABLISHED状态的已完成连接队列大小。互联网上常见的TCP SYN FLOOD恶意DOS攻击方式就是用/proc/sys/net/ipv4/tcp_max_syn_backlog来控制的。 在使用listen函数时，内核会根据传入参数的backlog跟系统配置参数/proc/sys/net/core/somaxconn中，二者取最小值，作为“ESTABLISHED状态之后，完成TCP连接，等待服务程序ACCEPT”的队列大小。在kernel 2.4.25之前，是写死在代码常量SOMAXCONN，默认值是128。在kernel 2.4.25之后，在配置文件/proc/sys/net/core/somaxconn (即 /etc/sysctl.conf 之类 )中可以修改。 How TCP backlog works in Linux backlog为0 时在linux上表明允许不受限制的连接数，这是一个缺陷，因为它可能会导致SYN Flooding(拒绝服务型攻击。 tcp_tw_recycle ：BOOLEAN 默认值是0。 打开快速 TIME-WAIT sockets 回收。除非得到技术专家的建议或要求﹐请不要随意修改这个值。(做NAT的时候，建议打开它)。 tcp_tw_reuse：BOOLEAN 默认值是0。 该文件表示是否允许重新应用处于TIME-WAIT状态的socket用于新的TCP连接(这个对快速重启动某些服务,而启动后提示端口已经被使用的情形非常有帮助)。 tcp_max_orphans ：INTEGER 缺省值是8192。 系统所能处理不属于任何进程的TCP sockets最大数量。假如超过这个数量﹐那么不属于任何进程的连接会被立即reset，并同时显示警告信息。之所以要设定这个限制﹐纯粹为了抵御那些简单的 DoS 攻击﹐千万不要依赖这个或是人为的降低这个限制(这个值Redhat AS版本中设置为32768，但是很多防火墙修改的时候,，议该值修改为2000)。 tcp_abort_on_overflow ：BOOLEAN 缺省值是0。 当守护进程太忙而不能接受新的连接，就向对方发送reset消息，默认值是false。这意味着当溢出的原因是因为一个偶然的猝发，那么连接将恢复状态。只有在你确信守护进程真的不能完成连接请求时才打开该选项，该选项会影响客户的使用。(对待已经满载的sendmail，apache这类服务的时候，这个可以很快让客户端终止连接，可以给予服务程序处理已有连接的缓冲机会,所以很多防火墙上推荐打开它)。 TCP_NODELAY 神秘的40毫秒延迟与TCP_NODELAY Nagle’s Algorithm 是为了提高带宽利用率设计的算法，其做法是合并小的TCP 包为一个，避免了过多的小报文的 TCP 头所浪费的带宽。如果开启了这个算法 （默认），则协议栈会累积数据直到以下两个条件之一满足的时候才真正发送出 去： 积累的数据量到达最大的 TCP Segment Size。 收到了一个 Ack。 TCP Delayed Acknoledgement 也是为了类似的目的被设计出来的，它的作用就是延迟 Ack 包的发送，使得协议栈有机会合并多个 Ack，提高网络性能。 如果一个 TCP 连接的一端启用了 Nagle‘s Algorithm，而另一端启用了 TCP Delayed Ack，而发送的数据包又比较小，则可能会出现这样的情况：发送端在等待接收端对上一个packet 的 Ack 才发送当前的 packet，而接收端则正好延迟了 此 Ack 的发送，那么这个正要被发送的 packet 就会同样被延迟。当然 Delayed Ack 是有个超时机制的，而默认的超时正好就是 40ms。 现代的 TCP/IP 协议栈实现，默认几乎都启用了这两个功能，你可能会想，按我上面的说法，当协议报文很小的时候，岂不每次都会触发这个延迟问题？事实不是那样的。仅当协议的交互是发送端连续发送两个 packet，然后立刻 read 的时候才会出现问题。 Nagle算法主要是避免发送小的数据包，要求TCP连接上最多只能有一个未被确认的小分组，在该分组的确认到达之前不能发送其他的小分组。相反，TCP收集这些少量的小分组，并在确认到来时以一个分组的方式发出去。 SO_LINGER 设置函数close()关闭TCP连接时的行为。缺省close()的行为是，如果有数据残留在socket发送缓冲区中则系统将继续发送这些数据给对方，等待被确认，然后返回。SO_LINGER选项用来改变此缺省设置。使用如下结构： 1234struct linger &#123; int l_onoff; &#x2F;* 0 &#x3D; off, nozero &#x3D; on *&#x2F; int l_linger; &#x2F;* linger time *&#x2F;&#125;; l_onoff l_linger closesocket行为 发送队列 底层行为 零 忽略 立即返回。 保持直至发送完成。 系统接管套接字并保证将数据发送至对端。 非零 零 立即返回。 立即放弃。 直接发送RST包，自身立即复位，不用经过2MSL状态。对端收到复位错误号。 非零 非零 阻塞直到l_linger时间超时或数据发送完成。(套接字必须设置为阻塞) 在超时时间段内保持尝试发送，若超时则立即放弃。 超时则同第二种情况，若发送完成则皆大欢喜。 第二种设置主要是为了避免主动断开连接方进入TIME_WAIT状态。丢失缓冲区中未丢失数据只是一种副作用。 SO_REUSEADDR / SO_REUSEPORT 浅析套接字中SO_REUSEPORT和SO_REUSEADDR的区别 SO_KEEPALIVE 貌似是由发起连接方（客户端）主动发给服务端的，就是一个data size为0的packet，服务器收到这个packet也会回复一个同样data size为0的packet表明连接仍存活。 SO_KEEPALIVE和心跳线程 SO_KEEPALIVE是系统底层的机制，用于系统维护每一个tcp连接的。 心跳线程属于应用层，主要用于终端和服务器连接的检查。 即使SO_KEEPALIVE检测到连接正常，但并不能保证终端和服务器连接的正常。有一种情况，服务器进程死了，但它和客户端的tcp连接还连着（该连接由系统维护的）。 这就是SO_KEEPALIVE不能取代心跳线程的原因吧。 TCP协议 四次挥手 其实也可以看成两次过程，任何一方发送FIN表明自己不会再发送数据。 TIME_WAIT（涉及主动断开连接一方） TCP协议在关闭连接的四次握手过程中，最终的ACK 是由 主动关闭连接 的一端（后面统称A端）发出的，如果这个ACK丢失，对方（后面统称B端）将重发出最终的FIN，因此A端必须维护状态信息（TIME_WAIT）允许它重发最终的ACK。如果A端不维持TIME_WAIT状态，而是处于CLOSED 状态，那么A端将响应RST分节，B端收到后将此分节解释成一个错误（在java中会抛出connection reset的SocketException)。 因而，要实现TCP全双工连接的正常终止，必须处理终止过程中四个分节任何一个分节的丢失情况，主动关闭连接的A端必须维持TIME_WAIT状态。 主动关闭的一方要负责处于TIME_WAIT状态中。MSL就是maximum segment lifetime(最大分节生命期），这是一个IP数据包能在互联网上生存的最长时间，超过这个时间IP数据包将在网络中消失 。MSL在RFC 1122上建议是2分钟，而源自berkeley的TCP实现传统上使用30秒。Windows使用的是2分钟，而Linux则是30秒。 CLOSE_WAIT（涉及被动断开连接一方） 在被动关闭连接情况下，在已经接收到FIN，但是还没有发送自己的FIN的时刻，连接处于CLOSE_WAIT状态。 出现大量CLOSE_WAIT的现象，主要原因是某种情况下对方关闭socket链接，但是我方忙与读或者写，没有关闭连接。代码需要判断socket，一旦读到0，断开连接，read返回负，检查一下errno，如果不是AGAIN，就断开连接。 服务器TIME_WAIT和CLOSE_WAIT详解和解决办法 拥塞控制 当cwnd&lt;ssthresh时，使用慢开始算法。 当cwnd&gt;ssthresh时，改用拥塞避免算法。 当cwnd=ssthresh时，慢开始与拥塞避免算法任意。 快重传和快恢复 快重传要求接收方在收到一个失序的报文段后就立即发出重复确认（为的是使发送方及早知道有报文段没有到达对方）而不要等到自己发送数据时捎带确认。快重传算法规定，发送方只要一连收到三个重复确认就应当立即重传对方尚未收到的报文段，而不必继续等待设置的重传计时器时间到期。 快重传配合使用的还有快恢复算法，有以下两个要点: 当发送方连续收到三个重复确认时，就执行“乘法减小”算法，把ssthresh门限减半。但是接下去并不执行慢开始算法。 考虑到如果网络出现拥塞的话就不会收到好几个重复的确认，所以发送方现在认为网络可能没有出现拥塞。所以此时不执行慢开始算法，而是将cwnd设置为ssthresh的大小，然后执行拥塞避免算法。如下图： 随机早期检测RED 若发生路由器的尾部丢弃，可能影响到很多条TCP连接，结果就是这许多的TCP连接在同一时间进入慢开始状态。这在术语中称为全局同步。全局同步会使得网络的通信量突然下降很多，而在网络恢复正常之后，其通信量又突然增大很多。 为避免发生网路中的全局同步现象，路由器采用随机早期检测(RED:randomearly detection)。 msl、ttl及rtt的区别 MSL 是Maximum Segment Lifetime英文的缩写，中文可以译为“报文最大生存时间”，他是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。例如RIP协议用经过的最大路由节点数作为MSL。 IP头中有一个TTL域，TTL是 time to live的缩写，中文可以译为“生存时间”，这个生存时间是由源主机设置初始值但不是存的具体时间，而是存储了一个ip数据报可以经过的最大路由数，每经 过一个处理他的路由器此值就减1，当此值为0则数据报将被丢弃，同时发送ICMP报文通知源主机。RFC 793中规定MSL为2分钟，实际应用中常用的是30秒，1分钟和2分钟等。 TTL与MSL是有关系的但不是简单的相等的关系，MSL要大于等于TTL。 RTT是客户到服务器往返所花时间（round-trip time，简称RTT），TCP含有动态估算RTT的算法。TCP还持续估算一个给定连接的RTT，这是因为RTT受网络传输拥塞程序的变化而变化。 本文地址：http://xnerv.wang/tcp-model-knowledge-summary/","categories":[{"name":"网络协议","slug":"网络协议","permalink":"https://xnerv.wang/categories/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"}],"tags":[{"name":"网络协议","slug":"网络协议","permalink":"https://xnerv.wang/tags/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"},{"name":"原创","slug":"原创","permalink":"https://xnerv.wang/tags/%E5%8E%9F%E5%88%9B/"},{"name":"TCP","slug":"TCP","permalink":"https://xnerv.wang/tags/TCP/"},{"name":"IP","slug":"IP","permalink":"https://xnerv.wang/tags/IP/"},{"name":"UDP","slug":"UDP","permalink":"https://xnerv.wang/tags/UDP/"}]},{"title":"线程同步与原子操作","slug":"thread-synchronization-and-atomic-operation","date":"2019-09-25T15:25:00.000Z","updated":"2023-08-21T02:24:20.992Z","comments":true,"path":"thread-synchronization-and-atomic-operation/","link":"","permalink":"https://xnerv.wang/thread-synchronization-and-atomic-operation/","excerpt":"volatile volatile使得代码每次在读写volatile变量时都需要从内存读写，而不能使用寄存器中缓存的值。并且也禁止编译器对volatible做编译优化。volatile本身并不是用于线程同步，也不保证原子读写（例如volatile a++这种需要几个指令才能完成的操作）。volatile主要用于access to memory mapped devices和variables in signal handlers and between setjmp and longjmp。C++标准禁止编译器reorder同一个线程内的volatile变量的读写，但不同线程则没有限制。non-volatile变量则有可能发生reorder（Stay away from Volatile in threaded code?）。 而根据为什么volatile++不是原子性的？中的说法，volatile的读操作后会插入LoadLoad和LoadStore屏障，避免volatile读操作与后面的普通读写发生reorder。而volatile的写操作前会插入StoreLoad和StoreStore屏障，避免volatile写操作与后面的普通读写发生reorder（我不太确定这种说法的正确性，毕竟在wikipediavolatile (computer programming)中并没有提到volatile会插入内存屏障，或者只有Java等语言才会这样做？）。 内存屏障中有提到 C与C++语言中，volatile关键字意图允许内存映射的I/O操作。这要求编译器对此的数据读写按照程序中的先后顺序执行，不能对volatile内存的读写重排序。因此关键字volatile并不保证是一个内存屏障。[4] 对于Visual Studio 2003，编译器保证对volatile的操作是有序的，但是不能保证处理器的乱序执行。因此，可以使用InterlockedCompareExchange或InterlockedExchange函数。 对于Visual Studio 2005及以后版本，编译器对volatile变量的读操作使用acquire semantics，对写操作使用release semantics。","text":"volatile volatile使得代码每次在读写volatile变量时都需要从内存读写，而不能使用寄存器中缓存的值。并且也禁止编译器对volatible做编译优化。volatile本身并不是用于线程同步，也不保证原子读写（例如volatile a++这种需要几个指令才能完成的操作）。volatile主要用于access to memory mapped devices和variables in signal handlers and between setjmp and longjmp。C++标准禁止编译器reorder同一个线程内的volatile变量的读写，但不同线程则没有限制。non-volatile变量则有可能发生reorder（Stay away from Volatile in threaded code?）。 而根据为什么volatile++不是原子性的？中的说法，volatile的读操作后会插入LoadLoad和LoadStore屏障，避免volatile读操作与后面的普通读写发生reorder。而volatile的写操作前会插入StoreLoad和StoreStore屏障，避免volatile写操作与后面的普通读写发生reorder（我不太确定这种说法的正确性，毕竟在wikipediavolatile (computer programming)中并没有提到volatile会插入内存屏障，或者只有Java等语言才会这样做？）。 内存屏障中有提到 C与C++语言中，volatile关键字意图允许内存映射的I/O操作。这要求编译器对此的数据读写按照程序中的先后顺序执行，不能对volatile内存的读写重排序。因此关键字volatile并不保证是一个内存屏障。[4] 对于Visual Studio 2003，编译器保证对volatile的操作是有序的，但是不能保证处理器的乱序执行。因此，可以使用InterlockedCompareExchange或InterlockedExchange函数。 对于Visual Studio 2005及以后版本，编译器对volatile变量的读操作使用acquire semantics，对写操作使用release semantics。 volatile跟const一样属于变量修饰符，因此也和const一样必须弄清楚修饰的是指针还是变量自身（或者甚至是第几级指针）。例如uchar * volatile reg;说明指针reg本身是volatile的，而volatile uchar *reg;说明*reg（也就是reg指向的变量）是volatile的。而且volatile也可以和const同时使用。 volatile陷阱一文中有提到几种volatile的陷阱和误用。 “Volatile” can be harmful…中提到可以将函数参数标记为volatile避免编译器优化，从而便于debug。 临界区块（Critical section） Windows的CRITICAL_SECTION首先会在用户态自旋尝试几次获取锁，如果最终还是失败的话就会内核模式等待。 内存屏障（Memory barrier） 首先要明白的一点，reorder不仅可以发生在编译时，也可以发生在运行时。CPU流水线也可以重排某些指令顺序。所以即使是同一段编译好的程序，不同的CPU内核也可能执行不同的指令顺序。（Volatile and memory barriers） 根据Memory barrier，如果只有单个CPU，即使发生reorder也不会有什么问题，问题都是发生在多个线程之间。而且内存屏障只在运行时生效？？？而编译时需要用volatile？并且这篇wiki也提到volatile并不能阻止volatile变量和non-volatile变量的reorder。本质上，C/C++标准中volatile是通过控制编译器而实现的（虽然编译器实现有可能引入内存屏障来实现volatile），而内存屏障是通过特殊CPU指令实现的）。 How do I Understand Read Memory Barriers and Volatile中将对内存的操作比喻成有一个queue（因为CPU比内存快），所以Acquire操作就是flush all read requests in the queue（实际上并不是flush，而只是加一个标记，所以叫做内存屏障），而Release操作就是flush all write requests in the queue。因此在Acquire(lfence)之前的read requests一定会完成在Acquire之前，而Release(sfence)之前的write requests一定会完成在Release之后。类似于“半透膜”的效果。（那也就是说Acquire是LoadLoad屏障，而Release是WriteWrite屏障？）而full barrier (or full fence, mfence)就是禁止在此之前的所有read/write操作被reorder到本操作之后。 而C++11中的Acquire和Release定义则与上面的回答不同。C++11中的Acquire是LoadLoad+StoreLoad，Release是LoadStore+StoreStore。结合C++11内存模型，内存屏障与内存模型，C++内存屏障（内存顺序）总结，std::memory_order等文章，6中内存屏障级别的区别是： （建议仔细再阅读最后一篇文章。不太明白的是，当论及other thread acquire/release the same atomic时，是指代码上有acquire/release操作的线程，还是指在某一个时刻瞬间进行了acquire/release的线程？就目前看来前者的可能性更大一些） 条件变量（Condition Variable） C++11的条件变量跟win32的Event有一个区别，就是必须在wait之后signal，否则就必须结合预测条件（从而检查是否在wait之前已经signal）。C++ Core Guidelines: Be Aware of the Traps of Condition Variables Atomic Does the C++ 11 standard guarantees that std::atomic&lt;&gt; is implemented as a lock-free operation?，C++11不保证std::atomic是lock-free的，而是由数据类型长度等因素决定的，可以用std::atomic::is_lock_free()来判断该类型是否lock-free。 What is the difference between explicit atomic load/store and usual operator= and operator T?，两者是相等的，后者使用默认的memory_order_seq_cst级别内存屏障。 本文地址：http://xnerv.wang/thread-synchronization-and-atomic-operation/","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://xnerv.wang/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"原创","slug":"原创","permalink":"https://xnerv.wang/tags/%E5%8E%9F%E5%88%9B/"},{"name":"操作系统","slug":"操作系统","permalink":"https://xnerv.wang/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"线程同步","slug":"线程同步","permalink":"https://xnerv.wang/tags/%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5/"},{"name":"原子操作","slug":"原子操作","permalink":"https://xnerv.wang/tags/%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/"},{"name":"volatile","slug":"volatile","permalink":"https://xnerv.wang/tags/volatile/"}]},{"title":"MySQL - 常用SQL语句的MDL加锁源码分析（转载）","slug":"analysis-of-mdl-source-code-for-common-sql-statements","date":"2018-03-22T02:40:00.000Z","updated":"2023-08-21T02:24:19.103Z","comments":true,"path":"analysis-of-mdl-source-code-for-common-sql-statements/","link":"","permalink":"https://xnerv.wang/analysis-of-mdl-source-code-for-common-sql-statements/","excerpt":"前言 MySQL5.5版本开始引入了MDL锁用来保护元数据信息，让MySQL能够在并发环境下多DDL、DML同时操作下保持元数据的一致性。本文用MySQL5.7源码分析了常用SQL语句的MDL加锁实现。 MDL锁粒度 MDL_key由namespace、db_name、name组成。 namespace包含： GLOBAL。用于global read lock，例如FLUSH TABLES WITH READ LOCK。 TABLESPACE/SCHEMA。用于保护tablespace/schema。 FUNCTION/PROCEDURE/TRIGGER/EVENT。用于保护function/procedure/trigger/event。 COMMIT。主要用于global read lock后，阻塞事务提交。（在DML的commit阶段也会获取COMMIT锁） USER_LEVEL_LOCK。用于user level lock函数的实现，GET_LOCK(str,timeout)， RELEASE_LOCK(str)。 LOCKING_SERVICE。用于locking service的实现。","text":"前言 MySQL5.5版本开始引入了MDL锁用来保护元数据信息，让MySQL能够在并发环境下多DDL、DML同时操作下保持元数据的一致性。本文用MySQL5.7源码分析了常用SQL语句的MDL加锁实现。 MDL锁粒度 MDL_key由namespace、db_name、name组成。 namespace包含： GLOBAL。用于global read lock，例如FLUSH TABLES WITH READ LOCK。 TABLESPACE/SCHEMA。用于保护tablespace/schema。 FUNCTION/PROCEDURE/TRIGGER/EVENT。用于保护function/procedure/trigger/event。 COMMIT。主要用于global read lock后，阻塞事务提交。（在DML的commit阶段也会获取COMMIT锁） USER_LEVEL_LOCK。用于user level lock函数的实现，GET_LOCK(str,timeout)， RELEASE_LOCK(str)。 LOCKING_SERVICE。用于locking service的实现。 MDL锁类型 MDL_INTENTION_EXCLUSIVE(IX) 意向排他锁，锁定一个范围，用在GLOBAL/SCHEMA/COMMIT粒度。 MDL_SHARED(S) 用在只访问元数据信息，不访问数据。例如CREATE TABLE t LIKE t1; MDL_SHARED_HIGH_PRIO(SH) 也是用于只访问元数据信息，但是优先级比排他锁高，用于访问information_schema的表。例如：select * from information_schema.tables; MDL_SHARED_READ(SR) 访问表结构并且读表数据，例如：SELECT * FROM t1; LOCK TABLE t1 READ LOCAL; MDL_SHARED_WRITE(SW) 访问表结构且写表数据， 例如：INSERT/DELETE/UPDATE t1 … ;SELECT * FROM t1 FOR UPDATE;LOCK TALE t1 WRITE MDL_SHARED_WRITE_LOW_PRIO(SWLP) 优先级低于MDL_SHARED_READ_ONLY。语句INSER/DELETE/UPDATE LOW_PRIORITY t1 …; LOCK TABLE t1 WRITE LOW_PRIORITY。 MDL_SHARED_UPGRADABLE(SU) 可升级锁，允许并发update/read表数据。持有该锁可以同时读取表metadata和表数据，但不能修改数据。可以升级到SNW、X锁。用在alter table的第一阶段，使alter table的时候不阻塞DML，防止其他DDL。（是mysql 5.6引入的新的metadata lock，在alter table/create index/drop index会加该锁，可以说是为了online ddl才引入的。特点是允许DML，防止DDL。） MDL_SHARED_READ_ONLY(SRO) 持有该锁可读取表数据，同时阻塞所有表结构和表数据的修改操作，用于LOCK TABLE t1 READ。 MDL_SHARED_NO_WRITE(SNW) 持有该锁可以读取表metadata和表数据，同时阻塞所有的表数据修改操作，允许读。可以升级到X锁。用在ALTER TABLE第一阶段，拷贝原始表数据到新表，允许读但不允许更新。 MDL_SHARED_NO_READ_WRITE(SNRW) 可升级锁，允许其他连接读取表结构但不可以读取数据，阻塞所有表数据的读写操作，允许INFORMATION_SCHEMA访问和SHOW语句。持有该锁的的连接可以读取表结构，修改和读取表数据。可升级为X锁。使用在LOCK TABLE WRITE语句。 MDL_EXCLUSIVE(X) 排他锁，持有该锁连接可以修改表结构和表数据，使用在CREATE/DROP/RENAME/ALTER TABLE 语句。 MDL锁持有时间 MDL_STATEMENT 语句中持有，语句结束自动释放 MDL_TRANSACTION 事务中持有，事务结束时释放 MDL_EXPLICIT 需要显示释放 MDL锁兼容性 Scope锁活跃锁和请求锁兼容性矩阵如下。 1234567891011 | Type of active |Request | scoped lock |type | IS(*) IX S X |---------+------------------+IS | + + + + |IX | + + - - |S | + - + - |X | + - - - |+号表示请求的锁可以满足。-号表示请求的锁无法满足需要等待。 Scope锁等待锁和请求锁优先级矩阵 12345678910 | Pending |Request | scoped lock |type | IS(*) IX S X |---------+-----------------+IS | + + + + |IX | + + - - |S | + + + - |X | + + + + |+号表示请求的锁可以满足。-号表示请求的锁无法满足需要等待。 object上已持有锁和请求锁的兼容性矩阵如下。 12345678910111213Request | Granted requests for lock | type | S SH SR SW SWLP SU SRO SNW SNRW X |----------+---------------------------------------------+S | + + + + + + + + + - |SH | + + + + + + + + + - |SR | + + + + + + + + - - |SW | + + + + + + - - - - |SWLP | + + + + + + - - - - |SU | + + + + + - + - - - |SRO | + + + - - + + + - - |SNW | + + + - - - + - - - |SNRW | + + - - - - - - - - |X | - - - - - - - - - - | object上等待锁和请求锁的优先级矩阵如下。 12345678910111213Request | Pending requests for lock | type | S SH SR SW SWLP SU SRO SNW SNRW X |----------+--------------------------------------------+S | + + + + + + + + + - |SH | + + + + + + + + + + |SR | + + + + + + + + - - |SW | + + + + + + + - - - |SWLP | + + + + + + - - - - |SU | + + + + + + + + + - |SRO | + + + - + + + + - - |SNW | + + + + + + + + + - |SNRW | + + + + + + + + + - |X | + + + + + + + + + + | 常用语句MDL锁加锁分析 使用performance_schema可以辅助分析加锁。利用下面语句打开MDL锁分析，可以看到在只有当前session访问的时候，SELECT语句对metadata_locks表加了TRANSACTION周期的SHARED_READ锁，即锁粒度、时间范围和锁类型分别为：TABLE, TRANSACTION, SHARED_READ，在代码位置sql_parse.cc:5996初始化锁。后面的锁分析也按照锁粒度-时间范围-锁类型介绍。 1234567891011121314UPDATE performance_schema.setup_consumers SET ENABLED &#x3D; &#39;YES&#39; WHERE NAME &#x3D;&#39;global_instrumentation&#39;;UPDATE performance_schema.setup_instruments SET ENABLED &#x3D; &#39;YES&#39; WHERE NAME &#x3D;&#39;wait&#x2F;lock&#x2F;metadata&#x2F;sql&#x2F;mdl&#39;;select * from performance_schema.metadata_locks\\G*************************** 1. row *************************** OBJECT_TYPE: TABLE OBJECT_SCHEMA: performance_schema OBJECT_NAME: metadata_locksOBJECT_INSTANCE_BEGIN: 46995934864720 LOCK_TYPE: SHARED_READ LOCK_DURATION: TRANSACTION LOCK_STATUS: GRANTED SOURCE: sql_parse.cc:5996 OWNER_THREAD_ID: 26 OWNER_EVENT_ID: 163 使用performance_schema很难完整分析语句执行中所有的加锁过程，可以借助gdb分析，在 MDL_context::acquire_lock设置断点。 下面会结合performance_schema和gdb分析常用语句的MDL加锁源码实现。 FLUSH TABLES WITH READ LOCK 语句执行会加锁GLOBAL-EXPLICIT-SHARED和COMMIT-EXPLICIT-SHARED。 1234567891011121314151617181920212223select * from performance_schema.metadata_locks\\G*************************** 1. row *************************** OBJECT_TYPE: GLOBAL OBJECT_SCHEMA: NULL OBJECT_NAME: NULLOBJECT_INSTANCE_BEGIN: 46996001973424 LOCK_TYPE: SHARED LOCK_DURATION: EXPLICIT LOCK_STATUS: GRANTED SOURCE: lock.cc:1110 OWNER_THREAD_ID: 27 OWNER_EVENT_ID: 92*************************** 2. row *************************** OBJECT_TYPE: COMMIT OBJECT_SCHEMA: NULL OBJECT_NAME: NULLOBJECT_INSTANCE_BEGIN: 46996001973616 LOCK_TYPE: SHARED LOCK_DURATION: EXPLICIT LOCK_STATUS: GRANTED SOURCE: lock.cc:1194 OWNER_THREAD_ID: 27 OWNER_EVENT_ID: 375 相关源码实现剖析。当FLUSH语句是FLUSH TABLES WITH READ LOCK的时候，lex-&gt;type会添加REFRESH_TABLES和REFRESH_READ_LOCK标记，当没有指定表即进入reload_acl_and_cache函数，通过调用lock_global_read_lock和make_global_read_lock_block_commit加对应锁，通过对应的锁来阻止元数据修改和表数据更改。DDL语句执行时会请求GLOBAL的INTENTION_EXCLUSIVE锁，事务提交和外部XA需要记录binlog的语句执行会请求COMMIT的INTENTION_EXCLUSIVE锁。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149sql/sql_yacc.yyflush_options: table_or_tables &#123; Lex-&gt;type|= REFRESH_TABLES; /* Set type of metadata and table locks for FLUSH TABLES table_list [WITH READ LOCK]. */ YYPS-&gt;m_lock_type= TL_READ_NO_INSERT; YYPS-&gt;m_mdl_type= MDL_SHARED_HIGH_PRIO; &#125; opt_table_list &#123;&#125; opt_flush_lock &#123;&#125; | flush_options_list ;opt_flush_lock: /* empty */ &#123;&#125; | WITH READ_SYM LOCK_SYM &#123; TABLE_LIST *tables= Lex-&gt;query_tables; Lex-&gt;type|= REFRESH_READ_LOCK;sql/sql_parse.cc ... case SQLCOM_FLUSH: if (first_table &amp;&amp; lex-&gt;type &amp; REFRESH_READ_LOCK)//当指定表的时候，对指定表加锁。 &#123; if (flush_tables_with_read_lock(thd, all_tables)) &#125; ... if (!reload_acl_and_cache(thd, lex-&gt;type, first_table, &amp;write_to_binlog))sql/sql_reload.ccreload_acl_and_cache&#123; if (options &amp; (REFRESH_TABLES | REFRESH_READ_LOCK)) &#123; if ((options &amp; REFRESH_READ_LOCK) &amp;&amp; thd) &#123; ... if (thd-&gt;global_read_lock.lock_global_read_lock(thd))//当未指定表的时候，加全局锁 return 1; ... if (thd-&gt;global_read_lock.make_global_read_lock_block_commit(thd))//当未指定表的时候，加COMMIT锁&#125;//对GLOBAL加EXPLICIT的S锁。sql/lock.ccbool Global_read_lock::lock_global_read_lock(THD *thd)&#123; ... MDL_REQUEST_INIT(&amp;mdl_request, MDL_key::GLOBAL, &quot;&quot;, &quot;&quot;, MDL_SHARED, MDL_EXPLICIT); ...&#125;//对COMMIT加EXPLICIT的S锁。bool Global_read_lock::make_global_read_lock_block_commit(THD *thd)&#123; ... MDL_REQUEST_INIT(&amp;mdl_request, MDL_key::COMMIT, &quot;&quot;, &quot;&quot;, MDL_SHARED, MDL_EXPLICIT); ...&#125;sql/handler.cc事务提交和外部XA事务的commit\\rollback\\prepare均需要加COMMIT的IX锁.int ha_commit_trans(THD *thd, bool all, bool ignore_global_read_lock)&#123; ... if (rw_trans &amp;&amp; !ignore_global_read_lock) //对于内部表slave status table的更新可以忽略global read lock &#123; MDL_REQUEST_INIT(&amp;mdl_request, MDL_key::COMMIT, &quot;&quot;, &quot;&quot;, MDL_INTENTION_EXCLUSIVE, MDL_EXPLICIT); DBUG_PRINT(&quot;debug&quot;, (&quot;Acquire MDL commit lock&quot;)); if (thd-&gt;mdl_context.acquire_lock(&amp;mdl_request, thd-&gt;variables.lock_wait_timeout)) &#125; ...&#125;sql/xa.ccbool Sql_cmd_xa_commit::trans_xa_commit(THD *thd)&#123; ... MDL_request mdl_request; MDL_REQUEST_INIT(&amp;mdl_request, MDL_key::COMMIT, &quot;&quot;, &quot;&quot;, MDL_INTENTION_EXCLUSIVE, MDL_STATEMENT); if (thd-&gt;mdl_context.acquire_lock(&amp;mdl_request, thd-&gt;variables.lock_wait_timeout)) ...&#125;bool Sql_cmd_xa_rollback::trans_xa_rollback(THD *thd)&#123; ... MDL_request mdl_request; MDL_REQUEST_INIT(&amp;mdl_request, MDL_key::COMMIT, &quot;&quot;, &quot;&quot;, MDL_INTENTION_EXCLUSIVE, MDL_STATEMENT);&#125;bool Sql_cmd_xa_prepare::trans_xa_prepare(THD *thd)&#123; ... MDL_request mdl_request; MDL_REQUEST_INIT(&amp;mdl_request, MDL_key::COMMIT, &quot;&quot;, &quot;&quot;, MDL_INTENTION_EXCLUSIVE, MDL_STATEMENT);&#125;//写入语句的执行和DDL执行需要GLOBAL的IX锁，这与S锁不兼容。sql/sql_base.ccbool open_table(THD *thd, TABLE_LIST *table_list, Open_table_context *ot_ctx)&#123; if (table_list-&gt;mdl_request.is_write_lock_request() &amp;&amp; &#123; MDL_request protection_request; MDL_deadlock_handler mdl_deadlock_handler(ot_ctx); if (thd-&gt;global_read_lock.can_acquire_protection()) DBUG_RETURN(TRUE); MDL_REQUEST_INIT(&amp;protection_request, MDL_key::GLOBAL, &quot;&quot;, &quot;&quot;, MDL_INTENTION_EXCLUSIVE, MDL_STATEMENT); &#125;&#125;boollock_table_names(THD *thd, TABLE_LIST *tables_start, TABLE_LIST *tables_end, ulong lock_wait_timeout, uint flags)&#123; if (need_global_read_lock_protection) &#123; /* Protect this statement against concurrent global read lock by acquiring global intention exclusive lock with statement duration. */ if (thd-&gt;global_read_lock.can_acquire_protection()) return true; MDL_REQUEST_INIT(&amp;global_request, MDL_key::GLOBAL, &quot;&quot;, &quot;&quot;, MDL_INTENTION_EXCLUSIVE, MDL_STATEMENT); mdl_requests.push_front(&amp;global_request); &#125;&#125; LOCK TABLE t READ [LOCAL] LOCK TABLE t READ LOCAL会加锁TABLE-TRANSACTION-SHARED_READ。 123456789select * from performance_schema.metadata_locks\\G*************************** 1. row *************************** OBJECT_TYPE: TABLE OBJECT_SCHEMA: test OBJECT_NAME: t LOCK_TYPE: SHARED_READ LOCK_DURATION: TRANSACTION LOCK_STATUS: GRANTED SOURCE: sql_parse.cc:5996 LOCK TABLE t READ会加锁TABLE-TRANSACTION-SHARED_READ_ONLY。 123456789select * from performance_schema.metadata_locks\\G*************************** 1. row *************************** OBJECT_TYPE: TABLE OBJECT_SCHEMA: test OBJECT_NAME: t LOCK_TYPE: SHARED_READ_ONLY LOCK_DURATION: TRANSACTION LOCK_STATUS: GRANTED SOURCE: sql_parse.cc:5996 这两个的区别是对于MyISAM引擎，LOCAL方式的加锁与insert写入不冲突，而没有LOCAL的时候SHARED_READ_ONLY会阻塞写入。不过对于InnoDB引擎两种方式是一样的，带有LOCAL的语句执行后面会升级为SHARED_READ_ONLY。 源码分析 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081table_lock: table_ident opt_table_alias lock_option &#123; thr_lock_type lock_type= (thr_lock_type) $3; enum_mdl_type mdl_lock_type; if (lock_type &gt;= TL_WRITE_ALLOW_WRITE) &#123; /* LOCK TABLE ... WRITE/LOW_PRIORITY WRITE */ mdl_lock_type= MDL_SHARED_NO_READ_WRITE; &#125; else if (lock_type == TL_READ) &#123; /* LOCK TABLE ... READ LOCAL */ mdl_lock_type= MDL_SHARED_READ; &#125; else &#123; /* LOCK TABLE ... READ */ mdl_lock_type= MDL_SHARED_READ_ONLY; &#125; if (!Select-&gt;add_table_to_list(YYTHD, $1, $2, 0, lock_type, mdl_lock_type)) MYSQL_YYABORT; &#125;lock_option: READ_SYM &#123; $= TL_READ_NO_INSERT; &#125; | WRITE_SYM &#123; $= TL_WRITE_DEFAULT; &#125; | LOW_PRIORITY WRITE_SYM &#123; $= TL_WRITE_LOW_PRIORITY; push_deprecated_warn(YYTHD, &quot;LOW_PRIORITY WRITE&quot;, &quot;WRITE&quot;); &#125; | READ_SYM LOCAL_SYM &#123; $= TL_READ; &#125; ;sql/sql_parse.ccTABLE_LIST *st_select_lex::add_table_to_list(THD *thd, Table_ident *table, LEX_STRING *alias, ulong table_options, thr_lock_type lock_type, enum_mdl_type mdl_type, List&lt;Index_hint&gt; *index_hints_arg, List&lt;String&gt; *partition_names, LEX_STRING *option)&#123; // Pure table aliases do not need to be locked: if (!MY_TEST(table_options &amp; TL_OPTION_ALIAS)) &#123; MDL_REQUEST_INIT(&amp; ptr-&gt;mdl_request, MDL_key::TABLE, ptr-&gt;db, ptr-&gt;table_name, mdl_type, MDL_TRANSACTION); &#125;&#125;//对于Innodb引擎static bool lock_tables_open_and_lock_tables(THD *thd, TABLE_LIST *tables)&#123; ... else if (table-&gt;lock_type == TL_READ &amp;&amp; ! table-&gt;prelocking_placeholder &amp;&amp; table-&gt;table-&gt;file-&gt;ha_table_flags() &amp; HA_NO_READ_LOCAL_LOCK) &#123; /* In case when LOCK TABLE ... READ LOCAL was issued for table with storage engine which doesn&#x27;t support READ LOCAL option and doesn&#x27;t use THR_LOCK locks we need to upgrade weak SR metadata lock acquired in open_tables() to stronger SRO metadata lock. This is not needed for tables used through stored routines or triggers as we always acquire SRO (or even stronger SNRW) metadata lock for them. */ bool result= thd-&gt;mdl_context.upgrade_shared_lock( table-&gt;table-&gt;mdl_ticket, MDL_SHARED_READ_ONLY, thd-&gt;variables.lock_wait_timeout); ...&#125; LOCK TABLE t WITH WRITE LOCK TABLE t WITH WRITE会加锁：GLOBAL-STATEMENT-INTENTION_EXCLUSIVE，SCHEMA-TRANSACTION-INTENTION_EXCLUSIVE，TABLE-TRANSACTION-SHARED_NO_READ_WRITE。 12345678910111213141516171819202122select OBJECT_TYPE,OBJECT_SCHEMA,OBJECT_NAME,LOCK_TYPE,LOCK_DURATION,SOURCE from performance_schema.metadata_locks\\G*************************** 1. row *************************** OBJECT_TYPE: GLOBAL OBJECT_SCHEMA: NULL OBJECT_NAME: NULL LOCK_TYPE: INTENTION_EXCLUSIVE LOCK_DURATION: STATEMENT SOURCE: sql_base.cc:5497*************************** 2. row *************************** OBJECT_TYPE: SCHEMA OBJECT_SCHEMA: test OBJECT_NAME: NULL LOCK_TYPE: INTENTION_EXCLUSIVE LOCK_DURATION: TRANSACTION SOURCE: sql_base.cc:5482*************************** 3. row *************************** OBJECT_TYPE: TABLE OBJECT_SCHEMA: test OBJECT_NAME: ti LOCK_TYPE: SHARED_NO_READ_WRITE LOCK_DURATION: TRANSACTION SOURCE: sql_parse.cc:5996 相关源码 12345678910111213141516171819202122232425262728293031323334353637383940boollock_table_names(THD *thd, TABLE_LIST *tables_start, TABLE_LIST *tables_end, ulong lock_wait_timeout, uint flags)&#123; ... while ((table= it++)) &#123; MDL_request *schema_request= new (thd-&gt;mem_root) MDL_request; if (schema_request == NULL) return true; MDL_REQUEST_INIT(schema_request, MDL_key::SCHEMA, table-&gt;db, &quot;&quot;, MDL_INTENTION_EXCLUSIVE, MDL_TRANSACTION); mdl_requests.push_front(schema_request); &#125; if (need_global_read_lock_protection) &#123; /* Protect this statement against concurrent global read lock by acquiring global intention exclusive lock with statement duration. */ if (thd-&gt;global_read_lock.can_acquire_protection()) return true; MDL_REQUEST_INIT(&amp;global_request, MDL_key::GLOBAL, &quot;&quot;, &quot;&quot;, MDL_INTENTION_EXCLUSIVE, MDL_STATEMENT); mdl_requests.push_front(&amp;global_request); &#125; ... // Phase 3: Acquire the locks which have been requested so far. if (thd-&gt;mdl_context.acquire_locks(&amp;mdl_requests, lock_wait_timeout)) return true;&#125;在open_table中也会请求锁。SHARED_NO_READ_WRITE的加锁源码参考LOCK TABLE WITH READ的源码分析。 SELECT查询语句的执行 SELECT语句的执行加锁TABLE-TRANSACTION-SHARED_READ锁。 123456789select * from performance_schema.metadata_locks\\G*************************** 1. row *************************** OBJECT_TYPE: TABLE OBJECT_SCHEMA: test OBJECT_NAME: t1 LOCK_TYPE: SHARED_READ LOCK_DURATION: TRANSACTION LOCK_STATUS: GRANTED SOURCE: sql_parse.cc:5996 源码分析： 1234567891011121314151617181920212223class Yacc_state&#123; void reset() &#123; yacc_yyss= NULL; yacc_yyvs= NULL; yacc_yyls= NULL; m_lock_type= TL_READ_DEFAULT; m_mdl_type= MDL_SHARED_READ; m_ha_rkey_mode= HA_READ_KEY_EXACT; &#125;&#125;调用add_table_to_list初始化锁，调用open_table_get_mdl_lock获取锁。static boolopen_table_get_mdl_lock(THD *thd, Open_table_context *ot_ctx, TABLE_LIST *table_list, uint flags, MDL_ticket **mdl_ticket)&#123; bool result= thd-&gt;mdl_context.acquire_lock(mdl_request, ot_ctx-&gt;get_timeout());&#125; INSERT/UPDATE/DELETE语句 在open table阶段会获取GLOBAL-STATEMENT-INTENTION_EXCLUSIVE，TABLE-TRANSACTION-SHARED_WRITE。 在commit阶段获取COMMIT-MDL_EXPLICIT-INTENTION_EXCLUSIVE锁。 123456789101112131415161718192021select OBJECT_TYPE,OBJECT_SCHEMA,OBJECT_NAME,LOCK_TYPE,LOCK_DURATION,SOURCE from performance_schema.metadata_locks\\GOBJECT_TYPE: GLOBALOBJECT_SCHEMA: NULLOBJECT_NAME: NULL LOCK_TYPE: INTENTION_EXCLUSIVELOCK_DURATION: STATEMENT SOURCE: sql_base.cc:3190*************************** 2. row ***************************OBJECT_TYPE: TABLEOBJECT_SCHEMA: testOBJECT_NAME: ti LOCK_TYPE: SHARED_WRITELOCK_DURATION: TRANSACTION SOURCE: sql_parse.cc:5996*************************** 3. row ***************************OBJECT_TYPE: COMMITOBJECT_SCHEMA: NULLOBJECT_NAME: NULL LOCK_TYPE: INTENTION_EXCLUSIVELOCK_DURATION: EXPLICIT SOURCE: handler.cc:1758 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113sql/sql_yacc.yyinsert_stmt: INSERT /* #1 */ insert_lock_option /* #2 */ insert_lock_option: /* empty */ &#123; $= TL_WRITE_CONCURRENT_DEFAULT; &#125; | LOW_PRIORITY &#123; $= TL_WRITE_LOW_PRIORITY; &#125; | DELAYED_SYM &#123; $= TL_WRITE_CONCURRENT_DEFAULT; push_warning_printf(YYTHD, Sql_condition::SL_WARNING, ER_WARN_LEGACY_SYNTAX_CONVERTED, ER(ER_WARN_LEGACY_SYNTAX_CONVERTED), &quot;INSERT DELAYED&quot;, &quot;INSERT&quot;); &#125; | HIGH_PRIORITY &#123; $= TL_WRITE; &#125; ;//DELETE语句delete_stmt: DELETE_SYM opt_delete_options//UPDATEupdate_stmt: UPDATE_SYM /* #1 */ opt_low_priority /* #2 */ opt_ignore /* #3 */ join_table_list /* #4 */ SET /* #5 */ update_list /* #6 */opt_low_priority: /* empty */ &#123; $= TL_WRITE_DEFAULT; &#125; | LOW_PRIORITY &#123; $= TL_WRITE_LOW_PRIORITY; &#125; ;opt_delete_options: /* empty */ &#123; $= 0; &#125; | opt_delete_option opt_delete_options &#123; $= $1 | $2; &#125; ;opt_delete_option: QUICK &#123; $= DELETE_QUICK; &#125; | LOW_PRIORITY &#123; $= DELETE_LOW_PRIORITY; &#125; | IGNORE_SYM &#123; $= DELETE_IGNORE; &#125; ;sql/parse_tree_nodes.ccbool PT_delete::add_table(Parse_context *pc, Table_ident *table)&#123; ... const enum_mdl_type mdl_type= (opt_delete_options &amp; DELETE_LOW_PRIORITY) ? MDL_SHARED_WRITE_LOW_PRIO : MDL_SHARED_WRITE; ...&#125;bool PT_insert::contextualize(Parse_context *pc)&#123; if (!pc-&gt;select-&gt;add_table_to_list(pc-&gt;thd, table_ident, NULL, TL_OPTION_UPDATING, yyps-&gt;m_lock_type, yyps-&gt;m_mdl_type, NULL, opt_use_partition)) pc-&gt;select-&gt;set_lock_for_tables(lock_option);&#125;bool PT_update::contextualize(Parse_context *pc)&#123; pc-&gt;select-&gt;set_lock_for_tables(opt_low_priority);&#125;void st_select_lex::set_lock_for_tables(thr_lock_type lock_type)&#123; bool for_update= lock_type &gt;= TL_READ_NO_INSERT; enum_mdl_type mdl_type= mdl_type_for_dml(lock_type); ... tables-&gt;mdl_request.set_type(mdl_type); ...&#125;inline enum enum_mdl_type mdl_type_for_dml(enum thr_lock_type lock_type)&#123; return lock_type &gt;= TL_WRITE_ALLOW_WRITE ? (lock_type == TL_WRITE_LOW_PRIORITY ? MDL_SHARED_WRITE_LOW_PRIO : MDL_SHARED_WRITE) : MDL_SHARED_READ;&#125;最终调用open\\_table加锁bool open_table(THD *thd, TABLE_LIST *table_list, Open_table_context *ot_ctx)&#123; if (table_list-&gt;mdl_request.is_write_lock_request() &amp;&amp; ... &#123; MDL_REQUEST_INIT(&amp;protection_request, MDL_key::GLOBAL, &quot;&quot;, &quot;&quot;, MDL_INTENTION_EXCLUSIVE, MDL_STATEMENT); bool result= thd-&gt;mdl_context.acquire_lock(&amp;protection_request, ot_ctx-&gt;get_timeout()); &#125; ... if (open_table_get_mdl_lock(thd, ot_ctx, table_list, flags, &amp;mdl_ticket) || ...&#125;在commit阶段调用ha_commit_trans函数时加COMMIT的INTENTION_EXCLUSIVE锁，源码如FLUSH TABLES WITH READ LOCK所述。 如果INSERT/UPDATE/DELETE LOW_PRIORITY语句TABLE上加MDL_SHARED_WRITE_LOW_PRIO锁。 ALTER TABLE ALGORITHM=COPY[INPLACE] ALTER TABLE ALGORITHM=COPY COPY方式ALTER TABLE在open_table阶段加GLOBAL-STATEMENT-INTENTION_EXCLUSIVE锁，SCHEMA-TRANSACTION-INTENTION_EXCLUSIVE锁，TABLE-TRANSACTION-SHARED_UPGRADABLE锁。 在拷贝数据前将TABLE-TRANSACTION-SHARED_UPGRADABLE锁升级到SHARED_NO_WRITE。 拷贝完在交换表阶段将SHARED_NO_WRITE锁升级到EXCLUSIVE锁。 源码解析： 123456789101112131415161718192021222324252627282930313233343536373839404142GLOBAL、SCHEMA锁初始化位置和LOCK TABLE WRITE位置一致都是在lock_table_names函数中。在open_table中也会请求锁。sql/sql_yacc.yyalter: ALTER TABLE_SYM table_ident &#123; THD *thd= YYTHD; LEX *lex= thd-&gt;lex; lex-&gt;name.str= 0; lex-&gt;name.length= 0; lex-&gt;sql_command= SQLCOM_ALTER_TABLE; lex-&gt;duplicates= DUP_ERROR; if (!lex-&gt;select_lex-&gt;add_table_to_list(thd, $3, NULL, TL_OPTION_UPDATING, TL_READ_NO_INSERT, MDL_SHARED_UPGRADABLE))bool mysql_alter_table(THD *thd, const char *new_db, const char *new_name, HA_CREATE_INFO *create_info, TABLE_LIST *table_list, Alter_info *alter_info)&#123; //升级锁 if (thd-&gt;mdl_context.upgrade_shared_lock(mdl_ticket, MDL_SHARED_NO_WRITE, thd-&gt;variables.lock_wait_timeout) || lock_tables(thd, table_list, alter_ctx.tables_opened, 0)) ... if (wait_while_table_is_used(thd, table, HA_EXTRA_PREPARE_FOR_RENAME))&#125;bool wait_while_table_is_used(THD *thd, TABLE *table, enum ha_extra_function function)&#123; DBUG_ENTER(&quot;wait_while_table_is_used&quot;); DBUG_PRINT(&quot;enter&quot;, (&quot;table: &#x27;%s&#x27; share: 0x%lx db_stat: %u version: %lu&quot;, table-&gt;s-&gt;table_name.str, (ulong) table-&gt;s, table-&gt;db_stat, table-&gt;s-&gt;version)); if (thd-&gt;mdl_context.upgrade_shared_lock( table-&gt;mdl_ticket, MDL_EXCLUSIVE, thd-&gt;variables.lock_wait_timeout)) ALTER TABLE INPLACE的加锁： INPLACE方式在打开表的时候也是加GLOBAL-STATEMENT-INTENTION_EXCLUSIVE锁，SCHEMA-TRANSACTION-INTENTION_EXCLUSIVE锁，TABLE-TRANSACTION-SHARED_UPGRADABLE锁。 在prepare前将TABLE-TRANSACTION-SHARED_UPGRADABLE升级为TABLE-TRANSACTION-EXCLUSIVE锁。 在prepare后会再将EXCLUSIVE根据不同引擎支持情况降级为SHARED_NO_WRITE(不允许其他线程写入)或者SHARED_UPGRADABLE锁（其他线程可以读写，InnoDB引擎）。 在commit前，TABLE上的锁会再次升级到EXCLUSIVE锁。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950sql/sql_table.ccstatic bool mysql_inplace_alter_table(THD *thd, TABLE_LIST *table_list, TABLE *table, TABLE *altered_table, Alter_inplace_info *ha_alter_info, enum_alter_inplace_result inplace_supported, MDL_request *target_mdl_request, Alter_table_ctx *alter_ctx)&#123; ... else if (inplace_supported == HA_ALTER_INPLACE_SHARED_LOCK_AFTER_PREPARE || inplace_supported == HA_ALTER_INPLACE_NO_LOCK_AFTER_PREPARE) &#123; /* Storage engine has requested exclusive lock only for prepare phase and we are not under LOCK TABLES. Don&#x27;t mark TABLE_SHARE as old in this case, as this won&#x27;t allow opening of table by other threads during main phase of in-place ALTER TABLE. */ if (thd-&gt;mdl_context.upgrade_shared_lock(table-&gt;mdl_ticket, MDL_EXCLUSIVE, thd-&gt;variables.lock_wait_timeout)) ... if (table-&gt;file-&gt;ha_prepare_inplace_alter_table(altered_table, ha_alter_info)) ... if ((inplace_supported == HA_ALTER_INPLACE_SHARED_LOCK_AFTER_PREPARE || inplace_supported == HA_ALTER_INPLACE_NO_LOCK_AFTER_PREPARE) &amp;&amp; !(thd-&gt;locked_tables_mode == LTM_LOCK_TABLES || thd-&gt;locked_tables_mode == LTM_PRELOCKED_UNDER_LOCK_TABLES) &amp;&amp; (alter_info-&gt;requested_lock != Alter_info::ALTER_TABLE_LOCK_EXCLUSIVE)) &#123; /* If storage engine or user requested shared lock downgrade to SNW. */ if (inplace_supported == HA_ALTER_INPLACE_SHARED_LOCK_AFTER_PREPARE || alter_info-&gt;requested_lock == Alter_info::ALTER_TABLE_LOCK_SHARED) table-&gt;mdl_ticket-&gt;downgrade_lock(MDL_SHARED_NO_WRITE); else &#123; DBUG_ASSERT(inplace_supported == HA_ALTER_INPLACE_NO_LOCK_AFTER_PREPARE); table-&gt;mdl_ticket-&gt;downgrade_lock(MDL_SHARED_UPGRADABLE); &#125; &#125; ... // Upgrade to EXCLUSIVE before commit. if (wait_while_table_is_used(thd, table, HA_EXTRA_PREPARE_FOR_RENAME)) ... if (table-&gt;file-&gt;ha_commit_inplace_alter_table(altered_table, ha_alter_info, true))&#125; CREATE TABLE 加锁 CREATE TABLE先加锁GLOBAL-STATEMENT-INTENTION_EXCLUSIVE，SCHEMA-MDL_TRANSACTION-INTENTION_EXCLUSIVE，TABLE-TRANSACTION-SHARED。 表不存在则升级表上的SHARED锁到EXCLUSIVE。 1234567891011121314bool open_table(THD *thd, TABLE_LIST *table_list, Open_table_context *ot_ctx)&#123; ... if (!exists) &#123; ... bool wait_result= thd-&gt;mdl_context.upgrade_shared_lock( table_list-&gt;mdl_request.ticket, MDL_EXCLUSIVE, thd-&gt;variables.lock_wait_timeout); ... &#125; ...&#125; DROP TABLE 加锁 drop table语句执行加锁GLOBAL-STATEMENT-INTENTION_EXCLUSIVE，SCHEMA-MDL_TRANSACTION-INTENTION_EXCLUSIVE，TABLE-EXCLUSIVE。 12345678910drop: DROP opt_temporary table_or_tables if_exists &#123; LEX *lex=Lex; lex-&gt;sql_command = SQLCOM_DROP_TABLE; lex-&gt;drop_temporary= $2; lex-&gt;drop_if_exists= $4; YYPS-&gt;m_lock_type= TL_UNLOCK; YYPS-&gt;m_mdl_type= MDL_EXCLUSIVE; &#125; 本文地址：http://xnerv.wang/analysis-of-mdl-source-code-for-common-sql-statements/ 转载自：常用SQL语句的MDL加锁源码分析","categories":[{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/categories/DBMS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/tags/DBMS/"},{"name":"MySQL","slug":"MySQL","permalink":"https://xnerv.wang/tags/MySQL/"},{"name":"MDL","slug":"MDL","permalink":"https://xnerv.wang/tags/MDL/"}]},{"title":"MySQL - 利用gdb跟踪MDL加锁过程（转载）","slug":"trace-metadata-lock-procedure-using-gdb","date":"2018-03-22T02:38:00.000Z","updated":"2023-08-21T02:24:19.018Z","comments":true,"path":"trace-metadata-lock-procedure-using-gdb/","link":"","permalink":"https://xnerv.wang/trace-metadata-lock-procedure-using-gdb/","excerpt":"MDL(Meta Data LocK)的作用 在MySQL5.1及之前的版本中，如果有未提交的事务trx，当执行DROP/RENAME/ALTER TABLE RENAME操作时，不会被其他事务阻塞住。这会导致如下问题(MySQL bug#989) master： 未提交的事务，但SQL已经完成(binlog也准备好了)，表schema发生更改，在commit的时候不会被察觉到. slave： 在binlog里是以事务提交顺序记录的，DDL隐式提交，因此在备库先执行DDL，后执行事务trx，由于trx作用的表已经发生了改变，因此trx会执行失败。 在DDL时的主库DML压力越大，这个问题触发的可能性就越高 在5.5引入了MDL（meta data lock）锁来解决在这个问题","text":"MDL(Meta Data LocK)的作用 在MySQL5.1及之前的版本中，如果有未提交的事务trx，当执行DROP/RENAME/ALTER TABLE RENAME操作时，不会被其他事务阻塞住。这会导致如下问题(MySQL bug#989) master： 未提交的事务，但SQL已经完成(binlog也准备好了)，表schema发生更改，在commit的时候不会被察觉到. slave： 在binlog里是以事务提交顺序记录的，DDL隐式提交，因此在备库先执行DDL，后执行事务trx，由于trx作用的表已经发生了改变，因此trx会执行失败。 在DDL时的主库DML压力越大，这个问题触发的可能性就越高 在5.5引入了MDL（meta data lock）锁来解决在这个问题 MDL锁的类型 metadata lock也是一种锁。每个metadata lock都会定义锁住的对象，锁的持有时间和锁的类型 属性 范围 作用 GLOBAL 全局锁 主要作用是防止DDL和写操作的过程中执行 set golbal_read_only =on 或flush tables with read lock; commit 提交保护锁 主要作用是执行flush tables with read lock后，防止已经开始在执行的写事务提交 SCHEMA 库锁 对象 TABLE 表锁 对象 FUNCTION 函数锁 对象 PROCEDURE 存储过程锁 对象 TRIGGER 触发器锁 对象 EVENT 事件锁 对象 这些锁具有以下层级关系 MDL锁的简单示例 在实际工作中，最常见的MDL冲突就DDL的操作被没用提交的事务所阻塞。 我们下面通过一个具体的实例来演示DDL加MDL锁的过程。在这个实例中，利用gdb来跟踪DDL申请MDL锁的过程。 会话1: 12345678910111213141516171819mysql&gt; create table ti(id int primary key, c1 int, key(c1)) engine=InnoDBstats_auto_recalc=default;Query OK, 0 rows affected (0.03 sec)mysql&gt; insert into ti values (1,1), (2,2);Query OK, 2 rows affected (0.03 sec)Records: 2 Duplicates: 0 Warnings: 0mysql&gt; start transaction;Query OK, 0 rows affected (0.00 sec)mysql&gt; select * from ti;+----+------+| id | c1 |+----+------+| 1 | 1 || 2 | 2 |+----+------+2 rows in set (0.00 sec) 再开启第二个会话,利用gdb来跟踪mysql加MDL的过程 会话2： 12345678910111213141516[root@localhost mysql]# ps -ef|grep mysqlroot 3336 2390 0 06:33 pts&#x2F;2 00:00:01 &#x2F;u02&#x2F;mysql&#x2F;bin&#x2F;mysqld --basedir&#x3D;&#x2F;u02&#x2F;mysql&#x2F; --datadir&#x3D;&#x2F;u02&#x2F;mysql&#x2F;data--plugin-dir&#x3D;&#x2F;u02&#x2F;mysql&#x2F;&#x2F;lib&#x2F;plugin --user&#x3D;root--log-error&#x3D;&#x2F;u02&#x2F;mysql&#x2F;tmp&#x2F;error1.log --open-files-limit&#x3D;10240--pid-file&#x3D;&#x2F;u02&#x2F;mysql&#x2F;tmp&#x2F;mysql.pid--socket&#x3D;&#x2F;u02&#x2F;mysql&#x2F;tmp&#x2F;mysql.sock --port&#x3D;3306[root@localhost mysql]# gdb -p 3336----在GDB设置以下断点(gdb) b MDL_context::acquire_lockBreakpoint 1 at 0x730cab: file &#x2F;u02&#x2F;mysql-server-5.6&#x2F;sql&#x2F;mdl.cc, line 2187.(gdb) b lock_rec_lockBreakpoint 2 at 0xb5ef50: file &#x2F;u02&#x2F;mysql-server-5.6&#x2F;storage&#x2F;innobase&#x2F;lock&#x2F;lock0lock.cc, line 2296.(gdb) cContinuing..... 开启第三个会话 12mysql&gt; alter table ti stats_auto_recalc=1;这个操作被hang住 在会话2中执行下面的操作 12345678910111213141516171819202122232425(gdb) p mdl_request$1 &#x3D; (MDL_request *) 0x7f697d1c3bd0(gdb) p *mdl_request$2 &#x3D; &#123;type &#x3D; MDL_INTENTION_EXCLUSIVE, duration &#x3D; MDL_STATEMENT, next_in_list &#x3D; 0x7f697002a560, prev_in_list &#x3D; 0x7f697d1c3df8, ticket &#x3D; 0x0, key &#x3D; &#123;m_length &#x3D; 3, m_db_name_length &#x3D; 0, m_ptr &#x3D; &#39;\\000&#39; &lt;repeats 20 times&gt;, &quot;0|\\002p\\000\\000\\001\\000\\060&lt;\\034&#125;i\\177\\000\\000&gt;\\240\\344\\000\\000\\000\\000\\000\\000\\t\\000pi\\177\\000\\000\\000\\t\\000pi\\177\\000\\000&#96;&gt;\\034&#125;i\\177\\000\\000V\\312\\344\\000\\000\\000\\000\\000\\240&gt;\\034&#125;i\\177\\000\\000\\333\\361\\254\\000b\\001\\000\\000\\a?\\000\\001&quot;, &#39;\\000&#39; &lt;repeats 20 times&gt;, &quot;0|\\002p\\000\\000\\001\\000\\220&lt;\\034&#125;i\\177\\000\\000&gt;\\240\\344\\000\\000\\000\\000\\000\\340\\236\\002pi\\177\\000\\000\\333\\361\\254\\000\\000\\000\\000\\000\\a?\\000\\001&quot;, &#39;\\000&#39; &lt;repeats 12 times&gt;&quot;\\340, &gt;\\034&#125;i\\177\\000\\000\\060|\\002p\\000\\000\\001\\000\\350\\062\\220\\003\\000\\000\\000\\000\\333\\361\\254\\000\\000\\000\\000\\000$\\226\\363&quot;, &#39;\\000&#39; &lt;repeats 14 times&gt;,&quot;?\\034&#125;i\\177\\000\\000\\060|\\002p\\000\\000\\001\\000\\000&#x3D;\\034&#125;i\\177\\000\\000&gt;\\240\\344\\000\\000\\000\\000\\000\\000&quot;...,static m_namespace_to_wait_state_name &#x3D; &#123;&#123;m_key &#x3D; 101, m_name &#x3D; 0xf125a2 &quot;Waiting for global read lock&quot;, m_flags &#x3D; 0&#125;,&#123;m_key &#x3D; 102, m_name &#x3D; 0xf125c0 &quot;Waiting for schema metadata lock&quot;, m_flags &#x3D; 0&#125;,&#123;m_key &#x3D; 103, m_name &#x3D; 0xf125e8 &quot;Waiting for table metadata lock&quot;, m_flags &#x3D; 0&#125;,&#123;m_key &#x3D; 104, m_name &#x3D; 0xf12608 &quot;Waiting for stored function metadata lock&quot;, m_flags &#x3D; 0&#125;,&#123;m_key &#x3D; 105, m_name &#x3D; 0xf12638 &quot;Waiting for stored procedure metadata lock&quot;, m_flags &#x3D; 0&#125;,&#123;m_key &#x3D; 106, m_name &#x3D; 0xf12668 &quot;Waiting for trigger metadata lock&quot;, m_flags &#x3D; 0&#125;,&#123;m_key &#x3D; 107, m_name &#x3D; 0xf12690 &quot;Waiting for event metadata lock&quot;, m_flags &#x3D; 0&#125;,&#123;m_key &#x3D; 108, m_name &#x3D; 0xf126b0 &quot;Waiting for commit lock&quot;, m_flags &#x3D; 0&#125;&#125;&#125;&#125;(gdb) 从上面的输出中，我只能看到申请了一个语句级别的MDL_INTENTION_EXCLUSIVE。并没有看到什么其他有意义的信息。我们继续gdb跟踪 1234567891011121314151617181920(gdb) p *(mdl_request-&gt;next_in_list)$3 &#x3D; &#123;type &#x3D; MDL_INTENTION_EXCLUSIVE, duration &#x3D; MDL_TRANSACTION, next_in_list &#x3D; 0x7f697002a388, prev_in_list &#x3D; 0x7f697d1c3bd8, ticket &#x3D; 0x0, key &#x3D; &#123;m_length &#x3D; 7, m_db_name_length &#x3D; 4, m_ptr &#x3D; &quot;\\001test\\000\\000\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217\\217&quot;,static m_namespace_to_wait_state_name &#x3D; &#123;&#123;m_key &#x3D; 101, m_name &#x3D; 0xf125a2 &quot;Waiting for global read lock&quot;, m_flags &#x3D; 0&#125;,&#123;m_key &#x3D; 102, m_name &#x3D; 0xf125c0 &quot;Waiting for schema metadata lock&quot;, m_flags &#x3D; 0&#125;,&#123;m_key &#x3D; 103, m_name &#x3D; 0xf125e8 &quot;Waiting for table metadata lock&quot;, m_flags &#x3D; 0&#125;,&#123;m_key &#x3D; 104, m_name &#x3D; 0xf12608 &quot;Waiting for stored function metadata lock&quot;, m_flags &#x3D; 0&#125;,&#123;m_key &#x3D; 105, m_name &#x3D; 0xf12638 &quot;Waiting for stored procedure metadata lock&quot;, m_flags &#x3D; 0&#125;,&#123;m_key &#x3D; 106, m_name &#x3D; 0xf12668 &quot;Waiting for trigger metadata lock&quot;, m_flags &#x3D; 0&#125;,&#123;m_key &#x3D; 107, m_name &#x3D; 0xf12690 &quot;Waiting for event metadata lock&quot;, m_flags &#x3D; 0&#125;,&#123;m_key &#x3D; 108, m_name &#x3D; 0xf126b0 &quot;Waiting for commit lock&quot;, m_flags &#x3D; 0&#125;&#125;&#125;&#125; 从上面的输出中，我们看到了需要在test（见输出中的 m_ptr = “\\001test）数据库上加一把事务级的MDL_INTENTION_EXCLUSIVE锁。它并没有告诉我们最终的MDL会落在哪个对象上。我们继续跟踪 12345678910111213141516171819$4 &#x3D; &#123;type &#x3D; MDL_SHARED_UPGRADABLE, duration &#x3D; MDL_TRANSACTION, next_in_list &#x3D; 0x0, prev_in_list &#x3D; 0x7f697002a568, ticket &#x3D; 0x0, key &#x3D; &#123;m_length &#x3D; 9, m_db_name_length &#x3D; 4, m_ptr &#x3D; &quot;\\002test\\000ti&quot;, &#39;\\000&#39; &lt;repeats 378 times&gt;,static m_namespace_to_wait_state_name &#x3D; &#123;&#123;m_key &#x3D; 101, m_name &#x3D; 0xf125a2 &quot;Waiting for global read lock&quot;, m_flags &#x3D; 0&#125;,&#123;m_key &#x3D; 102, m_name &#x3D; 0xf125c0 &quot;Waiting for schema metadata lock&quot;, m_flags &#x3D; 0&#125;,&#123;m_key &#x3D; 103, m_name &#x3D; 0xf125e8 &quot;Waiting for table metadata lock&quot;, m_flags &#x3D; 0&#125;,&#123;m_key &#x3D; 104, m_name &#x3D; 0xf12608 &quot;Waiting for stored function metadata lock&quot;, m_flags &#x3D; 0&#125;,&#123;m_key &#x3D; 105, m_name &#x3D; 0xf12638 &quot;Waiting for stored procedure metadata lock&quot;, m_flags &#x3D; 0&#125;,&#123;m_key &#x3D; 106, m_name &#x3D; 0xf12668 &quot;Waiting for trigger metadata lock&quot;, m_flags &#x3D; 0&#125;,&#123;m_key &#x3D; 107, m_name &#x3D; 0xf12690 &quot;Waiting for event metadata lock&quot;, m_flags &#x3D; 0&#125;,&#123;m_key &#x3D; 108, m_name &#x3D; 0xf126b0 &quot;Waiting for commit lock&quot;, m_flags &#x3D; 0&#125;&#125;&#125;&#125; 从上面的输出中，我们可以看出最终是要在test数据库的ti对象上加一把MDL_SHARED_UPGRADABLE锁。在做DDL时会先加MDL_SHARED_UPGRADABLE锁，然后升级到MDL_EXCLUSIVE锁 我来执行下面的过程 会话1 12mysql&gt; commit;Query OK, 0 rows affected (5.51 sec) 会话2 1234567891011121314151617181920(gdb) p *mdl_request$5 &#x3D; &#123;type &#x3D; MDL_EXCLUSIVE, duration &#x3D; MDL_TRANSACTION, next_in_list &#x3D; 0x20302000000, prev_in_list &#x3D; 0x200000001, ticket &#x3D; 0x0, key &#x3D; &#123;m_length &#x3D; 9, m_db_name_length &#x3D; 4, m_ptr &#x3D; &quot;\\002test\\000ti\\000\\000\\000\\000@\\031\\220\\003\\000\\000\\000\\000\\333\\361\\254\\000\\000\\000\\000\\000\\260&lt;\\034&#125;i\\177\\000\\000\\302\\362\\254\\000\\000\\000\\000\\000\\300&lt;\\034&#125;i\\177\\000\\000\\060|\\002pi\\177\\000\\000\\320&lt;\\034&#125;i\\177\\000\\000\\360\\236\\344\\000\\000\\000\\000\\000\\000\\t\\000pi\\177\\000\\000(&#125;\\002pi\\177\\000\\000\\360&lt;\\034&#125;i\\177\\000\\000\\234\\312\\344\\000\\000\\000\\000\\000H\\245\\002pi\\177\\000\\000\\333\\361\\254\\000\\000\\000\\000\\000\\023\\360\\000\\001&quot;, &#39;\\000&#39; &lt;repeats 12 times&gt;, &quot;&#96;S\\005pi\\177\\000\\000\\060|\\002p\\000\\000\\001\\000\\060&#x3D;\\034&#125;i\\177\\000\\000&gt;\\240\\344\\000\\000\\000\\000\\000\\000\\t\\000pi\\177\\000\\000\\000\\t\\000pi\\177\\000\\000\\200&#x3D;\\034&#125;i\\177\\000\\000\\231\\310\\344\\000\\000\\000\\000\\000\\240&#x3D;\\034&#125;i\\177\\000\\000l-d0t\\b\\000\\000H\\344\\000\\001\\000\\000\\000\\000\\023\\360\\000\\001\\000\\000\\000\\000\\226&quot;...,static m_namespace_to_wait_state_name &#x3D; &#123;&#123;m_key &#x3D; 101, m_name &#x3D; 0xf125a2 &quot;Waiting for global read lock&quot;, m_flags &#x3D; 0&#125;,&#123;m_key &#x3D; 102, m_name &#x3D; 0xf125c0 &quot;Waiting for schema metadata lock&quot;, m_flags &#x3D; 0&#125;,&#123;m_key &#x3D; 103, m_name &#x3D; 0xf125e8 &quot;Waiting for table metadata lock&quot;, m_flags &#x3D; 0&#125;,&#123;m_key &#x3D; 104, m_name &#x3D; 0xf12608 &quot;Waiting for stored function metadata lock&quot;, m_flags &#x3D; 0&#125;,&#123;m_key &#x3D; 105, m_name &#x3D; 0xf12638 &quot;Waiting for stored procedure metadata lock&quot;, m_flags &#x3D; 0&#125;,&#123;m_key &#x3D; 106, m_name &#x3D; 0xf12668 &quot;Waiting for trigger metadata lock&quot;, m_flags &#x3D; 0&#125;,&#123;m_key &#x3D; 107, m_name &#x3D; 0xf12690 &quot;Waiting for event metadata lock&quot;, m_flags &#x3D; 0&#125;,&#123;m_key &#x3D; 108, m_name &#x3D; 0xf126b0 &quot;Waiting for commit lock&quot;, m_flags &#x3D; 0&#125;&#125;&#125;&#125; 从上面的输出中，我们看到了最终是在test.ti上申请了事务级别的MDL_EXCLUSIVE锁。 会话3 123mysql&gt; alter table ti stats_auto_recalc=1;Query OK, 0 rows affected (22 min 58.99 sec)Records: 0 Duplicates: 0 Warnings: 0 小结 本例只是简单的演示了，在同一个事务的不同时期加的不同的MDL的锁。MYSQL中DDL的操作不属于事务操作的范围。这就给mysql主备基于语句级别同步带来了困难。mysql主备在同步的过程中，为了保证主备结构一致性，而引入了MDL机制。为了尽可能的降低MDL带来的影响。请在业务低谷的时候，执行DDL操作。 本文地址：http://xnerv.wang/trace-metadata-lock-procedure-using-gdb/ 转载自：利用gdb跟踪MDL加锁过程","categories":[{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/categories/DBMS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/tags/DBMS/"},{"name":"MySQL","slug":"MySQL","permalink":"https://xnerv.wang/tags/MySQL/"},{"name":"MDL","slug":"MDL","permalink":"https://xnerv.wang/tags/MDL/"},{"name":"gdb","slug":"gdb","permalink":"https://xnerv.wang/tags/gdb/"}]},{"title":"MySQL - 跟踪Metadata lock（转载）","slug":"trace-metadata-lock","date":"2018-03-22T02:32:00.000Z","updated":"2023-08-21T02:24:19.126Z","comments":true,"path":"trace-metadata-lock/","link":"","permalink":"https://xnerv.wang/trace-metadata-lock/","excerpt":"背景 MySQL 从5.5.3版本，对Metadata lock进行了调整，主要是MDL锁持有的周期从语句变成了事务， 其原因主要是解决两个问题： 问题1: 破坏事务隔离级别 在repeatable read的隔离级别下，多次的select语句执行过程中，会因为其它session的DDL语句，而导致select语句执行的结果不相同，破坏了RR的隔离级别。 问题2: 破坏binlog的顺序 在对表的DML过程中，会因为其它session的DDL语句，导致binlog里的event顺序在备库执行的结果和主库不一致。 从MySQL 5.5.3开始，MDL锁的持有周期变成了事务，解决了上面提到的两个问题，但在autocommit=off的情况下，也大大增加了阻塞的可能性。DBA对于阻塞的case，处理起来又比较麻烦，原因就是MDL锁的阻塞情况没有暴露明确的信息。 从MySQL 5.7.6开始，可以通过performance schema来查询MDL锁的持有情况。 在开始介绍5.7的跟踪Metadata lock之前， 小编还想讨论一下前面提到的这两个问题，在Oracle数据库中是如何处理的。","text":"背景 MySQL 从5.5.3版本，对Metadata lock进行了调整，主要是MDL锁持有的周期从语句变成了事务， 其原因主要是解决两个问题： 问题1: 破坏事务隔离级别 在repeatable read的隔离级别下，多次的select语句执行过程中，会因为其它session的DDL语句，而导致select语句执行的结果不相同，破坏了RR的隔离级别。 问题2: 破坏binlog的顺序 在对表的DML过程中，会因为其它session的DDL语句，导致binlog里的event顺序在备库执行的结果和主库不一致。 从MySQL 5.5.3开始，MDL锁的持有周期变成了事务，解决了上面提到的两个问题，但在autocommit=off的情况下，也大大增加了阻塞的可能性。DBA对于阻塞的case，处理起来又比较麻烦，原因就是MDL锁的阻塞情况没有暴露明确的信息。 从MySQL 5.7.6开始，可以通过performance schema来查询MDL锁的持有情况。 在开始介绍5.7的跟踪Metadata lock之前， 小编还想讨论一下前面提到的这两个问题，在Oracle数据库中是如何处理的。 Oracle的处理方式 首先，Oracle只实现了两种隔离级别，即read committed和serializable，我们来看下serializable级别下，怎么来处理问题1: 先看如下的case: 123456789101112131415161718192021session 1: session 2:-- create table t1(id number);-- insert into t1 values(1);-- commit;SET TRANSACTION ISOLATION LEVEL SERIALIZABLE; --TEST/TEST@ORCL&gt;select * from t1; -- ID---------- 11 row selected.-- alter table t1 add col number;TEST/TEST@ORCL&gt;select * from t1; -- ID COL---------- ---------- 1-- alter table t1 add col1 number default 10;TEST/TEST@ORCL&gt;select * from t1; -- ID COL COL1---------- ---------- ---------- 1 可以看到，虽然session是serializable隔离级别，但并没有产生阻塞的情况，Oracle保证了session1的多次select查询的返回结果是一样的， 但t1表数据字典的变化是马上可见的，这个也是符合serializable的要求的，因为隔离级别只定义了数据的可见性，而没有定义数据字典的可见性。 那MySQL能否不要MDL锁，来达到这样的效果？ 答案是否定的，因为Oracle是堆表，alter的操作只更改了数据字典，数据记录没有发生变化，纵使加了default值，也是在原记录上进行的update，完全可以使用scn号来构建一致性读版本，这样就不会产生阻塞。 而MySQL是IOT表，alter的过程进行了表重建，无法完成read view的构建。 那我们再来看问题2，Oracle的处理方式: 对于redo日志，Oracle的处理方式和InnoDB的处理方式一致，也就是当使用redo的时候，日志的写入并不和事务的提交与否有必然的关系，也不用和提交的顺序保持一致。这一点就和binlog区别开来，也就是物理日志是可以避免使用逻辑日志(binlog)带来的问题。 MySQL如果要避免这两个问题，而不引入Metadata lock，可以有以下两个思路： DDL只更改数据字典，行记录的变更在原记录上进行，这样能够实现多版本，也就是我们常说的在线加字段； 使用物理redo日志，避免使用binlog。 这两种都会对现有的MySQL架构带来调整，仅供参考。 下面我们回来看下对5.7 MDL的tracing。 MySQL 5.7 首先，打开metadata locks的tracing功能。 123456mysql&gt; UPDATE performance_schema.setup_consumers SET ENABLED = &#x27;YES&#x27; WHERE NAME = &#x27;global_instrumentation&#x27;;Query OK, 0 rows affected (0.00 sec)Rows matched: 1 Changed: 0 Warnings: 0mysql&gt; UPDATE performance_schema.setup_instruments SET ENABLED = &#x27;YES&#x27; WHERE NAME = &#x27;wait/lock/metadata/sql/mdl&#x27;;Query OK, 0 rows affected (0.00 sec)Rows matched: 1 Changed: 0 Warnings: 0 打开两个session，一个select，一个truncate。因为MDL锁的情况，select会阻塞truncate的操作。 session 1: 操作如下： 12345678910111213141516mysql&gt; set session autocommit=0;Query OK, 0 rows affected (0.00 sec)mysql&gt; select @@autocommit, @@tx_isolation;+--------------+----------------+| @@autocommit | @@tx_isolation |+--------------+----------------+| 0 | READ-COMMITTED |+--------------+----------------+1 row in set (0.00 sec)mysql&gt; select * from t limit 1;+----+------+| id | val |+----+------+| 1 | 1 |+----+------+1 row in set (0.00 sec) session 2: 操作如下： 1mysql&gt; truncate table t; 结果看到的就是session2被阻塞， 接下来check一下performance schema的信息： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657mysql&gt; select * from performance_schema.metadata_locks\\G*************************** 1\\. row ***************************OBJECT_TYPE: TABLEOBJECT_SCHEMA: testOBJECT_NAME: tOBJECT_INSTANCE_BEGIN: 140450128308592LOCK_TYPE: SHARED_READLOCK_DURATION: TRANSACTIONLOCK_STATUS: GRANTEDSOURCE: sql_parse.cc:5585OWNER_THREAD_ID: 27OWNER_EVENT_ID: 17*************************** 2\\. row ***************************OBJECT_TYPE: GLOBALOBJECT_SCHEMA: NULLOBJECT_NAME: NULLOBJECT_INSTANCE_BEGIN: 140450195436144LOCK_TYPE: INTENTION_EXCLUSIVELOCK_DURATION: STATEMENTLOCK_STATUS: GRANTEDSOURCE: sql_base.cc:5224OWNER_THREAD_ID: 30OWNER_EVENT_ID: 8*************************** 3\\. row ***************************OBJECT_TYPE: SCHEMAOBJECT_SCHEMA: testOBJECT_NAME: NULLOBJECT_INSTANCE_BEGIN: 140450195434272LOCK_TYPE: INTENTION_EXCLUSIVELOCK_DURATION: TRANSACTIONLOCK_STATUS: GRANTEDSOURCE: sql_base.cc:5209OWNER_THREAD_ID: 30OWNER_EVENT_ID: 8*************************** 4\\. row ***************************OBJECT_TYPE: TABLEOBJECT_SCHEMA: testOBJECT_NAME: tOBJECT_INSTANCE_BEGIN: 140450195434368LOCK_TYPE: EXCLUSIVELOCK_DURATION: TRANSACTIONLOCK_STATUS: PENDINGSOURCE: sql_parse.cc:5585OWNER_THREAD_ID: 30OWNER_EVENT_ID: 8*************************** 5\\. row ***************************OBJECT_TYPE: TABLEOBJECT_SCHEMA: performance_schemaOBJECT_NAME: metadata_locksOBJECT_INSTANCE_BEGIN: 140450128262384LOCK_TYPE: SHARED_READLOCK_DURATION: TRANSACTIONLOCK_STATUS: GRANTEDSOURCE: sql_parse.cc:5585OWNER_THREAD_ID: 27OWNER_EVENT_ID: 185 rows in set (0.00 sec) 如上所示，在t表上，持有一个SHARE_READ lock，而且还有一个EXCULSIVE lock请求是pending状态，也就是我们被阻塞的session 2。 在5.7之前，我们可以通过show processlist，来查看MDL阻塞的情况，但无法获取session 1的信息: 1234567891011121314151617mysql&gt; SELECT OBJECT_TYPE, OBJECT_SCHEMA, OBJECT_NAME, LOCK_TYPE, LOCK_STATUS, THREAD_ID, PROCESSLIST_ID, PROCESSLIST_INFO FROM performance_schema.metadata_locks INNER JOIN performance_schema.threads ON THREAD_ID = OWNER_THREAD_ID WHERE PROCESSLIST_ID &lt;&gt; CONNECTION_ID();+-------------+---------------+-------------+---------------------+-------------+-----------+----------------+------------------+| OBJECT_TYPE | OBJECT_SCHEMA | OBJECT_NAME | LOCK_TYPE | LOCK_STATUS | THREAD_ID | PROCESSLIST_ID | PROCESSLIST_INFO |+-------------+---------------+-------------+---------------------+-------------+-----------+----------------+------------------+| GLOBAL | NULL | NULL | INTENTION_EXCLUSIVE | GRANTED | 30 | 8 | truncate table t || SCHEMA | test | NULL | INTENTION_EXCLUSIVE | GRANTED | 30 | 8 | truncate table t || TABLE | test | t | EXCLUSIVE | PENDING | 30 | 8 | truncate table t |+-------------+---------------+-------------+---------------------+-------------+-----------+----------------+------------------+3 rows in set (0.00 sec)mysql&gt; show processlist;+----+------+-----------+------+---------+------+---------------------------------+------------------+| Id | User | Host | db | Command | Time | State | Info |+----+------+-----------+------+---------+------+---------------------------------+------------------+| 5 | root | localhost | test | Query | 0 | starting | show processlist || 8 | root | localhost | test | Query | 50 | Waiting for table metadata lock | truncate table t |+----+------+-----------+------+---------+------+---------------------------------+------------------+2 rows in set (0.00 sec) 接下来当事务提交了后，释放MDL锁再查询，就看不到MDL锁的信息了。 123456mysql&gt; commit;Query OK, 0 rows affected (0.00 sec)mysql&gt; SELECT OBJECT_TYPE, OBJECT_SCHEMA, OBJECT_NAME, LOCK_TYPE, LOCK_STATUS, THREAD_ID, PROCESSLIST_ID, PROCESSLIST_INFO FROM performance_schema.metadata_locks INNER JOIN performance_schema.threads ON THREAD_ID = OWNER_THREAD_ID WHERE PROCESSLIST_ID &lt;&gt; CONNECTION_ID();Empty set (0.01 sec)mysql&gt; select * from t;Empty set (0.00 sec) MySQL 5.7可以通过performance schema来检索MDL锁阻塞情况，方便DBA来诊断问题。 本文地址：http://xnerv.wang/trace-metadata-lock/ 转载自：跟踪Metadata lock","categories":[{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/categories/DBMS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/tags/DBMS/"},{"name":"MySQL","slug":"MySQL","permalink":"https://xnerv.wang/tags/MySQL/"},{"name":"MDL","slug":"MDL","permalink":"https://xnerv.wang/tags/MDL/"}]},{"title":"MySQL - InnoDB mini transation（转载）","slug":"what-innodb-mini-transation","date":"2018-03-17T23:42:00.000Z","updated":"2023-08-21T02:24:18.950Z","comments":true,"path":"what-innodb-mini-transation/","link":"","permalink":"https://xnerv.wang/what-innodb-mini-transation/","excerpt":"前言 InnoDB有两个非常重要的日志，undo log 和 redo log；通过undo log可以看到数据较早版本，实现MVCC，或回滚事务等功能；redo log用来保证事务持久性 本文以一条insert语句为线索介绍 mini transaction mini transaction 简介 mini transation 主要用于innodb redo log 和 undo log写入，保证两种日志的ACID特性 mini-transaction遵循以下三个协议: The FIX Rules Write-Ahead Log Force-log-at-commit","text":"前言 InnoDB有两个非常重要的日志，undo log 和 redo log；通过undo log可以看到数据较早版本，实现MVCC，或回滚事务等功能；redo log用来保证事务持久性 本文以一条insert语句为线索介绍 mini transaction mini transaction 简介 mini transation 主要用于innodb redo log 和 undo log写入，保证两种日志的ACID特性 mini-transaction遵循以下三个协议: The FIX Rules Write-Ahead Log Force-log-at-commit The FIX Rules 修改一个页需要获得该页的x-latch 访问一个页是需要获得该页的s-latch或者x-latch 持有该页的latch直到修改或者访问该页的操作完成 Write-Ahead Log 持久化一个数据页之前，必须先将内存中相应的日志页持久化 每个页有一个LSN,每次页修改需要维护这个LSN,当一个页需要写入到持久化设备时，要求内存中小于该页LSN的日志先写入到持久化设备中 Force-log-at-commit 一个事务可以同时修改了多个页，Write-AheadLog单个数据页的一致性，无法保证事务的持久性 Force -log-at-commit要求当一个事务提交时，其产生所有的mini-transaction日志必须刷到持久设备中 这样即使在页数据刷盘的时候宕机，也可以通过日志进行redo恢复 代码简介 本文使用 MySQL 5.6.16 版本进行分析 mini transation 相关代码路径位于 storage/innobase/mtr/ 主要有 mtr0mtr.cc 和 mtr0log.cc 两个文件 另有部分代码在 storage/innobase/include/ 文件名以 mtr0 开头 mini transaction 的信息保存在结构体 mtr_t 中，结构体成员描述如下 成员属性 描述 state mini transaction所处状态 MTR_ACTIVE, MTR_COMMITTING, MTR_COMMITTED memo mtr 持有锁的栈 log mtr产生的日志 inside_ibuf insert buffer 是否修改 modifications 是否修改buffer pool pages made_dirty 是否产生buffer pool脏页 n_log_recs log 记录数 n_freed_pages 释放page数 log_mode 日志模式，默认MTR_LOG_ALL start_lsn lsn 起始值 end_lsn lsn 结束值 magic_n 魔术字 一个 mini transaction 从 mtr_start(mtr)开始，到 mtr_commit(mtr)结束 一条insert语句涉及的 mini transaction 下面涉及 mtr 的嵌套，在代码中，每个 mtr_t 对象变量名都叫 mtr，本文中为了区分不同 mtr，给不同的对象加编号 下面一般省略 mtr_t 以外的参数 第一个 mtr 从 row_ins_clust_index_entry_low 开始 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687mtr_start(mtr_1) &#x2F;&#x2F; mtr_1 贯穿整条insert语句row_ins_clust_index_entry_lowmtr_s_lock(dict_index_get_lock(index), mtr_1) &#x2F;&#x2F; 对index加s锁btr_cur_search_to_nth_levelrow_ins_clust_index_entry_lowmtr_memo_push(mtr_1) &#x2F;&#x2F; buffer RW_NO_LATCH 入栈buf_page_get_genbtr_cur_search_to_nth_levelrow_ins_clust_index_entry_lowmtr_memo_push(mtr_1) &#x2F;&#x2F; page RW_X_LATCH 入栈buf_page_get_genbtr_block_get_funcbtr_cur_latch_leavesbtr_cur_search_to_nth_levelrow_ins_clust_index_entry_low mtr_start(mtr_2) &#x2F;&#x2F; mtr_2 用于记录 undo log trx_undo_report_row_operation btr_cur_ins_lock_and_undo btr_cur_optimistic_insert row_ins_clust_index_entry_low mtr_start(mtr_3) &#x2F;&#x2F; mtr_3 分配或复用一个 undo log trx_undo_assign_undo trx_undo_report_row_operation btr_cur_ins_lock_and_undo btr_cur_optimistic_insert row_ins_clust_index_entry_low mtr_memo_push(mtr_3) &#x2F;&#x2F; 对复用（也可能是分配）的 undo log page 加 RW_X_LATCH 入栈 buf_page_get_gen trx_undo_page_get trx_undo_reuse_cached &#x2F;&#x2F; 这里先尝试复用，如果复用失败，则分配新的 undo log trx_undo_assign_undo trx_undo_report_row_operation trx_undo_insert_header_reuse(mtr_3) &#x2F;&#x2F; 写 undo log header trx_undo_reuse_cached trx_undo_assign_undo trx_undo_report_row_operation trx_undo_header_add_space_for_xid(mtr_3) &#x2F;&#x2F; 在 undo header 中预留 XID 空间 trx_undo_reuse_cached trx_undo_assign_undo trx_undo_report_row_operation mtr_commit(mtr_3) &#x2F;&#x2F; 提交 mtr_3 trx_undo_assign_undo trx_undo_report_row_operation btr_cur_ins_lock_and_undo btr_cur_optimistic_insert row_ins_clust_index_entry_low mtr_memo_push(mtr_2) &#x2F;&#x2F; 即将写入的 undo log page 加 RW_X_LATCH 入栈 buf_page_get_gen trx_undo_report_row_operation btr_cur_ins_lock_and_undo btr_cur_optimistic_insert row_ins_clust_index_entry_low trx_undo_page_report_insert(mtr_2) &#x2F;&#x2F; undo log 记录 insert 操作 trx_undo_report_row_operation btr_cur_ins_lock_and_undo btr_cur_optimistic_insert row_ins_clust_index_entry_low mtr_commit(mtr_2) &#x2F;&#x2F; 提交 mtr_2 trx_undo_report_row_operation btr_cur_ins_lock_and_undo btr_cur_optimistic_insert row_ins_clust_index_entry_low&#x2F;* mtr_2 提交后开始执行 insert 操作 page_cur_insert_rec_low 具体执行 insert 操作 在该函数末尾调用 page_cur_insert_rec_write_log 写 redo log*&#x2F;page_cur_insert_rec_write_log(mtr_1) &#x2F;&#x2F; insert 操作写 redo logpage_cur_insert_rec_lowpage_cur_tuple_insertbtr_cur_optimistic_insertmtr_commit(mtr_1) &#x2F;&#x2F; 提交 mtr_1row_ins_clust_index_entry_low 至此 insert 语句执行结束后 一条 insert 是一个单语句事务，事务提交时也会涉及 mini transaction 提交事务时，第一个 mtr 从 trx_prepare 开始 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273mtr_start(mtr_4) &#x2F;&#x2F; mtr_4 用于 prepare transactiontrx_preparetrx_prepare_for_mysqlinnobase_xa_prepareha_prepare_lowMYSQL_BIN_LOG::prepareha_commit_transtrans_commit_stmtmysql_execute_commandmtr_memo_push(mtr_4) &#x2F;&#x2F; undo page 加 RW_X_LATCH 入栈buf_page_get_gentrx_undo_page_gettrx_undo_set_state_at_preparetrx_preparemlog_write_ulint(seg_hdr + TRX_UNDO_STATE, undo-&gt;state, MLOG_2BYTES, mtr_4) 写入TRX_UNDO_STATEtrx_undo_set_state_at_preparetrx_preparemlog_write_ulint(undo_header + TRX_UNDO_XID_EXISTS, TRUE, MLOG_1BYTE, mtr_4) 写入 TRX_UNDO_XID_EXISTStrx_undo_set_state_at_preparetrx_preparetrx_undo_write_xid(undo_header, &amp;undo-&gt;xid, mtr_4) undo 写入 xidtrx_undo_set_state_at_preparetrx_preparemtr_commit(mtr_4) &#x2F;&#x2F; 提交 mtr_4trx_preparemtr_start(mtr_5) &#x2F;&#x2F; mtr_5 用于 commit transactiontrx_committrx_commit_for_mysqlinnobase_commit_lowinnobase_commitha_commit_lowMYSQL_BIN_LOG::process_commit_stage_queueMYSQL_BIN_LOG::ordered_commitMYSQL_BIN_LOG::commitha_commit_transtrans_commit_stmtmysql_execute_commandmtr_memo_push(mtr_5) &#x2F;&#x2F; undo page 加 RW_X_LATCH 入栈buf_page_get_gentrx_undo_page_gettrx_undo_set_state_at_finishtrx_write_serialisation_historytrx_commit_lowtrx_committrx_undo_set_state_at_finish(mtr_5) &#x2F;&#x2F; set undo state， 这里是 TRX_UNDO_CACHEDtrx_write_serialisation_historytrx_commit_lowtrx_commitmtr_memo_push(mtr_5) &#x2F;&#x2F; 系统表空间 transaction system header page 加 RW_X_LATCH 入栈buf_page_get_gentrx_sysf_gettrx_sys_update_mysql_binlog_offsettrx_write_serialisation_historytrx_commit_lowtrx_committrx_sys_update_mysql_binlog_offset &#x2F;&#x2F; 更新偏移量信息到系统表空间trx_write_serialisation_historytrx_commit_lowtrx_commitmtr_commit(mtr_5) &#x2F;&#x2F; 提交 mtr_5trx_commit_lowtrx_commit 至此 insert 语句涉及的 mini transaction 全部结束 总结 上面可以看到加锁、写日志到 mlog 等操作在 mini transaction 过程中进行 解锁、把日志刷盘等操作全部在 mtr_commit 中进行，和事务类似 mini transaction 没有回滚操作， 因为只有在 mtr_commit 才将修改落盘，如果宕机，内存丢失，无需回滚；如果落盘过程中宕机，崩溃恢复时可以看出落盘过程不完整，丢弃这部分修改 mtr_commit 主要包含以下步骤 mlog 中日志刷盘 释放 mtr 持有的锁，锁信息保存在 memo 中，以栈形式保存，后加的锁先释放 清理 mtr 申请的内存空间，memo 和 log mtr—&gt;state 设置为 MTR_COMMITTED 上面的步骤 1. 中，日志刷盘策略和 innodb_flush_log_at_trx_commit 有关 当设置该值为1时，每次事务提交都要做一次fsync，这是最安全的配置，即使宕机也不会丢失事务 当设置为2时，则在事务提交时只做write操作，只保证写到系统的page cache，因此实例crash不会丢失事务，但宕机则可能丢失事务 当设置为0时，事务提交不会触发redo写操作，而是留给后台线程每秒一次的刷盘操作，因此实例crash将最多丢失1秒钟内的事务 本文地址：http://xnerv.wang/what-innodb-mini-transation/ 转载自：InnoDB mini transation","categories":[{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/categories/DBMS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/tags/DBMS/"},{"name":"MySQL","slug":"MySQL","permalink":"https://xnerv.wang/tags/MySQL/"},{"name":"InnoDB","slug":"InnoDB","permalink":"https://xnerv.wang/tags/InnoDB/"},{"name":"MTR","slug":"MTR","permalink":"https://xnerv.wang/tags/MTR/"}]},{"title":"MySQL - 信号处理机制分析（转载）","slug":"what-analysis-of-signal-processing-mechanism","date":"2018-03-17T23:32:00.000Z","updated":"2023-08-21T02:24:18.998Z","comments":true,"path":"what-analysis-of-signal-processing-mechanism/","link":"","permalink":"https://xnerv.wang/what-analysis-of-signal-processing-mechanism/","excerpt":"背景 在 AliSQL 上面有人提交了一个 bug，在使用主备的时候 service stop mysql 不能关闭主库，一直显示 shutting down mysql …，到底怎么回事呢，先来看一下 service stop mysql 是怎么停止数据库的。配置 MySQL 在系统启动时启动需要把 MYSQL_BASEDIR/support-files 目录下的脚本 mysql.sever 放到 /etc/init.d/ 目录下，脚本来控制 mysqld 的启动和停止。看一下脚本中的代码 ： 1234567891011if test -s &quot;$mysqld_pid_file_path&quot; then mysqld_pid&#x3D;&#96;cat &quot;$mysqld_pid_file_path&quot;&#96; if (kill -0 $mysqld_pid 2&gt;&#x2F;dev&#x2F;null) then echo $echo_n &quot;Shutting down MySQL&quot; kill $mysqld_pid # mysqld should remove the pid file when it exits, so wait for it. wait_for_pid removed &quot;$mysqld_pid&quot; &quot;$mysqld_pid_file_path&quot;; return_value&#x3D;$? ... 实际上的关闭动作就是向 mysqld 进程发送一个 kill pid 的信号，也就是 TERM ， wait_for_pid 函数中就是不断检测 $MYSQL_DATADIR 下面的 pid 文件是否存在，并且打印 ‘.’，所以上述问题应该是 mysqld 没有正确处理接收到的信号。","text":"背景 在 AliSQL 上面有人提交了一个 bug，在使用主备的时候 service stop mysql 不能关闭主库，一直显示 shutting down mysql …，到底怎么回事呢，先来看一下 service stop mysql 是怎么停止数据库的。配置 MySQL 在系统启动时启动需要把 MYSQL_BASEDIR/support-files 目录下的脚本 mysql.sever 放到 /etc/init.d/ 目录下，脚本来控制 mysqld 的启动和停止。看一下脚本中的代码 ： 1234567891011if test -s &quot;$mysqld_pid_file_path&quot; then mysqld_pid&#x3D;&#96;cat &quot;$mysqld_pid_file_path&quot;&#96; if (kill -0 $mysqld_pid 2&gt;&#x2F;dev&#x2F;null) then echo $echo_n &quot;Shutting down MySQL&quot; kill $mysqld_pid # mysqld should remove the pid file when it exits, so wait for it. wait_for_pid removed &quot;$mysqld_pid&quot; &quot;$mysqld_pid_file_path&quot;; return_value&#x3D;$? ... 实际上的关闭动作就是向 mysqld 进程发送一个 kill pid 的信号，也就是 TERM ， wait_for_pid 函数中就是不断检测 $MYSQL_DATADIR 下面的 pid 文件是否存在，并且打印 ‘.’，所以上述问题应该是 mysqld 没有正确处理接收到的信号。 信号处理机制 多线程信号处理 进程中的信号处理是异步的，当信号发送给进程之后，就会中断进程当前的执行流程，跳到注册的对应信号处理函数中，执行完毕后再返回进程的执行流程。在多线程信号处理中，一般采用一个单独的线程阻塞的等待信号集，然后处理信号，重新阻塞等待。线程的信号处理有以下几个特点： 每个线程都有自己的信号屏蔽字（单个线程可以屏蔽某些信号） 信号的处理是整个进程中所有线程共享的（某个线程修改信号处理行为后，也会影响其它线程） 进程中的信号是递送到单个线程的，如果一个信号和硬件故障相关，那么该信号就会被递送到引起该事件的线程，否是是发送到任意一个线程。 1int pthread_sigmask(int how, const sigset_t * restrict set, sigset_t *restrict oset); 在进程中使用 sigprocmask 设置信号屏蔽字，在线程中使用 pthread_sigmask，他们的基本相同，pthread_sigmask 工作在线程中，失败时返回错误码，而 sigprocmask 会设置 errno 并返回 -1。参数 how 控制设置屏蔽字的行为，值为 SIG_BLOCK（把信号集添加到现有信号集中，取并集）, SIG_SET_MASK（设置信号集为 set）, SIG_UNBLOCK（从信号集中移除 set 中的信号）。set 表示需要操纵的信号集合。oset 返回设置之前的信号屏蔽字，如果设置 set 为 NULL，可以通过 oset 获得当前的信号屏蔽字。 1int sigwait(const sigset_t \\*restrict set, int \\*restrict sig) sigwait 将会挂起调用线程，直到接收到 set 中设置的信号，具体的信号将会通过 sig 返回，同时会从 set 中删除 sig 信号。 在调用 sigwait 之前，必须阻塞那些它正在等待的信号，否则在调用的时间窗口就可能接收到信号。 1int pthread_kill(pthread_t thread, int sig) 发送信号到指定线程，如果 sig 为 0，可以用来判断线程是否还活着。 man pthread_sigmask 里面给了一个例子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#include &lt;pthread.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;signal.h&gt;#include &lt;errno.h&gt;/* Simple error handling functions */#define handle_error_en(en, msg) \\ do &#123; errno = en; perror(msg); exit(EXIT_FAILURE); &#125; while (0) static void *sig_thread(void *arg)&#123; sigset_t *set = (sigset_t *) arg; int s, sig; for (;;) &#123; s = sigwait(set, &amp;sig); if (s != 0) handle_error_en(s, &quot;sigwait&quot;); printf(&quot;Signal handling thread got signal %d\\n&quot;, sig); &#125;&#125;int main(int argc, char *argv[])&#123; pthread_t thread; sigset_t set; int s; /* Block SIGINT; other threads created by main() will inherit * a copy of the signal mask. */ /* Block SIGINT; other threads created by main() will inherit * a copy of the signal mask. */ sigemptyset(&amp;set); sigaddset(&amp;set, SIGQUIT); sigaddset(&amp;set, SIGUSR1); s = pthread_sigmask(SIG_BLOCK, &amp;set, NULL); //s = sigprocmask(SIG_BLOCK, &amp;set, NULL); if (s != 0) handle_error_en(s, &quot;pthread_sigmask&quot;); s = pthread_create(&amp;thread, NULL, &amp;sig_thread, (void *) &amp;set); if (s != 0) handle_error_en(s, &quot;pthread_create&quot;); /* Main thread carries on to create other threads and/or do * other work */ pause(); /* Dummy pause so we can test program */ return 0;&#125; 执行一下： 12345678$ ./a.out &amp;[1] 5423$ kill -QUIT %1Signal handling thread got signal 3$ kill -USR1 %1Signal handling thread got signal 10$ kill -TERM %1[1]+ Terminated ./a.out 测试了一下，把上面代码的 pthread_sigmask 替换成 sigprocmask ，同样能够正确执行，说明线程也能够继承原进程的屏蔽字，不过还是尽量使用 pthread_sigmask, 表述清楚点，而且说不定还有其它坑。 MySQL 信号处理 MySQL 是典型的多线程处理，它的信号处理形式和上一小节介绍的差不多，在 mysqld 启动的时候调用 my_init_signal 初始化信号屏蔽字，把需要信号处理线程处理的信号屏蔽起来，然后启动信号处理函数，入口是 signal_hand 。 在 my_init_signal 函数中，设置 SIGSEGC, SIGABORT, SIGBUS, SIGILL, SIGFPE 的处理函数为 handle_fatal_signal，把 SIGPIPE，SIGQUIT, SIGHUP, SIGTERM, SIGTSTP 加入到信号屏蔽字里，调用 sigprocmask 和 pthread_sigmask 设置屏蔽字。这一系列动作是在 mysql 启动其它辅助线程之前完成的动作，意图很明显，就是让之后的线程都继承设置的信号屏蔽字，把所有的信号交给信号处理线程去处理。 signal_hand 函数首先把需要处理的信号放到信号集合里去，然后完成 create_pid_file ，data 目录下的 pid 文件实际上是由信号处理线程创建的。接着等待 mysqld 完成启动，各个线程之间需要同步，核心代码是一个死循环，通过 my_sigwait 调用 sigwait 阻塞的等待信号的到来。我们目前主要关心 SIGTERM 的处理，和 SIGQUIT, SIGKILL 处理方式相同，都是调用 kill_server 关闭整个数据库。 Bug Fix 文中开头的链接中提到 loose-rpl_semi_sync_master_enabled = 0 关闭就不会有问题， 如果为 1 就会出现无法关闭的情况，顺着这个线索寻找，rpl_semi_sync_master_enabled 在主备使用 semisync 情况下控制启动 Master 节点的 Ack Receiver 线程，初始化阶段的调用堆栈为: 12345init_common_variables | |----- ReplSemiSyncMaster::initObject | |----- Ack_receiver::start 而 init_common_variables 的调用是在 my_init_signal 之前，也就是 Ack Receiver 线程没有办法继承信号屏蔽字，不会屏蔽 SIGTERM 信号。在 my_init_signal 中还有一段这样的代码： 12345/* Fix signals if blocked by parents (can happen on Mac OS X) */ .... sa.sa_handler = print_signal_warning; sigaction(SIGTERM, &amp;sa, (struct sigaction\\*) 0); ... 对于信号的修改的作用于整个进程的，也就是说之前启动的 Ack Receiver 线程没有信号屏蔽字，而且注册了信号处理函数。当 SIGTERM 发生后，信号处理线程和 Ack Receiver 线程都可以接收信号处理，信号被随机的分发（测试高概率都是发给 Ack Receiver），print_signal_warning 仅仅打印信息到 errlog，就出现了无法关闭 mysqld 的情况了。 修改也比较简单，把 initObject 的操作放到 my_init_signal 之后就好，注意不能把 init_common_variables 整个移到 my_init_signal 之前，因为 my_init_signal 里面还有要初始化的变量呢。 本文地址：http://xnerv.wang/what-analysis-of-signal-processing-mechanism/ 转载自：信号处理机制分析","categories":[{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/categories/DBMS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/tags/DBMS/"},{"name":"MySQL","slug":"MySQL","permalink":"https://xnerv.wang/tags/MySQL/"},{"name":"Signal","slug":"Signal","permalink":"https://xnerv.wang/tags/Signal/"}]},{"title":"MySQL - InnoDB Adaptive hash index介绍（转载）","slug":"mysql-introduction-innodb-adaptive-hash-index","date":"2018-03-17T23:26:00.000Z","updated":"2023-08-21T02:24:18.919Z","comments":true,"path":"mysql-introduction-innodb-adaptive-hash-index/","link":"","permalink":"https://xnerv.wang/mysql-introduction-innodb-adaptive-hash-index/","excerpt":"前言 我们知道InnoDB的索引组织结构为Btree。通常情况下，我们需要根据查询条件，从根节点开始寻路到叶子节点，找到满足条件的记录。为了减少寻路开销，InnoDB本身做了几点优化。 首先，对于连续记录扫描，InnoDB在满足比较严格的条件时采用row cache的方式连续读取8条记录（并将记录格式转换成MySQL Format），存储在线程私有的row_prebuilt_t::fetch_cache中；这样一次寻路就可以获取多条记录，在server层处理完一条记录后，可以直接从cache中取数据而无需再次寻路，直到cache中数据取完，再进行下一轮。 另一种方式是，当一次进入InnoDB层获得数据后，在返回server层前，当前在btree上的cursor会被暂时存储到row_prebuilt_t::pcur中，当再次返回InnoDB层捞数据时，如果对应的Block没有发生任何修改，则可以继续沿用之前存储的cursor，无需重新定位。 上面这两种方式都是为了减少了重新寻路的次数，而对于一次寻路的开销，则使用Adaptive hash index来解决。AHI是一个内存结构，严格来说不是传统意义上的索引，可以把它理解为建立在Btree索引上的“索引”。","text":"前言 我们知道InnoDB的索引组织结构为Btree。通常情况下，我们需要根据查询条件，从根节点开始寻路到叶子节点，找到满足条件的记录。为了减少寻路开销，InnoDB本身做了几点优化。 首先，对于连续记录扫描，InnoDB在满足比较严格的条件时采用row cache的方式连续读取8条记录（并将记录格式转换成MySQL Format），存储在线程私有的row_prebuilt_t::fetch_cache中；这样一次寻路就可以获取多条记录，在server层处理完一条记录后，可以直接从cache中取数据而无需再次寻路，直到cache中数据取完，再进行下一轮。 另一种方式是，当一次进入InnoDB层获得数据后，在返回server层前，当前在btree上的cursor会被暂时存储到row_prebuilt_t::pcur中，当再次返回InnoDB层捞数据时，如果对应的Block没有发生任何修改，则可以继续沿用之前存储的cursor，无需重新定位。 上面这两种方式都是为了减少了重新寻路的次数，而对于一次寻路的开销，则使用Adaptive hash index来解决。AHI是一个内存结构，严格来说不是传统意义上的索引，可以把它理解为建立在Btree索引上的“索引”。 本文代码分析基于MySQL 5.7.7-rc，描述的逻辑适用于5.7.7之前及5.6版本。但在即将发布的MySQL-5.7.8版本中， InnoDB根据索引id对AHI进行了分区处理，以此来降低btr_search_latch读写锁竞争，由于尚未发布，本文暂不覆盖相关内容。 我们以一个干净启动的实例作为起点，分析下如何进行AHI构建的过程。 初始化 AHI在内存中表现就是一个普通的哈希表对象，存储在btr_search_sys_t::hash_index中，对AHI的查删改操作都是通过一个全局读写锁btr_search_latch来保护。 在实例启动，完成buffer pool初始化后，会初始化AHI子系统相关对象，并分配AHI内存，大小为buffer pool的1/64。 参考函数：btr_search_sys_create Tips：MySQL 5.7已经开始支持InnoDB buffer pool的动态调整，其策略是buffer pool的大小改变超过1倍，就重新分配AHI Hash内存（btr_search_sys_resize）。 触发AHI信息统计 在系统刚启动时，索引对象上没有足够的信息来启发是否适合进行AHI缓存，因此开始有个信息搜集的阶段，在索引对象上维护了dict_index_t::search_info，类型为btr_search_t，用于跟踪当前索引使用AHI的关键信息。 在第一次执行SQL时，需要从btree的root节点开始，当寻址到匹配的叶子节点时，会走如下逻辑： btr_cur_search_to_nth_level： 123if (btr_search_enabled &amp;&amp; !index-&gt;disable_ahi) &#123; btr_search_info_update(index, cursor);&#125; 这里会判断脏读AHI开关（btr_search_enabled）是否打开，以及index-&gt;diable_ahi是否为false。第二个条件是MySQL5.7对临时表的优化，避免临时表操作对全局对象的影响，针对临时表不做AHI构建。 我们看看函数btr_search_info_update的逻辑： 对info-&gt;hash_analysis++，当info-&gt;hash_analysis值超过BTR_SEARCH_HASH_ANALYSIS（17）时，也就是说对该索引寻路到叶子节点17次后，才会去做AHI分析（进入步骤2） 进入函数btr_search_info_update_slow 在连续执行17次对相同索引的操作后，满足info-&gt;hash_analysis大于等于BTR_SEARCH_HASH_ANALYSIS的条件，就会调用函数btr_search_info_update_slow来更新search_info，这主要是为了避免频繁的索引查询分析产生的过多CPU开销。 InnoDB通过索引条件构建一个可用于查询的tuple，而AHI需要根据tuple定位到叶子节点上记录的位置，既然AHI是构建在Btree索引上的索引，它的键值就是通过索引的前N列的值计算的来，所有的信息搜集统计都是为了确定一个合适的”Ｎ” ，这个值也是个动态的值，会跟随应用的负载自适应调整并触发block上的AHI重构建。 btr_search_info_update_slow包含三个部分：更新索引查询信息、block上的查询信息以及为当前block构建AHI，下面几小节分别介绍。 更新索引上的查询信息 参考函数：btr_search_info_update_hash 这里涉及到的几个search_info变量包括： btr_search_t::n_hash_potential 表示如果使用AHI构建索引，潜在的可能成功的次数； btr_search_t::hash_analysis 若设置了新的建议前缀索引模式，则重置为0，随后的17次查询分析可以忽略更新search_info。 下面两个字段表示推荐的前缀索引模式： btr_search_t::n_fields 推荐构建AHI的索引列数； btr_search_t::left_side 表示是否在相同索引前缀的最左索引记录构建AHI；值为true时，则对于相同前缀索引的记录，只存储最右的那个记录。 通过n_fields和left_side可以指导选择哪些列作为索引前缀来构建（fold, rec）哈希记录。如果用户的SQL的索引前缀列的个数大于等于构建AHI时的前缀索引，就可以用上AHI。 Tip1：在５.7之前的版本中，还支持索引中的字符串前缀作为构建AHI的键值的一部分，但上游认为带来的好处并不明显，因此将btr_search_t::n_bytes 移除了(参见commit 6f5f19b338543277a108a97710de8dd59b9dbb60, 42499d9394bf103a27d63cd38b0c3c6bd738a7c7）。 Tip2：然而上游在测试中发现，如果把n_bytes移除，可能在诸如顺序插入这样的场景存在性能退化(参阅commit 00ec81a9efc1108376813f15935b52c451a268cf)，因此在新发布的MySQL5.7.8版本中又重新引入，本文分析代码时统一基于MySQL5.7.7版本。 两种情况需要构建建议的前缀索引列： 当前是第一次为该索引做AHI分析，btr_search_t::n_hash_potential值为0，需要构建建议的前缀索引列； 新的记录匹配模式发生了变化(info-&gt;left_side == (info-&gt;n_fields &lt;=cursor-&gt;low_match))，需要重新设置前缀索引列。 相关代码段： 1234567891011121314151617181920212223242526272829303132333435if (cursor-&gt;up_match == cursor-&gt;low_match) &#123; info-&gt;n_hash_potential = 0; /* For extra safety, we set some sensible values here */ info-&gt;n_fields = 1; info-&gt;left_side = TRUE;&#125; else if (cursor-&gt;up_match &gt; cursor-&gt;low_match) &#123; info-&gt;n_hash_potential = 1; if (cursor-&gt;up_match &gt;= n_unique) &#123; info-&gt;n_fields = n_unique; &#125; else if (cursor-&gt;low_match &lt; cursor-&gt;up_match) &#123; info-&gt;n_fields = cursor-&gt;low_match + 1; &#125; else &#123; info-&gt;n_fields = cursor-&gt;low_match; &#125; info-&gt;left_side = TRUE;&#125; else &#123; info-&gt;n_hash_potential = 1; if (cursor-&gt;low_match &gt;= n_unique) &#123; info-&gt;n_fields = n_unique; &#125; else if (cursor-&gt;low_match &gt; cursor-&gt;up_match) &#123; info-&gt;n_fields = cursor-&gt;up_match + 1; &#125; else &#123; info-&gt;n_fields = cursor-&gt;up_match; &#125; info-&gt;left_side = FALSE;&#125; 从上述代码可以看到，在low_match和up_match之间，选择小一点match的索引列数的来进行设置，但不超过唯一确定索引记录值的列的个数： 当low_match小于up_match时，left_side设置为true，表示相同前缀索引的记录只缓存最左记录； 当low_match大于up_match时，left_side设置为false，表示相同前缀索引的记录只缓存最右记录。 如果不是第一次进入seach_info分析，有两种情况会递增btr_search_t::n_hash_potential： 本次查询的up_match和当前推荐的前缀索引都能唯一决定一条索引记录(例如唯一索引)，则根据search_info推荐的前缀索引列构建AHI肯定能命中，递增 info-&gt;n_hash_potential； 123456if (info-&gt;n_fields &gt;= n_unique &amp;&amp; cursor-&gt;up_match &gt;= n_unique) &#123;increment_potential: info-&gt;n_hash_potential++; return;&#125; 本次查询的tuple可以通过建议的前缀索引列构建的AHI定位到。 1234if (info-&gt;left_side == (info-&gt;n_fields &lt;= cursor-&gt;up_match)) &#123; goto increment_potential;&#125; 很显然，如果对同一个索引的查询交替使用不同的查询模式，可能上次更新的search_info很快就会被重新设置，具有固定模式的索引查询将会受益于AHI索引。 更新block上的查询信息 参考函数：btr_search_update_block_hash_info 更新数据页block上的查询信息，涉及到修改的变量包括： btr_search_info::last_hash_succ 最近一次成功(或可能成功)使用AHI； buf_block_t::n_hash_helps 计数值，如果使用当前推荐的前缀索引列构建AHI可能命中的次数，用于启发构建／重新构建数据页上的AHI记录项； buf_block_t::n_fields 推荐在block上构建AHI的前缀索引列数； buf_block_t::left_side 和search_info上对应字段含义相同。 函数主要流程包括： 首先设置btr_search_info::last_hash_succ 为FALSE 这会导致在分析过程中无法使用AHI进行检索，感觉这里的设置不是很合理。这意味着每次分析一个新的block，都会导致AHI短暂不可用。 初始化或更新block上的查询信息 123456789101112131415161718192021if ((block-&gt;n_hash_helps &gt; 0) &amp;&amp; (info-&gt;n_hash_potential &gt; 0) &amp;&amp; (block-&gt;n_fields == info-&gt;n_fields) &amp;&amp; (block-&gt;left_side == info-&gt;left_side)) &#123; if ((block-&gt;index) &amp;&amp; (block-&gt;curr_n_fields == info-&gt;n_fields) &amp;&amp; (block-&gt;curr_left_side == info-&gt;left_side)) &#123; /* The search would presumably have succeeded using the hash index */ info-&gt;last_hash_succ = TRUE; &#125; block-&gt;n_hash_helps++;&#125; else &#123; block-&gt;n_hash_helps = 1; block-&gt;n_fields = info-&gt;n_fields; block-&gt;left_side = info-&gt;left_side;&#125; 当block第一次被touch到并进入该函数时，设置block上的建议索引列值；以后再进入时，如果和索引上的全局search_info相匹配，则递增block-&gt;n_hash_helps，启发后续的创建或重构建AHI。 如果当前数据页block上已经构建了AHI记录项，且buf_block_t::curr_n_fields等字段和btr_search_info上对应字段值相同时，则认为当前SQL如果使用AHI索引能够命中，因此将btr_search_info::last_hash_succ设置为true，下次再使用相同索引检索btree时就会尝试使用AHI。 在初始化或更新block上的变量后，需要判断是否为整个page构建AHI索引： 123456789101112131415if ((block-&gt;n_hash_helps &gt; page_get_n_recs(block-&gt;frame) / BTR_SEARCH_PAGE_BUILD_LIMIT) &amp;&amp; (info-&gt;n_hash_potential &gt;= BTR_SEARCH_BUILD_LIMIT)) &#123; if ((!block-&gt;index) || (block-&gt;n_hash_helps &gt; 2 * page_get_n_recs(block-&gt;frame)) || (block-&gt;n_fields != block-&gt;curr_n_fields) || (block-&gt;left_side != block-&gt;curr_left_side)) &#123; /* Build a new hash index on the page */ return(TRUE); &#125;&#125; 简单来说，当满足下面三个条件时，就会去为整个block上构建AHI记录项： 分析使用AHI可以成功查询的次数(buf_block_t::n_hash_helps)超过block上记录数的16(BTR_SEARCH_PAGE_BUILD_LIMIT)分之一； btr_search_info::n_hash_potential大于等于BTR_SEARCH_BUILD_LIMIT (100)，表示连续100次潜在的成功使用AHI可能性； 尚未为当前block构造过索引、或者当前block上已经构建了AHI索引且block-&gt;n_hash_helps大于page上记录数的两倍、或者当前block上推荐的前缀索引列发生了变化 。 为数据页构建AHI索引 如果在上一阶段判断认为可以为当前page构建AHI索引（函数btr_search_update_block_hash_info返回值为TRUE），则根据当前推荐的索引前缀进行AHI构建。 参考函数：btr_search_build_page_hash_index 分为三个阶段： 检查阶段：加btr_search_latch的S锁，判断AHI开关是否打开；如果block上已经构建了老的AHI但前缀索引列和当前推荐的不同，则清空Block对应的AHI记录项（btr_search_drop_page_hash_index）；检查n_fields和page上的记录数；然后释放btr_search_latch的S锁； 搜集阶段：根据推荐的索引列数计算记录fold值，将对应的数据页记录内存地址到数组里； 根据left_mode值，相同的前缀索引列值会有不同的行为，举个简单的例子，假设page上记录为 (2,1), (2,2), (5, 3), (5, 4), (7, 5), (8, 6)，n_fields＝１ 若left_most为true，则hash存储的记录为(2,1) , (5, 3), (7, 5), (8,6) 若left_most为false，则hash存储的记录为(2, 2), (5, 4), (7,5), (8, 6) 插入阶段：加btr_search_latch的X锁，将第二阶段搜集的(fold, rec)插入到AHI中，并更新： 123456789if (!block-&gt;index) &#123; index-&gt;search_info-&gt;ref_count++;&#125;block-&gt;n_hash_helps = 0;block-&gt;curr_n_fields = n_fields;block-&gt;curr_left_side = left_side;block-&gt;index = index; PS：由于第二阶段释放了btr_search_latch锁，这里还得判断block上的AHI信息是否发生了变化，如果block上已经构建了AHI且block-&gt;curr_*几个变量和当前尝试构建的检索模式不同，则放弃本次构建。 使用AHI AHI的目的是根据用户提供的查询条件加速定位到叶子节点，一般如果有固定的查询pattern，都可以通过AHI受益，尤其是Btree高度比较大的时候。 入口函数：btr_cur_search_to_nth_level 相关代码： 12345678910111213141516171819 /* Use of AHI is disabled for intrinsic table as these tables re-use the index-id and AHI validation is based on index-id. */ if (rw_lock_get_writer(&amp;btr_search_latch) == RW_LOCK_NOT_LOCKED &amp;&amp; latch_mode &lt;= BTR_MODIFY_LEAF &amp;&amp; info-&gt;last_hash_succ &amp;&amp; !index-&gt;disable_ahi &amp;&amp; !estimate# ifdef PAGE_CUR_LE_OR_EXTENDS &amp;&amp; mode != PAGE_CUR_LE_OR_EXTENDS# endif /* PAGE_CUR_LE_OR_EXTENDS */ &amp;&amp; !dict_index_is_spatial(index) /* If !has_search_latch, we do a dirty read of btr_search_enabled below, and btr_search_guess_on_hash() will have to check it again. */ &amp;&amp; UNIV_LIKELY(btr_search_enabled) &amp;&amp; !modify_external &amp;&amp; btr_search_guess_on_hash(index, info, tuple, mode, latch_mode, cursor, has_search_latch, mtr)) &#123; 从代码段可以看出，需要满足如下条件才能够使用AHI： 没有加btr_search_latch写锁。如果加了写锁，可能操作时间比较耗时，走AHI检索记录就得不偿失了； latch_mode &lt;= BTR_MODIFY_LEAF，表明本次只是一次不变更BTREE结构的DML或查询（包括等值、RANGE等查询）操作； btr_search_info::last_hash_succ为true表示最近一次使用AHI成功（或可能成功）了； 打开AHI开关； 查询优化阶段的估值操作，例如计算range范围等，典型的堆栈包括：handler::multi_range_read_info_const –&gt; ha_innobase::records_in_range –&gt; btr_estimate_n_rows_in_range –&gt; btr_cur_search_to_nth_level； 不是spatial索引； 调用者无需分配外部存储页(BTR_MODIFY_EXTERNAL，主要用于辅助写入大的blob数据，参考struct btr_blob_log_check_t)。 当满足上述条件时，进入函数btr_search_guess_on_hash，根据当前的查询tuple对象计算fold，并查询AHI；只有当前检索使用的tuple列的个数大于等于构建AHI的列的个数时，才能够使用AHI索引。 btr_search_guess_on_hash： 首先用户提供的前缀索引查询条件必须大于等于构建AHI时的前缀索引列数，这里存在一种可能性：索引上的search_info的n_fields 和block上构建AHI时的cur_n_fields值已经不相同了，但是我们并不知道本次查询到底落在哪个block上，这里一致以search_info上的n_fields为准来计算fold，去查询AHI； 在检索AHI时需要加&amp;btr_search_latch的S锁； 如果本次无法命中AHI，就会将btr_search_info::last_hash_succ设置为false，这意味着随后的查询都不会去使用AHI了，只能等待下一路查询信息分析后才可能再次启动（btr_search_failure）； 对于从ahi中获得的记录指针，还需要根据当前的查询模式检查是否是正确的记录位置（btr_search_check_guess）。 如果本次查询使用了AHI，但查询失败了（cursor-&gt;flag == BTR_CUR_HASH_FAIL），并且当前block构建AHI索引的curr_n_fields等字段和btr_search_info上的相符合，则根据当前cursor定位到的记录插入AHI。参考函数：btr_search_update_hash_ref。 从上述分析可见，AHI如其名，完全是自适应的，如果检索模式不固定，很容易就出现无法用上AHI或者AHI失效的情况。 维护AHI 关闭选项innodb_adaptive_hash_index； 持有dict_sys-&gt;mutex和btr_search_latch的X锁； 遍历dict_sys-&gt;table_LRU和dict_sys-&gt;table_non_LRU链表，将每个表上的所有索引的index-&gt;search_info-&gt;ref_count设置为0； 释放dict_sys-&gt;mutex； 遍历buffer pool，将block上的index标记(buf_block_t::index)清空为NULL； 清空AHI中的哈希项，并释放为记录项分配的Heap； 释放btr_search_latch。 参考函数：btr_search_disable index-&gt;search_info的ref_count不为0时，无法从数据集词典cache中将对应的表驱逐，workaround的方式是临时关闭AHI开关； 参考函数：dict_table_can_be_evicted、dict_index_remove_from_cache_low 删除索引页上的记录，或者更新的是二级索引、或者更新了主键且影响了排序键值，则需要从AHI上将对应的索引记录删除； 参考函数：btr_search_update_hash_on_delete 插入新的记录时，如果本次插入未产生页面重组、操作的page为叶子节点，且本次插入操作使用过AHI定位成功，则先尝试更新再尝试插入，否则直接插入对应的AHI记录项； 参考函数：btr_search_update_hash_node_on_insert、btr_search_update_hash_on_insert 涉及索引树分裂或者节点合并，或从LRU中驱逐page（buf_LRU_free_page）时，需要清空AHI对应的page。 参考函数：btr_search_drop_page_hash_index shortcut查询模式 在row_search_mvcc函数中，首先会去判断在满足一定条件时，使用shortcut模式，利用AHI索引来进行检索。 只有满足严苛的条件时（例如需要唯一键查询、使用聚集索引、长度不超过八分之一的page size、隔离级别在RC及RC之上、活跃的Read view等等条件，具体的参阅代码），才能使用shortcut： 加btr_search_latch的S锁； 然后通过row_sel_try_search_shortcut_for_mysql检索记录；如果找到满足条件的记录，本次查询可以不释放 btr_search_latch，这意味着InnoDB/server层交互期间可能持有AHI锁，但最多在10000次（BTR_SEA_TIMEOUT）交互后释放AHI latch。一旦发现有别的线程在等待AHI X 锁，也会主动释放其拥有的S锁。 然而， Percona的开发Alexey Kopytov认为这种长时间拥有的btr_search_latch的方式是没有必要的，这种设计方式出现在很久之前加锁、解锁非常昂贵的时代，然而现在的CPU已经很先进了，完全没有必要，在Percona的版本中，一次shortcut的查询操作后都直接释放掉btr_search_latch（参阅bug#1218347）。 AHI监控项 我们可以通过information_schema.innodb_metrics来监控AHI模块的运行状态 首先打开监控： 1234567891011121314151617mysql&gt; set global innodb_monitor_enable = module_adaptive_hash;Query OK, 0 rows affected (0.00 sec)mysql&gt; select status, name, subsystem from INNODB_METRICS where subsystem like &#x27;%adaptive_hash%&#x27;;+---------+------------------------------------------+---------------------+| status | name | subsystem |+---------+------------------------------------------+---------------------+| enabled | adaptive_hash_searches | adaptive_hash_index || enabled | adaptive_hash_searches_btree | adaptive_hash_index || enabled | adaptive_hash_pages_added | adaptive_hash_index || enabled | adaptive_hash_pages_removed | adaptive_hash_index || enabled | adaptive_hash_rows_added | adaptive_hash_index || enabled | adaptive_hash_rows_removed | adaptive_hash_index || enabled | adaptive_hash_rows_deleted_no_hash_entry | adaptive_hash_index || enabled | adaptive_hash_rows_updated | adaptive_hash_index |+---------+------------------------------------------+---------------------+8 rows in set (0.00 sec) 重置所有的计数 12mysql&gt; set global innodb_monitor_reset_all = &#x27;adaptive_hash%&#x27;;Query OK, 0 rows affected (0.00 sec) 该表搜集了AHI子系统诸如AHI查询次数，更新次数等信息，可以很好的监控其运行状态，在某些负载下，AHI并不适合打开，关闭AHI可以避免额外的维护开销。当然这取决于你针对具体负载的性能测试。 本文地址：http://xnerv.wang/mysql-introduction-innodb-adaptive-hash-index/ 转载自：InnoDB Adaptive hash index介绍","categories":[{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/categories/DBMS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/tags/DBMS/"},{"name":"MySQL","slug":"MySQL","permalink":"https://xnerv.wang/tags/MySQL/"},{"name":"InnoDB","slug":"InnoDB","permalink":"https://xnerv.wang/tags/InnoDB/"},{"name":"Adaptive hash index","slug":"Adaptive-hash-index","permalink":"https://xnerv.wang/tags/Adaptive-hash-index/"},{"name":"Hash","slug":"Hash","permalink":"https://xnerv.wang/tags/Hash/"}]},{"title":"MySQL - set names 都做了什么（转载）","slug":"what-set-names-does","date":"2018-03-17T05:50:00.000Z","updated":"2023-08-21T02:24:18.975Z","comments":true,"path":"what-set-names-does/","link":"","permalink":"https://xnerv.wang/what-set-names-does/","excerpt":"背景 最近有同事问，set names 时会同时设置了3个session变量 123SET character_set_client = charset_name;SET character_set_results = charset_name;SET character_set_connection = charset_name; 就从变量名字来看，character_set_client 是设置客户端相关的字符集，character_set_results 是设置返回结果相关的字符集，character_set_connection 这个就有点不太明白了，这个有啥用呢？ 概念说明 通过官方文档来看: character_set_client 是指客户端发送过来的语句的编码; character_set_connection 是指mysqld收到客户端的语句后，要转换到的编码； 而 character_set_results 是指server执行语句后，返回给客户端的数据的编码。","text":"背景 最近有同事问，set names 时会同时设置了3个session变量 123SET character_set_client = charset_name;SET character_set_results = charset_name;SET character_set_connection = charset_name; 就从变量名字来看，character_set_client 是设置客户端相关的字符集，character_set_results 是设置返回结果相关的字符集，character_set_connection 这个就有点不太明白了，这个有啥用呢？ 概念说明 通过官方文档来看: character_set_client 是指客户端发送过来的语句的编码; character_set_connection 是指mysqld收到客户端的语句后，要转换到的编码； 而 character_set_results 是指server执行语句后，返回给客户端的数据的编码。 对人来说，能够理解的是各种各样的符号，而对计算机来说，只能理解二进制，二进制和符号之间的对应关系就是编码。不同地域国家都有自己的一套符号集合，每个都各自用一组二进制数字表示，从而形成了不同的编码，字符集就可以看作是编码和符号的对应关系集合。同一个二进制数在不同的字符集下可能对应完全不一样的字符，如在GBK字符集中，C4E3 对应的是你，而在big5字符集中对应的是斕，而 你在unicode中的编码是4F60，在Collation-Charts 这个网站有字符集和编码对应关系图，可以非常直观地看到不同编码下二进制数和符号的对应关系。 set names 设置的3个变量就是设置mysqld和客户端通信时，mysqld应该如何解读client发来的字符，以及返回给客户端什么样的编码。 实验测试 环境如下： 1234567891011mysql&gt; show variables like &#x27;character%&#x27;;+--------------------------+-------------------------------------+| Variable_name | Value |+--------------------------+-------------------------------------+| character_set_client | utf8 || character_set_connection | utf8 || character_set_database | utf8 || character_set_filesystem | binary || character_set_results | utf8 || character_set_server | utf8 || character_set_system | utf8 | server端的3个编码设置都是utf8。 另外，客户端是标准 mysql client，使用的编码是utf8，和sever端编码是一致的。 建一张表作为测试 123456789CREATE TABLE t1(id INT, name VARCHAR(200) CHARSET utf8) engine=InnoDB;INSERT INTO t1 VALUES(0, &#x27;你好&#x27;);mysql&gt; SELECT id, name, hex(name) FROM t1;+------+--------+--------------+| id | name | hex(name) |+------+--------+--------------+| 0 | 你好 | E4BDA0E5A5BD |+------+--------+--------------+ 下面我们分别改变这3个值，来看下结果会有什么变化 Case 1 只改变 character_set_client 12345678910SET character_set_client=gbk;INSERT INTO t1 VALUES(1, &#x27;你好&#x27;);mysql&gt; SELECT id, name, hex(name) FROM t1;+------+-----------+--------------------+| id | name | hex(name) |+------+-----------+--------------------+| 0 | 你好 | E4BDA0E5A5BD || 1 | 浣犲ソ | E6B5A3E78AB2E382BD |+------+-----------+--------------------+2 rows in set (0.00 sec) 可以看到返回的数据已经乱码了，并且数据库里存的确实和第一条记录不一样。 case 2 只改变 character_set_connection 12345678910111213SET names utf8;SET character_set_connection = gbk;INSERT INTO t1 VALUES(2, &#x27;你好&#x27;);mysql&gt; SELECT id, name, hex(name) FROM t1;+------+-----------+--------------------+| id | name | hex(name) |+------+-----------+--------------------+| 0 | 你好 | E4BDA0E5A5BD || 1 | 浣犲ソ | E6B5A3E78AB2E382BD || 2 | 你好 | E4BDA0E5A5BD |+------+-----------+--------------------+3 rows in set (0.00 sec) case 3 只改变 character_set_results 1234567891011121314SET names utf8;SET character_set_results = gbk;INSERT INTO t1 VALUES(3, &#x27;你好&#x27;);mysql&gt; select id, name, hex(name) from t1;+------+--------+--------------------+| id | name | hex(name) |+------+--------+--------------------+| 0 | | E4BDA0E5A5BD || 1 | 你好 | E6B5A3E78AB2E382BD || 2 | | E4BDA0E5A5BD || 3 | | E4BDA0E5A5BD |+------+--------+--------------------+4 rows in set (0.00 sec) 再改回原样，看下结果 1234567891011SET names utf8;mysql&gt; SELECT id, name, hex(name) FROM t1;+------+-----------+--------------------+| id | name | hex(name) |+------+-----------+--------------------+| 0 | 你好 | E4BDA0E5A5BD || 1 | 浣犲ソ | E6B5A3E78AB2E382BD || 2 | 你好 | E4BDA0E5A5BD || 3 | 你好 | E4BDA0E5A5BD |+------+-----------+--------------------+4 rows in set (0.00 sec) 分析 我们先理下字符集在整个过程中是怎样变化的，然后再分析上面的case 客户发送请求时： 1234A1 客户端发送出语句(总是以utf8)------&gt; A2 sever收到语句解析(按character_set_client指定编码) | vA4 数据进入mysqld内部存储&lt;--------- A3 sever判断是否需要转换编码(以character_set_connection 目标编码) server返回结果时： 1B1 server返回结果(按character_set_results 指定编码) -----&gt;B2客户端解析编码显示(总是以utf8) A3步是否需要转换编码，代码中的逻辑是这样的，在sql_yacc.yy文件中： 12345678910111213141516171819202122LEX_STRING tmp;THD *thd= YYTHD;const CHARSET_INFO *cs_con= thd-&gt;variables.collation_connection;const CHARSET_INFO *cs_cli= thd-&gt;variables.character_set_client;uint repertoire= thd-&gt;lex-&gt;text_string_is_7bit &amp;&amp; my_charset_is_ascii_based(cs_cli) ? MY_REPERTOIRE_ASCII : MY_REPERTOIRE_UNICODE30;if (thd-&gt;charset_is_collation_connection || (repertoire == MY_REPERTOIRE_ASCII &amp;&amp; my_charset_is_ascii_based(cs_con))) tmp= $1;else&#123; if (thd-&gt;convert_string(&amp;tmp, cs_con, $1.str, $1.length, cs_cli)) MYSQL_YYABORT;&#125;$= new (thd-&gt;mem_root) Item_string(tmp.str, tmp.length, cs_con, DERIVATION_COERCIBLE, repertoire);if ($ == NULL) MYSQL_YYABORT; 如果 character_set_client 和 character_set_connection 一样，或者当前的字符编码是和ASCII兼容，并且都是ASCII范围内的，就不转换，其它情况就转。 对于case1 实际上客户端发过来是UTF8的，但A2步骤server认为客户端的编码是GBK的，就按GBK来解析，同时满足A3步骤的转换条件，所以就误将UTF8编码认为是GBK，然后又给转成了UTF8。 你好的UTF8编码是 E4BDA0E5A5BD 6个字节，每个字符3个字节，按GBK来解析的话，因为GBK是固定2个字节，就认为有3个字符，然后转成UTF8，虽然UTF8是变长的，但是这里的3个GBK字符按值都是要占3个字节的，转出来一共9个字节。所以case1看到的实际存储的值一共9个字节，比原来的大。 在返回时，是按UTF8返回的，因为存了3个UTF8字符，所以客户端看到的就是3个。 对于case2 A2步骤没问题，问题是出在A3，按照转换逻辑，此时需要把UTF8转成GBK，这里因为character_set_client是正确的，所以转换的源不会识别错，转换成GBK自然也不会错，后面存储成UTF8时，再从GBK转成UTF8，也没错，因为UTF8和GBK字符集里都包含 ‘你’和’好’，所以相互转换也不会出错，只是多了2次转换。 对于case3 错在返回字符集设置的和客户端不匹配，在返回时，server将所有字符转成GBK的，结果客户端一根筋的认为是UTF8，就解析错了。 比较有意思的是第二条记录，即case1错误插进去的，显示出来是对的。 为什么呢，因为在case1中存的时候，是按 UTF8-&gt;强制解析为GBK-&gt;然后转为UTF8 这个逻辑存下去的，而返回的时候，因为server会将存的UTF8又给转回GBK，然后客户端又拿着这个GBK误以为是UTF8解析，实际上是case1的逆向过程，虽然2个方向都是错的，最终显示是好的，所谓的负负得正吧，哈哈。 对于case2 ，数据从客户端进入server的时候，多做了2次转换，最终显示还是对的，但不是所有场景都是这样，如下面这种 123456789101112131415set names utf8;set character_set_connection = latin1;INSERT INTO t1 VALUES(4, &#x27;你好&#x27;);set names utf8;mysql&gt; SELECT id, name, hex(name) FROM t1;+------+-----------+--------------------+| id | name | hex(name) |+------+-----------+--------------------+| 0 | 你好 | E4BDA0E5A5BD || 1 | 浣犲ソ | E6B5A3E78AB2E382BD || 2 | 你好 | E4BDA0E5A5BD || 3 | 你好 | E4BDA0E5A5BD || 4 | ?? | 3F3F |+------+-----------+--------------------+5 rows in set (0.00 sec) 为什么呢，因为在 UTF8转latin1时，信息丢失了，latin1字符编码所能表达的字符集是远小于utf8的，你 和 好就不在其中，这2个字符在转换中被转成了 ? 和 ?，之后存储转换成UTF8时，?只有一个字节3F，还原回去还是 3F。 总结 character_set_client 和 character_set_results 是一定要和客户端一致，不要依赖于负负得正，character_set_connection 设置和character_set_client 不一致，有丢失数据的风险，所以尽量也一致，总之这3个值就是要一样，还要和客户端一致，所以才有了 set names 这个快捷命令。关于为啥要有 character_set_connection 这一步转换，笔者目前还没看出来，以后理解了再更新，如果读者朋友知道的话，请不吝赐教。 本文地址：http://xnerv.wang/what-set-names-does/ 转载自：set names 都做了什么","categories":[{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/categories/DBMS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/tags/DBMS/"},{"name":"MySQL","slug":"MySQL","permalink":"https://xnerv.wang/tags/MySQL/"},{"name":"MySQL charset","slug":"MySQL-charset","permalink":"https://xnerv.wang/tags/MySQL-charset/"}]},{"title":"MySQL - Index Condition Pushdown (ICP)（转载）","slug":"mysql-index-condition-pushdown","date":"2018-03-17T05:46:00.000Z","updated":"2023-08-21T02:24:18.864Z","comments":true,"path":"mysql-index-condition-pushdown/","link":"","permalink":"https://xnerv.wang/mysql-index-condition-pushdown/","excerpt":"前言 上一篇文章 提过，我们在之后的文章中会从 optimizer 的选项出发，系统的介绍 optimizer 的各个变量，包括变量的原理、作用以及源码实现等，然后再进一步的介绍优化器的工作过程（SQL 语句扁平化处理、索引选择、代价计算、多表连接顺序选择以及物理执行等内容），本期我们先看一下众所周知的 ICP，官方文档请参考这里。 ICP 测试 首先，咱们来看一下打开 ICP 与关闭 ICP 之间的性能区别，以下是测试过程： 准备数据： 123456789create table icp(id int, age int, name varchar(30), memo varchar(600)) engine=innodb;alter table icp add index aind(age, name, memo);--let $i= 100000while ($i)&#123; --eval insert into icp values($i, 1, &#x27;a$i&#x27;, repeat(&#x27;a$i&#x27;, 100)) --dec $i&#125; PS: MySQL 有一个叫profile的东东，可以用来监视 SQL 语句在各个阶段的执行情况，咱们可以使用这个工具来观察 SQL 语句在各个阶段的运行情况，关于 profile 的详细说明可以参考官方文档。","text":"前言 上一篇文章 提过，我们在之后的文章中会从 optimizer 的选项出发，系统的介绍 optimizer 的各个变量，包括变量的原理、作用以及源码实现等，然后再进一步的介绍优化器的工作过程（SQL 语句扁平化处理、索引选择、代价计算、多表连接顺序选择以及物理执行等内容），本期我们先看一下众所周知的 ICP，官方文档请参考这里。 ICP 测试 首先，咱们来看一下打开 ICP 与关闭 ICP 之间的性能区别，以下是测试过程： 准备数据： 123456789create table icp(id int, age int, name varchar(30), memo varchar(600)) engine=innodb;alter table icp add index aind(age, name, memo);--let $i= 100000while ($i)&#123; --eval insert into icp values($i, 1, &#x27;a$i&#x27;, repeat(&#x27;a$i&#x27;, 100)) --dec $i&#125; PS: MySQL 有一个叫profile的东东，可以用来监视 SQL 语句在各个阶段的执行情况，咱们可以使用这个工具来观察 SQL 语句在各个阶段的运行情况，关于 profile 的详细说明可以参考官方文档。 打开 ICP 的性能测试： 12345678910111213141516set profiling=on;set optimizer_switch=&#x27;index_condition_pushdown=on&#x27;; （default enabled）select * from icp where age = 1 and memo like &#x27;%9999%&#x27;;mysql&gt; show profile cpu,block io for query 7;+----------------------+-----------+-----------+------------+--------------+---------------+| Status | Duration | CPU_user | CPU_system | Block_ops_in | Block_ops_out |+----------------------+-----------+-----------+------------+--------------+---------------+| executing | 0.000009 | 0.000000 | 0.000000 | 0 | 0 || Sending data | 3.225383 | 3.507467 | 0.037994 | 0 | 0 |+----------------------+-----------+-----------+------------+--------------+---------------+mysql&gt; show session status like &#x27;%handler%&#x27;;show session status like &#x27;%handler%&#x27;;+----------------------------+--------+| Handler_read_next | 19 || Handler_read_rnd_next | 30 |+----------------------------+--------+18 rows in set (0.00 sec) 关闭 ICP 的性能测试： 1234567891011121314151617mysql&gt; set optimizer_switch=&#x27;index_condition_pushdown=off&#x27;;mysql&gt; select * from icp where age = 1 and memo like &#x27;%9999%&#x27;;mysql&gt; show profile cpu, block io for query 20;+----------------------+----------+----------+------------+--------------+---------------+| Status | Duration | CPU_user | CPU_system | Block_ops_in | Block_ops_out |+----------------------+----------+----------+------------+--------------+---------------+| Sending data | 15.327345 | 17.443348 | 0.165975 | 0 | 0 |+----------------------+----------+----------+------------+--------------+---------------+15 rows in set, 1 warning (0.00 sec)mysql&gt; show session status like &#x27;%handler%&#x27;;+----------------------------+--------+| Variable_name | Value |+----------------------------+--------+| Handler_read_next | 100019 || Handler_read_rnd_next | 47 |+----------------------------+--------+18 rows in set (0.01 sec) 测试结论：由以上测试情况可以看到，在二级索引是复合索引且前面的条件过滤性较低的情况下，打开 ICP 可以有效的降低 server 层和 engine 层之间交互的次数，从而有效的降低在运行时间。 ICP 原理 5.6 之前，在 SQL 语句的执行过程中，server 层通过 engine 的 api 获取数据，然后再进行 where_cond 的判断（具体判断逻辑在: evaluate_join_record），每一条数据都需要从engine层返回server层做判断。我们回顾一下上面把 ICP 关掉的测试，可以看到 Handler_read_next 的值陡增，其原因是第 1 个字段区分度不高，且 memo 字段无法使用索引，造成了类似 index 扫描的的情况，性能较低。 5.6 之后，在利用索引扫描的过程中，如果发现 where_cond 中含有这个 index 相关的条件，则将此条件记录在 handler 接口中，在索引扫描的过程中，只有满足索引与handler接口的条件时，才会返回到 server 层做进一步的处理，在前缀索引区分度不够，其它字段区分度高的情况下可以有效的减少 server &amp; engine之间的开销，提升查询性能。 ICP 源码实现 我们在上小节提到，index condition down 所用的条件是记在handler接口中的，咱们分析一下“记录”的过程是如何实现的。 首先，优化器计算代价后会生成一个 JOIN_TAB 的左支树，每一个 JOIN_TAB 包含相关表的指针、表的读取方式、访问表所包含的索引等信息，优化器会在 make_join_readinfo 中对JOIN_TAB中表的访问方式进行相应的修正，并进一步将 where cond 中和索引相关的条件记录到 table 的句柄中，堆栈如下： 123456#0 make_cond_for_index (cond=0x2b69680179e8, table=0x2b6968012100, keyno=0, other_tbls_ok=true)#1 in push_index_cond (tab=0x2b696802aa48, keyno=0, other_tbls_ok=true, trace_obj=0x2b696413ec30)#2 in make_join_readinfo (join=0x2b6968017db0, options=0, no_jbuf_after=4294967295)#3 in JOIN::optimize (this=0x2b6968017db0)#4 in mysql_execute_select (thd=0x3176760, select_lex=0x3179470, free_join=true) 其次， make_cond_for_index 是一个递归的过程，对 where_cond中的每一个条件进行判断，对满足条件的 cond 重新组合成一个新的cond，最后将新的 cond 挂在table-&gt;file 下面（table-&gt;file 指的是操作物理表的接口函数，此变量为thd下私有的，不共享，共享的是tab-&gt;table-&gt;s），详细参考make_cond_for_index 的详细实现，设置的堆栈如下： 123456#0 ha_innobase::idx_cond_push (this=0x2b696800e810, keyno=0, idx_cond=0x2b69680179e8)#1 0x0000000000a60a55 in push_index_cond (tab=0x2b696802aa48, keyno=0, other_tbls_ok=true, trace_obj=0x2b696413ec30)#2 0x0000000000a6362f in make_join_readinfo (join=0x2b6968017db0, options=0, no_jbuf_after=4294967295)#3 0x0000000000d9b8bd in JOIN::optimize (this=0x2b6968017db0#4 0x0000000000a5b9ae in mysql_execute_select (thd=0x3176760, select_lex=0x3179470, free_join=true) 再次，server 层根据生成的 JOIN_TAB 读取engine层的内容，在engine读取的时候，会进行index_condition_pushdown的调用，即 ICP 的调用，堆栈如下： 12345678910111213#0 Item_func_like::val_int (this=0x2b6978005a28)#1 0x0000000001187b66 in innobase_index_cond (file=0x2b696800e810)#2 0x0000000001393566 in row_search_idx_cond_check (mysql_rec=0x2b69680129f0 &lt;incomplete sequence \\361&gt;, prebuilt=0x2b69680130f8, rec=0x2b692b56e4cf &quot;\\200&quot;, offsets=0x2b697008d450)#3 0x0000000001397e2b in row_search_for_mysql (buf=0x2b69680129f0 &lt;incomplete sequence \\361&gt;, mode=2, prebuilt=0x2b69680130f8, match_mode=1, direction=0)#4 0x00000000011696b9 in ha_innobase::index_read (this=0x2b696800e810, buf=0x2b69680129f0 &lt;incomplete sequence \\361&gt;, key_ptr=0x2b697800a660 &quot;&quot;, key_len=5, find_flag=HA_READ_KEY_EXACT)#5 0x00000000006ecc58 in handler::index_read_map (this=0x2b696800e810, buf=0x2b69680129f0 &lt;incomplete sequence \\361&gt;, key=0x2b697800a660 &quot;&quot;, keypart_map=1, find_flag=HA_READ_KEY_EXACT)#6 0x00000000006d6bb4 in handler::ha_index_read_map (this=0x2b696800e810, buf=0x2b69680129f0 &lt;incomplete sequence \\361&gt;, key=0x2b697800a660 &quot;&quot;, keypart_map=1, find_flag=HA_READ_KEY_EXACT)#7 0x00000000009a1870 in join_read_always_key (tab=0x2b697800a1b8)#8 0x000000000099d480 in sub_select (join=0x2b6978005df0, join_tab=0x2b697800a1b8, end_of_records=false)#9 0x000000000099c6c0 in do_select (join=0x2b6978005df0)#10 0x00000000009980a4 in JOIN::exec (this=0x2b6978005df0)#11 0x0000000000a5bac0 in mysql_execute_select (thd=0x32801a0, select_lex=0x3282eb0, free_join=true) 可见在 ICP 的判断是调用相关item的函数的，虽然同是调用 server 层的函数，但是没有 ICP 的调用需要根据主建找到记录，然后再匹配，而有了 ICP 可以省略一次主键查找数据的过程，进而提升效率。 ICP 使用限制及问题 只支持 select 语句； 5.6 中只支持 MyISAM 与 InnoDB 引擎; ICP的优化策略可用于range、ref、eq_ref、ref_or_null 类型的访问数据方法； 不支持主建索引的 ICP； 当 SQL 使用覆盖索引时但只检索部分数据时，ICP 无法使用，详细的分析可以参考 bug#68554 中 Olav Sandstå的分析，代码实现部分可以参考 make_join_readinfo； 在查询的时候即使正确的使用索引的前Ｎ个字段（即遵循前缀索引的原则），还是会用到 ICP，无故的多了 ICP 相关的判断，这应该是一个退化的问题，例： 12345678mysql&gt; explain select * from icp where age&#x3D;1 and name &#x3D; &#39;a1&#39;;+----+-------------+-------+------+---------------+------+---------+-------------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+------+---------------+------+---------+-------------+------+-----------------------+| 1 | SIMPLE | icp | ref | aind | aind | 38 | const,const | 1 | Using index condition |+----+-------------+-------+------+---------------+------+---------+-------------+------+-----------------------+1 row in set (3.26 sec) PS: engine condition pushdown 是 NDB 使用的，其它引擎不支持。 本文地址：http://xnerv.wang/mysql-index-condition-pushdown/ 转载自：Index Condition Pushdown (ICP)","categories":[{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/categories/DBMS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/tags/DBMS/"},{"name":"MySQL","slug":"MySQL","permalink":"https://xnerv.wang/tags/MySQL/"},{"name":"Index Condition Pushdown","slug":"Index-Condition-Pushdown","permalink":"https://xnerv.wang/tags/Index-Condition-Pushdown/"}]},{"title":"InnoDB 事务锁系统简介（转载）","slug":"introduction-of-innodb-transaction-lock","date":"2018-03-17T05:28:00.000Z","updated":"2023-08-21T02:24:18.794Z","comments":true,"path":"introduction-of-innodb-transaction-lock/","link":"","permalink":"https://xnerv.wang/introduction-of-innodb-transaction-lock/","excerpt":"前言 本文的目的是对 InnoDB 的事务锁模块做个简单的介绍，使读者对这块有初步的认识。本文先介绍行级锁和表级锁的相关概念，再介绍其内部的一些实现；最后以两个有趣的案例结束本文。 本文所有的代码和示例都是基于当前最新的 MySQL5.7.10 版本。 行级锁 InnoDB 支持到行级别粒度的并发控制，本小节我们分析下几种常见的行级锁类型，以及在哪些情况下会使用到这些类型的锁。 LOCK_REC_NOT_GAP 锁带上这个 FLAG 时，表示这个锁对象只是单纯的锁在记录上，不会锁记录之前的 GAP。在 RC 隔离级别下一般加的都是该类型的记录锁（但唯一二级索引上的 duplicate key 检查除外，总是加 LOCK_ORDINARY 类型的锁）。","text":"前言 本文的目的是对 InnoDB 的事务锁模块做个简单的介绍，使读者对这块有初步的认识。本文先介绍行级锁和表级锁的相关概念，再介绍其内部的一些实现；最后以两个有趣的案例结束本文。 本文所有的代码和示例都是基于当前最新的 MySQL5.7.10 版本。 行级锁 InnoDB 支持到行级别粒度的并发控制，本小节我们分析下几种常见的行级锁类型，以及在哪些情况下会使用到这些类型的锁。 LOCK_REC_NOT_GAP 锁带上这个 FLAG 时，表示这个锁对象只是单纯的锁在记录上，不会锁记录之前的 GAP。在 RC 隔离级别下一般加的都是该类型的记录锁（但唯一二级索引上的 duplicate key 检查除外，总是加 LOCK_ORDINARY 类型的锁）。 LOCK_GAP 表示只锁住一段范围，不锁记录本身，通常表示两个索引记录之间，或者索引上的第一条记录之前，或者最后一条记录之后的锁。可以理解为一种区间锁，一般在RR隔离级别下会使用到GAP锁。 你可以通过切换到RC隔离级别，或者开启选项innodb_locks_unsafe_for_binlog来避免GAP锁。这时候只有在检查外键约束或者duplicate key检查时才会使用到GAP LOCK。 LOCK_ORDINARY(Next-Key Lock) 也就是所谓的 NEXT-KEY 锁，包含记录本身及记录之前的GAP。当前 MySQL 默认情况下使用RR的隔离级别，而NEXT-KEY LOCK正是为了解决RR隔离级别下的幻读问题。所谓幻读就是一个事务内执行相同的查询，会看到不同的行记录。在RR隔离级别下这是不允许的。 假设索引上有记录1, 4, 5, 8，12 我们执行类似语句：SELECT… WHERE col &gt; 10 FOR UPDATE。如果我们不在(8, 12)之间加上Gap锁，另外一个 Session 就可能向其中插入一条记录，例如9，再执行一次相同的SELECT FOR UPDATE，就会看到新插入的记录。 这也是为什么插入一条记录时，需要判断下一条记录上是否加锁了。 LOCK_S（共享锁） 共享锁的作用通常用于在事务中读取一条行记录后，不希望它被别的事务锁修改，但所有的读请求产生的LOCK_S锁是不冲突的。在InnoDB里有如下几种情况会请求S锁。 普通查询在隔离级别为 SERIALIZABLE 会给记录加 LOCK_S 锁。但这也取决于场景：非事务读（auto-commit）在 SERIALIZABLE 隔离级别下，无需加锁(不过在当前最新的5.7.10版本中，SHOW ENGINE INNODB STATUS 的输出中不会打印只读事务的信息，只能从informationschema.innodb_trx表中获取到该只读事务持有的锁个数等信息)。 类似 SQL SELECT … IN SHARE MODE，会给记录加S锁，其他线程可以并发查询，但不能修改。基于不同的隔离级别，行为有所不同: RC隔离级别： LOCK_REC_NOT_GAP | LOCK_S； RR隔离级别：如果查询条件为唯一索引且是唯一等值查询时，加的是 LOCK_REC_NOT_GAP | LOCK_S；对于非唯一条件查询，或者查询会扫描到多条记录时，加的是LOCK_ORDINARY | LOCK_S锁，也就是记录本身+记录之前的GAP； 通常INSERT操作是不加锁的，但如果在插入或更新记录时，检查到 duplicate key（或者有一个被标记删除的duplicate key），对于普通的INSERT/UPDATE，会加LOCK_S锁，而对于类似REPLACE INTO或者INSERT … ON DUPLICATE这样的SQL加的是X锁。而针对不同的索引类型也有所不同： 对于聚集索引（参阅函数row_ins_duplicate_error_in_clust），隔离级别小于等于RC时，加的是LOCK_REC_NOT_GAP类似的S或者X记录锁。否则加LOCK_ORDINARY类型的记录锁（NEXT-KEY LOCK）； 对于二级唯一索引，若检查到重复键，当前版本总是加 LOCK_ORDINARY 类型的记录锁(函数 row_ins_scan_sec_index_for_duplicate)。实际上按照RC的设计理念，不应该加GAP锁（bug#68021），官方也事实上尝试修复过一次，即对于RC隔离级别加上LOCK_REC_NOT_GAP，但却引入了另外一个问题，导致二级索引的唯一约束失效(bug#73170)，感兴趣的可以参阅我写的这篇博客，由于这个严重bug，官方很快又把这个fix给revert掉了。 外键检查 当我们删除一条父表上的记录时，需要去检查是否有引用约束(row_pd_check_references_constraints)，这时候会扫描子表(dict_table_t::referenced_list)上对应的记录，并加上共享锁。按照实际情况又有所不同。我们举例说明 使用RC隔离级别，两张测试表： 1234create table t1 (a int, b int, primary key(a));create table t2 (a int, b int, primary key (a), key(b), foreign key(b) references t1(a));insert into t1 values (1,2), (2,3), (3,4), (4,5), (5,6), (7,8), (10,11);insert into t2 values (1,2), (2,2), (4,4); 执行SQL：delete from t1 where a = 10; 在t1表记录10上加 LOCKREC_NOT_GAP|LOCK_X 在t2表的supremum记录（表示最大记录）上加 LOCK_ORDINARY|LOCK_S，即锁住(4, ~)区间 执行SQL：delete from t1 where a = 2; 在t1表记录(2,3)上加 LOCK_REC_NOT_GAP|LOCK_X 在t2表记录(1,2)上加 LOCK_REC_NOT_GAP|LOCK_S锁，这里检查到有引用约束，因此无需继续扫描(2,2)就可以退出检查，判定报错。 执行SQL：delete from t1 where a = 3; 在t1表记录(3,4)上加 LOCK_REC_NOT_GAP|LOCK_X 在t2表记录(4,4)上加 LOCK_GAP|LOCK_S锁 另外从代码里还可以看到，如果扫描到的记录被标记删除时，也会加LOCK_ORDINARY|LOCK_S 锁。具体参阅函数row_ins_check_foreign_constraint INSERT … SELECT插入数据时，会对SELECT的表上扫描到的数据加LOCK_S锁 LOCK_X（排他锁） 排他锁的目的主要是避免对同一条记录的并发修改。通常对于UPDATE或者DELETE操作，或者类似SELECT … FOR UPDATE操作，都会对记录加排他锁。 我们以如下表为例： 12create table t1 (a int, b int, c int, primary key(a), key(b));insert into t1 values (1,2,3), (2,3,4),(3,4,5), (4,5,6),(5,6,7); 执行SQL（通过二级索引查询）：update t1 set c = c +1 where b = 3; RC隔离级别：1. 锁住二级索引记录，为NOT GAP X锁；2.锁住对应的聚集索引记录，也是NOT GAP X锁。 RR隔离级别下：1.锁住二级索引记录，为LOCK_ORDINARY|LOCK_X锁；2.锁住聚集索引记录，为NOT GAP X锁 执行SQL（通过聚集索引检索，更新二级索引数据）：update t1 set b = b +1 where a = 2; 对聚集索引记录加 LOCK_REC_NOT_GAP | LOCK_X锁; 在标记删除二级索引时，检查二级索引记录上的锁（lock_sec_rec_modify_check_and_lock），如果存在和LOCK_X | LOCK_REC_NOT_GAP冲突的锁对象，则创建锁对象并返回等待错误码；否则无需创建锁对象； 当到达这里时，我们已经持有了聚集索引上的排他锁，因此能保证别的线程不会来修改这条记录。（修改记录总是先聚集索引，再二级索引的顺序），即使不对二级索引加锁也没有关系。但如果已经有别的线程已经持有了二级索引上的记录锁，则需要等待。 在标记删除后，需要插入更新后的二级索引记录时，依然要遵循插入意向锁的加锁原则。 我们考虑上述两种 SQL 的混合场景，一个是先锁住二级索引记录，再锁聚集索引；另一个是先锁聚集索引，再检查二级索引冲突，因此在这类并发更新场景下，可能会发生死锁。 不同场景，不同隔离级别下的加锁行为都有所不同，例如在RC隔离级别下，不符合WHERE条件的扫描到的记录，会被立刻释放掉，但RR级别则会持续到事务结束。你可以通过GDB，断点函数lock_rec_lock来查看某条SQL如何执行加锁操作。 LOCK_INSERT_INTENTION(插入意向锁) INSERT INTENTION锁是GAP锁的一种，如果有多个session插入同一个GAP时，他们无需互相等待，例如当前索引上有记录4和8，两个并发session同时插入记录6，7。他们会分别为(4,8)加上GAP锁，但相互之间并不冲突（因为插入的记录不冲突）。 当向某个数据页中插入一条记录时，总是会调用函数lock_rec_insert_check_and_lock进行锁检查（构建索引时的数据插入除外），会去检查当前插入位置的下一条记录上是否存在锁对象，这里的下一条记录不是指的物理连续，而是按照逻辑顺序的下一条记录。 如果下一条记录上不存在锁对象：若记录是二级索引上的，先更新二级索引页上的最大事务ID为当前事务的ID；直接返回成功。 如果下一条记录上存在锁对象，就需要判断该锁对象是否锁住了GAP。如果GAP被锁住了，并判定和插入意向GAP锁冲突，当前操作就需要等待，加的锁类型为LOCK_X | LOCK_GAP | LOCK_INSERT_INTENTION，并进入等待状态。但是插入意向锁之间并不互斥。这意味着在同一个GAP里可能有多个申请插入意向锁的会话。 锁表更新 我们知道GAP锁是在一个记录上描述的，表示记录及其之前的记录之间的GAP。但如果记录之前发生了插入或者删除操作，之前描述的GAP就会发生变化，InnoDB需要对锁表进行更新。 对于数据插入，假设我们当前在记录[3,9]之间有会话持有锁(不管是否和插入意向锁冲突)，现在插入一条新的记录5，需要调用函数lock_update_insert。这里会遍历所有在记录9上的记录锁，如果这些锁不是插入意向锁并且是LOCK_GAP或者NEXT-KEY LOCK（没有设置LOCK_REC_NOT_GAP标记)(lock_rec_inherit_to_gap_if_gap_lock)，就会为这些会话的事务增加一个新的锁对象，锁的类型为LOCK_REC | LOCK_GAP，锁住的GAP范围在本例中为(3,5)。所有符合条件的会话都继承了这个新的GAP，避免之前的GAP锁失效。 对于数据删除操作，调用函数lock_update_delete，这里会遍历在被删除记录上的记录锁，当符合如下条件时，需要为这些锁对应的事务增加一个新的GAP锁，锁的Heap No为被删除记录的下一条记录： 123456789101112131415161718lock_rec_inherit_to_gap for (lock = lock_rec_get_first(lock_sys-&gt;rec_hash, block, heap_no); lock != NULL; lock = lock_rec_get_next(heap_no, lock)) &#123; if (!lock_rec_get_insert_intention(lock) &amp;&amp; !((srv_locks_unsafe_for_binlog || lock-&gt;trx-&gt;isolation_level &lt;= TRX_ISO_READ_COMMITTED) &amp;&amp; lock_get_mode(lock) == (lock-&gt;trx-&gt;duplicates ? LOCK_S : LOCK_X))) &#123; lock_rec_add_to_queue( LOCK_REC | LOCK_GAP | lock_get_mode(lock), heir_block, heir_heap_no, lock-&gt;index, lock-&gt;trx, FALSE); &#125; &#125; 从上述判断可以看出，即使在RC隔离级别下，也有可能继承LOCK GAP锁，这也是当前版本InnoDB唯一的意外：判断Duplicate key时目前容忍GAP锁。上面这段代码实际上在最近的版本中才做过更新，更早之前的版本可能存在二级索引损坏，感兴趣的可以阅读我的这篇博客 完成GAP锁继承后，会将所有等待该记录的锁对象全部唤醒(lock_rec_reset_and_release_wait)。 LOCK_PREDICATE 从 MySQL5.7 开始MySQL整合了boost.geometry库以更好的支持空间数据类型，并支持在在Spatial数据类型的列上构建索引，在InnoDB内，这个索引和普通的索引有所不同，基于R-TREE的结构，目前支持对2D数据的描述，暂不支持3D. R-TREE和BTREE不同，它能够描述多维空间，而多维数据并没有明确的数据顺序，因此无法在RR隔离级别下构建NEXT-KEY锁以避免幻读，因此InnoDB使用称为Predicate Lock的锁模式来加锁，会锁住一块查询用到的被称为MBR(minimum boundingrectangle/box)的数据区域。 因此这个锁不是锁到某个具体的记录之上的，可以理解为一种Page级别的锁。 Predicate Lock和普通的记录锁或者表锁（如上所述）存储在不同的lock hash中，其相互之间不会产生冲突。 Predicate Lock相关代码见lock/lock0prdt.cc文件 关于Predicate Lock的设计参阅官方WL#6609。 由于这块的代码量比较庞大，目前小编对InnoDB的spatial实现了解有限，本文暂不对此展开，将在后面单独专门介绍spatial index时，再细细阐述这块内容。 隐式锁 InnoDB 通常对插入操作无需加锁，而是通过一种“隐式锁”的方式来解决冲突。聚集索引记录中存储了事务id，如果另外有个session查询到了这条记录，会去判断该记录对应的事务id是否属于一个活跃的事务，并协助这个事务创建一个记录锁，然后将自己置于等待队列中。该设计的思路是基于大多数情况下新插入的记录不会立刻被别的线程并发修改，而创建锁的开销是比较昂贵的，涉及到全局资源的竞争。 关于隐式锁转换，上一期的月报InnoDB 事务子系统介绍我们已经介绍过了，这里不再赘述。 锁的冲突判定 锁模式的兼容性矩阵通过如下数组进行快速判定： 123456789static const byte lock_compatibility_matrix[5][5] = &#123;/** IS IX S X AI // IS / &#123; TRUE, TRUE, TRUE, FALSE, TRUE&#125;,/ IX / &#123; TRUE, TRUE, FALSE, FALSE, TRUE&#125;,/ S / &#123; TRUE, FALSE, TRUE, FALSE, FALSE&#125;,/ X / &#123; FALSE, FALSE, FALSE, FALSE, FALSE&#125;,/ AI / &#123; TRUE, TRUE, FALSE, FALSE, FALSE&#125;&#125;; 对于记录锁而言，锁模式只有LOCK_S 和LOCK_X，其他的 FLAG 用于锁的描述，如前述 LOCK_GAP、LOCK_REC_NOT_GAP 以及 LOCK_ORDINARY、LOCK_INSERT_INTENTION 四种描述。在比较两个锁是否冲突时，即使不满足兼容性矩阵，在如下几种情况下，依然认为是相容的，无需等待（参考函数lock_rec_has_to_wait） 对于GAP类型（锁对象建立在supremum上或者申请的锁类型为LOCK_GAP）且申请的不是插入意向锁时，无需等待任何锁，这是因为不同Session对于相同GAP可能申请不同类型的锁，而GAP锁本身设计为不互相冲突； LOCK_ORDINARY 或者LOCK_REC_NOT_GAP类型的锁对象，无需等待LOCK_GAP类型的锁； LOCK_GAP类型的锁无需等待LOCK_REC_NOT_GAP类型的锁对象； 任何锁请求都无需等待插入意向锁。 表级锁 InnoDB的表级别锁包含五种锁模式：LOCK_IS、LOCK_IX、LOCK_X、LOCK_S以及LOCK_AUTO_INC锁，锁之间的相容性遵循数组lock_compatibility_matrix中的定义。 InnoDB表级锁的目的是为了防止DDL和DML的并发问题。但从5.5版本开始引入MDL锁后，InnoDB层的表级锁的意义就没那么大了，MDL锁本身已经覆盖了其大部分功能。以下我们介绍下几种InnoDB表锁类型。 LOCK_IS/LOCK_IX 也就是所谓的意向锁，这实际上可以理解为一种“暗示”未来需要什么样行级锁，IS表示未来可能需要在这个表的某些记录上加共享锁，IX表示未来可能需要在这个表的某些记录上加排他锁。意向锁是表级别的，IS和IX锁之间相互并不冲突，但与表级S/X锁冲突。 在对记录加S锁或者X锁时，必须保证其在相同的表上有对应的意向锁或者锁强度更高的表级锁。 LOCK_X 当加了LOCK_X表级锁时，所有其他的表级锁请求都需要等待。通常有这么几种情况需要加X锁： DDL操作的最后一个阶段(ha_innobase::commit_inlace_alter_table)对表上加LOCK_X锁，以确保没有别的事务持有表级锁。通常情况下Server层MDL锁已经能保证这一点了，在DDL的commit 阶段是加了排他的MDL锁的。但诸如外键检查或者刚从崩溃恢复的事务正在进行某些操作，这些操作都是直接InnoDB自治的，不走server层，也就无法通过MDL所保护； 当设置会话的autocommit变量为OFF时，执行LOCK TABLE tbname WRITE这样的操作会加表级的LOCK_X锁(ha_innobase::external_lock)； 对某个表空间执行discard或者import操作时，需要加LOCK_X锁(ha_innobase::discard_or_import_tablespace)。 LOCK_S 在DDL的第一个阶段，如果当前DDL不能通过ONLINE的方式执行，则对表加LOCK_S锁(prepare_inplace_alter_table_dict)； 设置会话的autocommit为OFF，执行LOCK TABLE tbname READ时，会加LOCK_S锁(ha_innobase::external_lock)。 从上面的描述我们可以看到LOCK_X及LOCK_S锁在实际的大部分负载中都很少会遇到。主要还是互相不冲突的LOCK_IS及LOCK_IX锁。一个有趣的问题是，每次加表锁时，却总是要扫描表上所有的表级锁对象，检查是否有冲突的锁。很显然，如果我们在同一张表上的更新并发度很高，这个链表就会非常长。 基于大多数表锁不冲突的事实，我们在RDS MYSQL中对各种表锁对象进行计数，在检查是否有冲突时，例如当前申请的是意向锁，如果此时LOCK_S和LOCK_X的锁计数都是0，就可以认为没有冲突，直接忽略检查。由于检查是在持有全局大锁lock_sys-&gt;mutex下进行的。在单表大并发下，这个优化的效果还是非常明显的，可以减少持有全局大锁的时间。 LOCK_AUTO_INC AUTO_INC锁加在表级别，和AUTO_INC、表级S锁以及X锁不相容。锁的范围为SQL级别，SQL结束后即释放。AUTO_INC的加锁逻辑和InnoDB的锁模式相关，这里在简单介绍一下。 通常对于自增列，我们既可以显式指定该值，也可以直接用NULL，系统将自动递增并填充该列。我们还可以在批量插入时混合使用者两种方式。不同的分配方式，其具体行为受到参数innodb_autoinc_lock_mode的影响。但在基于STATEMENT模式复制时，可能会影响到复制的数据一致性，官方文档 有详细描述，不再赘述，只说明下锁的影响。 自增锁模式通过参数innodb_autoinc_lock_mode来控制，加锁选择参阅函数ha_innobase::innobase_lock_autoinc 具体的，有以下几个值： AUTOINC_OLD_STYLE_LOCKING（0） 也就是所谓的传统加锁模式（在5.1版本引入这个参数之前的策略），在该策略下，会在分配前加上AUTO_INC锁，并在SQL结束时释放掉。该模式保证了在STATEMENT复制模式下，备库执行类似INSERT … SELECT这样的语句时的一致性，因为这样的语句在执行时无法确定到底有多少条记录，只有在执行过程中不允许别的会话分配自增值，才能确保主备一致。 很显然这种锁模式非常影响并发插入的性能，但却保证了一条SQL内自增值分配的连续性。 AUTOINC_NEW_STYLE_LOCKING（1） 这是InnoDB的默认值。在该锁模式下 普通的 INSERT 或 REPLACE 操作会先加一个dict_table_t::autoinc_mutex，然后去判断表上是否有别的线程加了LOCK_AUTO_INC锁，如果有的话，释放autoinc_mutex，并使用OLD STYLE的锁模式。否则，在预留本次插入需要的自增值之后，就快速的将autoinc_mutex释放掉。很显然，对于普通的并发INSERT操作，都是无需加LOCK_AUTO_INC锁的。因此大大提升了吞吐量； 但是对于一些批量插入操作，例如LOAD DATA，INSERT …SELECT 等还是使用OLD STYLE的锁模式，SQL执行期间加LOCK_AUTO_INC锁。 和传统模式相比，这种锁模式也能保证STATEMENT模式下的复制安全性，但却无法保证一条插入语句内的自增值的连续性，并且在执行一条混合了显式指定自增值和使用系统分配两种方式的插入语句时，可能存在一定的自增值浪费。 例如执行SQL： 1INSERT INTO t1 (c1,c2) VALUES (1,&#x27;a&#x27;), (NULL,&#x27;b&#x27;), (5,&#x27;c&#x27;), (NULL,’d’） 假设当前AUTO_INCREMENT值为101，在传统模式下执行完后，下一个自增值为103，而在新模式下，下一个可用的自增值为105，因为在开始执行SQL时，会先预取了[101, 104] 4个自增值，这和插入行的个数相匹配，然后将AUTO_INCREMENT设为105，导致自增值103和104被浪费掉。 AUTOINC_NO_LOCKING（2） 这种模式下只在分配时加个mutex即可，很快就释放，不会像NEW STYLE那样在某些场景下会退化到传统模式。因此设为2不能保证批量插入的复制安全性。 关于自增锁的小BUG 这是Mariadb的Jira上报的一个小bug，在row模式下，由于不走parse的逻辑，我们不知道行记录是通过什么批量导入还是普通INSERT产生的，因此command类型为SQLCOM_END，而在判断是否加自增锁时的逻辑时，是通过COMMAND类型是否为SQLCOM_INSERT或者SQLCOM_REPLACE来判断是否忽略加AUTO_INC锁。这个额外的锁开销，会导致在使用ROW模式时，InnoDB总是加AUTO_INC锁，加AUTO_INC锁又涉及到全局事务资源的开销，从而导致性能下降。 修复的方式也比较简单，将SQLCOM_END这个command类型也纳入考虑。 具体参阅Jira链接。 事务锁管理 InnoDB 所有的事务锁对象都是挂在全局对象lock_sys上，同时每个事务对象上也维持了其拥有的事务锁，每个表对象(dict_table_t)上维持了构建在其上的表级锁对象。 如下图所示： innodb 锁 加表级锁 首先从当前事务的trx_lock_t::table_locks中查找是否已经加了等同或更高级别的表锁，如果已经加锁了，则直接返回成功（lock_table_has）； 检查当前是否有和正在申请的锁模式冲突的表级锁对象（lock_table_other_has_incompatible）； 直接遍历链表dict_table_t::locks链表 如果存在冲突的锁对象，则需要进入等待队列（lock_table_enqueue_waiting） 创建等待锁对象 （lock_table_create） 检查是否存在死锁（DeadlockChecker::check_and_resolve），当存在死锁时：如果当前会话被选作牺牲者，就移除锁请求(lock_table_remove_low)，重置当前事务的wait_lock为空，并返回错误码DB_DEADLOCK；若被选成胜利者，则锁等待解除，可以认为当前会话已经获得了锁，返回成功； 若没有发生死锁，设置事务对象的相关变量后，返回错误码DB_LOCK_WAIT，随后进入锁等待状态 如果不存在冲突的锁，则直接创建锁对象（lock_table_create），加入队列。 lock_table_create: 创建锁对象 当前请求的是AUTO-INC锁时； 递增dict_table_t::n_waiting_or_granted_auto_inc_locks。前面我们已经提到过，当这个值非0时，对于自增列的插入操作就会退化到OLD-STYLE; 锁对象直接引用已经预先创建好的dict_table_t::autoinc_lock，并加入到trx_t::autoinc_locks集合中; 对于非AUTO-INC锁，则从一个pool中分配锁对象 在事务对象trx_t::lock中，维持了两个pool，一个是trx_lock_t::rec_pool，预分配了一组锁对象用于记录锁分配，另外一个是trx_lock_t::table_pool，用于表级锁的锁对象分配。通过预分配内存的方式，可以避免在持有全局大锁时(lock_sys-&gt;mutex)进行昂贵的内存分配操作。rec_pool和table_pool预分配的大小都为8个锁对象。（lock_trx_alloc_locks）; 如果table_pool已经用满，则走内存分配，创建一个锁对象； 构建好的锁对象分别加入到事务的trx_t::lock.trx_locks链表上以及表对象的dict_table_t::locks链表上； 构建好的锁对象加入到当前事务的trx_t::lock.table_locks集合中。 可以看到锁对象会加入到不同的集合或者链表中，通过挂载到事务对象上，可以快速检查当前事务是否已经持有表锁；通过挂到表对象的锁链表上，可以用于检查该表上的全局冲突情况。 加行级锁 行级锁加锁的入口函数为lock_rec_lock，其中第一个参数impl如果为TRUE，则当当前记录上已有的锁和LOCK_X | LOCK_REC_NOT_GAP不冲突时，就无需创建锁对象。（见上文关于记录锁LOCK_X相关描述部分），为了描述清晰，下文的流程描述，默认impl为FALSE。 lock_rec_lock： 首先尝试fast lock的方式，对于冲突少的场景，这是比较普通的加锁方式(lock_rec_lock_fast), 符合如下情况时，可以走fast lock: 记录所在的page上没有任何记录锁时，直接创建锁对象，加入rec_hash，并返回成功; 记录所在的page上只存在一个记录锁，并且属于当前事务，且这个记录锁预分配的bitmap能够描述当前的heap no（预分配的bit数为创建锁对象时的page上记录数 + 64，参阅函数RecLock::lock_size），则直接设置对应的bit位并返回; 无法走fast lock时，再调用slow lock的逻辑(lock_rec_lock_slow) 判断当前事务是否已经持有了一个优先级更高的锁，如果是的话，直接返回成功（lock_rec_has_expl）; 检查是否存在和当前申请锁模式冲突的锁（lock_rec_other_has_conflicting），如果存在的话，就创建一个锁对象（RecLock::RecLock），并加入到等待队列中（RecLock::add_to_waitq），这里会进行死锁检测; 如果没有冲突的锁，则入队列（lock_rec_add_to_queue）：已经有在同一个Page上的锁对象且没有别的会话等待相同的heap no时，可以直接设置对应的bitmap（lock_rec_find_similar_on_page）；否则需要创建一个新的锁对象; 返回错误码，对于DB_LOCK_WAIT, DB_DEADLOCK等错误码，会在上层进行处理。 等待及死锁判断 当发现有冲突的锁时，调用函数RecLock::add_to_waitq进行判断 如果持有冲突锁的线程是内部的后台线程（例如后台dict_state线程），这个线程不会被一个高优先级的事务取消掉，因为总是优先保证内部线程正常执行； 比较当前会话和持有锁的会话的事务优先级，调用函数trx_arbitrate 返回被选作牺牲者的事务； 当前发起请求的会话是后台线程，但持有锁的会话设置了高优先级时，选择当前线程作为牺牲者； 持有锁的线程为后台线程时，在第一步已经判断了，不会选作牺牲者； 如果两个会话都设置了优先级，低优先级的被选做牺牲者，优先级相同时，请求者被选做牺牲者(thd_tx_arbitrate)； PS: 目前最新版本的5.7还不支持用户端设置线程优先级（但增加一个配置session变量的接口非常容易)； 如果当前会话的优先级较低，或者另外一个持有锁的会话为后台线程，这时候若当前会话设置了优先级，直接报错，并返回错误码DB_DEADLOCK； 默认不设置优先级时，请求锁的会话也会被选作victim_trx，但只创建锁等待对象，不会直接返回错误； 当持有锁的会话被选作牺牲者时，说明当前会话肯定设置了高优先级，这时候会走RecLock::enqueue_priority的逻辑； 如果持有锁的会话在等待另外一个不同的锁时，或者持有锁的事务不是readonly的，当前会话会被回滚掉； 开始跳队列，直到当前会话满足加锁条件（RecLock::jump_queue）； 请求的锁对象跳过阻塞它的锁对象，直接操作hash链表，将锁对象往前挪； 从当前lock，向前遍历链表，逐个判断是否有别的会话持有了相同记录上的锁（RecLock::is_on_row），并将这些会话标记为回滚（mark_trx_for_rollback）,同时将这些事务对象搜集下来，以待后续处理（但直接阻塞当前会话的事务会被立刻回滚掉）； 高优先级的会话非常具有杀伤力，其他低优先级会话即使拿到了锁，也会被它所干掉。 不过实际场景中，我们并没有多少机会去设置事务的优先级，这里先抛开这个话题，只考虑默认的场景，即所有的事务优先级都未设置。 在创建了一个处于WAIT状态的锁对象后，我们需要进行死锁检测（RecLock::deadlock_check），死锁检测采用深度优先遍历的方式，通过事务对象上的trx_t::lock.wait_lock构造事务的wait-for graph进行判断，当最终发现一个锁请求等待闭环时，可以判定发生了死锁。另外一种情况是，如果检测深度过长（即锁等待的会话形成的检测链路非常长），也会认为发生死锁，最大深度默认为LOCK_MAX_DEPTH_IN_DEADLOCK_CHECK，值为200。 当发生死锁时，需要选择一个牺牲者（DeadlockChecker::select_victim()）来解决死锁，通常事务权重低的回滚（trx_weight_ge）。 修改了非事务表的会话具有更高的权重； 如果两个表都修改了、或者都没有修改事务表，那么就根据的事务的undo数量加上持有的事务锁个数来决定权值（TRX_WEIGHT）； 低权重的事务被回滚，高权重的获得锁对象。 Tips：对于一个经过精心设计的应用，我们可以从业务上避免死锁，而死锁检测本身是通过持有全局大锁来进行的，代价非常高昂，在阿里内部的应用中，由于有专业的团队来保证业务SQL的质量，我们可以选择性的禁止掉死锁检测来提升性能，尤其是在热点更新场景，带来的性能提升非常明显，极端高并发下，甚至能带来数倍的提升。 当无法立刻获得锁时，会将错误码传到上层进行处理（row_mysql_handle_errors） DB_LOCK_WAIT： 具有高优先级的事务已经搜集了会阻塞它的事务链表，这时候会统一将这些事务回滚掉（trx_kill_blocking）； 将当前的线程挂起（lock_wait_suspend_thread），等待超时时间取决于session级别配置（innodb_lock_wait_timeout），默认为50秒； 如果当前会话的状态设置为running，一种是被选做死锁检测的牺牲者，需要回滚当前事务，另外一种是在进入等待前已经获得了事务锁，也无需等待； 获得等待队列的一个空闲slot。（lock_wait_table_reserve_slot） 系统启动时，已经创建好了足够用的slot数组，类型为srv_slot_t，挂在lock_sys-&gt;waiting_threads上； 分配slot时，从slot数组的第一个元素开始遍历，直到找到一个空闲的slot。注意这里存在的一个性能问题是，如果挂起的线程非常多，每个新加入挂起等待的线程都需要遍历直到找到一个空闲的slot。 实际上如果每次遍历都从上次分配的位置往后找，到达数组末尾在循环到数组头，这样可以在高并发高锁冲突场景下获得一定的性能提升； 如果会话在innodb层（通常为true），则强制从InnoDB层退出，确保其不占用innodb_thread_concurrency的槽位。然后进入等待状态。被唤醒后，会再次强制进入InnoDB层； 被唤醒后，释放slot（lock_wait_table_release_slot）； 若被选作死锁的牺牲者了，返回上层回滚事务；若等待超时了，则根据参数innodb_rollback_on_timeout的配置，默认为OFF只回滚当前SQL，设置为ON表示回滚整个事务。 DB_DEADLOCK: 直接回滚当前事务 释放锁及唤醒 大多数情况下事务锁都是在事务提交时释放，但有两种意外： AUTO-INC锁在SQL结束时直接释放（innobase_commit --&gt; lock_unlock_table_autoinc）； 在RC隔离级别下执行DML语句时，从引擎层返回到Server层的记录，如果不满足where条件，则需要立刻unlock掉（ha_innobase::unlock_row）。 除这两种情况外，其他的事务锁都是在事务提交时释放的(lock_trx_release_locks --&gt; lock_release)。事务持有的所有锁都维护在链表trx_t::lock.trx_locks上，依次遍历释放即可。 对于行锁，从全局hash中删除后，还需要判断别的正在等待的会话是否可以被唤醒（lock_rec_dequeue_from_page）。例如如果当前释放的是某个记录的X锁，那么所有的S锁请求的会话都可以被唤醒。 这里的移除锁和检查的逻辑开销比较大，尤其是大量线程在等待少量几个锁时。当某个锁从hash链上移除时，InnoDB实际上通过遍历相同page上的所有等待的锁，并判断这些锁等待是否可以被唤醒。而判断唤醒的逻辑又一次遍历，这是因为当前的链表维护是基于&lt;space, page no&gt;的，并不是基于Heap no构建的。关于这个问题的讨论，可以参阅bug#53825。官方开发Sunny也提到虽然使用&lt;space, page no, heap no&gt;来构建链表，移除Bitmap会浪费更多的内存，但效率更高，而且现在的内存也没有以前那么昂贵。 对于表锁，如果表级锁的类型不为LOCK_IS，且当前事务修改了数据，就将表对象的dict_table_t::query_cache_inv_id设置为当前最大的事务id。在检查是否可以使用该表的Query Cache时会使用该值进行判断（row_search_check_if_query_cache_permitted），如果某个用户会话的事务对象的low_limit_id（即最大可见事务id）比这个值还小，说明它不应该使用当前table cache的内容，也不应该存储到query cache中。 表级锁对象的释放调用函数lock_table_dequeue。 注意在释放锁时，如果该事务持有的锁对象太多，每释放1000（LOCK_RELEASE_INTERVAL）个锁对象，会暂时释放下lock_sys-&gt;mutex再重新持有，防止InnoDB hang住。 两个有趣的案例 本小节我们来分析几个比较有趣的死锁案例。 普通的并发插入导致的死锁 create table t1 (a int primary key); 开启三个会话执行： insert into t1(a) values (2); session 1 session 2 session 3 BEGIN; INSERT… INSERT (block),为session1创建X锁，并等待S锁 INSERT (block， 同上等待S锁) ROLLBACK，释放锁 获得S锁 获得S锁 申请插入意向X锁，等待session3 申请插入意向X锁，等待session2 上述描述了互相等待的场景，因为插入意向X锁和S锁是不相容的。这也是一种典型的锁升级导致的死锁。如果session1执行COMMIT的话，则另外两个线程都会因为duplicate key失败。 这里需要解释下为何要申请插入意向锁，因为ROLLBACK时原记录回滚时是被标记删除的。而我们尝试插入的记录和这个标记删除的记录是相邻的(键值相同)，根据插入意向锁的规则，插入位置的下一条记录上如果存在与插入意向X锁冲突的锁时，则需要获取插入意向X锁。 另外一种类似（但产生死锁的原因不同）的场景是在一张同时存在聚集索引和唯一索引的表上，通过replace into的方式插入冲突的唯一键，可能会产生死锁，在3月份的月报，我已经专门描述过这个问题，感兴趣的可以延伸阅读下。 又一个并发插入的死锁现象 两个会话参与。在RR隔离级别下 例表如下： 12create table t1 (a int primary key ,b int);insert into t1 values (1,2),(2,3),(3,4),(11,22); session 1 session 2 begin;select * from t1 where a = 5 for update;(获取记录(11,22)上的GAP X锁) begin;select * from t1 where a = 5 for update; (同上,GAP锁之间不冲突 insert into t1 values (4,5); (block，等待session1) insert into t1 values (4,5);（需要等待session2，死锁） 引起这个死锁的原因是非插入意向的GAP X锁和插入意向X锁之间是冲突的。 本文地址：http://xnerv.wang/introduction-of-innodb-transaction-lock/ 转载自：InnoDB 事务锁系统简介","categories":[{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/categories/DBMS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/tags/DBMS/"},{"name":"MySQL","slug":"MySQL","permalink":"https://xnerv.wang/tags/MySQL/"},{"name":"Transaction","slug":"Transaction","permalink":"https://xnerv.wang/tags/Transaction/"},{"name":"InnoDB","slug":"InnoDB","permalink":"https://xnerv.wang/tags/InnoDB/"},{"name":"Lock","slug":"Lock","permalink":"https://xnerv.wang/tags/Lock/"}]},{"title":"MySQL 5.6中Binlog Group Commit实现（转载）","slug":"implement-of-binlog-group-commit-in-mysql56","date":"2018-03-17T05:19:00.000Z","updated":"2023-08-21T02:24:19.147Z","comments":true,"path":"implement-of-binlog-group-commit-in-mysql56/","link":"","permalink":"https://xnerv.wang/implement-of-binlog-group-commit-in-mysql56/","excerpt":"背景 在MySQL 5.1中，如果配置项sync_binlog=1，并且innodb_flush_log_at_trx_commit=1，那么MySQL的TPS将会下降到几十每秒，完全不可接受。这是因为InnoDB提交事务时，不仅需要将REDO刷盘，还需要将Binlog刷盘，每个事务都需要2次sync操作。机械磁盘的IOPS也就为几百的水平，所以InnoDB的性能极差。 这个问题，在MySQL 5.6中得到了比较好的解决。在了解Binlog Group Commit之前，需要先了解MySQL Binlog和InnoDB的两阶段提交。MySQL为了保证主库和从库的数据一致性，就必须保证Binlog和InnoDB的一致性，即如果一个事务写入了Binlog，InnoDB中就必须提交该事务；相反，如果一个事务没有写入Binlog，InnoDB就不能提交该事务。做法是：","text":"背景 在MySQL 5.1中，如果配置项sync_binlog=1，并且innodb_flush_log_at_trx_commit=1，那么MySQL的TPS将会下降到几十每秒，完全不可接受。这是因为InnoDB提交事务时，不仅需要将REDO刷盘，还需要将Binlog刷盘，每个事务都需要2次sync操作。机械磁盘的IOPS也就为几百的水平，所以InnoDB的性能极差。 这个问题，在MySQL 5.6中得到了比较好的解决。在了解Binlog Group Commit之前，需要先了解MySQL Binlog和InnoDB的两阶段提交。MySQL为了保证主库和从库的数据一致性，就必须保证Binlog和InnoDB的一致性，即如果一个事务写入了Binlog，InnoDB中就必须提交该事务；相反，如果一个事务没有写入Binlog，InnoDB就不能提交该事务。做法是： InnoDB先执行Prepare，将Redo日志写磁盘。然后再将Binlog写磁盘，最后InnoDB再执行Commit，将事务标记为提交。这样，可以保证Binlog和InnoDB的一致性。具体原因，可以分三种情况考虑： 情况1： 如果MySQL在InnoDB Prepare阶段Crash。MySQL在启动时做崩溃恢复，InnoDB会回滚这些事务，同时由于事务也没有写到binlog，InnoDB和Binlog一致。 情况2： 如果MySQL在Binlog写磁盘阶段Crash。MySQL在启动时做崩溃恢复，在恢复时会扫描未成功提交的事务，和当时未成功关闭的binlog文件，如果事务已经Prepare了，并且也已经在Binlog中了，InnoDB会提交该事务；相反，如果事务已经在Prepare中了，但是不在Binlog中，InnoDB会回滚该事务。结果就是InnoDB和Binlog一致。 情况3： 如果MySQL在InnoDB执行Commit阶段Crash，和情况2类似，由于事务已经成功Prepare，并且存在Binlog文件中，InnoDB在崩溃恢复时，仍然会提交该事务，确保Binlog和InnoDB一致。 MySQL在实现时，将mysql_bin_log作为2阶段提交的协调者，可以参考MySQL的代码：sql/handler.cc:ha_commit_trans。内部分别调用tc_log-&gt;prepare()和tc_log-&gt;commit()实现2阶段提交，这里的tc_log就是MySQL源码中的全局对象mysql_bin_log。 伪代码如下： 12345678910111213ha_commit_trans() --&gt; tc_log-&gt;prepare() --&gt; ha_prepare_low() for () &#123; ht-&gt;prepare() //存储引擎 hton-&gt;prepare() &#125; --&gt; tc_log-&gt;commit() --&gt; MYSQL_BINLOG::ordered_commit()//做Group Commit --&gt; MYSQL_BINLOG::process_commit_stage_queue() //Group Commit的Commit阶段，会调用InnoDB提交 --&gt; ha_commit_low() for () &#123; ht-&gt;commit(); //存储引擎 hton-&gt;commit() &#125; 两阶段提交的参与者分别为：binlog_hton和innobase_hton，它们实现了MySQL的存储引擎接口。如果你再深入调研一下，就会发现binlog_hton在2阶段提交时，啥也没干。所有binlog操作都是由协调者mysql_bin_log干的，包括Group Commit，也都是在mysql_bin_log中实现的。下面我们就来分析一下，mysql_bin_log是如何做到Group Commit的，也就是上面的函数ordered_commit()。 实现 和Level DB的Group Commit类似，MySQL的Group Commit也是维护了一个队列，第一个进入队列的线程就是Leader，负责写binlog。其他的线程是Flower，Flower不需要操作，只需要等待完成的通知即可。但是如果只用一个队列的话，在Group Commit进行中的时候，后来的线程就得等待，还可以进一步优化，MySQL把这个过程分裂成了3个阶段：FLUSH_STAGE，SYNC_STAGE和COMMIT_STAGE。它们像流水线一样工作，每个阶段都会涉及一批事务，它们组成一个Group。可以这样理解，事务刚提交时，处于FLUSH阶段，同时处于FLUSH阶段的事务为一个队列，形成一个Group，只有队列的头，Leader在干活，FLUSH完成以后，Leader进入SYNC阶段（所有的Flower也都进入SYNC阶段）。这时，新提交的事务可以进入FLUSH阶段，它们又会产生一个新的Leader，如此不断的推进。每个阶段都需要一个队列，所以MySQL在Group Commit时，需要3个队列。如下图所示，队列通过thd-&gt;next_to_commit连接： MySQL把队列命名为Mutex_queue，这是一个C++的类，定义如下： 12345class Mutex_queue &#123; THD *m_first; //队列的头指针 THD **m_last; //队列尾指针的地址。如果队列为空，相当于&amp;m_first，否则，相当于&amp;last-&gt;next_to_commit mysql_mutex_t m_lock;&#125;; 在Group Commit时，事务的状态首先转为FLUSH_STAGE，然后为SYNC_STAGE，最后为COMMIT_STAGE。在状态转变时，都会调用如下函数Stage_manager::enroll_for： 123456789101112131415161718192021222324bool Stage_manager::enroll_for(StageID stage, THD *thd, mysql_mutex_t *stage_mutex) &#123; // 只有队列的第一个元素为Leader，其他情况均为false bool leader= m_queue[stage].append(thd); // The stage mutex can be NULL if we are enrolling for the first stage. if (stage_mutex) mysql_mutex_unlock(stage_mutex); /** * 如果不是Leader的话，只需等待Leader完成操作的通知 * Leader完成以后，会设置thd-&gt;transaction.flags.pending = false */ if (!leader) &#123; mysql_mutex_lock(&amp;m_lock_done); while (thd-&gt;transaction.flags.pending) mysql_cond_wait(&amp;m_cond_done, &amp;m_lock_done); mysql_mutex_unlock(&amp;m_lock_done); &#125; return leader;&#125; 从上面的代码可以看出，Flower线程什么也不干，所有的事情都要靠Leader去做。上述代码有一个细节需要注意，先把自己添加到队列中，然后再释放锁stage_mutex，这个在后面会有解释。下面逐个分析一下，在每个阶段Leader线程所做的事情。 FLUSH阶段 因为InnoDB在事务执行过程中，要保证事务的原子性。对于INSERT/UPDATE/DELETE操作，会先将Binlog写事务日志（binlog_cache_mngr），事务提交时，也就是在FLUSH阶段，再把事务日志复制到binlog文件中，然后通知Dump线程去发送binlog，由于要写Binlog文件，这个过程需要锁定LOCK_log锁。这也就是FLUSH阶段要做的事情，可参考函数：MYSQL_BIN_LOG::process_flush_stage_queue。 在这个阶段，Leader线程遍历遍历FLUSH_STAGE链表，依次取出thd对应的事务日志，并写到binlog的IOCACHE中，然后flush IOCACHE。代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647int MYSQL_BIN_LOG::ordered_commit(THD *thd, bool all, bool skip_commit)&#123; //... /* Stage #1: flushing transactions to binary log While flushing, we allow new threads to enter and will process them in due time. Once the queue was empty, we cannot reap anything more since it is possible that a thread entered and appointed itself leader for the flush phase. */ if (change_stage(thd, Stage_manager::FLUSH_STAGE, thd, NULL, &amp;LOCK_log)) &#123; DBUG_PRINT(&quot;return&quot;, (&quot;Thread ID: %lu, commit_error: %d&quot;, thd-&gt;thread_id, thd-&gt;commit_error)); DBUG_RETURN(finish_commit(thd)); &#125; THD *wait_queue= NULL; flush_error= process_flush_stage_queue(&amp;total_bytes, &amp;do_rotate, &amp;wait_queue); my_off_t flush_end_pos= 0; if (flush_error == 0 &amp;&amp; total_bytes &gt; 0) flush_error= flush_cache_to_file(&amp;flush_end_pos); /* If the flush finished successfully, we can call the after_flush hook. Being invoked here, we have the guarantee that the hook is executed before the before/after_send_hooks on the dump thread preventing race conditions among these plug-ins. */ if (flush_error == 0) &#123; const char *file_name_ptr= log_file_name + dirname_length(log_file_name); DBUG_ASSERT(flush_end_pos != 0); if (RUN_HOOK(binlog_storage, after_flush, (thd, file_name_ptr, flush_end_pos))) &#123; sql_print_error(&quot;Failed to run &#x27;after_flush&#x27; hooks&quot;); flush_error= ER_ERROR_ON_WRITE; &#125; signal_update(); DBUG_EXECUTE_IF(&quot;crash_commit_after_log&quot;, DBUG_SUICIDE();); &#125;&#125; 在这个过程中有一个问题需要考虑，就是：一方面，Leader线程从链表中取出thd，将日志写binlog IOCACHE，另一方面，新提交的事务仍然会往FLUSH_STAGE链表中添加thd。如果MySQL的并发事务比较多，Leader线程写binlog的速度，小于新事务的提交速度，可能会造成事务停留在FLUSH阶段的时间过长。所以MySQL通过配置项binlog_max_flush_queue_time来控制这个时间，如果Leader线程在取THD时，发现超时了，Leader线程就将队列整个端走，再做处理。这样，当前已经处于FLUSH阶段的事务还用现在的Leader，而新提交的事务，会用新的Leader。因为LOCK_log锁的存在，所有新的Leader只能等当前的FLUSH执行完成才能开始执行。具体代码如下： 1234567891011121314151617181920212223242526272829303132333435int MYSQL_BIN_LOG::process_flush_stage_queue(my_off_t *total_bytes_var, bool *rotate_var, THD **out_queue_var) bool has_more= true; THD *first_seen= NULL; while ((max_udelay == 0 || my_micro_time() &lt; start_utime + max_udelay) &amp;&amp; has_more) &#123; std::pair&lt;bool,THD*&gt; current= stage_manager.pop_front(Stage_manager::FLUSH_STAGE); std::pair&lt;int,my_off_t&gt; result= flush_thread_caches(current.second); has_more= current.first; total_bytes+= result.second; if (flush_error == 1) flush_error= result.first; if (first_seen == NULL) first_seen= current.second; &#125; /* Either the queue is empty, or we ran out of time. If we ran out of time, we have to fetch the entire queue (and flush it) since otherwise the next batch will not have a leader. */ if (has_more) &#123; THD *queue= stage_manager.fetch_queue_for(Stage_manager::FLUSH_STAGE); for (THD *head= queue ; head ; head = head-&gt;next_to_commit) &#123; std::pair&lt;int,my_off_t&gt; result= flush_thread_caches(head); total_bytes+= result.second; if (flush_error == 1) flush_error= result.first; &#125; if (first_seen == NULL) first_seen= queue; &#125;&#125; 写完binlog IOCACHE后，还要将IOCACHE写文件，最后通知Dump线程读取binlog，FLUSH阶段完成。 SYNC阶段 SYNC阶段的任务比较简单,但是却非常耗时，就是将binlog文件sync到磁盘。这个操作由配置项sync_binlog = N 来控制每隔N个binlog只sync一次。如果sync_binlog=1的话，MySQL在SYNC阶段不释放锁LOCK_log，而Dump线程为了读取binlog，必须先申请锁LOCK_log，所以可以保证主库先将binlog sync到磁盘，然后Dump线程才能读取Binlog，确保即使在主库操作系统Crash情况下，仍然保证主库和从库数据一致。其他情况会释放LOCK_log锁，这时Dump线程可以读取并发送binlog，同时新提交的事务也可以进入FLUSH阶段。所以SYNC阶段需要考虑有多个FLUSH阶段的Leader同时进入SYNC阶段的情况。MySQL将这些Leader合并为一个新的Leader，做法是：FLUSH阶段的Leader线程进入SYNC阶段前，需要将自己加入到SYNC_STAGE队列中，第一个进入SYNC_STAGE队列的线程为SYNC阶段的Leader，后进入的为Flower。由Leader完成后续操作，Flower线程只需等待通知即可。回忆前面的函数enroll_for()，在状态转变时，Leader先把自己添加到SYNC队列中，然后才释放锁stage_mutex，这里就是LOCK_log，其他事务才可以进入FLUSH阶段，这可以保证，第一个进入FLUSH阶段的Leader，在SYNC阶段仍然是Leader，同样，在COMMIT阶段还是Leader。这对于保证Binlog和InnoDB提交顺序一致非常重要。 SYNC阶段的代码如下： 1234567891011121314151617181920212223242526272829303132333435int MYSQL_BIN_LOG::ordered_commit(THD *thd, bool all, bool skip_commit)&#123; // ... /* Stage #2: Syncing binary log file to disk */ bool need_LOCK_log= (get_sync_period() == 1); //只有sync_binlog=1，才不释放LOCK_log锁 /* LOCK_log is not released when sync_binlog is 1. It guarantees that the events are not be replicated by dump threads before they are synced to disk. */ //不管怎样，都要申请锁LOCK_sync if (change_stage(thd, Stage_manager::SYNC_STAGE, wait_queue, need_LOCK_log ? NULL : &amp;LOCK_log, &amp;LOCK_sync)) &#123; DBUG_PRINT(&quot;return&quot;, (&quot;Thread ID: %lu, commit_error: %d&quot;, thd-&gt;thread_id, thd-&gt;commit_error)); DBUG_RETURN(finish_commit(thd)); &#125; THD *final_queue= stage_manager.fetch_queue_for(Stage_manager::SYNC_STAGE); if (flush_error == 0 &amp;&amp; total_bytes &gt; 0) &#123; DEBUG_SYNC(thd, &quot;before_sync_binlog_file&quot;); std::pair&lt;bool, bool&gt; result= sync_binlog_file(false); flush_error= result.first; &#125; if (need_LOCK_log) mysql_mutex_unlock(&amp;LOCK_log); //...&#125; COMMIT阶段 经过前面2个阶段，Binlog已经顺利sync到磁盘了，COMMIT阶段的任务就是让InnoDB存储引擎完成Commit。COMMIT阶段的逻辑通过MySQL的配置项binlog_order_commits控制。如果配置项为1，MySQL要保证InnoDB的提交顺序和Binlog的写入顺序一致，这个特性在InnoDB热备中使用。下面只分析binlog_order_commits=1的情况。 MySQL释放锁LOCK_sync，申请锁LOCK_commit。由于释放锁LOCK_sync，所以需要考虑多个线程同时完成SYNC阶段的情况，处理逻辑和SYNC阶段类似，将当前SYNC阶段的Leader合并，关于Leader的产生和SYNC阶段类似。Leader产生以后，遍历THD，完成事务提交，等所有事务都提交完成以后，再遍历thd，设置thd-&gt;transaction.flags.pending=false，最后广播通知Flower线程提交完成，自此，Group Commit完成。 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142int MYSQL_BIN_LOG::ordered_commit(THD *thd, bool all, bool skip_commit)&#123; //... /* Stage #3: Commit all transactions in order. This stage is skipped if we do not need to order the commits and each thread have to execute the handlerton commit instead. Howver, since we are keeping the lock from the previous stage, we need to unlock it if we skip the stage. */ if (opt_binlog_order_commits) &#123; if (change_stage(thd, Stage_manager::COMMIT_STAGE, final_queue, &amp;LOCK_sync, &amp;LOCK_commit)) &#123; DBUG_PRINT(&quot;return&quot;, (&quot;Thread ID: %lu, commit_error: %d&quot;, thd-&gt;thread_id, thd-&gt;commit_error)); DBUG_RETURN(finish_commit(thd)); &#125; THD *commit_queue= stage_manager.fetch_queue_for(Stage_manager::COMMIT_STAGE); DBUG_EXECUTE_IF(&quot;semi_sync_3-way_deadlock&quot;, DEBUG_SYNC(thd, &quot;before_process_commit_stage_queue&quot;);); process_commit_stage_queue(thd, commit_queue); mysql_mutex_unlock(&amp;LOCK_commit); /* Process after_commit after LOCK_commit is released for avoiding 3-way deadlock among user thread, rotate thread and dump thread. */ process_after_commit_stage_queue(thd, commit_queue); final_queue= commit_queue; &#125; else mysql_mutex_unlock(&amp;LOCK_sync); /* Commit done so signal all waiting threads */ stage_manager.signal_done(final_queue); //...&#125; Leader产生以后，Leader线程通过next_to_commit遍历thd，对每个thd完成事务提交ha_commit_low(),代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243voidMYSQL_BIN_LOG::process_commit_stage_queue(THD *thd, THD *first)&#123; for (THD *head= first ; head ; head = head-&gt;next_to_commit) &#123; if (head-&gt;commit_error == THD::CE_NONE) &#123; excursion.try_to_attach_to(head); bool all= head-&gt;transaction.flags.real_commit; if (head-&gt;transaction.flags.commit_low) &#123; /* head is parked to have exited append() */ DBUG_ASSERT(head-&gt;transaction.flags.ready_preempt); /* storage engine commit */ if (ha_commit_low(head, all, false)) head-&gt;commit_error= THD::CE_COMMIT_ERROR; &#125; &#125; /* Decrement the prepared XID counter after storage engine commit. We also need decrement the prepared XID when encountering a flush error or session attach error for avoiding 3-way deadlock among user thread, rotate thread and dump thread. */ if (head-&gt;transaction.flags.xid_written) dec_prep_xids(head); &#125;&#125;class Stage_manager &#123;public: //遍历THD，标记提交完成，并广播通知 void signal_done(THD *queue) &#123; mysql_mutex_lock(&amp;m_lock_done); for (THD *thd= queue ; thd ; thd = thd-&gt;next_to_commit) thd-&gt;transaction.flags.pending= false; mysql_mutex_unlock(&amp;m_lock_done); mysql_cond_broadcast(&amp;m_cond_done); &#125;&#125; 本文地址：http://xnerv.wang/implement-of-binlog-group-commit-in-mysql56/ 转载自：MySQL 5.6中Binlog Group Commit实现","categories":[{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/categories/DBMS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/tags/DBMS/"},{"name":"MySQL","slug":"MySQL","permalink":"https://xnerv.wang/tags/MySQL/"},{"name":"Binlog Group Commit","slug":"Binlog-Group-Commit","permalink":"https://xnerv.wang/tags/Binlog-Group-Commit/"}]},{"title":"一些流行引擎存储格式简介（转载）","slug":"introduction-of-some-popular-engine-storage-formats","date":"2018-03-17T03:47:00.000Z","updated":"2023-08-21T02:24:20.552Z","comments":true,"path":"introduction-of-some-popular-engine-storage-formats/","link":"","permalink":"https://xnerv.wang/introduction-of-some-popular-engine-storage-formats/","excerpt":"概述 本文简要介绍了一些存储引擎存储结构，包括InnoDB, TokuDB, RocksDB, TiDB, CockroachDB, 供大家对比分析 InnoDB InnoDB 底层存储结构为B+树，结构如下","text":"概述 本文简要介绍了一些存储引擎存储结构，包括InnoDB, TokuDB, RocksDB, TiDB, CockroachDB, 供大家对比分析 InnoDB InnoDB 底层存储结构为B+树，结构如下 B树的每个节点对应innodb的一个page，page大小是固定的，一般设为16k。 其中非叶子节点只有键值，叶子节点包含完整数据。 InnoDB按segment, extent, page方式管理page 每个数据节点page结构如下 数据记录record按行存储，record具体格式由row_format决定. 详情可以参考数据内核月报 TokuDB TokuDB 底层存储结构为Fractal Tree Fractal Tree的结构与B+树有些类似, 在Fractal Tree中，每一个child指针除了需要指向一个child节点外，还会带有一个Message Buffer ，这个Message Buffer 是一个FIFO的队列，用来缓存更新操作。 例如，一次插入操作只需要落在某节点的Message Buffer就可以马上返回了，并不需要搜索到叶子节点。这些缓存的更新会在查询时或后台异步合并应用到对应的节点中。 RocksDB RockDB的存储结构如下 RocksDB写入数据时，先写到memtable中,memtable一般为skiplist, memtable写满时转为immutable memtable并刷入Level 0. Level0中的SST文件中的数据都是有序的，Level0中SST文件之间的数据范围可能存在重叠。 其他Level中的SST文件之间的数据范围不重叠。 RocksDB会以一定的机制从低level compact数据到高level中。 RocksDB中SST文件的结构如下 MyRocks使用的存储引擎就是RocksDB, MyRocks的中RocksDB的数据映射关系参考 之前的月报 TiDB TiDB的存储结构 TiDB是分布式存储，分为两个部分TiKV和Placement Driver server。 TiKV用于存储真正的数据，TiKV由分布在不同机器上的RocksDB实例组成。 数据按范围划分为一个个Region. 并且会尽量保持每个 Region 中保存的数据不超过一定的大小(这个大小可以配置，目前默认是 64MB). 同一Region分布在不同的RocksDB实例中，一个RocksDB实例包含多个Region. 图中，Region4有三个副本分布在三个RocksDB实例中，这三个Region副本组成一个RaftGroup，副本间通过Raft协议保证一致性。 Placement Driver server（PD）， 也是一个集群，也通过Raft协议保证一致性。PD主要有以下作用： 存储region的位置等元数据信息 调度和rebalance regions, TiKV中的Raft leader等信息 分配全局事务ID TiDB的数据映射关系 以下表为例 123create table user(user_id int primary key, name varchar(100), email varchar(200));INSERT INTO user VALUES (1, “bob”, “huang@pingcap.com”);INSERT INTO user VALUES (2, “tom”, “tom@pingcap.com”); 对应到RocksDB中的KV结构如下 Key Values user/1 bob huang@pingcap.com user/2 tom tom@pingcap.com CockroachDB CockroachDB的存储结构 CockroachDB的也是分布式存储，其结构和TiDB类似。CockroachDB按范围划分为Range，Range默认为64M，Range的存储为RocksDB， CockroachDB的一个node包含多个RocksDB实例。 Range副本分布在不同的node中，通过Raft协议保证一致。 Range的元数据信息也保存在Range中(靠前的Range中). System keys come in several subtypes: Global keys store cluster-wide data such as the “meta1” and “meta2” keys as well as various other system-wide keys such as the node and store ID allocators. Store local keys are used for unreplicated store metadata (e.g. the StoreIdent structure). “Unreplicated” indicates that these values are not replicated across multiple stores because the data they hold is tied to the lifetime of the store they are present on. Range local keys store range metadata that is associated with a global key. Range local keys have a special prefix followed by a global key and a special suffix. For example, transaction records are range local keys which look like: \\x01ktxn-. Replicated Range ID local keys store range metadata that is present on all of the replicas for a range. These keys are updated via Raft operations. Examples include the range lease state and abort cache entries. Unreplicated Range ID local keys store range metadata that is local to a replica. The primary examples of such keys are the Raft state and Raft log. CockroachDB的数据映射关系 以下表为例 12create table mydb.customers(name varchar(100) primary key, address varchar(100) , URL varchar(100));insert into mydb.customers values(&#x27;Apple&#x27;,&#x27;1 Infinite Loop, Cupertino, CA&#x27;,&#x27;http://apple.com/&#x27;); 表结构信息 Key Values /system/databases/mydb/id 51 /system/tables/customer/id 42 /system/desc/51/42/address 69 /system/desc/51/42/url 66 表中的数据 Key Values /51/42/Apple/69 1 Infinite Loop, Cupertino, CA /51/42/Apple/66 http://apple.com/ 最后 本文简要介绍了各存储引擎的结构，供大家参考，有错误之处请指正. 参考文档 https://github.com/facebook/rocksdb https://www.percona.com/doc/percona-server/LATEST/tokudb/tokudb_intro.html https://github.com/cockroachdb/cockroach/blob/master/docs/design.md https://github.com/pingcap/tidb https://www.percona.com/live/plam16/sessions/how-we-build-tidb https://dev.mysql.com/doc/internals/en/innodb.html http://img3.tbcdn.cn/L1/461/1/d0069515c04809a449eda659386afbe966e0d1df 本文地址：http://xnerv.wang/introduction-of-some-popular-engine-storage-formats/ 转载自：一些流行引擎存储格式简介","categories":[{"name":"分布式及存储","slug":"分布式及存储","permalink":"https://xnerv.wang/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8F%8A%E5%AD%98%E5%82%A8/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"分布式及存储","slug":"分布式及存储","permalink":"https://xnerv.wang/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8F%8A%E5%AD%98%E5%82%A8/"},{"name":"InnoDB","slug":"InnoDB","permalink":"https://xnerv.wang/tags/InnoDB/"},{"name":"TokuDB","slug":"TokuDB","permalink":"https://xnerv.wang/tags/TokuDB/"},{"name":"RocksDB","slug":"RocksDB","permalink":"https://xnerv.wang/tags/RocksDB/"},{"name":"TiDB","slug":"TiDB","permalink":"https://xnerv.wang/tags/TiDB/"},{"name":"CockroachDB","slug":"CockroachDB","permalink":"https://xnerv.wang/tags/CockroachDB/"}]},{"title":"分布式键值存储 Dynamo 的实现原理（转载）","slug":"dynamo-implement","date":"2018-03-17T01:40:00.000Z","updated":"2023-08-21T02:24:20.600Z","comments":true,"path":"dynamo-implement/","link":"","permalink":"https://xnerv.wang/dynamo-implement/","excerpt":"在最近的一周时间里，一直都在研究和阅读 Amazon 的一篇论文 Dynamo: Amazon’s Highly Available Key-value Store，论文中描述了 Amazon 的高可用分布式键值存储服务 Dynamo 的实现原理。 之前在阅读 Google 的 Bigtable: A Distributed Storage System for Structured Data 时写了一篇 浅析 Bigtable 和 LevelDB 的实现 文章分析了 Bigtable 的单机版 LevelDB 的实现原理；在研究 Dynamo 时，作者发现 Dynamo 虽然和 Bigtable 同为 NoSQL，但是它们的实现却有着很大的不同，最主要的原因来自不同的应用场景和不同的目的。","text":"在最近的一周时间里，一直都在研究和阅读 Amazon 的一篇论文 Dynamo: Amazon’s Highly Available Key-value Store，论文中描述了 Amazon 的高可用分布式键值存储服务 Dynamo 的实现原理。 之前在阅读 Google 的 Bigtable: A Distributed Storage System for Structured Data 时写了一篇 浅析 Bigtable 和 LevelDB 的实现 文章分析了 Bigtable 的单机版 LevelDB 的实现原理；在研究 Dynamo 时，作者发现 Dynamo 虽然和 Bigtable 同为 NoSQL，但是它们的实现却有着很大的不同，最主要的原因来自不同的应用场景和不同的目的。 Bigtable 和 Dynamo Bigtable 和 Dynamo 两者分别是 Google 和 Amazon 两大巨头给出的存储海量数据的解决方法，作为 NoSQL 两者都具有分布式、容错以及可扩展的几大特性。 虽然两者都是 NoSQL，并且有着相似的特性，但是它们在侧重的方向上有非常明显的不同，从两个数据库论文的标题中，我们就能看到 Amazon 的 Dynamo 追求的是高可用性并且提供的是类似 MongoDB 的 Key-value 文档存储，而 Bigtable 中描述的数据库却可以用于结构化的数据存储。 由于 Bigtable 和 Dynamo 都属于同一个类别 - NoSQL，所以它们经常会被放在一起进行对比，这篇文章不仅会介绍 Dynamo 的设计理念以及架构等问题，还会就其中的部分问题与 Bigtable 中相对应的概念进行对比，这样能够让我们更加清楚地了解不同的数据库对不同问题，因设计理念的差异做出的权衡。 架构 在数据库领域中尤其是分布式数据库，最重要的就是服务的架构，多数的分布式系统在设计时都会假设服务运行在廉价的节点上，并没有出众的性能和也不能提供稳定的服务，所以水平扩展和容错的能力是分布式数据库的标配；但是不同的分布式数据库选用了不同的架构来组织大量的节点。 很多的分布式服务例如 GFS 和 Bigtable 都使用了带有主节点的架构来维护整个系统中的元数据，包括节点的位置等信息，而 Dynamo 的实现不同于这些中心化的分布式服务，在 Dynamo 中所有的节点都有着完全相同的职责，会对外界提供同样的服务，所以在整个系统中并不会出现单点故障的问题。 去中心化的架构使得系统的水平扩展非常容易，节点可以在任何时候直接加入到整个 Dynamo 的集群中，并且只会造成集群中少量数据的迁移。 Bigtable 使用了中心化的架构，通过主节点来维护整个系统中全部的元数据信息，但是 Bigtable 本身其实并不会处理来自客户端的读写请求，所有请求都会由客户端直接和从节点通信，不过由于有了中心化的主节点，所以主节点一旦发生故障宕机就会造成服务的不可用，虽然 Bigtable 以及类似的服务通过其他方式解决这个问题，但是这个问题仍然是中心化的设计所造成的。 中心化或者去中心化并不是一个绝对好或者绝对坏的选择，选择中心化的解决方案能够降低系统实现的复杂度，而去中心化的方式能够避免单点故障，让系统能够更好更快地增加新的节点，提供优秀的水平扩展能力。 分片和复制 Dynamo 在设计之初就定下了增量扩展（Incremental Scalability）的核心需求，这也就需要一种能够在一组节点中动态分片的机制，Dynamo 的分片策略依赖于_一致性哈希_，通过这种策略 Dynamo 能够将负载合理的分配到不同的存储节点上。 所有的键在存储之前都会通过哈希函数得到一个唯一的值，哈希函数的输出被看做是一个固定长度的环，也就是其输出的最大值和最小值是『连接』到一起的： 每一个节点都会被 Dynamo 在这个环中分配一个随机的位置，而这个节点会处理从哈希的输出在当前节点前的所有键；假设我们有一个键值对 (draven, developer)，Hash(draven) 的结果位于上图中的绿色区域，从环中的位置开始按照顺时针的顺序寻找，找到的以第一个节点 B 就会成为协调者（coordinator）负责处理当前的键值对，上图中的每一个节点都会负责与其颜色相同的部分。 由于 Dynamo 系统中的每一个节点在刚刚加入当前的集群时，会被分配一个随机的位置，所以由于算法的随机性可能会导致不同节点处理的范围有所不同，最终每一个节点的负载也并不相同；为了解决这个问题，Dynamo 使用了一致性哈希算法的变种，将同一个物理节点分配到环中的多个位置（标记），成为多个虚拟节点，但是在这种策略下，如果当前的 Dynamo 节点一天处理上百万的请求，那么新增节点为了不影响已有节点的性能，会在后台进行启动，整个过程大约会消耗一整天的时间，这其实是很难接受的，除此之外这种策略还会造成系统进行日常归档极其缓慢。 为了解决负载的不均衡的问题，除了上面使用虚拟节点的策略之外，Dynamo 论文中还提供了另外两种策略，其中性能相对较好的是将数据的哈希分成 Q 个大小相等的区域，S 个节点每一个处理 Q/S 个分区，当某一个节点因为故障或者其他原因需要退出集群时，会将它处理的数据分片随机分配给其它的节点，当有节点加入系统时，会从其它的节点中『接管』对应的数据分片。上图只是对这种策略下的分片情况简单展示，在真实环境中分片数 Q 的值远远大于节点数 S。 Dynamo 为了达到高可用性和持久性，防止由于节点宕机故障或者数据丢失，将同一份数据在协调者和随后的 N-1 个节点上备份了多次，N 是一个可以配置的值，在一般情况下都为 3。 也就是说，上图中黄色区域的值会存储在三个节点 A、B 和 C 中，绿色的区域会被 B、C、D 三个节点处理，从另一个角度来看，A 节点会处理范围在 (C, A] 之间的值，而 B 节点会处理从 (D, B] 区域内的值。 负责存储某一个特定键值对的节点列表叫做偏好列表（preference list），因为虚拟节点在环中会随机存在，为了保证出现节点故障时不会影响可用性和持久性，偏好列表中的全部节点必须都为不同的物理节点。 Bigtable 中对分片和复制的实现其实就与 Dynamo 中完全不同，这不仅是因为 Bigtable 的节点有主从之分，还因为 Bigtable 的设计理念与 Dynamo 完全不同。在 Bigtable 中，数据是按照键的顺序存储的，数据存储的单位都是 tablet，每一张表都由多个 tablet 组成，而每一个的 tablet 都有一个 tablet 服务器来处理，而 tablet 的位置都存储在 METADATA 表中。 在 Bigtable 中，所有的 tablet 都在 GFS 中以 SSTable 的格式存储起来，这些 SSTable 都被分成了固定大小的块在 chunkserver 上存储，而每一个块也都会在存储在多个 chunkserver 中。 读写请求的执行 Dynamo 集群中的任意节点都能够接受来自客户端的对于任意键的读写请求，所有的请求都通过 RPC 调用执行，客户端在选择节点时有两种不同的策略：一种是通过一个负载均衡器根据负载选择不同的节点，另一种是通过一个清楚当前集群分片的库直接请求相应的节点。 从上面我们就已经知道了处理读写请求的节点就叫做协调者（coordinator），前 N 个『健康』的节点会参与读写请求的处理；Dynamo 使用了 Quorum 一致性协议来保证系统中的一致性，协议中有两个可以配置的值：R 和 W，其中 R 是成功参与一个读请求的最小节点数，而 W 是成功参与写请求的最小节点数。 当 R = 2 时，所有的读请求必须等待两个节点成功返回对应键的结果，才认为当前的请求结束了，也就是说读请求的时间取决于返回最慢的节点，对于写请求来说也是完全相同的；当协调者接收到了来自客户端的写请求 put() 时，它会创建一个新的向量时钟（vector clock），然后将新版本的信息存储在本地，之后向偏好列表（preference list）中的前 N-1 个节点发送消息，直到其中的 W-1 个返回这次请求才成功结束，读请求 get() 与上述请求的唯一区别就是，如果协调者发现节点中的数据出现了冲突，就会对冲突尝试进行解决并将结果重新写回对应的节点。 冲突和向量时钟 Dynamo 与目前的绝大多数分布式系统一样都提供了最终一致性，最终一致性能够允许我们异步的更新集群中的节点，put() 请求可能会在所有的节点后更新前就返回对应的结果了，在这时随后的 get() 就可能获取到过期的数据。 如果在系统中出现了节点故障宕机，那么数据的更新可能在一段时间内都不会到达失效的节点，这也是在使用 Dynamo 或者使用相似原理的系统时会遇到的问题，Amazon 中的很多应用虽然都能够忍受这种数据层面可能发生的不一致性，但是有些对业务数据一致性非常高的应用在选择 Dynamo 时就需要好好考虑了。 因为 Dynamo 在工作的过程中不同的节点可能会发生数据不一致的问题，这种问题肯定是需要解决的，Dynamo 能够确保一旦数据之间发生了冲突不会丢失，但是可能会有已被删除的数据重新出现的问题。 在多数情况下，Dynamo 中的最新版本的数据都会取代之前的版本，系统在这时可以通过语法调解（syntactic reconcile）数据库中的正确版本。但是版本也可能会出现分支，在这时，Dynamo 就会返回所有它无法处理的数据版本，由客户端在多个版本的数据中选择或者创建（collapse）合适的版本返回给 Dynamo，其实这个过程比较像出现冲突的 git merge 操作，git 没有办法判断当前的哪个版本是合适的，所以只能由开发者对分支之间的冲突进行处理。 上图中的每一个对象的版本 Dx 中存储着一个或多个向量时钟 [Sn, N]，每次 Dynamo 对数据进行写入时都会更新向量时钟的版本，节点 Sx 第一次写入时向量时钟为 [Sx, 1]，第二次为 [Sx, 2]，在这时假设节点 Sy 和 Sz 都不知道 Sx 已经对节点进行写入了，它们接收到了来自其他客户端的请求，在本地也对同样键做出了写入并分别生成了不同的时钟 [Sy, 1] 和 [Sz, 1]，当客户端再次使用 get() 请求时就会发现数据出现了冲突，由于 Dynamo 无法根据向量时钟自动解决，所以它需要手动合并三个不同的数据版本。 论文中对 24 小时内的请求进行了统计，其中 99.94% 的请求仅会返回一个版本，0.00057% 的请求会返回两个版本，0.00047 的请求会返回三个版本，0.000009% 的请求会返回四个版本，虽然论文中说： This shows that divergent versions are created rarely. 但是作者仍然认为在海量的数据面前 99.94% 并不是一个特别高的百分比，处理分歧的数据版本仍然会带来额外的工作量和负担。虽然在这种情况下，数据库本身确实没有足够的信息来解决数据的不一致问题，也确实只能由客户端去解决冲突，但是这种将问题抛给上层去解决的方式并不友好，论文中也提到了 Amazon 中使用 Dynamo 的应用程序也都是能够适应并解决这些数据不一致的问题的，不过对于作者来说，仅仅这一个问题就成为不选择 Dynamo 的理由了。 节点的增删 因为在分布式系统中节点的失效是非常常见的事情，而节点也很少会因为某些原因永久失效，往往大部分节点会临时宕机然后快速重新加入系统；由于这些原因，Dynamo 选择使用了显式的机制向系统中添加和移除节点。 添加节点时可以使用命令行工具或者浏览器连接 Dynamo 中的任意节点后触发一个成员变动的事件，这个事件会从当前的环中移除或者向环中添加一个新的节点，当节点的信息发生改变时，该节点会通过 Gossip 协议通知它所能通知的最多的节点。 在 Gossip 协议中，每次通讯的两个节点会对当前系统中的节点信息达成一致；通过节点之间互相传递成员信息，最终整个 Dyanmo 的集群中所有的节点都会就成员信息达成一致，如上图所示，”gossip” 首先会被 C 节点接收，然后它会传递给它能接触到的最多的节点 A、D、F、G 四个节点，然后 “gossip” 会进行二次传播传递给系统中的灰色节点，到此为止系统中的所有节点都得到了最新的 “gossip” 消息。 当我们向 Dynamo 中加入了新的节点时，会发生节点之间的分片转移，假设我们连接上了 Dynamo 数据库，然后添加了一个 X 节点，该节点被分配到了如下图所示的 A 和 B 节点之间。 新引入的节点 X 会从三个节点 C、D、E 中接受它们管理的分片的一部分，也就是上图中彩色的 (E, A]、(A, B] 和 (B, X] 三个部分，在 X 节点加入集群之前分别属于与其颜色相同的节点管理。 Dynamo 由于其去中心化的架构，节点增删的事件都需要通过 Gossip 协议进行传递，然而拥有主从节点之分的 Bigtable 就不需要上述的方式对集群中的节点进行增删了，它可以直接通过用于管理其他从节点的服务直接注册新的节点或者撤下已有的节点。 副本同步 在 Dynamo 运行的过程中，由于一些情况会造成不同节点中的数据不一致的问题，Dynamo 使用了反信息熵（anti-entropy）的策略保证所有的副本存储的信息都是同步的。 为了快速确认多个副本之间的数据的一致性并避免大量的数据传输，Dynamo 使用了 Merkle tree 对不同节点中的数据进行快速验证。 在 Merkle 树中，所有父节点中的内容都是叶子节点的哈希，通过这种方式构建的树形结构能够保证整棵树不会被篡改，任何的改动都能被立刻发现。 Dynamo 中的每一个节点都为其持有的键的范围维护了一颗 Merkle 树，在验证两份节点中的数据是否相同时，只需要发送根节点中的哈希值，如果相同那么说明两棵树的内容全部相同，否则就会依次对比不同层级节点中的内容，直到找出不同的副本，这种做法虽然能够减少数据的传输并能够快速找到副本之间的不同，但是当有新的节点加入或者旧的节点退出时会导致大量的 Merkle 树重新计算。 总结 在 Dynamo 的论文公开之后，有一篇文章将 Dynamo 的设计称作 “A flawed architecture”，这篇文章的作者在文中对 Dynamo 的实现进行了分析，主要对其最终一致性和 Quorom 机制进行了批评，它在 HackerNews 上也引起了广泛的讨论，帖子中的很多内容都值得一看，能够帮助我们了解 Dynamo 的设计原理，而 Amazon 的 CTO 对于这篇文章也发了一条 Twitter： 不管如何，Dynamo 作为支撑亚马逊业务的底层服务，其实现原理和思想对于整个社区都是非常有价值的，然而它使用的去中心化的策略也带了很多问题，虽然作者可能会因为这个原因在选择数据库时不会 Dynamo，不过相信它也是有合适的应用场景的。 Reference Dynamo: Amazon’s Highly Available Key-value Store Dynamo: A flawed architecture – Part I Dynamo – Part I: a followup and re-rebuttals Dynamo and BigTable - Review and Comparison DynamoDB vs. BigTable · vsChart Merkle tree A Digital Signature Based on a Conventional Encryption Function Dynamo 的实现技术和去中心化 浅析 Bigtable 和 LevelDB 的实现 本文地址：http://xnerv.wang/dynamo-implement/ 转载自：分布式键值存储 Dynamo 的实现原理","categories":[{"name":"分布式及存储","slug":"分布式及存储","permalink":"https://xnerv.wang/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8F%8A%E5%AD%98%E5%82%A8/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"分布式及存储","slug":"分布式及存储","permalink":"https://xnerv.wang/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8F%8A%E5%AD%98%E5%82%A8/"},{"name":"Dynamo","slug":"Dynamo","permalink":"https://xnerv.wang/tags/Dynamo/"}]},{"title":"浅析 Bigtable 和 LevelDB 的实现（转载）","slug":"bigtable-leveldb","date":"2018-03-17T01:30:00.000Z","updated":"2023-08-21T02:24:20.670Z","comments":true,"path":"bigtable-leveldb/","link":"","permalink":"https://xnerv.wang/bigtable-leveldb/","excerpt":"在 2006 年的 OSDI 上，Google 发布了名为 Bigtable: A Distributed Storage System for Structured Data 的论文，其中描述了一个用于管理结构化数据的分布式存储系统 - Bigtable 的数据模型、接口以及实现等内容。 本文会先对 Bigtable 一文中描述的分布式存储系统进行简单的描述，然后对 Google 开源的 KV 存储数据库 LevelDB 进行分析；LevelDB 可以理解为单点的 Bigtable 的系统，虽然其中没有 Bigtable 中与 tablet 管理以及一些分布式相关的逻辑，不过我们可以通过对 LevelDB 源代码的阅读增加对 Bigtable 的理解。","text":"在 2006 年的 OSDI 上，Google 发布了名为 Bigtable: A Distributed Storage System for Structured Data 的论文，其中描述了一个用于管理结构化数据的分布式存储系统 - Bigtable 的数据模型、接口以及实现等内容。 本文会先对 Bigtable 一文中描述的分布式存储系统进行简单的描述，然后对 Google 开源的 KV 存储数据库 LevelDB 进行分析；LevelDB 可以理解为单点的 Bigtable 的系统，虽然其中没有 Bigtable 中与 tablet 管理以及一些分布式相关的逻辑，不过我们可以通过对 LevelDB 源代码的阅读增加对 Bigtable 的理解。 Bigtable Bigtable 是一个用于管理结构化数据的分布式存储系统，它有非常优秀的扩展性，可以同时处理上千台机器中的 PB 级别的数据；Google 中的很多项目，包括 Web 索引都使用 Bigtable 来存储海量的数据；Bigtable 的论文中声称它实现了四个目标： 在作者看来这些目标看看就好，其实并没有什么太大的意义，所有的项目都会对外宣称它们达到了高性能、高可用性等等特性，我们需要关注的是 Bigtable 到底是如何实现的。 数据模型 Bigtable 与数据库在很多方面都非常相似，但是它提供了与数据库不同的接口，它并没有支持全部的关系型数据模型，反而使用了简单的数据模型，使数据可以被更灵活的控制和管理。 在实现中，Bigtable 其实就是一个稀疏的、分布式的、多维持久有序哈希。 A Bigtable is a sparse, distributed, persistent multi-dimensional sorted map. 它的定义其实也就决定了其数据模型非常简单并且易于实现，我们使用 row、column 和 timestamp 三个字段作为这个哈希的键，值就是一个字节数组，也可以理解为字符串。 这里最重要的就是 row 的值，它的长度最大可以为 64KB，对于同一 row 下数据的读写都可以看做是原子的；因为 Bigtable 是按照 row 的值使用字典顺序进行排序的，每一段 row 的范围都会被 Bigtable 进行分区，并交给一个 tablet 进行处理。 实现 在这一节中，我们将介绍 Bigtable 论文对于其本身实现的描述，其中包含很多内容：tablet 的组织形式、tablet 的管理、读写请求的处理以及数据的压缩等几个部分。 tablet 的组织形式 我们使用类似 B+ 树的三层结构来存储 tablet 的位置信息，第一层是一个单独的 Chubby 文件，其中保存了根 tablet 的位置。 Chubby 是一个分布式锁服务，我们可能会在后面的文章中介绍它。 每一个 METADATA tablet 包括根节点上的 tablet 都存储了 tablet 的位置和该 tablet 中 key 的最小值和最大值；每一个 METADATA 行大约在内存中存储了 1KB 的数据，如果每一个 METADATA tablet 的大小都为 128MB，那么整个三层结构可以存储 2^61 字节的数据。 tablet 的管理 既然在整个 Bigtable 中有着海量的 tablet 服务器以及数据的分片 tablet，那么 Bigtable 是如何管理海量的数据呢？Bigtable 与很多的分布式系统一样，使用一个主服务器将 tablet 分派给不同的服务器节点。 为了减轻主服务器的负载，所有的客户端仅仅通过 Master 获取 tablet 服务器的位置信息，它并不会在每次读写时都请求 Master 节点，而是直接与 tablet 服务器相连，同时客户端本身也会保存一份 tablet 服务器位置的缓存以减少与 Master 通信的次数和频率。 读写请求的处理 从读写请求的处理，我们其实可以看出整个 Bigtable 中的各个部分是如何协作的，包括日志、memtable 以及 SSTable 文件。 当有客户端向 tablet 服务器发送写操作时，它会先向 tablet 服务器中的日志追加一条记录，在日志成功追加之后再向 memtable 中插入该条记录；这与现在大多的数据库的实现完全相同，通过顺序写向日志追加记录，然后再向数据库随机写，因为随机写的耗时远远大于追加内容，如果直接进行随机写，可能由于发生设备故障造成数据丢失。 当 tablet 服务器接收到读操作时，它会在 memtable 和 SSTable 上进行合并查找，因为 memtable 和 SSTable 中对于键值的存储都是字典顺序的，所以整个读操作的执行会非常快。 表的压缩 随着写操作的进行，memtable 会随着事件的推移逐渐增大，当 memtable 的大小超过一定的阈值时，就会将当前的 memtable 冻结，并且创建一个新的 memtable，被冻结的 memtable 会被转换为一个 SSTable 并且写入到 GFS 系统中，这种压缩方式也被称作 Minor Compaction。 每一个 Minor Compaction 都能够创建一个新的 SSTable，它能够有效地降低内存的占用并且降低服务进程异常退出后，过大的日志导致的过长的恢复时间。既然有用于压缩 memtable 中数据的 Minor Compaction，那么就一定有一个对应的 Major Compaction 操作。 Bigtable 会在后台周期性地进行 Major Compaction，将 memtable 中的数据和一部分的 SSTable 作为输入，将其中的键值进行归并排序，生成新的 SSTable 并移除原有的 memtable 和 SSTable，新生成的 SSTable 中包含前两者的全部数据和信息，并且将其中一部分标记未删除的信息彻底清除。 小结 到这里为止，对于 Google 的 Bigtable 论文的介绍就差不多完成了，当然本文只介绍了其中的一部分内容，关于压缩算法的实现细节、缓存以及提交日志的实现等问题我们都没有涉及，想要了解更多相关信息的读者，这里强烈推荐去看一遍 Bigtable 这篇论文的原文 Bigtable: A Distributed Storage System for Structured Data 以增强对其实现的理解。 LevelDB 文章前面对于 Bigtable 的介绍其实都是对 LevelDB 这部分内容所做的铺垫，当然这并不是说前面的内容就不重要，LevelDB 是对 Bigtable 论文中描述的键值存储系统的单机版的实现，它提供了一个极其高速的键值存储系统，并且由 Bigtable 的作者 Jeff Dean 和 Sanjay Ghemawat 共同完成，可以说高度复刻了 Bigtable 论文中对于其实现的描述。 因为 Bigtable 只是一篇论文，同时又因为其实现依赖于 Google 的一些不开源的基础服务：GFS、Chubby 等等，我们很难接触到它的源代码，不过我们可以通过 LevelDB 更好地了解这篇论文中提到的诸多内容和思量。 概述 LevelDB 作为一个键值存储的『仓库』，它提供了一组非常简单的增删改查接口： 1234567class DB &#123; public: virtual Status Put(const WriteOptions&amp; options, const Slice&amp; key, const Slice&amp; value) = 0; virtual Status Delete(const WriteOptions&amp; options, const Slice&amp; key) = 0; virtual Status Write(const WriteOptions&amp; options, WriteBatch* updates) = 0; virtual Status Get(const ReadOptions&amp; options, const Slice&amp; key, std::string* value) = 0;&#125; Put 方法在内部最终会调用 Write 方法，只是在上层为调用者提供了两个不同的选择。 Get 和 Put 是 LevelDB 为上层提供的用于读写的接口，如果我们能够对读写的过程有一个非常清晰的认知，那么理解 LevelDB 的实现就不是那么困难了。 在这一节中，我们将先通过对读写操作的分析了解整个工程中的一些实现，并在遇到问题和新的概念时进行解释，我们会在这个过程中一步一步介绍 LevelDB 中一些重要模块的实现以达到掌握它的原理的目标。 从写操作开始： 12345678910Status DB::Put(const WriteOptions&amp; opt, const Slice&amp; key, const Slice&amp; value) &#123; WriteBatch batch; batch.Put(key, value); return Write(opt, &amp;batch);&#125;Status DBImpl::Write(const WriteOptions&amp; options, WriteBatch* my_batch) &#123; ...&#125; 正如上面所介绍的，DB::Put 方法将传入的参数封装成了一个 WritaBatch，然后仍然会执行 DBImpl::Write 方法向数据库中写入数据；写入方法 DBImpl::Write 其实是一个是非常复杂的过程，包含了很多对上下文状态的判断，我们先来看一个写操作的整体逻辑： 从总体上看，LevelDB 在对数据库执行写操作时，会有三个步骤： 调用 MakeRoomForWrite 方法为即将进行的写入提供足够的空间； 在这个过程中，由于 memtable 中空间的不足可能会冻结当前的 memtable，发生 Minor Compaction 并创建一个新的 MemTable 对象； 在某些条件满足时，也可能发生 Major Compaction，对数据库中的 SSTable 进行压缩； 通过 AddRecord 方法向日志中追加一条写操作的记录； 再向日志成功写入记录后，我们使用 InsertInto 直接插入 memtable 中，完成整个写操作的流程； 在这里，我们并不会提供 LevelDB 对于 Put 方法实现的全部代码，只会展示一份精简后的代码，帮助我们大致了解一下整个写操作的流程： 123456789101112131415161718Status DBImpl::Write(const WriteOptions&amp; options, WriteBatch* my_batch) &#123; Writer w(&amp;mutex_); w.batch = my_batch; MakeRoomForWrite(my_batch == NULL); uint64_t last_sequence = versions_-&gt;LastSequence(); Writer* last_writer = &amp;w; WriteBatch* updates = BuildBatchGroup(&amp;last_writer); WriteBatchInternal::SetSequence(updates, last_sequence + 1); last_sequence += WriteBatchInternal::Count(updates); log_-&gt;AddRecord(WriteBatchInternal::Contents(updates)); WriteBatchInternal::InsertInto(updates, mem_); versions_-&gt;SetLastSequence(last_sequence); return Status::OK();&#125; 不可变的 memtable 在写操作的实现代码 DBImpl::Put 中，写操作的准备过程 MakeRoomForWrite 是我们需要注意的一个方法： 1234567891011121314151617Status DBImpl::MakeRoomForWrite(bool force) &#123; uint64_t new_log_number = versions_-&gt;NewFileNumber(); WritableFile* lfile = NULL; env_-&gt;NewWritableFile(LogFileName(dbname_, new_log_number), &amp;lfile); delete log_; delete logfile_; logfile_ = lfile; logfile_number_ = new_log_number; log_ = new log::Writer(lfile); imm_ = mem_; has_imm_.Release_Store(imm_); mem_ = new MemTable(internal_comparator_); mem_-&gt;Ref(); MaybeScheduleCompaction(); return Status::OK();&#125; 当 LevelDB 中的 memtable 已经被数据填满导致内存已经快不够用的时候，我们会开始对 memtable 中的数据进行冻结并创建一个新的 MemTable 对象。 你可以看到，与 Bigtable 中论文不同的是，LevelDB 中引入了一个不可变的 memtable 结构 imm，它的结构与 memtable 完全相同，只是其中的所有数据都是不可变的。 在切换到新的 memtable 之后，还可能会执行 MaybeScheduleCompaction 来触发一次 Minor Compaction 将 imm 中数据固化成数据库中的 SSTable；imm 的引入能够解决由于 memtable 中数据过大导致压缩时不可写入数据的问题。 引入 imm 后，如果 memtable 中的数据过多，我们可以直接将 memtable 指针赋值给 imm，然后创建一个新的 MemTable 实例，这样就可以继续接受外界的写操作，不再需要等待 Minor Compaction 的结束了。 日志记录的格式 作为一个持久存储的 KV 数据库，LevelDB 一定要有日志模块以支持错误发生时恢复数据，我们想要深入了解 LevelDB 的实现，那么日志的格式是一定绕不开的问题；这里并不打算展示用于追加日志的方法 AddRecord 的实现，因为方法中只是实现了对表头和字符串的拼接。 日志在 LevelDB 是以块的形式存储的，每一个块的长度都是 32KB，固定的块长度也就决定了日志可能存放在块中的任意位置，LevelDB 中通过引入一位 RecordType 来表示当前记录在块中的位置： 123456789enum RecordType &#123; // Zero is reserved for preallocated files kZeroType = 0, kFullType = 1, // For fragments kFirstType = 2, kMiddleType = 3, kLastType = 4&#125;; 日志记录的类型存储在该条记录的头部，其中还存储了 4 字节日志的 CRC 校验、记录的长度等信息： 上图中一共包含 4 个块，其中存储着 6 条日志记录，我们可以通过 RecordType 对每一条日志记录或者日志记录的一部分进行标记，并在日志需要使用时通过该信息重新构造出这条日志记录。 12345678virtual Status Sync() &#123; Status s = SyncDirIfManifest(); if (fflush_unlocked(file_) != 0 || fdatasync(fileno(file_)) != 0) &#123; s = Status::IOError(filename_, strerror(errno)); &#125; return s;&#125; 因为向日志中写新记录都是顺序写的，所以它写入的速度非常快，当在内存中写入完成时，也会直接将缓冲区的这部分的内容 fflush 到磁盘上，实现对记录的持久化，用于之后的错误恢复等操作。 记录的插入 当一条数据的记录写入日志时，这条记录仍然无法被查询，只有当该数据写入 memtable 后才可以被查询，而这也是这一节将要介绍的内容，无论是数据的插入还是数据的删除都会向 memtable 中添加一条记录。 添加和删除的记录的区别就是它们使用了不用的 ValueType 标记，插入的数据会将其设置为 kTypeValue，删除的操作会标记为 kTypeDeletion；但是它们实际上都向 memtable 中插入了一条数据。 12345678virtual void Put(const Slice&amp; key, const Slice&amp; value) &#123; mem_-&gt;Add(sequence_, kTypeValue, key, value); sequence_++;&#125;virtual void Delete(const Slice&amp; key) &#123; mem_-&gt;Add(sequence_, kTypeDeletion, key, Slice()); sequence_++;&#125; 我们可以看到它们都调用了 memtable 的 Add 方法，向其内部的数据结构 skiplist 以上图展示的格式插入数据，这条数据中既包含了该记录的键值、序列号以及这条记录的种类，这些字段会在拼接后存入 skiplist；既然我们并没有在 memtable 中对数据进行删除，那么我们是如何保证每次取到的数据都是最新的呢？首先，在 skiplist 中，我们使用了自己定义的一个 comparator： 12345678910111213int InternalKeyComparator::Compare(const Slice&amp; akey, const Slice&amp; bkey) const &#123; int r = user_comparator_-&gt;Compare(ExtractUserKey(akey), ExtractUserKey(bkey)); if (r == 0) &#123; const uint64_t anum = DecodeFixed64(akey.data() + akey.size() - 8); const uint64_t bnum = DecodeFixed64(bkey.data() + bkey.size() - 8); if (anum &gt; bnum) &#123; r = -1; &#125; else if (anum &lt; bnum) &#123; r = +1; &#125; &#125; return r;&#125; 比较的两个 key 中的数据可能包含的内容都不完全相同，有的会包含键值、序列号等全部信息，但是例如从 Get 方法调用过来的 key 中可能就只包含键的长度、键值和序列号了，但是这并不影响这里对数据的提取，因为我们只从每个 key 的头部提取信息，所以无论是完整的 key/value 还是单独的 key，我们都不会取到 key 之外的任何数据。 该方法分别从两个不同的 key 中取出键和序列号，然后对它们进行比较；比较的过程就是使用 InternalKeyComparator 比较器，它通过 user_key 和 sequence_number 进行排序，其中 user_key 按照递增的顺序排序、sequence_number 按照递减的顺序排序，因为随着数据的插入序列号是不断递增的，所以我们可以保证先取到的都是最新的数据或者删除信息。 在序列号的帮助下，我们并不需要对历史数据进行删除，同时也能加快写操作的速度，提升 LevelDB 的写性能。 数据的读取 从 LevelDB 中读取数据其实并不复杂，memtable 和 imm 更像是两级缓存，它们在内存中提供了更快的访问速度，如果能直接从内存中的这两处直接获取到响应的值，那么它们一定是最新的数据。 LevelDB 总会将新的键值对写在最前面，并在数据压缩时删除历史数据。 数据的读取是按照 MemTable、Immutable MemTable 以及不同层级的 SSTable 的顺序进行的，前两者都是在内存中，后面不同层级的 SSTable 都是以 *.ldb 文件的形式持久存储在磁盘上，而正是因为有着不同层级的 SSTable，所以我们的数据库的名字叫做 LevelDB。 精简后的读操作方法的实现代码是这样的，方法的脉络非常清晰，作者相信这里也不需要过多的解释： 12345678910111213Status DBImpl::Get(const ReadOptions&amp; options, const Slice&amp; key, std::string* value) &#123; LookupKey lkey(key, versions_-&gt;LastSequence()); if (mem_-&gt;Get(lkey, value, NULL)) &#123; // Done &#125; else if (imm_ != NULL &amp;&amp; imm_-&gt;Get(lkey, value, NULL)) &#123; // Done &#125; else &#123; versions_-&gt;current()-&gt;Get(options, lkey, value, NULL); &#125; MaybeScheduleCompaction(); return Status::OK();&#125; 当 LevelDB 在 memtable 和 imm 中查询到结果时，如果查询到了数据并不一定表示当前的值一定存在，它仍然需要判断 ValueType 来确定当前记录是否被删除。 多层级的 SSTable 当 LevelDB 在内存中没有找到对应的数据时，它才会到磁盘中多个层级的 SSTable 中进行查找，这个过程就稍微有一点复杂了，LevelDB 会在多个层级中逐级进行查找，并且不会跳过其中的任何层级；在查找的过程就涉及到一个非常重要的数据结构 FileMetaData： FileMetaData 中包含了整个文件的全部信息，其中包括键的最大值和最小值、允许查找的次数、文件被引用的次数、文件的大小以及文件号，因为所有的 SSTable 都是以固定的形式存储在同一目录下的，所以我们可以通过文件号轻松查找到对应的文件。 查找的顺序就是从低到高了，LevelDB 首先会在 Level0 中查找对应的键。但是，与其他层级不同，Level0 中多个 SSTable 的键的范围有重合部分的，在查找对应值的过程中，会依次查找 Level0 中固定的 4 个 SSTable。 但是当涉及到更高层级的 SSTable 时，因为同一层级的 SSTable 都是没有重叠部分的，所以我们在查找时可以利用已知的 SSTable 中的极值信息 smallest/largest 快速查找到对应的 SSTable，再判断当前的 SSTable 是否包含查询的 key，如果不存在，就继续查找下一个层级直到最后的一个层级 kNumLevels（默认为 7 级）或者查询到了对应的值。 SSTable 的『合并』 既然 LevelDB 中的数据是通过多个层级的 SSTable 组织的，那么它是如何对不同层级中的 SSTable 进行合并和压缩的呢；与 Bigtable 论文中描述的两种 Compaction 几乎完全相同，LevelDB 对这两种压缩的方式都进行了实现。 无论是读操作还是写操作，在执行的过程中都可能调用 MaybeScheduleCompaction 来尝试对数据库中的 SSTable 进行合并，当合并的条件满足时，最终都会执行 BackgroundCompaction 方法在后台完成这个步骤。 这种合并分为两种情况，一种是 Minor Compaction，即内存中的数据超过了 memtable 大小的最大限制，改 memtable 被冻结为不可变的 imm，然后执行方法 CompactMemTable() 对内存表进行压缩。 1234567void DBImpl::CompactMemTable() &#123; VersionEdit edit; Version* base = versions_-&gt;current(); WriteLevel0Table(imm_, &amp;edit, base); versions_-&gt;LogAndApply(&amp;edit, &amp;mutex_); DeleteObsoleteFiles();&#125; CompactMemTable 会执行 WriteLevel0Table 将当前的 imm 转换成一个 Level0 的 SSTable 文件，同时由于 Level0 层级的文件变多，可能会继续触发一个新的 Major Compaction，在这里我们就需要在这里选择需要压缩的合适的层级： 123456789101112Status DBImpl::WriteLevel0Table(MemTable* mem, VersionEdit* edit, Version* base) &#123; FileMetaData meta; meta.number = versions_-&gt;NewFileNumber(); Iterator* iter = mem-&gt;NewIterator(); BuildTable(dbname_, env_, options_, table_cache_, iter, &amp;meta); const Slice min_user_key = meta.smallest.user_key(); const Slice max_user_key = meta.largest.user_key(); int level = base-&gt;PickLevelForMemTableOutput(min_user_key, max_user_key); edit-&gt;AddFile(level, meta.number, meta.file_size, meta.smallest, meta.largest); return Status::OK();&#125; 所有对当前 SSTable 数据的修改由一个统一的 VersionEdit 对象记录和管理，我们会在后面介绍这个对象的作用和实现，如果成功写入了就会返回这个文件的元数据 FileMetaData，最后调用 VersionSet 的方法 LogAndApply 将文件中的全部变化如实记录下来，最后做一些数据的清理工作。 当然如果是 Major Compaction 就稍微有一些复杂了，不过整理后的 BackgroundCompaction 方法的逻辑非常清晰： 12345678910111213void DBImpl::BackgroundCompaction() &#123; if (imm_ != NULL) &#123; CompactMemTable(); return; &#125; Compaction* c = versions_-&gt;PickCompaction(); CompactionState* compact = new CompactionState(c); DoCompactionWork(compact); CleanupCompaction(compact); DeleteObsoleteFiles();&#125; 我们从当前的 VersionSet 中找到需要压缩的文件信息，将它们打包存入一个 Compaction 对象，该对象需要选择两个层级的 SSTable，低层级的表很好选择，只需要选择大小超过限制的或者查询次数太多的 SSTable；当我们选择了低层级的一个 SSTable 后，就在更高的层级选择与该 SSTable 有重叠键的 SSTable 就可以了，通过 FileMetaData 中数据的帮助我们可以很快找到待压缩的全部数据。 查询次数太多的意思就是，当客户端调用多次 Get 方法时，如果这次 Get 方法在某个层级的 SSTable 中找到了对应的键，那么就算做上一层级中包含该键的 SSTable 的一次查找，也就是这次查找由于不同层级键的覆盖范围造成了更多的耗时，每个 SSTable 在创建之后的 allowed_seeks 都为 100 次，当 allowed_seeks &lt; 0 时就会触发该文件的与更高层级和合并，以减少以后查询的查找次数。 LevelDB 中的 DoCompactionWork 方法会对所有传入的 SSTable 中的键值使用归并排序进行合并，最后会在高高层级（图中为 Level2）中生成一个新的 SSTable。 这样下一次查询 17~40 之间的值时就可以减少一次对 SSTable 中数据的二分查找以及读取文件的时间，提升读写的性能。 存储 db 状态的 VersionSet LevelDB 中的所有状态其实都是被一个 VersionSet 结构所存储的，一个 VersionSet 包含一组 Version 结构体，所有的 Version 包括历史版本都是通过双向链表连接起来的，但是只有一个版本是当前版本。 当 LevelDB 中的 SSTable 发生变动时，它会生成一个 VersionEdit 结构，最终执行 LogAndApply 方法： 1234567891011121314151617181920Status VersionSet::LogAndApply(VersionEdit* edit, port::Mutex* mu) &#123; Version* v = new Version(this); Builder builder(this, current_); builder.Apply(edit); builder.SaveTo(v); std::string new_manifest_file; new_manifest_file = DescriptorFileName(dbname_, manifest_file_number_); env_-&gt;NewWritableFile(new_manifest_file, &amp;descriptor_file_); std::string record; edit-&gt;EncodeTo(&amp;record); descriptor_log_-&gt;AddRecord(record); descriptor_file_-&gt;Sync(); SetCurrentFile(env_, dbname_, manifest_file_number_); AppendVersion(v); return Status::OK();&#125; 该方法的主要工作是使用当前版本和 VersionEdit 创建一个新的版本对象，然后将 Version 的变更追加到 MANIFEST 日志中，并且改变数据库中全局当前版本信息。 MANIFEST 文件中记录了 LevelDB 中所有层级中的表、每一个 SSTable 的 Key 范围和其他重要的元数据，它以日志的格式存储，所有对文件的增删操作都会追加到这个日志中。 SSTable 的格式 SSTable 中其实存储的不只是数据，其中还保存了一些元数据、索引等信息，用于加速读写操作的速度，虽然在 Bigtable 的论文中并没有给出 SSTable 的数据格式，不过在 LevelDB 的实现中，我们可以发现 SSTable 是以这种格式存储数据的： 当 LevelDB 读取 SSTable 存在的 ldb 文件时，会先读取文件中的 Footer 信息。 整个 Footer 在文件中占用 48 个字节，我们能在其中拿到 MetaIndex 块和 Index 块的位置，再通过其中的索引继而找到对应值存在的位置。 TableBuilder::Rep 结构体中就包含了一个文件需要创建的全部信息，包括数据块、索引块等等： 1234567891011struct TableBuilder::Rep &#123; WritableFile* file; uint64_t offset; BlockBuilder data_block; BlockBuilder index_block; std::string last_key; int64_t num_entries; bool closed; FilterBlockBuilder* filter_block; ...&#125; 到这里，我们就完成了对整个数据读取过程的解析了；对于读操作，我们可以理解为 LevelDB 在它内部的『多级缓存』中依次查找是否存在对应的键，如果存在就会直接返回，唯一与缓存不同可能就是，在数据『命中』后，它并不会把数据移动到更近的地方，而是会把数据移到更远的地方来减少下一次的访问时间，虽然这么听起来却是不可思议，不过仔细想一下确实是这样。 小结 在这篇文章中，我们通过对 LevelDB 源代码中读写操作的分析，了解了整个框架的绝大部分实现细节，包括 LevelDB 中存储数据的格式、多级 SSTable、如何进行合并以及管理版本等信息，不过由于篇幅所限，对于其中的一些问题并没有展开详细地进行介绍和分析，例如错误恢复以及缓存等问题；但是对 LevelDB 源代码的阅读，加深了我们对 Bigtable 论文中描述的分布式 KV 存储数据库的理解。 LevelDB 的源代码非常易于阅读，也是学习 C++ 语言非常优秀的资源，如果对文章的内容有疑问，可以在博客下面留言。 Reference Bigtable: A Distributed Storage System for Structured Data LevelDB The Chubby lock service for loosely-coupled distributed systems LevelDB · Impl leveldb 中的 SSTable 本文地址：http://xnerv.wang/bigtable-leveldb/ 转载自：浅析 Bigtable 和 LevelDB 的实现","categories":[{"name":"分布式及存储","slug":"分布式及存储","permalink":"https://xnerv.wang/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8F%8A%E5%AD%98%E5%82%A8/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"分布式及存储","slug":"分布式及存储","permalink":"https://xnerv.wang/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8F%8A%E5%AD%98%E5%82%A8/"},{"name":"Bigtable","slug":"Bigtable","permalink":"https://xnerv.wang/tags/Bigtable/"},{"name":"LevelDB","slug":"LevelDB","permalink":"https://xnerv.wang/tags/LevelDB/"}]},{"title":"LevelDB 实现分析（转载）","slug":"leveldb-analysis","date":"2018-03-17T01:20:00.000Z","updated":"2023-08-21T02:24:20.511Z","comments":true,"path":"leveldb-analysis/","link":"","permalink":"https://xnerv.wang/leveldb-analysis/","excerpt":"LevelDB 介绍 LevelDB 是由 Google 开发的 key-value 非关系型数据库存储系统，是基于 LSM(Log-Structured-Merge Tree) 的典型实现，LSM 的原理是：当读写数据库时，首先纪录读写操作到 Op log 文件中，然后再操作内存数据库，当达到 checkpoint 时，则写入磁盘，同时删除相应的 Op log 文件，后续重新生成新的内存文件和 Op log 文件。 LevelDB 内部采用了内存缓存机制，也就是在写数据库时，首先会存储在内存中，内存的存储结构采用了 skip list 结构，待达到 checkpoint 时，才进行落盘操作，保证了数据库的高效运转。 LevelDB 总体架构","text":"LevelDB 介绍 LevelDB 是由 Google 开发的 key-value 非关系型数据库存储系统，是基于 LSM(Log-Structured-Merge Tree) 的典型实现，LSM 的原理是：当读写数据库时，首先纪录读写操作到 Op log 文件中，然后再操作内存数据库，当达到 checkpoint 时，则写入磁盘，同时删除相应的 Op log 文件，后续重新生成新的内存文件和 Op log 文件。 LevelDB 内部采用了内存缓存机制，也就是在写数据库时，首先会存储在内存中，内存的存储结构采用了 skip list 结构，待达到 checkpoint 时，才进行落盘操作，保证了数据库的高效运转。 LevelDB 总体架构 如上图所示，整个 LevelDB 由以下几部分组成： Write(k,v)，对外的接口 Op log，操作日志记录文件 memtable，数据库存储的内存结构 Immutable memtable，待落盘的数据库内存数据 sstable，落盘后的磁盘存储结构 manifest，LevelDB 元信息清单，包括数据库的配置信息和中间使用的文件列表 current，当前正在使用的文件清单 整体结构清晰紧凑，非常容易理解。 对外接口 123456789101112131415DB() &#123; &#125;;virtual ~DB();static Status Open(const Options&amp; options, const std::string&amp; name, DB** dbptr);virtual Status Put(const WriteOptions&amp; options, const Slice&amp; key, const Slice&amp; value) = 0;virtual Status Delete(const WriteOptions&amp; options, const Slice&amp; key) = 0;virtual Status Write(const WriteOptions&amp; options, WriteBatch* updates) = 0;virtual Status Get(const ReadOptions&amp; options, const Slice&amp; key, std::string* value) = 0;virtual Iterator* NewIterator(const ReadOptions&amp; options) = 0;virtual const Snapshot* GetSnapshot() = 0;virtual void ReleaseSnapshot(const Snapshot* snapshot) = 0; 整体接口分为： 数据库创建和删除 123DB() &#123; &#125;;virtual ~DB(); 数据库打开 123static Status Open(const Options&amp; options, const std::string&amp; name, DB** dbptr); 数据库读写删除操作 123456virtual Status Put(const WriteOptions&amp; options, const Slice&amp; key, const Slice&amp; value) = 0;virtual Status Delete(const WriteOptions&amp; options, const Slice&amp; key) = 0;virtual Status Get(const ReadOptions&amp; options, const Slice&amp; key, std::string* value) = 0; 数据库批处理操作 1virtual Status Write(const WriteOptions&amp; options, WriteBatch* updates) = 0; 数据库遍历操作 12virtual Iterator* NewIterator(const ReadOptions&amp; options) = 0; 获取快照操作 12virtual const Snapshot* GetSnapshot() = 0;virtual void ReleaseSnapshot(const Snapshot* snapshot) = 0; Op log结构分析 LevelDB 使用的 Op log 日志采用了文件记录的方式，且文件使用了 mmap 方式操作，以提高效率。 Op log 存储切分为 32KB 大小的数据块，每个 32KB 数据块存储着 Op log，每 个Op log 格式如下： 其中： CRC32 为 crc 校验码，保证数据的完整性 Length，为 Op log 的数据长度 Log Type，Op log 的类型，之所以会有类型，是由于 32KB 可能存不下一条 Op log，Op log 有可能跨数据块，类型分为： FULL：代表 Data 包含了所有的数据 FIRST：代表该 Data 是 Op log 的开始数据 MIDDLE：代表该 Data 是 Op log 的中间数据 LAST: 代表该 Data 是 Op log 的结束数据 Data，为 Op log 的实际数据 memtable 结构分析 memtable 是 LevelDB 数据库的内存存储结构，采用了 skip list 结构存储，如下图所示： skip list 是一种可以代替平衡树的存储结构，它采用概率的方式来保证平衡，而平衡树则是采用严格的旋转树结构来保证平衡，复杂度会高一些。 对于 skip list，会有 n 层链表，其中 0 层保存所有的值，越往上层，保存的值越少。每当插入一个值时，会通过概率计算该值需要插入的最高层级 k，然后从 0~k-1 层，分别插入该值。 其中每个表项的存储结构如下： 1key_size | key_value | sequence_num&amp;type | value_size | value 其中： sequence_num：表示操作的序列号，每一个数据项都会带有一个序列号，用以表示数据的新旧程度。 type：表示数据的类型，分为： kTypeValue：表明数据有效 kTypeDeletion：表明数据已经失效，在数据进行 delete 操作时会打上该标识 sstable 结构分析 sstable 作为落盘的存储结构，每个 sstable 最大 2MB，从宏观来看，它属于分层的结构，即： level 0：最多存储 4 个 sstable level 1：存储不超过 10MB 大小的 sstable level 2：存储不超过 100MB 大小的 sstable level 3 及之后：存储大小不超过上一级大小的 10 倍 之所以这样分层，是为了提高查找效率，也是 LevelDB 名称的由来。当每一层超过限制时，会进行 compaction 操作，合并到上一层，递归进行。 从微观的角度看，每个 sstable 文件结构入下图所示： 其中： Data Block 存储具体的 k-v 数据 Meta Block 存储索引过滤信息，用于快速定位 key 是否存在于 Data Block 中 Meta Index Block 存储 Meta Block 的偏移位置及大小 Index Block 存储 Data Block 的偏移位置及大小 Footer 则存储 Meta Index Block 和 Index Block 的偏移位置及大小，相当于二级索引，Footer 的结构如下： 另外 Data Block 及 Meta Block 的存储格式是统一的，都是如下格式： 其中 type 表示是否是压缩存储，目前 LevelDB 支持 key 值的 snappy 压缩或者不压缩。 而上图中的 Block data 的格式则为： 上图有几点要说明： 对于 Block data 中的第一项总是不压缩存储的，不压缩存储的项称为 restarts，会被记录在上图的最尾部，同时每隔 k 个值（k 值可定制），都会存储一个不压缩的项，这些都称为 restarts，都会被记录在最尾部。 每个 restarts 表项会作为索引项存储。 除了 restarts 表项以外，其它的表项则基于该 restarts 项，计算跟他相同部分和不同部分，上图中的 shared_bytes 和 unshared_bytes 记录了相同部分长度和不同部分的长度，key_delta 则记录了不同的部分的值，value_length 和 value 则记录了 value 部分的值。 压不压缩是可选的，默认会进行 snappy 压缩。 对于 Meta Block 来说，它保存了用于快速定位 key 是否在 Data Block 中的信息，具体方法是： 采用了 bloom filter 的过滤机制，bloom filter 是一种 hash 机制，它对每一个 key，会计算 k 个 hash 值，然后在 k 个 bit 位记录为 1。当查找时，相应计算出 k 个 hash 值，然后比对 k 个 bit 位是否为 1，只要有一个不为 1，则不存在。 对于每一个 Data Block，所有的 key 值会传入进行 bloom filter 的 hash 计算，每个 key 存储 k 个 bit 位值。 版本管理 对于 LevelDB 来说，它采用了简单的 sequence num 机制来管理，具体为： 对于 Op log 文件，每一个 Op log 文件名中会包含一个唯一的 sequence num，每创建一个新的 Op log 文件，sequence num 则加 1，sequence num 越大，则表示文件越新，同时最新的 sequence num 会记录下来。 对于每个 key-value 对，也会对应一个 sequence num，对于同一个 key，如果后续更新值时，sequence num 也会相应更新，这样就可以根据 sequence num 的大小，找到最新的 key-value 对 新增特性 支持模糊查询 该功能支持 key 以模糊规则匹配的方式进行数据库查询，支持＊和？两种模糊规则查询。 支持 JSON 格式数据存储 该功能支持 k-v 中，v以json格式传入，后续可以通过关键字，查询json里面的数据。 结束语 LevelDB 短小精悍，代码运行效率高效，且通俗易懂，是一个非常不错的 k-v 存储系统。 注：图片来源于网络 题图：https://unsplash.com/photos/9wwF-VmSOrY By @eberhard grossgasteiger 本文地址：http://xnerv.wang/leveldb-analysis/ 转载自：LevelDB 实现分析","categories":[{"name":"分布式及存储","slug":"分布式及存储","permalink":"https://xnerv.wang/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8F%8A%E5%AD%98%E5%82%A8/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"分布式及存储","slug":"分布式及存储","permalink":"https://xnerv.wang/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8F%8A%E5%AD%98%E5%82%A8/"},{"name":"LevelDB","slug":"LevelDB","permalink":"https://xnerv.wang/tags/LevelDB/"}]},{"title":"(Stack Overflow) Understanding SIX lock in SQL Server","slug":"understanding-six-lock-in-sql-server","date":"2018-02-22T00:53:00.000Z","updated":"2023-08-21T02:24:18.609Z","comments":true,"path":"understanding-six-lock-in-sql-server/","link":"","permalink":"https://xnerv.wang/understanding-six-lock-in-sql-server/","excerpt":"Question Can somebody explain me how a process can acquire SIX lock on a page? I my deadlock-graph xml file I see that a process running under RC isolation level (executing a select statement at the moment of deadlock) holds a SIX lock on a page. What does this mean and how that lock could have been acquired? From what I got from http://msdn.microsoft.com/en-us/library/aa213039%28v=sql.80%29.aspx SIX locks protects S-locks on all resources and IX locks on some resources lower in the hierarchy. For my case that would be IX-locks on rows? Can IX-lock be placed on a row? (I guess no). I am confused. Another thing is that I expect several X-locks on rows and no S-locks at all (since the IL is ReadCommited). Why do I have the whole page locked with SIX if I only inserted several records in previous statement? up!","text":"Question Can somebody explain me how a process can acquire SIX lock on a page? I my deadlock-graph xml file I see that a process running under RC isolation level (executing a select statement at the moment of deadlock) holds a SIX lock on a page. What does this mean and how that lock could have been acquired? From what I got from http://msdn.microsoft.com/en-us/library/aa213039%28v=sql.80%29.aspx SIX locks protects S-locks on all resources and IX locks on some resources lower in the hierarchy. For my case that would be IX-locks on rows? Can IX-lock be placed on a row? (I guess no). I am confused. Another thing is that I expect several X-locks on rows and no S-locks at all (since the IL is ReadCommited). Why do I have the whole page locked with SIX if I only inserted several records in previous statement? up! Answer by Sebastian Meine To answer that I have to take a little detour, so bear with me. If two sessions take a lock on the same resource SQL Server checks the lock compatibility map and if the second request is not “compatible” with the first, the second session has to wait. There are three lock types &quot;S&quot;hared, &quot;U&quot;pdate and e&quot;X&quot;clusive. S locks are taken to read from a resource and X locks are taken to write to a resource. S locks are compatible with each other, X locks are not compatible with anything else. U locks are a hybrid that is used in some cases for deadlock prevention. Now, SQL Server can take locks on several levels:Table, Partition, Page and Row. So if session one takes a table lock and session two takes a non-compatible lock on one row of the table, those two locks are not on the same resource and SQL Server won’t detect the collision. To protect against that, SQL Server always starts to take a lock on the table level and works its way down the hierarchy. Now the point of page and row locks is higher concurrency, so if one session wants to write to one row and another session wants to write to another row, they should not block each other. If a session in addition to taking a lock on a row also has to take the same lock on the table, that advantage is gone. So instead of taking an exclusive lock (X) on the table, the session requests an intend-exclusive lock (IX). This lock is compatible with other intend locks but not with other “real” locks. So another session can take an intend-exclusive lock on the same table as well. The intend-exclusive lock says, that the session intends to take an exclusive lock on a lower level resource. The same happens on the page level, if the intended lock is a row lock, so after all is done, the session has an IX lock on the table and on one of the pages and an X lock on one of the rows in that page. This also means, that you will never find an intend lock on a row as rows are the lowest level in the lock hierarchy. In some circumstances a session holds an S lock on the table or a page. If the session now (within the same transaction) requests an X lock on a row in that same table, it first has to take an IX lock on the table/page. However, a session can hold only one lock on any given resource. So to take the IX lock, it would have to release the S lock wich is probably not desired, so SQL Server offers a combination: SIX. The reason why you have a page lock is due to SQL Server sometimes deciding that it would be better to lock the page instead of locking each row. That happens often if there are very many locks taken between all sessions already, but can have many other reasons too. So far the theory. Now in your case the SIX lock is held by a three table join select query. A select never takes any type of lock that is not a shared lock unless you explicitly tell it to (e.g. with a XLOCK hint). Such a hint is not visible within the input buffer, so I assume the IX part is a left over from the last batch on this connection. If you are using connection pooling and forget to cleanup all open transactions, such a lock can live potentially forever. But it becomes also very hard to troubleshoot. You could start by running an XEvent session that pairs OPEN TRANs with COMMITs and see if you can find the culprit that way. Article link: http://xnerv.wang/understanding-six-lock-in-sql-server/ Reprinted from: (StackOverflow) Understanding SIX lock in SQL Server","categories":[{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/categories/DBMS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/tags/DBMS/"},{"name":"SQL Server","slug":"SQL-Server","permalink":"https://xnerv.wang/tags/SQL-Server/"},{"name":"Transaction","slug":"Transaction","permalink":"https://xnerv.wang/tags/Transaction/"},{"name":"SIX Lock","slug":"SIX-Lock","permalink":"https://xnerv.wang/tags/SIX-Lock/"}]},{"title":"SQL Server - Buffer Management - Writing Pages（转载）","slug":"sql-server-buffer-management-writing-pages","date":"2018-02-21T09:16:00.000Z","updated":"2023-08-21T02:24:19.314Z","comments":true,"path":"sql-server-buffer-management-writing-pages/","link":"","permalink":"https://xnerv.wang/sql-server-buffer-management-writing-pages/","excerpt":"The I/O from an instance of the Database Engine includes logical and physical writes. A logical write occurs when data is modified in a page in the buffer cache. A physical write occurs when the page is written from the buffer cache to disk. When a page is modified in the buffer cache, it is not immediately written back to disk; instead, the page is marked as dirty. This means that a page can have more than one logical write made before it is physically written to disk. For each logical write, a transaction log record is inserted in the log cache that records the modification. The log records must be written to disk before the associated dirty page is removed from the buffer cache and written to disk. SQL Server uses a technique known as write-ahead logging that prevents writing a dirty page before the associated log record is written to disk. This is essential to the correct working of the recovery manager. For more information, see Write-Ahead Transaction Log.","text":"The I/O from an instance of the Database Engine includes logical and physical writes. A logical write occurs when data is modified in a page in the buffer cache. A physical write occurs when the page is written from the buffer cache to disk. When a page is modified in the buffer cache, it is not immediately written back to disk; instead, the page is marked as dirty. This means that a page can have more than one logical write made before it is physically written to disk. For each logical write, a transaction log record is inserted in the log cache that records the modification. The log records must be written to disk before the associated dirty page is removed from the buffer cache and written to disk. SQL Server uses a technique known as write-ahead logging that prevents writing a dirty page before the associated log record is written to disk. This is essential to the correct working of the recovery manager. For more information, see Write-Ahead Transaction Log. The following illustration shows the process for writing a modified data page. When the buffer manager writes a page, it searches for adjacent dirty pages that can be included in a single gather-write operation. Adjacent pages have consecutive page IDs and are from the same file; the pages do not have to be contiguous in memory. The search continues both forward and backward until one of the following events occurs: A clean page is found. 32 pages have been found. A dirty page is found whose log sequence number (LSN) has not yet been flushed in the log. A page is found that cannot be immediately latched. In this way, the entire set of pages can be written to disk with a single gather-write operation. Just before a page is written, the form of page protection specified in the database is added to the page. If torn page protection is added, the page must be latched EX(clusively) for the I/O. This is because the torn page protection modifies the page, making it unsuitable for any other thread to read. If checksum page protection is added, or the database uses no page protection, the page is latched with an UP(date) latch for the I/O. This latch prevents anyone else from modifying the page during the write, but still allows readers to use it. For more information about disk I/O page protection options, see Buffer Management. A dirty page is written to disk in one of three ways. Lazy writing The lazy writer is a system process that keeps free buffers available by removing infrequently used pages from the buffer cache. Dirty pages are first written to disk. Eager writing The eager write process writes dirty data pages associated with nonlogged operations such as bulk insert and select into. This process allows creating and writing new pages to take place in parallel. That is, the calling operation does not have to wait until the entire operation finishes before writing the pages to disk. Checkpoint The checkpoint process periodically scans the buffer cache for buffers with pages from a specified database and writes all dirty pages to disk. Checkpoints save time during a later recovery by creating a point at which all dirty pages are guaranteed to have been written to disk. The user may request a checkpoint operation by using the CHECKPOINT command, or the Database Engine may generate automatic checkpoints based on the amount of log space used and time elapsed since the last checkpoint. In addition, a checkpoint is generated when certain activities occur. For example, when a data or log file is added or removed from a database, or when the instance of SQL Server is stopped. For more information, see Checkpoints and the Active Portion of the Log. The lazy writing, eager writing, and checkpoint processes do not wait for the I/O operation to complete. They always use asynchronous (or overlapped) I/O and continue with other work, checking for I/O success later. This allows SQL Server to maximize both CPU and I/O resources for the appropriate tasks. 本文地址：http://xnerv.wang/sql-server-buffer-management-writing-pages/ 转载自：Writing Pages","categories":[{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/categories/DBMS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/tags/DBMS/"},{"name":"SQL Server","slug":"SQL-Server","permalink":"https://xnerv.wang/tags/SQL-Server/"},{"name":"Buffer Pool","slug":"Buffer-Pool","permalink":"https://xnerv.wang/tags/Buffer-Pool/"},{"name":"Lazy Writing","slug":"Lazy-Writing","permalink":"https://xnerv.wang/tags/Lazy-Writing/"},{"name":"Eager Writing","slug":"Eager-Writing","permalink":"https://xnerv.wang/tags/Eager-Writing/"},{"name":"Checkpoint","slug":"Checkpoint","permalink":"https://xnerv.wang/tags/Checkpoint/"}]},{"title":"SQL Server - Buffer Management - Reading Pages（转载）","slug":"sql-server-buffer-management-reading-pages","date":"2018-02-21T09:07:00.000Z","updated":"2023-08-21T02:24:19.299Z","comments":true,"path":"sql-server-buffer-management-reading-pages/","link":"","permalink":"https://xnerv.wang/sql-server-buffer-management-reading-pages/","excerpt":"The I/O from an instance of the SQL Server Database Engine includes logical and physical reads. A logical read occurs every time the Database Engine requests a page from the buffer cache. If the page is not currently in the buffer cache, a physical read first copies the page from disk into the cache. The read requests generated by an instance of the Database Engine are controlled by the relational engine and optimized by the storage engine. The relational engine determines the most effective access method (such as a table scan, an index scan, or a keyed read); the access methods and buffer manager components of the storage engine determine the general pattern of reads to perform, and optimize the reads required to implement the access method. The thread executing the batch schedules the reads.","text":"The I/O from an instance of the SQL Server Database Engine includes logical and physical reads. A logical read occurs every time the Database Engine requests a page from the buffer cache. If the page is not currently in the buffer cache, a physical read first copies the page from disk into the cache. The read requests generated by an instance of the Database Engine are controlled by the relational engine and optimized by the storage engine. The relational engine determines the most effective access method (such as a table scan, an index scan, or a keyed read); the access methods and buffer manager components of the storage engine determine the general pattern of reads to perform, and optimize the reads required to implement the access method. The thread executing the batch schedules the reads. Read-Ahead The Database Engine supports a performance optimization mechanism called read-ahead. Read-ahead anticipates the data and index pages needed to fulfill a query execution plan and brings the pages into the buffer cache before they are actually used by the query. This allows computation and I/O to overlap, taking full advantage of both the CPU and the disk. The read-ahead mechanism allows the Database Engine to read up to 64 contiguous pages (512KB) from one file. The read is performed as a single scatter-gather read to the appropriate number of (probably non-contiguous) buffers in the buffer cache. If any of the pages in the range are already present in the buffer cache, the corresponding page from the read will be discarded when the read completes. The range of pages may also be “trimmed” from either end if the corresponding pages are already present in the cache. There are two kinds of read-ahead: one for data pages and one for index pages. Reading Data Pages Table scans used to read data pages are very efficient in the Database Engine. The index allocation map (IAM) pages in a SQL Server database list the extents used by a table or index. The storage engine can read the IAM to build a sorted list of the disk addresses that must be read. This allows the storage engine to optimize its I/Os as large sequential reads that are performed in sequence, based on their location on the disk. For more information about IAM pages, see Managing Space Used by Objects. Reading Index Pages The storage engine reads index pages serially in key order. For example, this illustration shows a simplified representation of a set of leaf pages that contains a set of keys and the intermediate index node mapping the leaf pages. For more information about the structure of pages in an index, see Clustered Index Structures. The storage engine uses the information in the intermediate index page above the leaf level to schedule serial read-aheads for the pages that contain the keys. If a request is made for all the keys from ABC to DEF, the storage engine first reads the index page above the leaf page. However, it does not just read each data page in sequence from page 504 to page 556 (the last page with keys in the specified range). Instead, the storage engine scans the intermediate index page and builds a list of the leaf pages that must be read. The storage engine then schedules all the reads in key order. The storage engine also recognizes that pages 504/505 and 527/528 are contiguous and performs a single scatter read to retrieve the adjacent pages in a single operation. When there are many pages to be retrieved in a serial operation, the storage engine schedules a block of reads at a time. When a subset of these reads is completed, the storage engine schedules an equal number of new reads until all the required reads have been scheduled. The storage engine uses prefetching to speed base table lookups from nonclustered indexes. The leaf rows of a nonclustered index contain pointers to the data rows that contain each specific key value. As the storage engine reads through the leaf pages of the nonclustered index, it also starts scheduling asynchronous reads for the data rows whose pointers have already been retrieved. This allows the storage engine to retrieve data rows from the underlying table before it has completed the scan of the nonclustered index. Prefetching is used regardless of whether the table has a clustered index. SQL Server Enterprise uses more prefetching than other editions of SQL Server, allowing more pages to be read ahead. The level of prefetching is not configurable in any edition. For more information about nonclustered indexes, see Nonclustered Index Structures. Advanced Scanning In SQL Server Enterprise, the advanced scan feature allows multiple tasks to share full table scans. If the execution plan of a Transact-SQL statement requires a scan of the data pages in a table and the Database Engine detects that the table is already being scanned for another execution plan, the Database Engine joins the second scan to the first, at the current location of the second scan. The Database Engine reads each page one time and passes the rows from each page to both execution plans. This continues until the end of the table is reached. At that point, the first execution plan has the complete results of a scan, but the second execution plan must still retrieve the data pages that were read before it joined the in-progress scan. The scan for the second execution plan then wraps back to the first data page of the table and scans forward to where it joined the first scan. Any number of scans can be combined like this. The Database Engine will keep looping through the data pages until it has completed all the scans. This mechanism is also called “merry-go-round scanning” and demonstrates why the order of the results returned from a SELECT statement cannot be guaranteed without an ORDER BY clause. For example, assume that you have a table with 500,000 pages. UserA executes a Transact-SQL statement that requires a scan of the table. When that scan has processed 100,000 pages, UserB executes another Transact-SQL statement that scans the same table. The Database Engine schedules one set of read requests for pages after 100,001, and passes the rows from each page back to both scans. When the scan reaches the 200,000th page, UserC executes another Transact-SQL statement that scans the same table. Starting with page 200,001, the Database Engine passes the rows from each page it reads back to all three scans. After it reads the 500,000th row, the scan for UserA is complete, and the scans for UserB and UserC wrap back and start to read the pages starting with page 1. When the Database Engine gets to page 100,000, the scan for UserB is completed. The scan for UserC then keeps going alone until it reads page 200,000. At this point, all the scans have been completed. Without advanced scanning, each user would have to compete for buffer space and cause disk arm contention. The same pages would then be read once for each user, instead of read one time and shared by multiple users, slowing down performance and taxing resources. See Also Concepts Clustered Index Structures Nonclustered Index Structures Heap Structures Writing Pages 本文地址：http://xnerv.wang/sql-server-buffer-management-reading-pages/ 转载自：Reading Pages","categories":[{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/categories/DBMS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/tags/DBMS/"},{"name":"SQL Server","slug":"SQL-Server","permalink":"https://xnerv.wang/tags/SQL-Server/"},{"name":"Buffer Pool","slug":"Buffer-Pool","permalink":"https://xnerv.wang/tags/Buffer-Pool/"},{"name":"Read Ahead","slug":"Read-Ahead","permalink":"https://xnerv.wang/tags/Read-Ahead/"}]},{"title":"Monitoring Memory Clerk and Buffer Pool Allocations in SQL Server（转载）","slug":"monitoring-memory-clerk-and-buffer-pool-allocations-in-sql-server","date":"2018-02-21T07:05:00.000Z","updated":"2023-08-21T02:24:18.837Z","comments":true,"path":"monitoring-memory-clerk-and-buffer-pool-allocations-in-sql-server/","link":"","permalink":"https://xnerv.wang/monitoring-memory-clerk-and-buffer-pool-allocations-in-sql-server/","excerpt":"The following article applies to SQL Server versions 2008 + Adequate memory is one of the most important factors for a well-functioning instance of SQL Server. By design SQL Server manages its own memory allocations via the SQLOS rather than having the servers Operating System perform this task. Therefore it’s safe to say that monitoring SQL Servers memory use is a very important administrative task and in this post I am going to show you how to use Dynamic Management Views to take a closer look at how SQL is using memory and how these benefit troubleshooting activities. Before we do that we need to see how much memory is on our server and how much is allocated to be used by SQL itself. This relates to the very first paragraph in this post, the difference between total memory and SQL memory is allocated to the operating system and how much that should be is really dependent on the total memory in the server. I have always started with a setting of 4Gb or 10% of the total memory, whichever is more and tested regularly.","text":"The following article applies to SQL Server versions 2008 + Adequate memory is one of the most important factors for a well-functioning instance of SQL Server. By design SQL Server manages its own memory allocations via the SQLOS rather than having the servers Operating System perform this task. Therefore it’s safe to say that monitoring SQL Servers memory use is a very important administrative task and in this post I am going to show you how to use Dynamic Management Views to take a closer look at how SQL is using memory and how these benefit troubleshooting activities. Before we do that we need to see how much memory is on our server and how much is allocated to be used by SQL itself. This relates to the very first paragraph in this post, the difference between total memory and SQL memory is allocated to the operating system and how much that should be is really dependent on the total memory in the server. I have always started with a setting of 4Gb or 10% of the total memory, whichever is more and tested regularly. To view the total server memory use the following query against the sys.dm_os_sys_memory DMV: 12SELECT total_physical_memory_kb / 1024 AS MemoryMbFROM sys.dm_os_sys_memory To view SQLs allocation we can query the sys.configurations table to see how SQL has been configured: 12SELECT name, value_in_use FROM sys.configurationsWHERE name LIKE &#x27;max server memory%&#x27; This is an incredibly important setting for SQL Server because its default value at installation can cause performance problems. The reason is SQLOS and by default it will be allocated all of the RAM in the server and will dynamically release memory back to the Operating System by monitoring a memory thread. Whilst that is all well and good we can avoid this release of memory entirely be sensibly capping SQL Servers memory. It’s also worth noting what else is running on your server. I’m a huge advocate of having dedicated SQL instances without anything else running on them and that applies to items like Analysis, Integration or Reporting services too. Whilst that is perfectly good advice it isn’t always possible for a number of reasons but again just make sure you have adequate resource. A common misconception is that the maximum server memory setting applies to all of SQL Server, it doesn’t and its quite common to see, at the server level, SQL Server using more memory than this setting allows. The reason for this is that the configuration item only applies to the SQL Buffer Pool and various other components within SQL can consume more memory but it must be said that the Buffer Pool is mainly the biggest item of SQL memory allocation. To see how SQL is using memory internally we can query the sys.dm_os_memory_clerks DMV to view currently active memory clerks within SQL Server. A memory clerk sits between memory nodes and the memory components within SQL Server. Each component has its own memory clerk that interfaces with the memory nodes to allocate memory; these clerks can then be used to track resource consumption. This architecture also means that threads cannot directly interface with the low level memory allocators but must go to the clerks for memory requests. The test instance that will use has16Gb of RAM in the Server and I have allocated 8Gb to SQL Server, by running the following query I can see the top 5 memory consumers by clerk type and see how much they are using. 12345SELECT TOP(5) [type] AS [ClerkType],SUM(pages_kb) / 1024 AS [SizeMb]FROM sys.dm_os_memory_clerks WITH (NOLOCK)GROUP BY [type]ORDER BY SUM(pages_kb) DESC As I would expect the Buffer Pool is the largest consumer of memory within the instance with just over 4.5Gb allocated. The lock manager is next with just over 1Gb allocated for lock resources and the remaining clerks relate to allocations for the query plan. The CACHESTORE_OBJCP allocation refers to plans for stored procedures and functions. The CACHESTORE_SQLCP are plans not within those object types and refer to statements executed directly against SQL Server whilst the CACHESTORE_PHDR row shows algebrized trees for various objects. On a busy SQL Server this information is really useful for us to capture at regular intervals so we can closely monitor memory allocation under normal workloads. If we were to experience performance problems where we suspect memory pressure we can repeat the query to see if memory is being allocated differently. As an example here’s the same query taken when a full database consistency check is being ran against one of my test databases. We can see here that there’s a new memory clerk that is now in our top 5 allocations list, this particular clerk, SQLQERESERVATIONS is related to Memory Grant allocations within SQL Server. Upon seeing the SQLQERESERVATIONS we can query the current memory grants using the sys.dm_exec_query_memory_grants DMV and by using the CROSS APPLY function to sys.dm_exec_sql_text we can return the query text that is associated with the process. 1234SELECT session_id, requested_memory_kb / 1024 as RequestedMemMb,granted_memory_kb / 1024 as GrantedMemMb, textFROM sys.dm_exec_query_memory_grants qmgCROSS APPLY sys.dm_exec_sql_text(sql_handle) The query returns the following single result and with only one process running we know the consistency check has had a direct effect on our memory allocations. Here’s the query text: 12345678910DECLARE @BlobEater VARBINARY(8000)SELECT @BlobEater = CheckIndex (ROWSET_COLUMN_FACT_BLOB)FROM &#123; IRowset 0xF022EAB907000000 &#125;GROUP BY ROWSET_COLUMN_FACT_KEY&gt;&gt; WITH ORDER BY ROWSET_COLUMN_FACT_KEY, ROWSET_COLUMN_SLOT_ID, ROWSET_COLUMN_COMBINED_ID, ROWSET_COLUMN_FACT_BLOBOPTION (ORDER GROUP) This is one example of how a resource intensive process can affect the internal memory allocations within SQL Server but what about monitoring the allocations within the Buffer Pool itself? For that we use the sys.dm_os_buffer_descriptors DMV to see memory allocation broken down by database. Similar to the memory clerk view it is incredibly useful to capture and record this information at regular intervals and observe significant changes from what you have observed as “the norm”. 12345SELECT TOP 5 DB_NAME(database_id) AS [Database Name],COUNT(*) * 8/1024.0 AS [Cached Size (MB)]FROM sys.dm_os_buffer_descriptors WITH (NOLOCK)GROUP BY DB_NAME(database_id)ORDER BY [Cached Size (MB)] DESC OPTION (RECOMPILE); Here are the results on a test instance: I like to record the results of this query at regular intervals taking note of database memory allocations at various points during the working day or when intensive activities or maintenance is being undertaken. The key is understanding how SQL is working so that once these baseline values have been captured we can compare back to them during troubleshooting to see if any databases are utilising (or have been allocated) memory differently. By capturing memory clerk and buffer descriptor usage we can build a picture of how SQL is working under normal workloads. It also means we have this information readily available to us should we need to highlight issues and the effect that they are having on the system. See more To get 3 free licenses to a SQL Server monitoring tool, download ApexSQL Monitor and fill out this simple survey Further reading: Sys.dm_os_memory_clerks Sys.dm_os_buffer_descriptors SQL Memory Architecture 本文地址：http://xnerv.wang/monitoring-memory-clerk-and-buffer-pool-allocations-in-sql-server/ 转载自：Monitoring Memory Clerk and Buffer Pool Allocations in SQL Server","categories":[{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/categories/DBMS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/tags/DBMS/"},{"name":"SQL Server","slug":"SQL-Server","permalink":"https://xnerv.wang/tags/SQL-Server/"},{"name":"Memory Clerk","slug":"Memory-Clerk","permalink":"https://xnerv.wang/tags/Memory-Clerk/"},{"name":"Buffer Pool","slug":"Buffer-Pool","permalink":"https://xnerv.wang/tags/Buffer-Pool/"}]},{"title":"Disabling ROW and PAGE Level Locks in SQL Server（转载）","slug":"disabling-row-and-page-level-locks-in-sql-server","date":"2018-02-21T06:40:00.000Z","updated":"2023-08-21T02:24:18.687Z","comments":true,"path":"disabling-row-and-page-level-locks-in-sql-server/","link":"","permalink":"https://xnerv.wang/disabling-row-and-page-level-locks-in-sql-server/","excerpt":"Today I want to talk about another very interesting topic in SQL Server: disabling Row and Page Level Locks in SQL Server. Every time that you rebuild an Index in SQL Server, you can use the ALLOW_ROW_LOCKS and ALLOW_PAGE_LOCKS options to specify that SQL Server should acquire Row and Page Level Locks when accessing your data for reading and writing. Let’s look at what happens internally when we disable these locks.","text":"Today I want to talk about another very interesting topic in SQL Server: disabling Row and Page Level Locks in SQL Server. Every time that you rebuild an Index in SQL Server, you can use the ALLOW_ROW_LOCKS and ALLOW_PAGE_LOCKS options to specify that SQL Server should acquire Row and Page Level Locks when accessing your data for reading and writing. Let’s look at what happens internally when we disable these locks. Disable Row Level Locks 1234-- Disable row level locksALTER INDEX idx_ci ON Foo REBUILDWITH (ALLOW_ROW_LOCKS = OFF)GO As you know from the Locking Hierarchy, SQL Server acquires locks at the table level, the page level, and the row level. Now let’s run a simple SELECT statement in an explicit transaction and let’s hold the Shared Locks until the end of the transaction with the query hint HOLDLOCK. 123456789101112-- SQL Server acquires in Repeatable Read a Shared Lock on the Page Level,-- because Shared Row Locks are not possible anymore.BEGIN TRANSACTIONSELECT * FROM Foo WITH (HOLDLOCK)WHERE ID = 5000SELECT * FROM sys.dm_tran_locksWHERE request_session_id = @@SPIDROLLBACKGO When you look into the Lock Manager during that transaction, you can see that SQL Server has only acquired the IS Lock at the Table level, and a Shared Lock at the Page level. There are no Row Level locks anymore! The acquired locks are now more restrictive, because normally SQL Server acquires an IS lock at the page level and a Shared Lock on the row itself. The same concept applies when you change your data through a transaction: 12345678910111213-- SQL Server acquires for an UPDATE statement an Exclusive Lock on the Page Level,-- because Exclusive Row Locks are not possible anymore.BEGIN TRANSACTIONUPDATE FooSET Col2 = REPLICATE(&#x27;y&#x27;, 100)WHERE ID = 5000SELECT * FROM sys.dm_tran_locksWHERE request_session_id = @@SPIDROLLBACKGO In that case you again end up with an Exclusive Lock at the Page Level instead of an IX lock. Disable Page Level Locks Next let’s disable Page Level Locks: 1234-- Disable Page level locksALTER INDEX idx_ci ON Foo REBUILDWITH (ALLOW_PAGE_LOCKS = OFF)GO The first thing that I want to show you here is that an Index Reorganize operation is dependent on Page Level locks. Therefore a simple Reorganize of that index will fail: The index “idx_ci” on table “Foo” cannot be reorganized because page level locking is disabled. Now let’s run our SELECT statement again but this time with the query hint HOLDLOCK: 1234567891011-- There is no IS lock on the Page anymore.BEGIN TRANSACTIONSELECT * FROM Foo WITH (HOLDLOCK)WHERE ID = 5000SELECT * FROM sys.dm_tran_locksWHERE request_session_id = @@SPIDROLLBACKGO When you look again into the Lock Manager you can see that the IS lock at the Page level disappeared. We only have an IS lock at the Table level, and the S Lock on the row. Let’s try to change a record again: 123456789101112-- There is no IX lock on the Page anymore.BEGIN TRANSACTIONUPDATE FooSET Col2 = REPLICATE(&#x27;y&#x27;, 100)WHERE ID = 5000SELECT * FROM sys.dm_tran_locksWHERE request_session_id = @@SPIDROLLBACKGO The same thing has happened as previously: SQL Server has only acquired the IX Lock at the Table level, and the X Lock on the row. There is no lock at the Page level anymore… Disable Row and Page Level Locks And now let’s go overboard, and we disable Row and Page level Locks for our Clustered Index: 1234-- Disable Row and Page level locksALTER INDEX idx_ci ON Foo REBUILDWITH (ALLOW_ROW_LOCKS = OFF, ALLOW_PAGE_LOCKS = OFF)GO When you now read some data, SQL Server just acquires a Shared Lock at the Table level. Your whole table is read-only: And when you change a record without being able to acquire Page and Row level Locks, SQL Server acquires an X Lock on the whole table – ouch: Summary The moral of this story/blog post? There is not really a good reason why you should disable Page and Row level Locks in SQL Server. Just work with the default Locking Strategy that SQL Server offers, because otherwise the locking that is employed will be too restrictive and the throughput of your workload will suffer… Thanks for your time. 本文地址：http://xnerv.wang/disabling-row-and-page-level-locks-in-sql-server/ 转载自：Disabling ROW and PAGE Level Locks in SQL Server","categories":[{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/categories/DBMS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/tags/DBMS/"},{"name":"SQL Server","slug":"SQL-Server","permalink":"https://xnerv.wang/tags/SQL-Server/"},{"name":"Transaction","slug":"Transaction","permalink":"https://xnerv.wang/tags/Transaction/"},{"name":"Row Lock","slug":"Row-Lock","permalink":"https://xnerv.wang/tags/Row-Lock/"},{"name":"Page Lock","slug":"Page-Lock","permalink":"https://xnerv.wang/tags/Page-Lock/"}]},{"title":"Active FTP vs. Passive FTP, a Definitive Explanation（转载）","slug":"active-ftp-vs-passive-ftp-a-definitive-explanation","date":"2018-02-20T22:34:00.000Z","updated":"2023-08-21T02:24:21.859Z","comments":true,"path":"active-ftp-vs-passive-ftp-a-definitive-explanation/","link":"","permalink":"https://xnerv.wang/active-ftp-vs-passive-ftp-a-definitive-explanation/","excerpt":"Introduction One of the most commonly seen questions when dealing with firewalls and other Internet connectivity issues is the difference between active and passive FTP and how best to support either or both of them. Hopefully the following text will help to clear up some of the confusion over how to support FTP in a firewalled environment. This may not be the definitive explanation, as the title claims, however, I’ve heard enough good feedback and seen this document linked in enough places to know that quite a few people have found it to be useful. I am always looking for ways to improve things though, and if you find something that is not quite clear or needs more explanation, please let me know! Recent additions to this document include the examples of both active and passive command line FTP sessions. These session examples should help make things a bit clearer. They also provide a nice picture into what goes on behind the scenes during an FTP session. Now, on to the information…","text":"Introduction One of the most commonly seen questions when dealing with firewalls and other Internet connectivity issues is the difference between active and passive FTP and how best to support either or both of them. Hopefully the following text will help to clear up some of the confusion over how to support FTP in a firewalled environment. This may not be the definitive explanation, as the title claims, however, I’ve heard enough good feedback and seen this document linked in enough places to know that quite a few people have found it to be useful. I am always looking for ways to improve things though, and if you find something that is not quite clear or needs more explanation, please let me know! Recent additions to this document include the examples of both active and passive command line FTP sessions. These session examples should help make things a bit clearer. They also provide a nice picture into what goes on behind the scenes during an FTP session. Now, on to the information… The Basics FTP is a TCP based service exclusively. There is no UDP component to FTP. FTP is an unusual service in that it utilizes two ports, a ‘data’ port and a ‘command’ port (also known as the control port). Traditionally these are port 21 for the command port and port 20 for the data port. The confusion begins however, when we find that depending on the mode, the data port is not always on port 20. Active FTP In active mode FTP the client connects from a random unprivileged port (N &gt; 1023) to the FTP server’s command port, port 21. Then, the client starts listening to port N+1 and sends the FTP command PORT N+1 to the FTP server. The server will then connect back to the client’s specified data port from its local data port, which is port 20. From the server-side firewall’s standpoint, to support active mode FTP the following communication channels need to be opened: FTP server’s port 21 from anywhere (Client initiates connection) FTP server’s port 21 to ports &gt; 1023 (Server responds to client’s control port) FTP server’s port 20 to ports &gt; 1023 (Server initiates data connection to client’s data port) FTP server’s port 20 from ports &gt; 1023 (Client sends ACKs to server’s data port) When drawn out, the connection appears as follows: In step 1, the client’s command port contacts the server’s command port and sends the command PORT 1027. The server then sends an ACK back to the client’s command port in step 2. In step 3 the server initiates a connection on its local data port to the data port the client specified earlier. Finally, the client sends an ACK back as shown in step 4. The main problem with active mode FTP actually falls on the client side. The FTP client doesn’t make the actual connection to the data port of the server–it simply tells the server what port it is listening on and the server connects back to the specified port on the client. From the client side firewall this appears to be an outside system initiating a connection to an internal client–something that is usually blocked. Active FTP Example Below is an actual example of an active FTP session. The only things that have been changed are the server names, IP addresses, and user names. In this example an FTP session is initiated from testbox1.slacksite.com (192.168.150.80), a linux box running the standard FTP command line client, to testbox2.slacksite.com (192.168.150.90), a linux box running ProFTPd 1.2.2RC2. The debugging (-d) flag is used with the FTP client to show what is going on behind the scenes. Everything in red is the debugging output which shows the actual FTP commands being sent to the server and the responses generated from those commands. Normal server output is shown in black, and user input is in bold. There are a few interesting things to consider about this dialog. Notice that when the PORT command is issued, it specifies a port on the client (192.168.150.80) system, rather than the server. We will see the opposite behavior when we use passive FTP. While we are on the subject, a quick note about the format of the PORT command. As you can see in the example below it is formatted as a series of six numbers separated by commas. The first four octets are the IP address while the last two octets comprise the port that will be used for the data connection. To find the actual port multiply the fifth octet by 256 and then add the sixth octet to the total. Thus in the example below the port number is ( (14*256) + 178), or 3762. A quick check with netstat should confirm this information. testbox1: {/home/p-t/slacker/public_html} % ftp -d testbox2 Connected to testbox2.slacksite.com. 220 testbox2.slacksite.com FTP server ready. Name (testbox2:slacker): slacker —&gt; USER slacker 331 Password required for slacker. Password: TmpPass —&gt; PASS XXXX 230 User slacker logged in. —&gt; SYST 215 UNIX Type: L8 Remote system type is UNIX. Using binary mode to transfer files. ftp&gt; ls ftp: setsockopt (ignored): Permission denied —&gt; PORT 192,168,150,80,14,178 200 PORT command successful. —&gt; LIST 150 Opening ASCII mode data connection for file list. drwx------ 3 slacker users 104 Jul 27 01:45 public_html 226 Transfer complete. ftp&gt; quit —&gt; QUIT 221 Goodbye. Passive FTP In order to resolve the issue of the server initiating the connection to the client a different method for FTP connections was developed. This was known as passive mode, or PASV, after the command used by the client to tell the server it is in passive mode. In passive mode FTP the client initiates both connections to the server, solving the problem of firewalls filtering the incoming data port connection to the client from the server. When opening an FTP connection, the client opens two random unprivileged ports locally (N &gt; 1023 and N+1). The first port contacts the server on port 21, but instead of then issuing a PORT command and allowing the server to connect back to its data port, the client will issue the PASV command. The result of this is that the server then opens a random unprivileged port (P &gt; 1023) and sends P back to the client in response to the PASV command. The client then initiates the connection from port N+1 to port P on the server to transfer data. From the server-side firewall’s standpoint, to support passive mode FTP the following communication channels need to be opened: FTP server’s port 21 from anywhere (Client initiates connection) FTP server’s port 21 to ports &gt; 1023 (Server responds to client’s control port) FTP server’s ports &gt; 1023 from anywhere (Client initiates data connection to random port specified by server) FTP server’s ports &gt; 1023 to remote ports &gt; 1023 (Server sends ACKs (and data) to client’s data port) When drawn, a passive mode FTP connection looks like this: In step 1, the client contacts the server on the command port and issues the PASV command. The server then replies in step 2 with PORT 2024, telling the client which port it is listening to for the data connection. In step 3 the client then initiates the data connection from its data port to the specified server data port. Finally, the server sends back an ACK in step 4 to the client’s data port. While passive mode FTP solves many of the problems from the client side, it opens up a whole range of problems on the server side. The biggest issue is the need to allow any remote connection to high numbered ports on the server. Fortunately, many FTP daemons, including the popular WU-FTPD allow the administrator to specify a range of ports which the FTP server will use. See Appendix 1 for more information. The second issue involves supporting and troubleshooting clients which do (or do not) support passive mode. As an example, the command line FTP utility provided with Solaris does not support passive mode, necessitating a third-party FTP client, such as ncftp. NOTE: This is no longer the case–use the -p option with the Solaris FTP client to enable passive mode! With the massive popularity of the World Wide Web, many people prefer to use their web browser as an FTP client. Most browsers only support passive mode when accessing ftp:// URLs. This can either be good or bad depending on what the servers and firewalls are configured to support. Passive FTP Example Below is an actual example of a passive FTP session. The only things that have been changed are the server names, IP addresses, and user names. In this example an FTP session is initiated from testbox1.slacksite.com (192.168.150.80), a linux box running the standard FTP command line client, to testbox2.slacksite.com (192.168.150.90), a linux box running ProFTPd 1.2.2RC2. The debugging (-d) flag is used with the FTP client to show what is going on behind the scenes. Everything in red is the debugging output which shows the actual FTP commands being sent to the server and the responses generated from those commands. Normal server output is shown in black, and user input is in bold. Notice the difference in the PORT command in this example as opposed to the active FTP example. Here, we see a port being opened on the server (192.168.150.90) system, rather than the client. See the discussion about the format of the PORT command above, in the Active FTP Example section. testbox1: {/home/p-t/slacker/public_html} % ftp -d testbox2 Connected to testbox2.slacksite.com. 220 testbox2.slacksite.com FTP server ready. Name (testbox2:slacker): slacker —&gt; USER slacker 331 Password required for slacker. Password: TmpPass —&gt; PASS XXXX 230 User slacker logged in. —&gt; SYST 215 UNIX Type: L8 Remote system type is UNIX. Using binary mode to transfer files. ftp&gt; passive Passive mode on. ftp&gt; ls ftp: setsockopt (ignored): Permission denied —&gt; PASV 227 Entering Passive Mode (192,168,150,90,195,149). —&gt; LIST 150 Opening ASCII mode data connection for file list drwx------ 3 slacker users 104 Jul 27 01:45 public_html 226 Transfer complete. ftp&gt; quit —&gt; QUIT 221 Goodbye. Other Notes A reader, Maarten Sjouw, pointed out that active FTP will not function when used in conjunction with a client-side NAT (Network Address Translation) device which is not smart enough to alter the IP address info in FTP packets. Summary The following chart should help admins remember how each FTP mode works: Active FTP : command : client &gt;1023 -&gt; server 21 data : client &gt;1023 &lt;- server 20 Passive FTP : command : client &gt;1023 -&gt; server 21 data : client &gt;1024 -&gt; server &gt;1023 A quick summary of the pros and cons of active vs. passive FTP is also in order: Active FTP is beneficial to the FTP server admin, but detrimental to the client side admin. The FTP server attempts to make connections to random high ports on the client, which would almost certainly be blocked by a firewall on the client side. Passive FTP is beneficial to the client, but detrimental to the FTP server admin. The client will make both connections to the server, but one of them will be to a random high port, which would almost certainly be blocked by a firewall on the server side. Luckily, there is somewhat of a compromise. Since admins running FTP servers will need to make their servers accessible to the greatest number of clients, they will almost certainly need to support passive FTP. The exposure of high level ports on the server can be minimized by specifying a limited port range for the FTP server to use. Thus, everything except for this range of ports can be firewalled on the server side. While this doesn’t eliminate all risk to the server, it decreases it tremendously. See Appendix 1 for more information. References An excellent reference on how various internet protocols work and the issues involved in firewalling them can be found in the O’Reilly and Associates book, Building Internet Firewalls, 2nd Ed, by Brent Chapman and Elizabeth Zwicky. **Note 2012:**This book is VERY old and the information contained therein may be outdated! Finally, the definitive reference on FTP would be RFC 959, which sets forth the official specifications of the FTP protocol. RFCs can be downloaded from numerous locations, including http://www.faqs.org/rfcs/rfc959.html. Appendix 1 Introduction This appendix will describe methods used to configure various popular FTP servers to limit the number of passive ports they will listen on. As mentioned in the main text, FTP server admins will almost definitely need to support passive FTP in order to allow the greatest number of clients to access their FTP resources. In order to support passive FTP, however, a large number of high-numbered ports on the server must be opened through a firewall. Luckily, most FTP servers allow this port range to be specified so as to limit exposure to attacks. ProFTPd ProFTPd, http://www.proftpd.net, is an increasingly popular FTP server due to its modularity and Apache-style configuration directives. ProFTPd also supports virtual hosts “out of the box”, causing it to become one of the most common FTP servers used by web hosting companies. As of version 1.20RC3 and later (current version as of this writing is 1.2.4), ProFTPd supports a directive called PassivePorts. The PassivePorts directive is usually used in a global context in the proftpd.conf file (the location of which varies depending on how ProFTPd was configured and installed). PassivePorts takes two arguments, the minimum port number and the maximum port number, as in the below example: PassivePorts 51000 51999 The ProFTPd documentation has the following to say about the PassivePorts directive: PassivePorts restricts the range of ports from which the server will select when sent the PASV command from a client. The server will randomly choose a number from within the specified range until an open port is found. Should no open ports be found within the given range, the server will default to a normal kernel-assigned port, and a message logged. The port range selected must be in the non-privileged range (eg. greater than or equal to 1024); it is STRONGLY RECOMMENDED that the choosen range be large enough to handle many simultaneous passive connections (for example, 49152-65534, the IANA-registered ephemeral port range). If you are attempting to use SSH port forwarding to securely tunnel the FTP command channel over an SSH connection (so that passwords are not sent in clear text), be aware that you must set the AllowForeignAddress directive to “on” in the proftpd.conf file. If this is not set and a tunnelled connection is attempted, ProFTPd will log a message similar to the following: SECURITY VIOLATION: Passive connection from a.b.c.d rejected Important Note: Please read and understand the documentation about the AllowForeignAddress directive before implementing it. This can open your FTP server up to bounce attacks. It is strongly recommended that this option not be set on systems being used as anonymous FTP servers. 本文地址：http://xnerv.wang/active-ftp-vs-passive-ftp-a-definitive-explanation/ 转载自：Active FTP vs. Passive FTP, a Definitive Explanation","categories":[{"name":"网络协议","slug":"网络协议","permalink":"https://xnerv.wang/categories/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"网络协议","slug":"网络协议","permalink":"https://xnerv.wang/tags/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"},{"name":"FTP","slug":"FTP","permalink":"https://xnerv.wang/tags/FTP/"},{"name":"Active FTP","slug":"Active-FTP","permalink":"https://xnerv.wang/tags/Active-FTP/"},{"name":"Passive FTP","slug":"Passive-FTP","permalink":"https://xnerv.wang/tags/Passive-FTP/"}]},{"title":"InnoDB Log Block Structure(InnoDB日志Block结构详解)（转载）","slug":"innodb-log-block-structure","date":"2018-02-20T04:30:00.000Z","updated":"2023-08-21T02:24:18.751Z","comments":true,"path":"innodb-log-block-structure/","link":"","permalink":"https://xnerv.wang/innodb-log-block-structure/","excerpt":"","text":"本文地址：http://xnerv.wang/innodb-log-block-structure/ 转载自：InnoDB Log Block Structure(InnoDB日志Block结构详解)","categories":[{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/categories/DBMS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/tags/DBMS/"},{"name":"MySQL","slug":"MySQL","permalink":"https://xnerv.wang/tags/MySQL/"},{"name":"InnoDB","slug":"InnoDB","permalink":"https://xnerv.wang/tags/InnoDB/"},{"name":"InnoDB Log","slug":"InnoDB-Log","permalink":"https://xnerv.wang/tags/InnoDB-Log/"}]},{"title":"MySQL Row Format(MySQL行格式详解)（转载）","slug":"mysql-row-format","date":"2018-02-20T04:18:00.000Z","updated":"2023-08-21T02:24:19.170Z","comments":true,"path":"mysql-row-format/","link":"","permalink":"https://xnerv.wang/mysql-row-format/","excerpt":"","text":"本文地址：http://xnerv.wang/mysql-row-format/ 转载自：MySQL Row Format(MySQL行格式详解)","categories":[{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/categories/DBMS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/tags/DBMS/"},{"name":"MySQL","slug":"MySQL","permalink":"https://xnerv.wang/tags/MySQL/"},{"name":"Row Format","slug":"Row-Format","permalink":"https://xnerv.wang/tags/Row-Format/"}]},{"title":"数据库弱一致性四个隔离级别（转载）","slug":"four-isolation-levels-of-database-weak-consistency","date":"2018-02-20T03:21:00.000Z","updated":"2023-08-21T02:24:19.419Z","comments":true,"path":"four-isolation-levels-of-database-weak-consistency/","link":"","permalink":"https://xnerv.wang/four-isolation-levels-of-database-weak-consistency/","excerpt":"SQL-92标准中定义了四个隔离级别，这四个隔离级别在以前版本的SQL Server中即受到支持： READ UNCOMMITTED READ UNCOMMITTED是限制性最弱的隔离级别，因为该级别忽略其他事务放置的锁。使用READ UNCOMMITTED级别执行的事务，可以读取尚未由其他事务提交的修改后的数据值，这些行为称为“脏”读。这是因为在Read Uncommitted级别下，读取数据不需要加S锁，这样就不会跟被修改的数据上的X锁冲突。比如，事务1修改一行，事务2在事务1提交之前读取了这一行。如果事务1回滚，事务2就读取了一行没有提交的数据，这样的数据我们认为是不存在的。","text":"SQL-92标准中定义了四个隔离级别，这四个隔离级别在以前版本的SQL Server中即受到支持： READ UNCOMMITTED READ UNCOMMITTED是限制性最弱的隔离级别，因为该级别忽略其他事务放置的锁。使用READ UNCOMMITTED级别执行的事务，可以读取尚未由其他事务提交的修改后的数据值，这些行为称为“脏”读。这是因为在Read Uncommitted级别下，读取数据不需要加S锁，这样就不会跟被修改的数据上的X锁冲突。比如，事务1修改一行，事务2在事务1提交之前读取了这一行。如果事务1回滚，事务2就读取了一行没有提交的数据，这样的数据我们认为是不存在的。 READ COMMITTED READ COMMITTED(Nonrepeatable reads)是SQL Server默认的隔离级别。该级别通过指定语句不能读取其他事务已修改但是尚未提交的数据值，禁止执行脏读。在当前事务中的各个语句执行之间，其他事务仍可以修改、插入或删除数据，从而产生无法重复的读操作，或“影子”数据。比如，事务1读取了一行，事务2修改或者删除这一行并且提交。如果事务1想再一次读取这一行，它将获得修改后的数据或者发现这一样已经被删除，因此事务的第二次读取结果与第一次读取结果不同，因此也叫不可重复读。 实验1 query1：事务1 12345678910111213141516171819--step1:创建实验数据select * into Employee from AdventureWorks.HumanResources.Employeealter table Employee add constraint pk_Employee_EmployeeID primary key(EmployeeID)--step2:设置隔离级别,这是数据库的默认隔离界别SET TRANSACTION ISOLATION LEVEL READ COMMITTED--step3:开启第一个事务BEGIN TRAN tran1 --step4:执行select操作,查看VacationHours,对查找的记录加S锁，在语句执行完以后自动释放S锁 SELECT EmployeeID, VacationHours FROM Employee WHERE EmployeeID = 4; --step5:查看当前加锁情况,没有发现在Employee表上面有锁,这是因为当前的隔离界别是READ COMMITTED --在执行完step2以后马上释放了S锁. SELECT request_session_id, resource_type, resource_associated_entity_id, request_status, request_mode, resource_description FROM sys.dm_tran_locks 查看锁的情况如下图所示，我们发现在只有在数据库级别的S锁，而没有在表级别或者更低级别的锁，这是因为**在Read Committed级别下，S锁在语句执行完以后就被释放**。 query2：事务2 1234567891011--step6:开启第二个事务BEGIN TRAN tran2; --step7:修改VacationHours,需要获得排它锁X,在VacationHours上没有有S锁 UPDATE Employee SET VacationHours = VacationHours - 8 WHERE EmployeeID = 4; --step8:查看当前加锁情况 SELECT request_session_id, resource_type, resource_associated_entity_id, request_status, request_mode, resource_description FROM sys.dm_tran_locks 在开启另外一个update事务以后，我们再去查看当前的锁状况，如下图所示，我们发现在表(Object)级别上加了IX锁，在这张表所在的Page上也加了IX锁，因为表加了聚集索引，所以在叶子结点上加了X锁，这个锁的类型是KEY。 然后我们回到事务1当中再次执行查询语句，我们会发现查询被阻塞，我们新建一个查询query3来查看这个时候的锁状况，其查询结果如下，我们可以发现查询操作需要在KEY级别上申请S锁，在Page和表(Object)上面申请IS锁，但是因为Key上面原先有了X锁，与当前读操作申请的S锁冲突，所以这一步处于WAIT状态。 如果此时提交事务2的update操作，那么事务1的select操作不再被阻塞，得到查询结果，但是我们发现此时得到的查询结果与第一次得到的查询结果不同，这也是为什么将read committed称为不可重复读，因为同一个事物内的两次相同的查询操作的结果可能不同。 REPEATABLE READ REPEATABLE READ是比READ COMMITTED限制性更强的隔离级别。该级别包括READ COMMITTED，并且另外指定了在当前事务提交之前，其他任何事务均不可以修改或删除当前事务已读取的数据。并发性低于 READ COMMITTED，因为已读数据的共享锁在整个事务期间持有，而不是在每个语句结束时释放。比如，事务1读取了一行，事务2想修改或者删除这一行并且提交，但是因为事务1尚未提交，数据行中有事务1的锁，事务2无法进行更新操作，因此事务2阻塞。如果这时候事务1想再一次读取这一行，它读取结果与第一次读取结果相同，因此叫可重复读。 实验2 query1：事务1 12345678910111213141516171819--step1:创建实验数据select * into Employee from AdventureWorks.HumanResources.Employeealter table Employee add constraint pk_Employee_EmployeeID primary key(EmployeeID)--step2:设置隔离级别SET TRANSACTION ISOLATION LEVEL REPEATABLE READ--step3:开启第一个事务BEGIN TRAN tran1 --step4:执行select操作,查看VacationHours SELECT EmployeeID, VacationHours FROM Employee WHERE EmployeeID = 4; --step5:查看当前加锁情况,发现在Employee表上面有S锁,这是因为当前的隔离界别是REPEATABLE READ --S锁只有在事务执行完以后才会被释放. SELECT request_session_id, resource_type, resource_associated_entity_id, request_status, request_mode, resource_description FROM sys.dm_tran_locks 查询锁状态的结果如下图所示，我们发现在KEY上面加了S锁，在Page和Object上面加了IS锁，这是因为**在Repeatable Read级别下S锁要在事务执行完以后才会被释放**。 query2：事务2 123456--step6:开启第二个事务BEGIN TRAN tran2; --step7:修改VacationHours,需要获得排他锁X,在VacationHours上有S锁，出现冲突，所以update操作被阻塞 UPDATE Employee SET VacationHours = VacationHours - 8 WHERE EmployeeID = 4; 执行上述update操作的时候发现该操作被阻塞，这是因为update操作要加排它锁X，而因为原先的查询操作的S锁没有释放，所以两者冲突。我们新建一个查询3执行查询锁状态操作，发现结果如下图所示，我们可以发现是**WAIT**发生在对KEY加X锁的操作上面。 此时再次执行查询1中的select操作，我们发现查询结果跟第一次相同，所以这个叫做可重复读操作。但是可重复读操作并不是特定指两次读取的数据一模一样，Repeatable Read存在的一个问题是幻读，就是第二次读取的数据返回的条目数比第一次返回的条目数更多。 比如在Repeatable Read隔离级别下，事务1第一次执行查询select id from users where id&gt;1 and id &lt;10，返回的结果是2，4，6，8。这个时候事务1没有提交，那么对2，4，6，8上面依然保持有S锁。此时事务2执行一次插入操作insert into user(id) valuse(3)，插入成功。此时再次执行事务1中的查询，那么返回结果就是2，3，4，6，8。这里的3就是因为幻读而出现的。因此可以得出结论：REPEATABLE READ隔离级别保证了在相同的查询条件下，同一个事务中的两个查询，第二次读取的内容肯定包换第一次读到的内容。 SERIALIZABLE SERIALIZABLE 是限制性最强的隔离级别，因为该级别锁定整个范围的键，并一直持有锁，直到事务完成。该级别包括REPEATABLE READ，并增加了在事务完成之前，其他事务不能向事务已读取的范围插入新行的限制。比如，事务1读取了一系列满足搜索条件的行。事务2在执行SQL statement产生一行或者多行满足事务1搜索条件的行时会冲突，则事务2回滚。这时事务1再次读取了一系列满足相同搜索条件的行，第二次读取的结果和第一次读取的结果相同。 重复读与幻读 重复读是为了保证在一个事务中，相同查询条件下读取的数据值不发生改变，但是不能保证下次同样条件查询，结果记录数不会增加。 幻读就是为了解决这个问题而存在的，他将这个查询范围都加锁了，所以就不能再往这个范围内插入数据，这就是SERIALIZABLE 隔离级别做的事情。 隔离级别与锁的关系 在Read Uncommitted级别下，读操作不加S锁； 在Read Committed级别下，读操作需要加S锁，但是在语句执行完以后释放S锁； 在Repeatable Read级别下，读操作需要加S锁，但是在事务提交之前并不释放S锁，也就是必须等待事务执行完毕以后才释放S锁。 在Serialize级别下，会在Repeatable Read级别的基础上，添加一个范围锁。保证一个事务内的两次查询结果完全一样，而不会出现第一次查询结果是第二次查询结果的子集。 （xnerv：SQL Server的四种隔离级别的加锁区别应该都只针对READ操作，如果是INSERT或UPDATE，应该都会加X锁并持有到事务结束。可以假设有一个RU级别的事务A修改了一行数据，另一个RC级别的事务B尝试读取这行数据。如果事务A不加X锁，或者修改完后在事务结束前释放X锁，都会导致事务B在事务A提交前读取到新数据而违反RC原则。） 本文地址：http://xnerv.wang/four-isolation-levels-of-database-weak-consistency/ 转载自：数据库弱一致性四个隔离级别","categories":[{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/categories/DBMS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/tags/DBMS/"},{"name":"Transaction","slug":"Transaction","permalink":"https://xnerv.wang/tags/Transaction/"},{"name":"Isolation Level","slug":"Isolation-Level","permalink":"https://xnerv.wang/tags/Isolation-Level/"}]},{"title":"The Tale of Two Evaluators: Understanding MASM and C++ Expression Evaluators in WinDbg（转载）","slug":"the-tale-of-two-evaluators-understanding-masm-and-cpp-expression-evaluators-in-windbg","date":"2018-02-20T02:09:00.000Z","updated":"2023-08-21T02:24:19.997Z","comments":true,"path":"the-tale-of-two-evaluators-understanding-masm-and-cpp-expression-evaluators-in-windbg/","link":"","permalink":"https://xnerv.wang/the-tale-of-two-evaluators-understanding-masm-and-cpp-expression-evaluators-in-windbg/","excerpt":"Much like any powerful tool with a command line interface, the WinDBG expression syntax can sometimes seem quite cryptic. In the interest of trying to unlock the power of the command line interface, in this article we’re going to cover a fundamental WinDBG concept that is key to understanding those long, strange commands: the expression evaluators. As it turns out, there are two built in expression evaluators in WinDBG, the MASM evaluator and the C++ evaluator. In order to better understand the differences, we’ll first discuss the MASM evaluator in detail and then move on to the C++ evaluator. Please note that this article is not going to attempt to be definitive and list every possible operator or feature of the expression evaluators. We’re simply going to focus on what we find to be the most useful in the hopes of getting you started. However, once you’ve gotten your feet wet full details are available in the WinDBG documentation.","text":"Much like any powerful tool with a command line interface, the WinDBG expression syntax can sometimes seem quite cryptic. In the interest of trying to unlock the power of the command line interface, in this article we’re going to cover a fundamental WinDBG concept that is key to understanding those long, strange commands: the expression evaluators. As it turns out, there are two built in expression evaluators in WinDBG, the MASM evaluator and the C++ evaluator. In order to better understand the differences, we’ll first discuss the MASM evaluator in detail and then move on to the C++ evaluator. Please note that this article is not going to attempt to be definitive and list every possible operator or feature of the expression evaluators. We’re simply going to focus on what we find to be the most useful in the hopes of getting you started. However, once you’ve gotten your feet wet full details are available in the WinDBG documentation. MASM Expression Evaluator The MASM expression evaluator is the expression evaluator enabled by default. The main thing that you have to remember when dealing with the MASM expression evaluator is that every expression you type results in nothing but an address. So, for example, MASM has no idea what data type your variable is, it just knows the address of your local variable. In addition, in the case of symbols the address returned is the address of the symbol and not the contents of the address. We can see exactly what this means if we give the evaluate expression command (?) the name of a local: 1231: kd&gt; ?irpSpEvaluate expression: -109664056 &#x3D; f976a8c8 If we then cross reference that with the dv output, we see that the value returned here is indeed the address of the variable and not the pointer contents: 1231: kd&gt; dv &#x2F;v &#x2F;t irpSpf976a8c8 struct _IO_STACK_LOCATION * irpSp &#x3D; 0x825c6fdc There are a couple of other interesting features of the MASM expression evaluator. The first is that the default radix used by the evaluator is hex, thus all values passed to the expression evaluator are first assumed to be hex values: 1234567891011121314151: kd&gt; nbase is 161: kd&gt; ?1234Evaluate expression: 4660 &#x3D; 000012341: kd&gt; n 10base is 101: kd&gt; ?1234Evaluate expression: 1234 &#x3D; 000004d2 You can change the default radix for the MASM evaluator with the n command, or you can override the radix for individual values. This is done by using one of the following prefixes: 12345670x for hexadecimal0t for octal0y binary0n for decimnal (the &#39;n&#39; is silent) There’s also another override which might come in handy, !. By prefixing a value with ! you’re telling the MASM evaluator that the following value is a symbol name and not a hexadecimal number. This can be helpful if you have an ambiguous local variable name, such as fcb: 12345671: kd&gt; ? fcbEvaluate expression: 4043 &#x3D; 00000fcb1: kd&gt; ? !fcbEvaluate expression: -109664276 &#x3D; f976a7ec Lastly, there’s the poi operator in MASM that you’ll likely find useful. As mentioned, MASM gives you the address of a symbol but not the contents. In most cases, you’ll find yourself more interested in the contents of the address. Take for example a local NTSTATUS value, if you give the name of the variable to the expression evaluator command you will get the local variable address: 1231: kd&gt; ?statusEvaluate expression: -138300352 &#x3D; f7c1b440 In order to see the NTSTATUS value you will need to dereference that address, which is exactly what poi does: 1231: kd&gt; ?poi(status)Evaluate expression: -1073741823 &#x3D; c0000001 C++ Expression Evaluator The C++ expression evaluator provides lots of added features over the MASM expression evaluator. The most important feature is that the evaluator is type aware. This means that you are able to access your variables in the same way as you are in your C/C++ code. In addition, in contrast to the MASM evaluator, symbol expressions result not in the address of the symbol but the contents of the address, thus there is no need for the poi operator. In order to begin playing with the C++ evaluator, we’ll need to switch the current evaluator from MASM to C++: 1231: kd&gt; .expr &#x2F;s c++Current expression evaluator: C++ - C++ source expressions Now we can try looking at our status variable again: 1231: kd&gt; ?statusEvaluate expression: -1073741823 &#x3D; c0000001 And we can immediately start to see the differences; for example, instead of the address of the local variable we’re given the contents. What’s also particularly important to know about the C++ evaluator is that all numeric values default to decimal, regardless of what your default radix is. There is no way to change this, which means that any hex value must have an explicit 0x prefix: 12345678910111: kd&gt; nbase is 161: kd&gt; ?1234Evaluate expression: 1234 &#x3D; 000004d21: kd&gt; ?0x1234Evaluate expression: 4660 &#x3D; 00001234 As mentioned, the C++ evaluator also allows for you to treat your variables the way you would in your C/C++ code. For example, I have a local PFILE_OBJECT variable and I can easily dereference fields of that structure: 12345671: kd&gt; ? fileObject-&gt;DeviceObjectEvaluate expression: -2117322120 &#x3D; 81cc3a781: kd&gt; ? fileObject-&gt;FileName.BufferEvaluate expression: -505429944 &#x3D; e1dfc048 This idea also extends out to pointer arithmetic, since the evaluator is aware of the types of your pointers. Let’s see what happens if we add one to the PWCHAR Buffer field of our UNICODE_STRING above: 1231: kd&gt; ?fileObject-&gt;FileName.Buffer+1Evaluate expression: -505429942 &#x3D; e1dfc04a The last nice feature that we’ll discuss is the ability to cast virtual addresses into structure types and then dereference fields of the structure. Here we take an address, cast it as an ETHREAD, then view a field: 1231: kd&gt; ? ((nt!_ethread *)0x81d11020)-&gt;Win32StartAddressEvaluate expression: 1979339677 &#x3D; 75fa539d Switching Evaluators As you can see, there are probably reasons to use the MASM evaluator in some cases and the C++ evaluator in others. Due to the MASM evaluator’s symbol evaluation and respect of the default radix, it’s likely to be the evaluator that you want to use on a daily basis. In fact, the WinDBG documentation recommends sticking to the MASM evaluator for day to day use. However, the C++ evaluator can be handy and lead to much more powerful WinDBG scripting. Thus, the question becomes how to incorporate both evaluators into your normal usage? We’ve already seen the first choice in switching between evaluators, which is to use the .expr command to change the default evaluator: 12345678910111213141516171: kd&gt; .expr &#x2F;qAvailable expression evaluators:MASM - Microsoft Assembler expressionsC++ - C++ source expressionsCurrent expression evaluator: MASM - Microsoft Assembler expressions1: kd&gt; .expr &#x2F;s c++Current expression evaluator: C++ - C++ source expressions1: kd&gt; .expr &#x2F;s masmCurrent expression evaluator: MASM - Microsoft Assembler expressions That’s cumbersome though and you need to remember to switch back. The more convenient option is to leave MASM as the default evaluator but perform an evaluator override when you want to evaluate an express with C++. This is done with the _@@_ prefix, which has three forms: @@c++(expression) - Evaluate expression with the C++ evaluator @@masm() - Evaluate expression with the MASM evaluator @@() - Evaluate the expression with the opposite of the evaluator that should be used The third option is a bit tricky and less clear than the first two options. It simply looks to see what evaluator should be used for the current expression and uses the opposite. It basically just saves you some typing if you know what expression evaluator would be used and you want the opposite. We can see this in action in a goofy example. Let’s see what we’d get if we added four to the length of the file name in the file object. We’re currently in the MASM expression evaluator, so we’ll need to perform an override to get the name length out of the file object: 12345671: kd&gt; .exprCurrent expression evaluator: MASM - Microsoft Assembler expressions1: kd&gt; ? 4 + @@c++(fileObject-&gt;FileName.Length)Evaluate expression: 6 &#x3D; 00000006 Because we already knew that we were in the MASM evaluator, we could have omitted the c++ from the @@ prefix: 1231: kd&gt; ? 4 + @@(fileObject-&gt;FileName.Length)Evaluate expression: 6 &#x3D; 00000006 When the Default Isn’t the Default While most of the time we’re working with whatever the default expression evaluator is, there are three circumstances in which you will always get the C++ evaluator by default. The first is when using the ?? command, which is used specifically for evaluating C++ expressions. This command is nice because it goes beyond showing just the value of the expression and shows typed information. So, instead of seeing the file name buffer address in our earlier examples we can dump out the full Unicode string structure: 1234567891011121314151: kd&gt; .exprCurrent expression evaluator: MASM - Microsoft Assembler expressions1: kd&gt; ?? fileObject-&gt;FileNamestruct _UNICODE_STRING&quot;\\&quot;+0x000 Length : 2+0x002 MaximumLength : 0x38+0x004 Buffer : 0xe1dfc048 &quot;\\&quot; You are allowed to specify MASM expressions to this command, you just need to wrap them in a @@masm() or @@() prefix. Confusingly enough, @@() works here because that simply chooses the opposite evaluator from what would be used, which in this case is C++: 12345678910111: kd&gt; ?? statuslong 0xc00000011: kd&gt; ?? @@masm(status)unsigned int64 0xffffffff&#96;f7c1b4401: kd&gt; ?? @@(status)unsigned int64 0xffffffff&#96;f7c1b440 The other two places where you get the C++ expression evaluator are the Watch and Locals windows. This is what allows you to type addresses as structures and browse them via the GUI in those windows. Hopefully A Little Less Cryptic The debugger syntax is indeed rich and full of nuances, which can make diving in a bit of a challenge at first. Hopefully this intro to the expression evaluators provides a good starting point for some experimentation of your own. 本文地址：http://xnerv.wang/the-tale-of-two-evaluators-understanding-masm-and-cpp-expression-evaluators-in-windbg/ 转载自：The Tale of Two Evaluators: Understanding MASM and C++ Expression Evaluators in WinDbg","categories":[{"name":"WinDbg","slug":"WinDbg","permalink":"https://xnerv.wang/categories/WinDbg/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/tags/Windows/"},{"name":"WinDbg","slug":"WinDbg","permalink":"https://xnerv.wang/tags/WinDbg/"}]},{"title":"Lock Escalations（转载）","slug":"lock-escalations","date":"2018-02-02T07:00:00.000Z","updated":"2023-08-21T02:24:18.818Z","comments":true,"path":"lock-escalations/","link":"","permalink":"https://xnerv.wang/lock-escalations/","excerpt":"In todays blog posting I want to talk about Lock Escalations in SQL Server. Lock Escalations are an optimization technique used by SQL Server to control the amount of locks that are held within the Lock Manager of SQL Server. Let’s start in the first step with the description of the so-called Lock Hierarchy in SQL Server, because that’s the reason why the concept of the Lock Escalations exists in a relational database like SQL Server. Lock Hierarchy The following picture shows you the lock hierarchy used by SQL Server:","text":"In todays blog posting I want to talk about Lock Escalations in SQL Server. Lock Escalations are an optimization technique used by SQL Server to control the amount of locks that are held within the Lock Manager of SQL Server. Let’s start in the first step with the description of the so-called Lock Hierarchy in SQL Server, because that’s the reason why the concept of the Lock Escalations exists in a relational database like SQL Server. Lock Hierarchy The following picture shows you the lock hierarchy used by SQL Server: As you can see from the picture, the lock hierarchy starts at the database level, and goes down to the row level. You always have a Shared Lock (S) on the database level itself. When your query is connected to a database (e.g. USE MyDatabase), the Shared Lock prevents the dropping of the database, or that backups are restored over that database. And underneath the database level, you have locks on the table, on the pages, and the records when you are performing an operation. When you are executing a SELECT statement, you have an Intent Shared Lock (IS) on the table and page level, and a Shared Lock (S) on the record itself. When you are performing a data modification statement (INSERT, UPDATE, DELETE), you have an Intent Exclusive or Update Lock (IX or IU) on the table and page level, and a Exclusive or Update Lock (X or U) on the changed records. SQL Server always acquires locks from top to bottom to prevent so-called Race Conditions, when multiple threads trying to acquire locks concurrently within the locking hierarchy. Imagine now how the lock hierarchy would look like, when you perform a DELETE operation on a table against 20.000 rows. Let’s assume that a row is 400 bytes long, means that 20 records fit onto one page of 8kb: You have one S Lock on the database, 1 IX Lock on the table, 1.000 IX locks on the pages (20.000 records are spread across 1.000 pages), and you have finally 20.000 X locks on the records itself. In sum you have acquired 21.002 locks for the DELETE operation. Every lock needs in SQL Server 96 bytes of memory, so we look at 1.9 MB of locks just for 1 simple query. This will not scale indefinitely when you run multiple queries in parallel. For that reason SQL Server implements now the so-called Lock Escalation. Lock Escalations As soon as you have more than 5.000 locks on one level in your locking hierarchy, SQL Server escalates these many fine-granularity locks into a simple coarse-granularity lock. By default SQL Server always escalates to the table level. This mean that your locking hierarchy from the previous example looks like the following after the Lock Escalation has been successfully performed. As you can see, you have only one big lock on the table itself. In the case of the DELETE operation, you have one Exclusive Lock (X) on the table level. This will hurt the concurrency of your database in a very negative way! Holding an Exclusive Lock on the table level means that no other session is able anymore to access that table – every other query will just block. When you are running your SELECT statement in the Repeatable Read Isolation Level, you are also holding your Shared Locks till the end of the transaction, means you will have a Lock Escalation as soon as you have read more than 5.000 rows. The result is here one Shared Lock on the table itself! Your table is temporary readonly, because every other data modification on that table will be blocked! There is also a misconception that SQL Server will escalate from the row level to the page level, and finally to the table level. Wrong! Such a code path doesn’t exist in SQL Server! SQL Server will by default always escalate directly to the table level. An escalation policy to the page level just doesn’t exist. If you have your table partitioned (Enterprise Edition only!), then you can configure an escalation to the partition level. But you have to test here very carefully your data access pattern, because a Lock Escalation to the partition level can cause a deadlock. Therefore this option is also not enabled by default. Since SQL Server 2008 you can also control how SQL Server performs the Lock Escalation – through the ALTER TABLE statement and the property LOCK_ESCALATION. There are 3 different options available: TABLE AUTO DISABLE 1234567-- Controllling Lock EscalationALTER TABLE Person.PersonSET( LOCK_ESCALATION = AUTO -- or TABLE or DISABLE)GO The default option is TABLE, means that SQL Server always performs the Lock Escalation to the table level – even when the table is partitioned. If you have your table partitioned, and you want to have a Partition Level Lock Escalation (because you have tested your data access pattern, and you don’t cause deadlocks with it), then you can change the option to AUTO. AUTO means that the Lock Escalation is performed to the partition level, if the table is partitioned, and otherwise to the table level. And with the option DISABLE you can completely disable the Lock Escalation for that specific table. But disabling Lock Escalations is not the very best option, because the Lock Manager of SQL Server can then consume a huge amount of memory, if you are not very carefully with your queries and your indexing strategy. Conclusion Lock Escalation in SQL Server is mainly a nightmare. How will you delete more than 5.000 rows from a table without running into Lock Escalations? You can disable Lock Escalation temporarily, but you have to be very careful here. Another option (that I’m suggesting) is to make your DELETE/UPDATE statements in a loop as different, separate transactions: DELETE/UPDATE less than 5.000 rows, so that you can prevent Lock Escalations. As a very nice side-effect your huge, big transaction will be splitted into multiple smaller ones, which will also help you with Auto Growth issues that you maybe have with your transaction log. Thanks for reading 本文地址：http://xnerv.wang/lock-escalations/ 转载自：Lock Escalations","categories":[{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/categories/DBMS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/tags/DBMS/"},{"name":"SQL Server","slug":"SQL-Server","permalink":"https://xnerv.wang/tags/SQL-Server/"},{"name":"Transaction","slug":"Transaction","permalink":"https://xnerv.wang/tags/Transaction/"},{"name":"Lock Escalations","slug":"Lock-Escalations","permalink":"https://xnerv.wang/tags/Lock-Escalations/"}]},{"title":"Why do we need Intent Locks in SQL Server?（转载）","slug":"why-do-we-need-intent-locks-in-sql-server","date":"2018-02-01T07:00:00.000Z","updated":"2023-08-21T02:24:19.369Z","comments":true,"path":"why-do-we-need-intent-locks-in-sql-server/","link":"","permalink":"https://xnerv.wang/why-do-we-need-intent-locks-in-sql-server/","excerpt":"I blogged 2 years ago about why we need UPDATE locks in SQL Server. Today I want to continue this discussion by talking about Intent Locks in SQL Server, and why they are needed. The Lock Hierarchy in SQL Server When I talked about Lock Escalations in SQL Server, I started by briefly mentioning that SQL Server uses a Lock Hierarchy when you read or change your data. When you read a row, SQL Server always acquires by default a Shared Lock (S), and when you change a row SQL Server acquires an Exclusive Lock (X). Those Locks are incompatible with each other, and that will introduce blocking situations when you want to read/write a row concurrently. In addition to the row level locks, SQL Server also acquires so-called Intent Locks at higher levels within the Lock Hierarchy: at the page and at the table level. SQL Server acquires the following Intent-Locks based on the requested row level lock: Intent Shared Lock (IS), when you have a Shared Lock at the row level Intent Update Lock (IU), when you have an Update Lock at the row level Intent Exclusive Lock (IX), when you have an Exclusive Lock at the row level Therefore you always get the Lock Hierarchy as shown above when you read and write your records. But why is SQL Server using these Intent Locks?","text":"I blogged 2 years ago about why we need UPDATE locks in SQL Server. Today I want to continue this discussion by talking about Intent Locks in SQL Server, and why they are needed. The Lock Hierarchy in SQL Server When I talked about Lock Escalations in SQL Server, I started by briefly mentioning that SQL Server uses a Lock Hierarchy when you read or change your data. When you read a row, SQL Server always acquires by default a Shared Lock (S), and when you change a row SQL Server acquires an Exclusive Lock (X). Those Locks are incompatible with each other, and that will introduce blocking situations when you want to read/write a row concurrently. In addition to the row level locks, SQL Server also acquires so-called Intent Locks at higher levels within the Lock Hierarchy: at the page and at the table level. SQL Server acquires the following Intent-Locks based on the requested row level lock: Intent Shared Lock (IS), when you have a Shared Lock at the row level Intent Update Lock (IU), when you have an Update Lock at the row level Intent Exclusive Lock (IX), when you have an Exclusive Lock at the row level Therefore you always get the Lock Hierarchy as shown above when you read and write your records. But why is SQL Server using these Intent Locks? Intent Locks in SQL Server From a technical perspective the Intent Locks are not really needed by SQL Server. They have to do with performance optimization. Let’s have a look on that in more detail. With an Intent Lock SQL Server just indicates at a higher level within the Lock Hierarchy that you have acquired a Lock somewhere else. A Intent Shared Lock tells SQL Server that there is a Shared Lock somewhere else. A Intent Update or Intent Exclusive Lock does the same, but this time SQL Server knows that there is an Update Lock or an Exclusive Lock somewhere. It is just an indication, nothing more. But how does that indication help SQL Server with performance optimization? Imagine you want to acquire an Exclusive Lock at the table level. In that case, SQL Server has to know if there is an incompatible lock (like a Shared or Update Lock) somewhere else on a record. Without Intent Locks SQL Server would have to check every record to see if an incompatible lock has been granted. But with an Intent Shared Lock on the table level, SQL Server knows immediately that a Shared Lock has been granted somewhere else, and therefore an Exclusive Lock can’t be granted at the table level. That’s the whole reason why Intent Locks exist in SQL Server: to allow efficient checking if an incompatible lock exists somewhere within the Lock Hierarchy. Quite easy, isn’t it? Summary Intent Locks are technically not needed by SQL Server, because they only indicate if there is some other specific Lock Type somewhere else within the Lock Hierarchy. But based on that fact SQL Server can check in a much more efficient way if an incompatible Lock exists somewhere else if you want to acquire a specific lock at the page or table level. Thanks for your time, 本文地址：http://xnerv.wang/why-do-we-need-intent-locks-in-sql-server/ 转载自：Why do we need Intent Locks in SQL Server?","categories":[{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/categories/DBMS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/tags/DBMS/"},{"name":"SQL Server","slug":"SQL-Server","permalink":"https://xnerv.wang/tags/SQL-Server/"},{"name":"Transaction","slug":"Transaction","permalink":"https://xnerv.wang/tags/Transaction/"},{"name":"Intent Lock","slug":"Intent-Lock","permalink":"https://xnerv.wang/tags/Intent-Lock/"}]},{"title":"Why do we need UPDATE Locks in SQL Server?（转载）","slug":"why-do-we-need-update-locks-in-sql-server","date":"2018-01-31T07:21:00.000Z","updated":"2023-08-21T02:24:19.388Z","comments":true,"path":"why-do-we-need-update-locks-in-sql-server/","link":"","permalink":"https://xnerv.wang/why-do-we-need-update-locks-in-sql-server/","excerpt":"Today I want to talk about a specific question that I almost get every time when I teach about Locking &amp; Blocking in SQL Server: Why does SQL Server need to have Update Locks? Before we go down to the details of why they are needed, I first want to give you a basic overview of when an Update (U) Lock is acquired, and how the lock itself behaves regarding its compatibility. In general an Update Lock is used in SQL Server when performing an UPDATE statement. When you look at the underlying query plan, you can see that such a plan always consists of 3 parts: Reading data Calculating new values Writing data When SQL Server initially reads the data to be changed in the first part of the query plan, Update Locks are acquired on the individual records. And finally these Update Locks are converted to Exclusive (X) Locks when the data is changed in the third part of the query plan. The question that arrises with this approach is always the same: why does SQL Server acquire Update Locks instead of Shared (S) Locks in the first phase? When you normally read data through a SELECT statement, a Shared Lock is also good enough. Why is there now a different approach with UPDATE query plans? Let’s have a more detailed look at it.","text":"Today I want to talk about a specific question that I almost get every time when I teach about Locking &amp; Blocking in SQL Server: Why does SQL Server need to have Update Locks? Before we go down to the details of why they are needed, I first want to give you a basic overview of when an Update (U) Lock is acquired, and how the lock itself behaves regarding its compatibility. In general an Update Lock is used in SQL Server when performing an UPDATE statement. When you look at the underlying query plan, you can see that such a plan always consists of 3 parts: Reading data Calculating new values Writing data When SQL Server initially reads the data to be changed in the first part of the query plan, Update Locks are acquired on the individual records. And finally these Update Locks are converted to Exclusive (X) Locks when the data is changed in the third part of the query plan. The question that arrises with this approach is always the same: why does SQL Server acquire Update Locks instead of Shared (S) Locks in the first phase? When you normally read data through a SELECT statement, a Shared Lock is also good enough. Why is there now a different approach with UPDATE query plans? Let’s have a more detailed look at it. Deadlock Avoidance First of all UPDATE Locks are needed to avoid deadlock situations in UPDATE query plans. Let’s try to imagine what happens when multiple UPDATE query plans acquire Shared Locks in the first phase of the plan, and afterwards convert these Shared Locks to Exclusive Locks when the data is finally changed in the third phase of the query plan: The 1st query can’t convert the Shared Lock to an Exclusive Lock, because the 2nd query has already acquired a Shared Lock. The 2nd query can’t convert the Shared Lock to an Exclusive Lock, because the 1st query has already acquired a Shared Lock. That approach would lead to a traditional deadlock situation in a relational database: That’s one of the main reasons why implementers of relational database engines have introduced Update Locks to avoid that specific deadlock situation. An Update Lock is only compatible with a Shared Lock, but isn’t compatible with another Update or Exclusive Lock. Therefore a deadlock situation can be avoided, because 2 UPDATE query plans can’t run concurrently at the same time. The 2nd query will just wait until the Update Lock can be acquired in the 1st phase of the query plan. An unpublished study of System R also showed that this kind of deadlock was the most prominent one. System R was initially implemented without any Update Locks. Improved Concurrency Instead of acquiring an Update Lock during the 1st phase, it would be also a viable option to acquire an Exclusive Lock directly in that phase. This will also overcome the deadlock problem, because an Exclusive Lock is not compatible with another Exclusive Lock. But the problem with that approach is limited concurrency, because in the mean time no other SELECT query can read the data that is currently exclusively locked. Therefore there is also the need for the Update Lock, because this specific lock is compatible with the traditional Shared Lock. As a result this means that other SELECT queries can read data, as long as individual Update Locks are not yet converted to Exclusive Locks. As a side-effect this will improve the concurrency of our parallel running queries. In traditional relational literature an Update Lock is a so-called Asymmetric Lock. In the context of the Update Lock that means that the Update Lock is compatible with the Shared Lock, but not vice-versa: the Shared Lock is not compatible with the Update Lock. But SQL Server doesn’t implement the Update Lock as an asymmetric one. The Update Lock is a symmetric one, which means that Update and Shared Locks are compatible in both directions. This will also improve the overall concurrency of the system, because it doesn’t introduce blocking situations between both lock types. Summary In todays blog posting I gave you an overview of Update Locks in SQL Server, and why they are needed. As you have seen there is a really strong need for Update Locks in a relational database, because otherwise it would yield to deadlock situations and decreased concurrency. I hope that you now have a better understanding of Update Locks, and how they are used in SQL Server. Thanks for reading! 本文地址：http://xnerv.wang/why-do-we-need-update-locks-in-sql-server/ 转载自：Why do we need UPDATE Locks in SQL Server?","categories":[{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/categories/DBMS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/tags/DBMS/"},{"name":"SQL Server","slug":"SQL-Server","permalink":"https://xnerv.wang/tags/SQL-Server/"},{"name":"Transaction","slug":"Transaction","permalink":"https://xnerv.wang/tags/Transaction/"},{"name":"Update Lock","slug":"Update-Lock","permalink":"https://xnerv.wang/tags/Update-Lock/"}]},{"title":"Snapshot Isolation Level in SQL Server - What, Why and How - Part 1（转载）","slug":"snapshot-isolation-level-in-sql-server-what-why-and-how-part-1","date":"2018-01-30T08:37:00.000Z","updated":"2023-08-21T02:24:19.255Z","comments":true,"path":"snapshot-isolation-level-in-sql-server-what-why-and-how-part-1/","link":"","permalink":"https://xnerv.wang/snapshot-isolation-level-in-sql-server-what-why-and-how-part-1/","excerpt":"Introduction Snapshot Isolation level was introduced in SQL Server 2005 and has been available ever since. Snapshot isolation levels improve performance but there are some things to take into consideration when using this feature. Some people use it frequently as it minimizes blocking and improves performance/concurrency without knowing its impact on maintaining versions in tempdb, whereas some people stay away from it because of this extra overhead. Some people get confused about the two variants of snapshot isolation level (Read Committed Snapshot Isolation (RCSI) and Snapshot Isolation (SI)) and use one where the other is needed or vice versa. In this article series, I am going to discuss what snapshot isolation levels are, their variants, why and when we should use them and how we should start using this feature with examples.","text":"Introduction Snapshot Isolation level was introduced in SQL Server 2005 and has been available ever since. Snapshot isolation levels improve performance but there are some things to take into consideration when using this feature. Some people use it frequently as it minimizes blocking and improves performance/concurrency without knowing its impact on maintaining versions in tempdb, whereas some people stay away from it because of this extra overhead. Some people get confused about the two variants of snapshot isolation level (Read Committed Snapshot Isolation (RCSI) and Snapshot Isolation (SI)) and use one where the other is needed or vice versa. In this article series, I am going to discuss what snapshot isolation levels are, their variants, why and when we should use them and how we should start using this feature with examples. Understanding Snapshot Isolation Level Isolation level controls how two or more transactions running simultaneously should be isolated from each other in terms of locking and blocking resources. Isolation level determines the level of concurrency and data consistency. Prior to SQL Server we had four isolation levels as briefly discussed below: Read Uncommitted – The transaction that uses this isolation level neither acquires shared locks to prevent others from modifying the data nor is blocked by conflicting locks acquired by other transactions. As this transaction can see the data that has been changed by other transactions but has not been committed, there is a possibility of dirty reads. This isolation level assures higher concurrency at the cost of data consistency. Read Committed – This is the default transaction isolation level in SQL Server and prevents dirty reads by not allowing reading of modified but not yet committed data by other transactions in the current transaction. A transaction with this isolation level acquires shared locks to prevent other transactions from modifying the data during read operation by that transaction. As a shared lock can be acquired only if there is no exclusive lock (needed for data modification) by other transactions, it ensures it reads only committed data. Repeatable Read – A transaction with Read Committed isolation level might face a problem of repeatable read, which means in a single transaction two data reads might see different sets of data if the data is changed between these two data reads. This happens as shared lock is acquired only while data is processed and released immediately after it. In other words, after the first data read the shared lock will be released and if the other transaction modifies the data (as other transactions can acquire exclusive locks) before a subsequent data read, the subsequent data read will see a different set of data than the previous data read. To avoid a repeatable read issue, you can use Repeatable Read isolation level with your transaction. The difference between Read Committed and Repeatable Read is, in the Read Committed isolation level the shared lock is released once the data gets processed without waiting for transaction completion whereas in Repeatable Read isolation levels the shared lock is held until the transaction completes either by committing or roll backing. Of course holding the shared lock till the end of the transaction improves the data consistency but reduces concurrency as well. Serializable – Even though Repeatable Read isolation level provides consistency in repeatable reads, there is still the possibility of phantom read ( Phantom read means two reads from a single transaction return a different number of records even though they use exactly same predicates). Serializable isolation level provides the highest level of data consistency but at the cost of greatly reduced concurrency. To avoid phantom, it uses range locks in the range of key values that match predicates of each statement executed in the current transaction. This way this isolation level blocks other transactions from inserting or updating any rows, which qualify the predicates used in the current transaction and hence the current transaction will keep on getting exactly the same records in the current transaction for the query. Please note the range locks are acquired and held till the completion of the transaction and hence this isolation is most restrictive and should be used only when absolutely necessary. If you notice, in all of the above isolation levels (except in the case of Read Uncommitted where there is a possibility of dirty reads anyway), the data writers (exclusive lock) block data readers (shared lock) and data readers block data writers. SQL Server 2005 introduced two new snapshot based isolation levels. The idea behind these isolation levels is to not let data writers block data readers and vice versa. These new snapshot isolation levels use a row versioning concept where they maintain the version of previously committed data in version store (tempdb) and hence it allows data readers to continue reading the older committed/consistent version of data before the current transaction/statement began, even though current version is locked and being changed by other data writers. Why, When and Where Should We Use Snapshot Isolation Levels? When using snapshot isolation levels, when the same data is modified by many data writers, SQL Server might need to maintain multiple versions of the old data and hence proper envisioning and planning needs to be done for the tempdb database size and storage before utilizing this feature. SQL Server needs to maintain versions in version store as long as they might be needed by currently running operations and if they are not needed they are removed from the version store by the SQL Server. When you use snapshot (how to use is discussed in the next section) isolation levels, any update (please note many updates to the same data in a single transaction does not create multiple versions but rather many updates from multiple transactions do) will be marked with a timestamp and will create a version with old committed data in version store and a pointer (14 bytes needed for pointer and additional overhead) is stored with the changed/new data. This storage of pointers will also add to the cost of using snapshot isolation level. If changes are very frequent, successive prior versions are stored in tempdb using a linked list structure and the newest committed value is always stored in a page in the database. There are two variants of using snapshot isolation levels as discussed below: Read Committed Snapshot Isolation (RCSI) – This is also frequently referred to as statement level snapshot isolation level. This is, you can say, an extension of Read Committed isolation level but with increased concurrency. In this data readers don’t get blocked by data writers but rather once enabled at database level, SQL Server starts maintaining a version of data being changed by data writers and data readers get data (last committed version before the statement starts, irrespective of the timing of transaction start) from versions stored in version store. As I said, to use it you just have to enable at database level and no code changes are required as it applies to all the transactions by default. Snapshot Isolation (SI) – This is also frequently referred to as transaction level snapshot isolation level. This increases concurrency along with data consistency at transaction level. Like RCSI, data readers aren’t blocked by data writers but rather once enabled at database level and specified to be used in the code, SQL Server start maintaining a version of the data being changed by data writers and data readers get data (last committed version before the transaction start, in other words it provides transaction level consistent view of data) from versions stored in version store. This requires code changes to use the SET command to with the transaction with which you want to use it. Using either of the snapshot isolation levels requires enabling it at database level first though another important difference between RCSI and SI is, SI requires you to explicitly change the isolation level to SNAPSHOT for each transaction that you want to execute at the transaction level with SI and usage of this in legacy system requires code changes, whereas RCSI becomes the default for all the transaction without doing any code changes. Now at the end, just to summarize the benefit of using snapshot isolation, data readers can get consistent data without being blocked by data writers running at the same time as the versions get stored in version store in the tempdb database before data is changed, whereas the cost of using snapshot isolation is more overheard on SQL Server in creating and maintaining versions and the increased size of tempdb for storage for version data and the increased size of each row for pointers. The overhead even becomes more when you use SI or transaction level snapshot, in which case versions are maintained till the end of the transaction as opposed to completion of the statement, as in case of RCSI. Conclusion SQL Server 2005 and later versions have 6 different isolation levels. I briefly talked about the earlier four, which have been in SQL Server for a long time and talked in detail about two new snapshot based isolation levels. To learn more about how to use this great feature and how they differ from each other with an example, please refer to the next article on the series. In closing, I would like to reiterate, these new snapshot based isolation levels are great as they provide better concurrency but come at a cost. Please ensure you do a thorough study before using this feature and consider the size and load on tempdb. especially in the case of SI. Resources SET TRANSACTION ISOLATION LEVEL (Transact-SQL) Isolation Levels in the Database Engine See all articles by Arshad Ali 本文地址：http://xnerv.wang/snapshot-isolation-level-in-sql-server-what-why-and-how-part-1/ 转载自：Snapshot Isolation Level in SQL Server - What, Why and How - Part 1","categories":[{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/categories/DBMS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/tags/DBMS/"},{"name":"SQL Server","slug":"SQL-Server","permalink":"https://xnerv.wang/tags/SQL-Server/"},{"name":"Transaction","slug":"Transaction","permalink":"https://xnerv.wang/tags/Transaction/"},{"name":"Isolation Level","slug":"Isolation-Level","permalink":"https://xnerv.wang/tags/Isolation-Level/"}]},{"title":"Read Committed Snapshot Isolation（转载）","slug":"read-committed-snapshot-isolation","date":"2018-01-30T07:47:00.000Z","updated":"2023-08-21T02:24:19.223Z","comments":true,"path":"read-committed-snapshot-isolation/","link":"","permalink":"https://xnerv.wang/read-committed-snapshot-isolation/","excerpt":"SQL Server provides two physical implementations of the read committed isolation level defined by the SQL standard, locking read committed and read committed snapshot isolation (RCSI). While both implementations meet the requirements laid down in the SQL standard for read committed isolation behaviours, RCSI has quite different physical behaviours from the locking implementation we looked at in the previous post in this series.","text":"SQL Server provides two physical implementations of the read committed isolation level defined by the SQL standard, locking read committed and read committed snapshot isolation (RCSI). While both implementations meet the requirements laid down in the SQL standard for read committed isolation behaviours, RCSI has quite different physical behaviours from the locking implementation we looked at in the previous post in this series. Logical Guarantees The SQL standard requires that a transaction operating at the read committed isolation level not experience any dirty reads. Another way to express this requirement is to say a read committed transaction must only encounter committed data. The standard also says that read committed transactions might experience the concurrency phenomena known as non-repeatable reads and phantoms (though they are not actually required to do so). As it happens, both physical implementations of read committed isolation in SQL Server can experience non-repeatable reads and phantom rows, though the precise details are quite different. A point-in-time view of committed data If the database option READ_COMMITTED_SNAPSHOT in ON, SQL Server uses a row-versioning implementation of the read committed isolation level. When this is enabled, transactions requesting read committed isolation automatically use the RCSI implementation; no changes to existing T-SQL code is required to use RCSI. Note carefully though that this is not the same as saying that code will behave the same under RCSI as when using the locking implementation of read committed, in fact this is quite generally not the case. There is nothing in the SQL standard that requires the data read by a read committed transaction to be the most-recently committed data. The SQL Server RCSI implementation takes advantage of this to provide transactions with a point-in-time view of committed data, where that point in time is the moment the current statement began execution (not the moment any containing transaction started). This is quite different from the behaviour of the SQL Server locking implementation of read committed, where the statement sees the most-recently committed data as of the moment each item is physically read. Locking read committed releases shared locks as quickly as possible, so the set of data encountered may come from very different points in time. To summarize, locking read committed sees each row as it was at the time it was briefly locked and physically read; RCSI sees all rows as they were at the time the statement began. Both implementations are guaranteed to never see uncommitted data, but the data they encounter can be very different. The implications of a point-in-time view Seeing a point-in-time view of committed data might seem self-evidently superior to the more complex behaviour of the locking implementation. It is clear, for example, that a point-in-time view cannot suffer from the problems of missing rows or encountering the same row multiple times, which are both possible under locking read committed isolation. A second important advantage of RCSI is that it does not acquire shared locks when reading data, because the data comes from the row version store rather than being accessed directly. The lack of shared locks can dramatically improve concurrency by eliminating conflicts with concurrent transactions looking to acquire incompatible locks. This advantage is commonly summarized by saying that readers do not block writers under RCSI, and vice-versa. As a further consequence of reducing blocking due to incompatible lock requests, the opportunity for deadlocks is usually greatly reduced when running under RCSI. However, these benefits do not come without costs and caveats. For one thing, maintaining versions of committed rows consumes system resources, so it is important that the physical environment is configured to cope with this, primarily in terms of tempdb performance and memory/disk space requirements. The second caveat is a little more subtle: RCSI provides a snapshot view of committed data as it was at the start of the statement, but there is nothing to prevent the real data from being changed (and those changes committed) while the RCSI statement is executing. There are no shared locks, remember. An immediate consequence of this second point is that T-SQL code running under RCSI might make decisions based on out of date information, as compared with the current committed state of the database. We will talk more about this shortly. There is one last (implementation-specific) observation I want to make about RCSI before we move on. Scalar and multi-statement functions execute using a different internal T-SQL context from the containing statement. This means that the point-in-time view seen inside a scalar or multi-statement function invocation can be later than the point-in-time view seen by the rest of the statement. This can result in unexpected inconsistencies, as different parts of the same statement see data from different points in time. This weird and confusing behaviour does not apply to in-line functions, which see the same snapshot as the statement they appear in. Non-repeatable reads and phantoms Given a statement-level point-in-time view of the committed state of the database, it might not be immediately apparent how a read committed transaction under RCSI might experience the non-repeatable read or phantom row phenomena. Indeed, if we limit our thinking to the scope of a single statement, neither of these phenomena are possible under RCSI. Reading the same data multiple times within the same statement under RCSI will always return the same data values, no data will disappear between those reads, and no new data will appear either. If you are wondering what sort of statement might read the same data more than once, think about queries that reference the same table more than once, perhaps in a subquery. Statement-level read consistency is an obvious consequence of the reads being issued against a fixed snapshot of the data. The reason that RCSI does not provide protection from non-repeatable reads and phantoms is that these SQL standard phenomena are defined at the transaction level. Multiple statements within a transaction running at RCSI may see different data, because each statement sees a point-in-time view as of the moment that particular statement started. To summarize, each statement within an RCSI transaction sees a static committed data set, but that set can change between statements inside the same transaction. Out-of-date data The possibility of our T-SQL code making an important decision based on out-of-date information is more than a little unsettling. Consider for a moment that the point-in-time snapshot used by a single statement running under RCSI might be arbitrarily old. A statement that runs for a considerable period a time will continue to see the committed state of the database as it was when the statement began. Meanwhile, the statement is missing all the committed changes that occurred in the database since that time. This is not to say that problems associated with accessing stale data under RCSI are limited to long-running statements, but the issues certainly might be more pronounced in such cases. A question of timing This issue of out-of-date data applies to all RCSI statements in principle, no matter how quickly they might complete. How ever small the time window is, there is always a chance that a concurrent operation might modify the data set we are working with, without us being aware of that change. Let us look again at one of the simple examples we used before when exploring the behaviour of locking read committed: 123456789INSERT dbo.OverdueInvoicesSELECT I.InvoiceNumberFROM dbo.Invoices AS IWHERE I.TotalDue &gt;( SELECT SUM(P.Amount) FROM dbo.Payments AS P WHERE P.InvoiceNumber = I.InvoiceNumber); When run under RCSI, this statement cannot see any committed database modifications that occur after the statement starts executing. While we will not encounter the problems of missed or multiply-encountered rows possible under the locking implementation, a concurrent transaction might add a payment that ought to prevent a customer from being sent a stern warning letter about an overdue payment after the statement above starts executing. You can probably think of many other potential problems that might occur in this scenario, or in others that are conceptually similar. The longer the statement runs for, the more out-of-date its view of the database becomes, and the greater the scope for possibly-unintended consequences. Of course, there are plenty of mitigating factors in this specific example. The behaviour might well be seen as perfectly acceptable. After all, sending a reminder letter because a payment arrived a few seconds too late is an easily defended action. The principle remains however. Business Rule Failures and Integrity Risks More serious issues can arise from the use of out-of-date information than sending a warning letter a few seconds early. A good example of this class of weakness can be seen with trigger code used to enforce an integrity rule that is perhaps too complex to enforce with declarative referential integrity constraints. To illustrate, consider the following code, which uses a trigger to enforce a variation of a foreign key constraint, but one that enforces the relationship for only certain child table rows: 123456789101112131415161718192021222324252627282930313233343536373839ALTER DATABASE SandpitSET READ_COMMITTED_SNAPSHOT ONWITH ROLLBACK IMMEDIATE;GOSET TRANSACTION ISOLATION LEVEL READ COMMITTED;GOCREATE TABLE dbo.Parent (ParentID integer PRIMARY KEY);GOCREATE TABLE dbo.Child( ChildID integer IDENTITY PRIMARY KEY, ParentID integer NOT NULL, CheckMe bit NOT NULL);GOCREATE TRIGGER dbo.Child_AION dbo.ChildAFTER INSERTASBEGIN -- Child rows with CheckMe = true -- must have an associated parent row IF EXISTS ( SELECT ins.ParentID FROM inserted AS ins WHERE ins.CheckMe = 1 EXCEPT SELECT P.ParentID FROM dbo.Parent AS P ) BEGIN RAISERROR (&#x27;Integrity violation!&#x27;, 16, 1); ROLLBACK TRANSACTION; ENDEND;GO-- Insert parent row #1INSERT dbo.Parent (ParentID) VALUES (1); Now consider a transaction running in another session (use another SSMS window for this if you are following along) which deletes parent row #1, but does not commit yet: 1234SET TRANSACTION ISOLATION LEVEL READ COMMITTED;BEGIN TRANSACTION;DELETE FROM dbo.ParentWHERE ParentID = 1; Back in our original session, we try to insert a (checked) child row that references this parent: 12INSERT dbo.Child (ParentID, CheckMe)VALUES (1, 1); The the trigger code executes, but because RCSI sees only committed data as of the time the statement started, it still sees the parent row (not the uncommitted deletion) and the insert succeeds! The transaction that deleted the parent row can now commit its change successfully, leaving the database in an inconsistent state in terms of our trigger logic: 123COMMIT TRANSACTION;SELECT P.* FROM dbo.Parent AS P;SELECT C.* FROM dbo.Child AS C; This is a simplified example of course, and one which could easily be circumvented using the built-in constraint facilities. Much more complex business rules and pseudo-integrity constraints can be written inside and outside of triggers. The potential for incorrect behaviour under RCSI should be obvious. Blocking behaviour and latest-committed data I mentioned earlier that T-SQL code is not guaranteed to behave in the same way under RCSI read committed as it did using the locking implementation. The preceding trigger code example is a good illustration of that, but I need to emphasise that the general problem is not limited to triggers. RCSI is typically not a good choice for any T-SQL code whose correctness depends on blocking if a concurrent uncommitted change exists. RCSI might also not be the right choice if the code depends on reading current committed data, rather than the latest committed data as at the time the statement started. These two considerations are related, but they are not the same thing. Locking read committed under RCSI SQL Server provides one way to request locking read committed when RCSI is enabled, using the table hint READCOMMITTEDLOCK. We can modify our trigger to avoid the problems shown above by adding this hint to the table that needs blocking behaviour to perform correctly: 123456789101112131415161718192021ALTER TRIGGER dbo.Child_AION dbo.ChildAFTER INSERTASBEGIN -- Child rows with CheckMe = true -- must have an associated parent row IF EXISTS ( SELECT ins.ParentID FROM inserted AS ins WHERE ins.CheckMe = 1 EXCEPT SELECT P.ParentID FROM dbo.Parent AS P WITH (READCOMMITTEDLOCK) -- NEW!! ) BEGIN RAISERROR (&#x27;Integrity violation!&#x27;, 16, 1); ROLLBACK TRANSACTION; ENDEND; With this change in place, the attempt to insert the potentially-orphaned child row blocks until the deleting transaction commits (or aborts). If the delete commits, the trigger code detects the integrity violation and raises the expected error. Identifying queries that might not perform correctly under RCSI is a non-trivial task that may require extensive testing to get right (and please remember these issues are quite general and not confined to trigger code!) Also, adding the READCOMMITTEDLOCK hint to every table that needs it can be a tedious and error-prone process. Until SQL Server provides a more broadly-scoped option to request the locking implementation where needed, we are stuck with using the table hints. Next Time The next post in this series continues our examination of read committed snapshot isolation, with a look at the surprising behaviour of data modification statements under RCSI. [ See the index for the whole series ] 本文地址：http://xnerv.wang/read-committed-snapshot-isolation/ 转载自：Read Committed Snapshot Isolation","categories":[{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/categories/DBMS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/tags/DBMS/"},{"name":"SQL Server","slug":"SQL-Server","permalink":"https://xnerv.wang/tags/SQL-Server/"},{"name":"Transaction","slug":"Transaction","permalink":"https://xnerv.wang/tags/Transaction/"},{"name":"Isolation Level","slug":"Isolation-Level","permalink":"https://xnerv.wang/tags/Isolation-Level/"}]},{"title":"Snapshot isolation transaction aborted due to update conflict.（转载）","slug":"snapshot-isolation-transaction-aborted-due-to-update-conflict","date":"2018-01-30T07:34:00.000Z","updated":"2023-08-21T02:24:19.283Z","comments":true,"path":"snapshot-isolation-transaction-aborted-due-to-update-conflict/","link":"","permalink":"https://xnerv.wang/snapshot-isolation-transaction-aborted-due-to-update-conflict/","excerpt":"Question To avoid deadlocks, we switched from ReadCommittedSnapshot isolation to SnapShot isolation for a SQL Server database at the database level and transaction level in the client code. Now, when two users perform concurrent operations on the database through the client, one of the clients get this error: “Snapshot isolation transaction aborted due to update conflict. You cannot use snapshot isolation to access table ‘dbo.cust_table’ directly or indirectly in database ‘cust_database’ to update, delete, or insert the row that has been modified or deleted by another transaction. Retry the transaction or change the isolation level for the update/delete statement.” What can we do to avoid deadlocks and update conflicts at the same time? (The same code with Oracle database and Oracle client works without any issues with the default Read Committed Snapshot isolation level) - SusmithaP","text":"Question To avoid deadlocks, we switched from ReadCommittedSnapshot isolation to SnapShot isolation for a SQL Server database at the database level and transaction level in the client code. Now, when two users perform concurrent operations on the database through the client, one of the clients get this error: “Snapshot isolation transaction aborted due to update conflict. You cannot use snapshot isolation to access table ‘dbo.cust_table’ directly or indirectly in database ‘cust_database’ to update, delete, or insert the row that has been modified or deleted by another transaction. Retry the transaction or change the isolation level for the update/delete statement.” What can we do to avoid deadlocks and update conflicts at the same time? (The same code with Oracle database and Oracle client works without any issues with the default Read Committed Snapshot isolation level) - SusmithaP All replies Hello Ok, you might be confused: READ_COMMITED_SNAPSHOT and the transaction isolation level SNAPSHOT are different. When you set a database to READ_COMMITED_SNAPSHOT your database will add 16 bytes to each row in the table for version storing. So whenever modifications to the data happen the readers are able to access a version of the row. This is statement level meaning the phantom rows and repeatable reads within the default transaction level (now READ_COMMITTED_SNAPSHOT) are possible. SET TRANSACTION ISOLATION SNAPSHOT is different. There is no version store initially, it is on request. So modifications will create the versioning so readers will not be blocked. This transaction level (similar to SERIALIZABLE without the blocking) phantom rows and non-repeatable reads are not possible. SNAPSHOT isolation is transaction level. With this transaction level you can run into problems when you have two (or more) writers, when the first transaction one creates a row version the next transaction comes along it also uses the version store. When the first transaction tries to commit it’s fine as the its version is up to date but as the second user/transaction goes to commit its transaction details (from the version it read) are now out of date. The second transaction won’t commit as it’s not consistent. So what your problem is you are using both methods but when you are setting the transaction isolation level (from your clients) to SNAPSHOT the READ_COMMITED_SNAPSHOT is ignored. You need to ask whether your procedures require the snapshot equivalent of COMMITED or SERIALIZABLE? Hope this helps - Rob 本文地址：http://xnerv.wang/snapshot-isolation-transaction-aborted-due-to-update-conflict/ 转载自：Snapshot isolation transaction aborted due to update conflict.","categories":[{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/categories/DBMS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/tags/DBMS/"},{"name":"SQL Server","slug":"SQL-Server","permalink":"https://xnerv.wang/tags/SQL-Server/"},{"name":"Transaction","slug":"Transaction","permalink":"https://xnerv.wang/tags/Transaction/"},{"name":"Isolation Level","slug":"Isolation-Level","permalink":"https://xnerv.wang/tags/Isolation-Level/"}]},{"title":"Understanding MySQL Isolation Levels: Repeatable-Read（转载）","slug":"understanding-mysql-isolation-levels-repeatable-read","date":"2018-01-30T07:21:00.000Z","updated":"2023-08-21T02:24:19.346Z","comments":true,"path":"understanding-mysql-isolation-levels-repeatable-read/","link":"","permalink":"https://xnerv.wang/understanding-mysql-isolation-levels-repeatable-read/","excerpt":"Isolation levels are a rare subject in MySQL literature. The documentation provides a terse description and focuses mainly on locking issues, but does not discuss the semantics of each isolation level. This is not only a problem that affects MySQL documentation but also the SQL standard itself. Both the lack of documentation and the absence of a deeper description of the expected behavior in the SQL standard make isolation levels a topic that is more assumed than known by database administrators and developers. In this blog post, I aim to help you understand how the default isolation level in MySQL works and show you some surprising facts about it. But first let’s see how isolation levels are described in the standard: “The transaction isolation level of a SQL-transaction defines the degree to which the operations on SQL-data, or schemas in that SQL-transaction are affected by the effects of and can affect operations on SQL-data or schemas in concurrent SQL-transactions”. To put it in plain words, isolation levels define how concurrent transactions interact while modifying data. MySQL uses Repeatable-read as the default level. In the standard, this level forbids dirty reads (non committed data) and non repeatable reads (executing the same query twice should return the same values) and allows phantom reads (new rows are visible). But MySQL implements it in a different way. Let’s see how it is implemented with some examples.","text":"Isolation levels are a rare subject in MySQL literature. The documentation provides a terse description and focuses mainly on locking issues, but does not discuss the semantics of each isolation level. This is not only a problem that affects MySQL documentation but also the SQL standard itself. Both the lack of documentation and the absence of a deeper description of the expected behavior in the SQL standard make isolation levels a topic that is more assumed than known by database administrators and developers. In this blog post, I aim to help you understand how the default isolation level in MySQL works and show you some surprising facts about it. But first let’s see how isolation levels are described in the standard: “The transaction isolation level of a SQL-transaction defines the degree to which the operations on SQL-data, or schemas in that SQL-transaction are affected by the effects of and can affect operations on SQL-data or schemas in concurrent SQL-transactions”. To put it in plain words, isolation levels define how concurrent transactions interact while modifying data. MySQL uses Repeatable-read as the default level. In the standard, this level forbids dirty reads (non committed data) and non repeatable reads (executing the same query twice should return the same values) and allows phantom reads (new rows are visible). But MySQL implements it in a different way. Let’s see how it is implemented with some examples. MySQL Repeatable-Read tests We create two connections against a MySQL server. We will call them Session Blue and Session Red (The fact that these are the colors of FC Barcelona is purely coincidental). In Session Blue we will create the database isolation and the table repeatable_read, both will be required for this test. No phantom reads… only phantom writes! We start a transaction, verify current isolation level by checking the value of the variable tx_isolation, and retrieve the contents of repeatable_read table, this way we create a snapshot of that table for the whole transaction. Now we move to Session Red and insert two rows into the table. We commit the transaction to make sure data is updated. Next we check what data is retrieved in Session Blue. As we can see, repeatable-read in MySQL avoids Phantom Reads, as rows are not retrieved. This is more restrictive than the standard description of the isolation level. But, what happens if we try to update the table contents? Our intuition is that we should not update any rows. Surprise! The update command tells us that one row matched and one row was changed. Let’s select table contents to view what is happening. We see just one row, the row that was modified by the update executed before. This is quite unexpected and counter-intuitive as the table never had one single row committed; we inserted and committed two rows. We are seeing a view of the table that never existed. As expected, when we commit, we see both rows, the one we modified and the other that was inserted before in Session Red. More phantom writes! Now we will begin another transaction and retrieve table contents. We retrieve table contents to create a snapshot (probably we should call it a version) of the table. Back in Session Red, we will run a transaction to update the contents of the table. Note: Phantom reads only affect new rows, not the ones already existing. Let’s find what Session Blue retrieves and what can update. Initially, we see the table unchanged. But no rows matched when we try to update the table using the data we retrieved in the select. We see one row with value “modified” for the text column, but the update finds no rows. When we update the table using a column value that was not modified by any transaction, in this case id, then we are able to proceed. Now we see the new value for the text column. WYSINWYW (What you see is NOT what you write) We will return our table to the original values and we will create an additional table required for the next test. As usual we start a new transaction and we retrieve table contents to create the snapshot. Now we go back to Session Red to update the contents of the whole table. Returning to Session Blue, we “clone” the contents of the table repeatable_read to repeatable_read_copy table using an insert into … select statement. After that we retrieve the values of both tables using a select. The values of rows inserted into the copy table using an insert into … as select is different than the values of rows retrieved using a regular select statement. Once we commit the transaction, as expected, we are able to see the modified data in the original table too. Conclusions After these tests, we have found about MySQL implementation of Repeatable-read isolation level: When using just select statements is even more restrictive than SQL Standard, as Phantom Reads do not happen. Besides the snapshot is used for all tables and all rows, as we find while we use a mysqldump with –single-transaction. When the transaction modifies data, the behavior is a mix of Repeatable-read (rows not modified are not visible) and Read committed (modified rows are visible). We cannot say that this is not the standard as these situations are not described in it and do not fit in the three concurrency phenomena: Dirty Read, Non-repeatable Read and Phantom Read. When the transaction writes new data based on existing data, it uses the committed data, instead of the snapshot retrieved previously. This is valid both for modified and new rows, mimicking Read committed behavior. The way MySQL implements Repeatable Read is not intuitive and, although it is required to support statement replication, can lead to some problems while running data modification and transfer to other tables in concurrent transactions. If your application can face these issues, you will need to modify your queries using select … for update statements and thus increase the number of locks in the database. 本文地址：http://xnerv.wang/understanding-mysql-isolation-levels-repeatable-read/ 转载自：Understanding MySQL Isolation Levels: Repeatable-Read","categories":[{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/categories/DBMS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/tags/DBMS/"},{"name":"MySQL","slug":"MySQL","permalink":"https://xnerv.wang/tags/MySQL/"},{"name":"Transaction","slug":"Transaction","permalink":"https://xnerv.wang/tags/Transaction/"},{"name":"Isolation Level","slug":"Isolation-Level","permalink":"https://xnerv.wang/tags/Isolation-Level/"}]},{"title":"SQL TRANSACTION ISOLATION LEVELS EXPLAINED（转载）","slug":"sql-transaction-isolation-levels-explained","date":"2018-01-30T06:36:00.000Z","updated":"2023-08-21T02:24:19.329Z","comments":true,"path":"sql-transaction-isolation-levels-explained/","link":"","permalink":"https://xnerv.wang/sql-transaction-isolation-levels-explained/","excerpt":"This article is part of a series. You do not have to read them in order but I will be referring to topics and explanations in previous articles: Implementing Your Own Transactions with MVCC SQL Transaction Isolation Levels Explained Implementing Repeatable Read and Serializable Transaction Isolation","text":"This article is part of a series. You do not have to read them in order but I will be referring to topics and explanations in previous articles: Implementing Your Own Transactions with MVCC SQL Transaction Isolation Levels Explained Implementing Repeatable Read and Serializable Transaction Isolation The Bad Stuff Before I can explain what’s different about the isolation levels we have to understand what it is that can go wrong with concurrent transactions (more than one client viewing or modifying the data at the same time). Let’s say we have a table users: id name age 1 Joe 20 2 Jill 25 Dirty Reads A dirty read is when the current transaction reads a row written by another uncommitted transaction that’s currently in-flight. For example: Transaction 1 Transaction 2 BEGIN; BEGIN; SELECT age FROM users WHERE id = 1; /* will read 20 */ UPDATE users SET age = 21 WHERE id = 1; /* No commit here */ SELECT age FROM users WHERE id = 1; /* will read 21 */ Basically, if the database is not able to keep track of who is changing the data (by keeping multiple versions of the same row with different visibilities) then rows be read even through they should not yet be visible to that other transaction. Non-repeatable Reads A non-repeatable read occurs when the current transaction reads the same data but this time it is different. It is different because another transaction has been committed during the life of the current transaction: Transaction 1 Transaction 2 BEGIN; BEGIN; SELECT * FROM users WHERE id = 1; UPDATE users SET age = 21 WHERE id = 1; COMMIT; SELECT * FROM users WHERE id = 1; /* will read 21 */ Basically, the database does not maintain what the transaction has already seen so each time the data is read (such as multiple SELECT statements in the same transaction) the same visibility check is done on those rows but some rows may have changed in the mean time. Phantom Reads A phantom read happens when the current transaction re-executes a query returning a set of rows that satisfy a search condition and finds that the set of rows satisfying the condition has changed due to another recently-committed transaction. Transaction 1 Transaction 2 BEGIN; BEGIN; SELECT COUNT(*) FROM users WHERE age BETWEEN 10 AND 30; /* will return 5 */ INSERT INTO users(id,name,age) VALUES ( 3, 'Bob', 27 ); COMMIT; SELECT COUNT(*) FROM users WHERE age BETWEEN 10 AND 30; /* will return 6 */ This can be thought of as a special type of non-repeatable read, but it is important to distinguish it as a different case because the possibility of it occurring depends on the isolation levels explained next. The distinction is that that the original rows are re-read correctly (even if they had been changed) but a new row (or rows) have been inserted into the range it previously selected. So it hasn’t broken the re-read rule, but the data returned is still different. The SQL Standard The SQL Standard defines four isolation levels which are described by what type of errors they are allowed to permit. It does not specify how the implementations themselves work. In most databases MVCC is used. Sometimes with some complex rules to handle higher isolation levels built on top. However, there are other mechanisms; SQLite3 use a separate journal, for example. Read uncommitted permits dirty reads, non repeatable reads and phantom reads. Read committed permits non repeatable reads and phantom reads. Repeatable read permits only phantom reads. Serializable does not permit any read errors. Read Uncommitted “Read uncommitted&quot; is the most loose transaction isolation and is in fact not even implemented in some databases because it is too loose to be useful. A transaction in this mode is susceptible to all types of read errors mentioned above since it is not required to even check if a row is committed or not. If the database itself was not able to keep multiple version of a row but you still wanted the virtual container of a transaction to be able to perform a ROLLBACK this might be your only choice. Read Committed “Read committed&quot; is the most common transaction isolation and usually is the default as it has the best balance between locking and performance. It will never read versions of rows that are currently uncommitted, however it is still susceptible to other transactions changing data between statements. Repeatable Read “Repeatable read&quot; ensures rows that have been read (rather than modified) will still be read, even if they are changed by other transactions. This provides an almost true snapshot of the data however it does incur overhead with locking and/or causing other transactions to be rolled back more commonly. Serializable In a perfect world (at least for a database) we would like only one client to be connected to the database at a any given time. This would guarantee there are no side effects between concurrent clients, since there are none. This is “Serializable&quot;, the most strict mode and works exactly like this - client perform transactions in serial. In practicality a database could not serve just one person at a time so some fancy internal algorithms are used to make sure that clients are not overlapping in anyway. This is tricky but means clients can concurrently work with the database but have the same guarantees as if the transactions were formed one after another. Depending on the database system it may rollback a transaction at the time of, or at the end of the transaction if it thinks the transaction isolation has been compromised. Applications that need this level of isolation should be built carefully so that they can always retry the transaction on failure. Database Vendors Here are some of the common database vendors and what levels of isolation they support. DB2 DB2 supports all four database isolation levels, but has different names for each of the levels: Uncommitted read (UR) -&gt; Read uncommitted. Cursor stability (CS) -&gt; Read committed (default). Read stability (RS) -&gt; Repeatable read. Repeatable read (RR) -&gt; Serializable. Links: IBM Knowledge Center Derby Derby supports all four levels of isolation, with read committed being the default. Links: Isolation levels and concurrency H2 H2 supports only three isolation levels; read uncommitted, read committed and serializable which can only be set at the connection level (not per transaction). The default is read committed. Links: H2 - Advanced MySQL and MariaDB When MySQL and MariaDB are using InnoDB they support all four transaction isolation levels with repeatable read as the default. Even though MySQL and MariaDB allow other pluggable engines, InnoDB is the most common when transactions are required and is usually the default engine on modern versions of either. The original engine, MyISAM does not support transactions but it still used and available. Transaction commands will be ignored rather than result in an error. This is effectively running a database in read uncommitted mode. Links: MySQL :: MySQL 5.7 Reference Manual :: 15.5.2.1 Transaction Isolation Levels SET TRANSACTION - MariaDB Knowledge Base Oracle Oracle only supports read committed (default) and serializable. Links: Ask Tom: On Transaction Isolation Levels PostgreSQL PostgreSQL supports three levels; read committed, repeatable read and serializable. The syntax allows you to choose read read uncommitted but it will function as read committed. Read committed is the default. Links: PostgreSQL: Documentation: 9.1: Transaction Isolation Redshift Redshift is a modified version of PostgreSQL that can only be used on Amazon Web Services (AWS). Redshift only supports serializable. Selecting a different isolation level will have no effect. Links: BEGIN - Amazon Redshift SQL Server Microsoft SQL Server (which is also commonly used as a backend for Microsoft Access instead of the in-built engine) supports all four transaction isolation levels with the default being read committed. SQL Server also has a fifth option called “snapshot&quot; which allows transactions to see a consistent read state. It is unclear if this is an alias of another isolation level or has a specific implementation. Links: SET TRANSACTION ISOLATION LEVEL (Transact-SQL) SQLite3 The default implementation of SQLite3 (since you can compile it from source with lots of different options) only works in Serialized mode. Anyone that has dealt with SQLite3 a lot will understand that this can cause a lot of locking issues but it also the most safe safe considering all the mission-critical systems that SQLite3 is deployed in. Since v3.7.0 write-ahead logging (WAL) has been introduced to make concurrent environments work more like other database vendors. Links: Isolation In SQLite Write-Ahead Logging If your favourite database vendor is not listed, please let me know. Other Resources GitHub - ept/hermitage: What are the differences between the transaction isolation levels in databases? This is a suite of test cases which differentiate isolation levels. 本文地址：http://xnerv.wang/sql-transaction-isolation-levels-explained/ 转载自：SQL TRANSACTION ISOLATION LEVELS EXPLAINED","categories":[{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/categories/DBMS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/tags/DBMS/"},{"name":"SQL Server","slug":"SQL-Server","permalink":"https://xnerv.wang/tags/SQL-Server/"},{"name":"Transaction","slug":"Transaction","permalink":"https://xnerv.wang/tags/Transaction/"},{"name":"Isolation Level","slug":"Isolation-Level","permalink":"https://xnerv.wang/tags/Isolation-Level/"}]},{"title":"IMPLEMENTING REPEATABLE READ AND SERIALIZABLE TRANSACTION ISOLATION（转载）","slug":"implementing-repeatable-read-and-serializable-transaction-isolation","date":"2018-01-30T06:33:00.000Z","updated":"2023-08-21T02:24:18.712Z","comments":true,"path":"implementing-repeatable-read-and-serializable-transaction-isolation/","link":"","permalink":"https://xnerv.wang/implementing-repeatable-read-and-serializable-transaction-isolation/","excerpt":"This article is part of a series. You do not have to read them in order but I will be referring to topics and explanations in previous articles: Implementing Your Own Transactions With MVCC SQL Transaction Isolation Levels Explained Implementing Repeatable Read and Serializable Transaction Isolation","text":"This article is part of a series. You do not have to read them in order but I will be referring to topics and explanations in previous articles: Implementing Your Own Transactions With MVCC SQL Transaction Isolation Levels Explained Implementing Repeatable Read and Serializable Transaction Isolation Repeatable read and serializable are usually higher than most databases use by default (if they are available at all). You can read a full explanation of the differences in the levels in the previous article, SQL Transaction Isolation Levels Explained. This article will focus on a real Python implementation of all four levels but mainly focused on repeatable read and serializable. I’m going to use the code from the original article on MVCC which implemented read-committed and refactor it to allow us to set the transaction isolation. You should understand how MVCC works an how it is implemented for read committed in that article before proceeding. You can view the entire program here. Implementing the Isolation Levels as Classes The majority of the logic of a transaction will remain in the Transaction class. There have been some new methods added: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273class Transaction: def __init__(self, table, xid): self.table = table self.xid = xid self.rollback_actions = [] def add_record(self, id, name): record = &#123; &#x27;id&#x27;: id, &#x27;name&#x27;: name, &#x27;created_xid&#x27;: self.xid, &#x27;expired_xid&#x27;: 0 &#125; self.rollback_actions.append([&quot;delete&quot;, len(self.table.records)]) self.table.records.append(record) def delete_record(self, id): for i, record in enumerate(self.table.records): if self.record_is_visible(record) and record[&#x27;id&#x27;] == id: if self.record_is_locked(record): raise RollbackException(&quot;Row locked by another transaction.&quot;) else: record[&#x27;expired_xid&#x27;] = self.xid self.rollback_actions.append([&quot;add&quot;, i]) def update_record(self, id, name): self.delete_record(id) self.add_record(id, name) def fetch_record(self, id): for record in self.table.records: if self.record_is_visible(record) and record[&#x27;id&#x27;] is id: return record return None def count_records(self, min_id, max_id): count = 0 for record in self.table.records: if self.record_is_visible(record) and \\ min_id &lt;= record[&#x27;id&#x27;] &lt;= max_id: count += 1 return count def fetch_all_records(self): visible_records = [] for record in self.table.records: if self.record_is_visible(record): visible_records.append(record) return visible_records def fetch(self, expr): visible_records = [] for record in self.table.records: if self.record_is_visible(record) and expr(record): visible_records.append(record) return visible_records def commit(self): self.table.active_xids.discard(self.xid) def rollback(self): for action in reversed(self.rollback_actions): if action[0] == &#x27;add&#x27;: self.table.records[action[1]][&#x27;expired_xid&#x27;] = 0 elif action[0] == &#x27;delete&#x27;: self.table.records[action[1]][&#x27;expired_xid&#x27;] = self.xid self.table.active_xids.discard(self.xid) We also introduce a new type of error, the RollbackException. We will use this to indicate that some race condition has occurred, that if allowed to continue would break the requirements of the chosen isolation level. This type of error becomes more common as the isolation level increases and is caught by the DBMS so that the transaction can be safely rolled back and the client notified. It is really just an alias for Exception. However, we want to differentiate this type of error so that other errors are not mistakenly caught: 123class RollbackException(Exception): pass It’s not easy to demonstrate interfaces or abstract classes in Python. Two methods that are missing from the Transaction class above are record_is_locked(record) and record_is_visible(record) and are implemented by the child class. As long as we implement these two methods we can control the isolation behaviour of the transaction. For example the most simple isolation level is read uncommitted: 1234567class ReadUncommittedTransaction(Transaction): def record_is_locked(self, record): return record[&#x27;expired_xid&#x27;] != 0 def record_is_visible(self, record): return record[&#x27;expired_xid&#x27;] == 0 It may seem like an anti-transaction pattern to read data that is not yet confirmed by another transaction. Read uncommitted is useful for large reporting queries where you really don’t want to impose any locking that would affect in-flight or new transactions. In exchange for this your totals may be a little off. Sometimes this is insignificant enough (such as processing millions of rows) that it makes sense to trade accuracy for concurrency. Here is the implementation for read committed (if you have read the previous article this should look very familiar): 123456789101112131415161718192021class ReadCommittedTransaction(Transaction): def record_is_locked(self, record): return record[&#x27;expired_xid&#x27;] != 0 and \\ row[&#x27;expired_xid&#x27;] in self.table.active_xids def record_is_visible(self, record): # The record was created in active transaction that is not our # own. if record[&#x27;created_xid&#x27;] in self.table.active_xids and \\ record[&#x27;created_xid&#x27;] != self.xid: return False # The record is expired or and no transaction holds it that is # our own. if record[&#x27;expired_xid&#x27;] != 0 and \\ (record[&#x27;expired_xid&#x27;] not in self.table.active_xids or \\ record[&#x27;expired_xid&#x27;] == self.xid): return False return True I won’t explain this now as there was already a whole article dedicated to it. Now we can move onto the higher isolation levels that most databases do not use by default because concurrency and performance start to suffer in exchange for greater accuracy and isolations of the transactions: 12345678910111213class RepeatableReadTransaction(ReadCommittedTransaction): def record_is_locked(self, record): return ReadCommittedTransaction.record_is_locked(self, record) or \\ self.table.locks.exists(self, record[&#x27;id&#x27;]) def record_is_visible(self, record): is_visible = ReadCommittedTransaction.record_is_visible(self, record) if is_visible: self.table.locks.add(self, record[&#x27;id&#x27;]) return is_visible To achieve repeatable read we use the same visibility (and locking) checks as read committed (ReadCommittedTransaction), but we add a read lock on every record that is read (and hence, visible) by us. It’s important we don’t add read locks to records that would otherwise not be visible to us, so that another transaction cannot remove it if we need to rescan the data. Here is a very crude lock manager, it simply keeps track of which transactions hold a read lock on any particular row. How this lock manager performs isn’t important, and there many ways to make this mechanism work better and faster. For the purpose of this demonstration just know that there is something that remembers when transactions have seen a row: 123456789101112class LockManager: def __init__(self): self.locks = [] def add(self, transaction, record_id): if not self.exists(transaction, record_id): self.locks.append([transaction, record_id]) def exists(self, transaction, record_id): return any(lock[0] is transaction and lock[1] == record_id \\ for lock in self.locks) Finally, we can implement serializable: 123456789101112131415class SerializableTransaction(RepeatableReadTransaction): def __init__(self, table, xid): Transaction.__init__(self, table, xid) self.existing_xids = self.table.active_xids.copy() def record_is_visible(self, record): is_visible = ReadCommittedTransaction.record_is_visible(self, record) \\ and record[&#x27;created_xid&#x27;] &lt;= self.xid \\ and record[&#x27;created_xid&#x27;] in self.existing_xids if is_visible: self.table.locks.add(self, record[&#x27;id&#x27;]) return is_visible Once again it uses the logic of ReadCommittedTransaction (but not RepeatableReadTransaction as you might originally suspect). Serializable primarily enforces one main new restriction to prevent phantom reads. That is, we must ignore any record that is added or updated in a transaction that was created after the current transaction. Bring On the Tests To really demonstrate the isolation levels we need to create test cases. All of the tests start with the same fixture data: id name 1 Joe 3 Jill A transaction test is implemented as a class. It will setup the fixture data (in the table above) and clients on which the test will be performed. It also runs the test and returns a pretty tick or cross for the outcome (we will use this later). 12345678910111213141516171819202122class TransactionTest: def __init__(self, transaction_type): self.table = Table() client = self.table.new_transaction(ReadCommittedTransaction) client.add_record(id=1, name=&quot;Joe&quot;) client.add_record(id=3, name=&quot;Jill&quot;) client.commit() self.client1 = self.table.new_transaction(transaction_type) self.client2 = self.table.new_transaction(transaction_type) def run_test(self): try: return self.run() except RollbackException: return False def result(self): if self.run_test(): return u&#x27;✔&#x27; return u&#x27;✘&#x27; Individual error scenarios (dirty reads, non-repeatable reads and phantom reads) are implemented by extending the TransactionTest and providing the run() method: 1234567891011121314151617181920212223242526class DirtyRead(TransactionTest): def run(self): result1 = self.client1.fetch_record(id=1) self.client2.update_record(id=1, name=&quot;Joe 2&quot;) result2 = self.client1.fetch_record(id=1) return result1 != result2class NonRepeatableRead(TransactionTest): def run(self): result1 = self.client1.fetch_record(id=1) self.client2.update_record(id=1, name=&quot;Joe 2&quot;) self.client2.commit() result2 = self.client1.fetch_record(id=1) return result1 != result2class PhantomRead(TransactionTest): def run(self): result1 = len(self.client1.fetch(lambda r: 1 &lt;= r[&#x27;id&#x27;] &lt;= 3)) self.client2.add_record(id=2, name=&quot;John&quot;) self.client2.commit() result2 = self.client1.count_records(min_id=1, max_id=3) return result1 != result2 Running the complete program prints a table of results. The ✔ means that it could replicate that kind of error (a little counterintuitive, I know): 123456 Dirty Repeat Phantomread uncommitted ✔ ✔ ✔read committed ✘ ✔ ✔repeatable read ✘ ✘ ✔serializable ✘ ✘ ✘ 本文地址：http://xnerv.wang/implementing-repeatable-read-and-serializable-transaction-isolation/ 转载自：IMPLEMENTING REPEATABLE READ AND SERIALIZABLE TRANSACTION ISOLATION","categories":[{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/categories/DBMS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/tags/DBMS/"},{"name":"SQL Server","slug":"SQL-Server","permalink":"https://xnerv.wang/tags/SQL-Server/"},{"name":"Transaction","slug":"Transaction","permalink":"https://xnerv.wang/tags/Transaction/"},{"name":"Isolation Level","slug":"Isolation-Level","permalink":"https://xnerv.wang/tags/Isolation-Level/"}]},{"title":"IMPLEMENTING YOUR OWN TRANSACTIONS WITH MVCC（转载）","slug":"implementing-your-own-transactions-with-mvcc","date":"2018-01-30T06:21:00.000Z","updated":"2023-08-21T02:24:18.737Z","comments":true,"path":"implementing-your-own-transactions-with-mvcc/","link":"","permalink":"https://xnerv.wang/implementing-your-own-transactions-with-mvcc/","excerpt":"This article is part of a series. You do not have to read them in order but I will be referring to topics and explanations in previous articles: Implementing Your Own Transactions With MVCC SQL Transaction Isolation Levels Explained Implementing Repeatable Read and Serializable Transaction Isolation","text":"This article is part of a series. You do not have to read them in order but I will be referring to topics and explanations in previous articles: Implementing Your Own Transactions With MVCC SQL Transaction Isolation Levels Explained Implementing Repeatable Read and Serializable Transaction Isolation MVCC (MultiVersion Concurrency Control) is a simple and effective way to implement transactional isolation with any application managing a group of things. We usually think of these things as database records by they could be anything really. MVCC is one of the most widely implemented concurrency control algorithms because reads do not block other reads and writes do not block other reads or writes. This means that we can safely and concurrently have lots of clients reading and writing simultaneously without blocking each other. With some notable specific cases that I will explain later. What is Isolation? Isolation (the I from ACID) makes sure that when a client begins a transaction that the data it sees will always be the same. Event if other clients are modifying the data. If another client (or clients); Add a record: It will not be visible to you. Delete a record: It will remain visible to you. Modify a record: You will forever see the version before it was changed. However, inside your transaction those three things do to take affect. So if you add a record then it will be visible to you. Often overlooked, transactions applied to the correct applications can; Alleviate blocking if you previously relied on simply locking the whole database until your modifications where finished. Prevent read issues from arising if your application is affected by reading the same data twice and getting two different answers unexpectedly. Provide durability. At any time you can throw away the changes if you change your mind without explicitly undoing all the changes yourself. Terminology A record is a single entity. This would be best explained as a database record. It could also be a file, a JSON object, anything that encapsulates something. Most importantly here is that a record cannot be simultaneously modified by two separate clients. If you have a complex data structure you will want to make sure what you define as a record does not encapsulate too much. A collection is the set of records. This would be like a table in a database. The important thing here is that a transaction is not isolated to a single collection as long as each of the records share the same transaction IDs. A transaction ID (also called an XID) is the unique number for the transaction. All records that have been modified under the same transaction can be saved or rolled back as one atomic operation, which is ultimately what we want. Transaction IDs There are three categories of transaction ID numbering systems we can use; An incrementing number. Before a transaction starts (before any changes occur is important, even if there are no changes). This is the transaction ID for all the record changes. It doesn’t matter if the changes are saved or thrown away the same transaction ID can never be used again so it must be atomic. Also important is the value must be kept when the script/server restarts to prevent transaction IDs from being reused. A timestamp. We do not need to maintain an atomic counter. This has the added benefit of allowing us to restore the collections to some point in time. There is one caveat; if the timestamp does not have a high enough resolution (lets say your using whole seconds) transactions that start very close to each other would potentially share the same transaction ID and horrible things will happen… Custom transaction ID implementations. Special cases where you are working with distributed databases or have other requirements that are not satisfied by the two types above. This may include UUID based algorithms and may not even be strictly a number. Items 2 and 3 each deserve their own blog post so we won’t walk about it here. Just the mechanism of the most simple implementation of using an incrementing counter. It’s Time to Dive In I’m going to provide the code examples in Python. The full example program is available in this gist. But I will explain each piece below. In a nutshell what we need is the next transaction ID and an array (or set) of the transactions currently active to produce new transactions: 123456789101112131415next_xid = 1active_xids = set()records = []def new_transaction(): global next_xid next_xid += 1 active_xids.add(next_xid) return Transaction(next_xid)class Transaction: def __init__(self, xid): self.xid = xid self.rollback_actions = [] In your application you may decide that clients (if you have any) can control when transactions are opened or closed. It doesn’t affect how the transactions work and they are independent of this. Next we need to add two discreet pieces of information attached to each record. Called the created XID and expired XID. It’s best to store these directly with the record itself. However, that is not always possible so you can maintain them somewhere external as long as you are the only one modifying the data to maintain consistency. Row Visibility and Locking Ultimately this is what MVCC comes down to: The visibility of a row depending on who is looking at it. Ever row has to be tested if it is visible to the client looking at it: 1234567891011121314151617#class Transaction: def record_is_visible(self, record): # The record was created in active transaction that is not our # own. if record[&#x27;created_xid&#x27;] in active_xids and \\ record[&#x27;created_xid&#x27;] != self.xid: return False # The record is expired or and no transaction holds it that is # our own. if record[&#x27;expired_xid&#x27;] != 0 and \\ (record[&#x27;expired_xid&#x27;] not in active_xids or \\ record[&#x27;expired_xid&#x27;] == self.xid): return False return True (xnerv: created_id is the transaction ID which created this row. expired_id is the transaction ID which deleted this row.) Furthermore, we discussed before that simultaneous transactions cannot make a modification to the same record. If this happens there are two ways to handle this: Abort (rollback) the transaction that tried to make the most recent changes and propagate the error back to the original client. Wait (block) the second transaction until that record becomes available. This has some special challenges with performance and potential reread errors. The safest and easiest one is the first choice - so that’s what we’re going to use in this tutorial. When we do need to modify a record we need to check if it is locked by another transaction with this: 12345#class Transaction: def row_is_locked(self, record): return record[&#x27;expired_xid&#x27;] != 0 and \\ record[&#x27;expired_xid&#x27;] in active_xids Adding a Record This is an easy one. We set the created_xid to the current transaction ID and expired_xid to 0 : 1234567#class Transaction: def add_record(self, record): record[&#x27;created_xid&#x27;] = self.xid record[&#x27;expired_xid&#x27;] = 0 self.rollback_actions.append([&quot;delete&quot;, len(records)]) records.append(record) The rollback_actions will be explained later. Deleting a Record There are two possibilities: The expired_xid is 0 meaning the record has never been deleted by anyone. So by setting expired_xid to the current transaction ID we are marking it as deleted. The expired_xid it not 0 and expired_xid is an active transaction. The record has been deleted by another active transaction and so we cannot touch it. The third scenario where expired_xid is not an active transaction is not possible due to the normal visibility constraints. 12345678910#class Transaction: def delete_record(self, id): for i, record in enumerate(records): if self.record_is_visible(record) and record[&#x27;id&#x27;] == id: if self.row_is_locked(record): raise Error(&quot;Row locked by another transaction.&quot;) else: record[&#x27;expired_xid&#x27;] = self.xid self.rollback_actions.append([&quot;add&quot;, i]) The rollback_actions will be explained later. Updating a Record Updating is a combination of deleting the old one and adding a new one. This allows the existing record to still be viewed by other transactions. If the delete_record fails then the exception raised would cause the subsequent add_record not to happen which is what we want. (xnerv: Absolutely this implementation is different from popular DBMS like MySQL.) 12345#class Transaction: def update_record(self, id, name): self.delete_record(id) self.add_record(&#123;&quot;id&quot;: id, &quot;name&quot;: name&#125;) Committing Changes Once all the modifications have been made we need to commit all the changes so that future clients can see these new changes. Very easy, we simply remove our transaction ID from the active list of transactions: 1234#class Transaction: def commit(self): active_xids.discard(self.xid) Rollback Changes Rolling back can be done several ways, one way is to replay the changes in reverse: 12345678910#class Transaction: def rollback(self): for action in reversed(self.rollback_actions): if action[0] == &#x27;add&#x27;: records[action[1]][&#x27;expired_xid&#x27;] = 0 elif action[0] == &#x27;delete&#x27;: records[action[1]][&#x27;expired_xid&#x27;] = self.xid active_xids.discard(self.xid) Note: This is fine for an environment where the server can guarantee that the all rollback actions will be replayed and that when the server is shut down forcefully that all the active transactions will have rollback() invoked on them. If you want higher durability that can recover from a server randomly crashing you will need to keep the transaction ID stored somewhere atomically and have extra code on the server start up that checks to see if it was shutdown safely, and manually repairs the records if need be. I will not go into this as this article is already long enough, but it may be explained in a future article. Vacuuming or Reclaiming Space You have probably noticed with this algorithm is that it does not ever actually delete data, only mark it as deleted. This makes it fantastic for keeping lots of records on disk by only appending to the file for modifications. Overtime you will likely want to gain back all that dead space. If it’s in a memory database you could simply iterate through the records and permanently delete the records that are now fully dead. If your using a medium like the disk this isn’t so easy. If your application isn’t too complicated you may be able to rewrite all of the non-dead rows out to a new file and switch it underneath your server. There are tons of solutions for this that aren’t specific to how MVCC works so I’ll leave that up to you. In any case we need to be sensitive to row visibility here as well. Records that have an expired_xid that is not 0 and it does not appear in the active transactions are fully dead rows. 本文地址：http://xnerv.wang/implementing-your-own-transactions-with-mvcc/ 转载自：IMPLEMENTING YOUR OWN TRANSACTIONS WITH MVCC","categories":[{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/categories/DBMS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/tags/DBMS/"},{"name":"MVCC","slug":"MVCC","permalink":"https://xnerv.wang/tags/MVCC/"},{"name":"SQL Server","slug":"SQL-Server","permalink":"https://xnerv.wang/tags/SQL-Server/"},{"name":"Transaction","slug":"Transaction","permalink":"https://xnerv.wang/tags/Transaction/"}]},{"title":"SPV、SPV节点和SPV钱包 (初稿）（转载）","slug":"spv-spv-nodes-spv-purses","date":"2018-01-20T23:18:00.000Z","updated":"2023-08-21T02:24:20.797Z","comments":true,"path":"spv-spv-nodes-spv-purses/","link":"","permalink":"https://xnerv.wang/spv-spv-nodes-spv-purses/","excerpt":"一、什么是SPV SPV是 “Simplified Payment Verification”（简单支付验证） 的缩写。中本聪论文简要地提及了这一概念，指出：不运行完全节点也可验证支付，用户只需要保存所有的block header就可以了。用户虽然不能自己验证交易，但如果能够从区块链的某处找到相符的交易，他就可以知道网络已经认可了这笔交易，而且得到了网络的多少个确认。 按照中本聪的原文，这里有个细节需要注意，SPV指的是“支付验证“，而不是“交易验证”。这两种验证有很大区别。 &quot;交易验证”非常复杂，涉及到验证是否有足够余额可供支出、是否存在双花、脚本能否通过等等，通常由运行完全节点的矿工来完成。 “支付验证”则比较简单，只判断用于“支付”的那笔交易是否已经被验证过，并得到了多少的算力保护（多少确认数）。 考虑这样一种情况，A收到来自B的一个通知，B声称他已经从某某账户中汇款一定数额的钱给了A。去中心方式下，没有任何人能证明B的可靠。接到这一通知，A如何能判断B所说的是真的呢？ 在比特币系统中，这一通知是以一个固定格式的“交易&quot;来实现的，该交易中包含B的汇款账户、B的签名、汇给A的金额以及A的地址。 如果A想本人亲自验证这笔交易，首先，A要遍历区块链账本，定位到B的账户上，这样才能查看B所给的账户上是否曾经有足够的金额；接下来，A要遍历后续的所有账本，看B是否已经支出了这个账户上的钱给别人(是否存在双花欺骗）；然后还要验证脚本来判断B是否拥有该账户的支配权。这一过程要求A必须得到完整的区块链才行。 但是，如果A只想知道这笔支付是否已经得到了验证（如果验证了就发货），他可以依赖比特币系统来快速验证。即，检查发生此项支付的那笔交易是否已经收录于区块链中，并得到了多少个确认。","text":"一、什么是SPV SPV是 “Simplified Payment Verification”（简单支付验证） 的缩写。中本聪论文简要地提及了这一概念，指出：不运行完全节点也可验证支付，用户只需要保存所有的block header就可以了。用户虽然不能自己验证交易，但如果能够从区块链的某处找到相符的交易，他就可以知道网络已经认可了这笔交易，而且得到了网络的多少个确认。 按照中本聪的原文，这里有个细节需要注意，SPV指的是“支付验证“，而不是“交易验证”。这两种验证有很大区别。 &quot;交易验证”非常复杂，涉及到验证是否有足够余额可供支出、是否存在双花、脚本能否通过等等，通常由运行完全节点的矿工来完成。 “支付验证”则比较简单，只判断用于“支付”的那笔交易是否已经被验证过，并得到了多少的算力保护（多少确认数）。 考虑这样一种情况，A收到来自B的一个通知，B声称他已经从某某账户中汇款一定数额的钱给了A。去中心方式下，没有任何人能证明B的可靠。接到这一通知，A如何能判断B所说的是真的呢？ 在比特币系统中，这一通知是以一个固定格式的“交易&quot;来实现的，该交易中包含B的汇款账户、B的签名、汇给A的金额以及A的地址。 如果A想本人亲自验证这笔交易，首先，A要遍历区块链账本，定位到B的账户上，这样才能查看B所给的账户上是否曾经有足够的金额；接下来，A要遍历后续的所有账本，看B是否已经支出了这个账户上的钱给别人(是否存在双花欺骗）；然后还要验证脚本来判断B是否拥有该账户的支配权。这一过程要求A必须得到完整的区块链才行。 但是，如果A只想知道这笔支付是否已经得到了验证（如果验证了就发货），他可以依赖比特币系统来快速验证。即，检查发生此项支付的那笔交易是否已经收录于区块链中，并得到了多少个确认。 原理：block header中有三个关键字段，一是prev_block_hash(前一区块的hash值，确保了区块链所记录的交易次序）；二是bits（当前区块的计算难度）, 三是merkle_root_hash（借助merkle tree算法，确保收录与区块中所有交易的真实性）。 验证某个交易是否真实存在时，理论上，用户可以通过以下方式进行验证： 0. 从网络上获取并保存最长链的所有block header至本地； 计算该交易的hash值tx_hash； 定位到包含该tx_hash所在的区块，验证block header是否包含在已知的最长链中； 从区块中获取构建merkle tree所需的hash值； 根据这些hash值计算merkle_root_hash； 若计算结果与block header中的merkle_root_hash相等，则交易真实存在。 根据该block header所处的位置，确定该交易已经得到多少个确认。 优点：极大地节省存储空间。减轻终端用户的负担。无论未来的交易量有多大，block header的大小始终不变，只有80字节。按照每小时6个的出块速度，每年产出52560个区块。当只保存block header时，每年新增的存储需求约为4兆字节，100年后累计的存储需求仅为400兆，即使用户使用的是最低端的设备，正常情况下也完全能够负载。 问题：如何才能通过tx_hash定位到该交易所在的区块? 以往的比特币协议中缺少对此的支持。 二、比特币钱包 在进一步讨论SPV的实现之前，先要说明一下比特币的钱存放的是什么，钱包和私钥之间是什么关系？ 比特币钱包 既然用到“钱包”一词，那么应该与我们日常生活中使用的钱包有一定的相似之处。为了更直观说明，我们与日常生活中所使用的钱包做一下对比。 日常生活中里面存放的可能是纸币、支票、印鉴等等(为了简化说明，我们把银行卡排除在外，使用银行卡涉及到很多中间环节，增加表述上的复杂度)。 用纸币购物时， 从钱包中凑足若干张不同面值的纸币，计算总面值是否大于所需金额以及应找回多少零钱； 将这些纸币直接交给卖方； 卖方验证这些纸币的真伪； 卖方计算这些纸币的面值是否大于或等于商品价格，并找回相应的零钱。 将收到的零钱放回钱包。 比特币的钱包里存放的相当于是一张张标有面值的“一次性支票”和对应的“印鉴”。支付时， 用户从钱包中取出若干张“一次性支票”，自己计算总面值是否大于所需金额以及应找回多少零钱，注意要扣除比特币系统所收取的手续费; 给卖方开一张支票，注明卖方地址和支付金额；如果需要找零，给自己开一张找零支票（写上自己的地址和找零金额）； 在每张从钱包中取出的支票上加盖对应的印鉴，确认支付权； 将这些票据提交给比特币系统，比特币系统验证支票的真伪和支付是否有效。 若比特币系统验证通过，收款方将收到的支票放入钱包。用户则将自己钱包中的已支付的支票丢弃（这些支票已经被比特币系统视为无效了，无法继续使用）， 即使是刚接触比特币的人，估计也能猜出“印鉴”指的是“私钥”。但“一次性支票”是什么？ 比特币系统中，这种“一次性支票”的术语是UTXO，全称是Unspent Transaction Outputs(未花费的交易输出）。区块链是一个收录所有历史交易（Transaction)的总帐，每个区块（block)中包含若干笔交易记录。 每个交易记录由两部分构成：资金来源（可以有多个来源）和资金去向（可以有多个去向），术语为Tx_in(交易输入）和Tx_out（交易输出）。也就是说，每笔交易TX包含有若干个Tx_in和若干个Tx_out。 除创世区块中的交易（genesis block）外，每笔交易必须要有资金来源。资金来源有两种，一种是挖矿奖励（依照固定算法实现的货币发行），出现在每个block的第一笔交易中；另一种是先前的交易中未曾使用的某个Tx_out(交易输出），即UTXO。支出方要出示证据来证明自己对该Tx_out拥有所有权，而比特币系统则要验证该Tx_out是否真的未被花费（是否是UTXO）以及支出方是否有权将其花费。 资金去向（TX_out)包含两个部分，一是传递的金额，二是支配权(谁可以动用）。取款权通过比特币的脚本系统来实现。若收款方地址是以1开头的普通地址，则脚本中会包含地址所对应公钥的hash值（hash160)，动用款项时一般需要用对应的私钥进行签名；若收款方地址是以3开头的多重签名地址，则脚本中会包含某个特定脚本的hash值（hash160)，动用款项时，一般需要依照特定的脚本，用多个私钥来签名。 用户钱包中的比特币实际上是用户拥有支配权的、且尚未花费的Tx_out中记录的金额总和，即用户可支配的所有UTXO金额之和。 完整的钱包中应存有若干个UTXO和支配每个UTXO时所对应的私钥。当然，有时从安全角度出发，可能会把钱包划分为两个部分，在线钱包中只有UTXO，而离线钱包只存私钥。 但是，用户怎么才能把自己的所有UTXO都放到钱包中呢？ 三、用户如何收录自己的UTXO （一）去中心化方式： 实现方法： 在本地建立一个用于存储UTXO的数据库； 设置区块扫描起始点（区块链上的扫描起始高度)，从该点开始，依次下载该点之后所有区块（block)的完整数据。 解析每个block的所有TX数据，依次读取每个Tx_in的prev_Tx_out(［tx hash］ + ［tx_out的序号］），检索UTXO数据库中是否存在这个Tx_out，如果有，则从UTXO数据库中删除（或标记删除）。 依次解析每个Tx_out的脚本，若与用户相关，则将[tx hash] + ［Tx_out的序号］以及整个tx_out的内容记录到UTXO数据库； 备注：如果钱包中只有新创建的私钥，可以从最新的区块开始扫描（由于私钥发生碰撞的可能性可以视为0，在你告知他人比特币地址之前，该私钥对应的地址上不会有任何收入） 优点：不依赖于信任；数据准确。 缺点：速度慢，需要从比特币网络下载大量数据，对网络造成的压力大。 （二）中心化方式： 某个中心化机构（或个人）运行完整的比特币节点，建立一个收录所有UTXO的数据库。 用户用中心化机构提供的api来请求与自己有关的UTXO数据。 优点：速度快，不拖累比特币网络； 缺点：依赖于信任；数据不一定准确（有可能中心化服务器出现故障，或是与中心服务器的会话被劫持，数据遭篡改） 四、瘦客户端、SPV轻钱包和SPV节点是什么？ 瘦客户端：参考了SPV的机制，在监听收款地址时，客户端在本地只需保存与用户可支配交易相关的数据。因为本地没有完整的区块链，缺少发送方的相关数据，客户端无法亲自验证交易是否合法，只能判断交易是否是被收录，并且得到了几个确认。这与SPV有很多相似之处，因而很多场合下这种瘦客户端也常被成为是“SPV客户端”，不过，与SPV的区别是，在去中心化方式下，这些客户端仍需下载每个新区块的全部数据并进行解析，只是无需在本地保存全部数据而已。 “轻钱包” 是用瘦客户端模式实现的钱包，因为不存储完整区块链，就涉及到如何获取UTXO的问题。不同的开发者可能有各自的实现方法，但从效率上考虑，往往多用中心化的方式来实现。 SPV节点：支持使用布隆过滤器（Bloom filter)在快速检索并返回相关数据的节点。这样的节点可以为去中心化方式SPV查询提供必要的支持。 SPV在实现上涉及到一个问题，如何才能通过tx_hash来定位到该支付交易所在的区块?用中心化方式来实现很好解决，但用去中心化就不那么简单了，因为以往的比特币系统协议中缺少对SPV的支持。原有协议中，可以通过getheaders命令来获取block headers，可以通过getdata命令支持获取指定的block, 但不支持通过tx_hash反向查找所在的block。为了定位block，客户端往往不得不下载整个区块链。Bloom filter解决了客户端检索的问题，原理是Bloom filter可以快速判断出某检索值一定不存在于某个指定的集合，从而可以过滤掉大量无关数据，减少客户端不必要的下载量。 前文提到，SPV的用途是验证某个支付是否确实存在，并得到多少个确认。而钱包的用途则是用于管理自己的资产以及进行支付。简言之，SPV的应用场合往往是为发货做准备（知道钱到帐了），“轻钱包”的应用场合往往是数钱或花钱。虽然“轻钱包”中部分借鉴了SPV的机制，但和SPV是完全不用的应用方向，直接把这两个词连起略显牵强。这种钱包要么采用中心化的方式——提高了效率，但引入了信任的风险；要么采用去中心化方式——无需信任，但效率低，且增加网络的负担。 SPV节点的出现使以去中心化方式来实现高效、低负荷的“轻钱包”成为了可能。笔者认为将基于SPV节点来实现的&quot;轻钱包&quot;简称为“SPV轻钱包”可能会更为合适些。 本文地址：http://xnerv.wang/spv-spv-nodes-spv-purses/ 转载自：SPV、SPV节点和SPV钱包 (初稿）","categories":[{"name":"区块链与比特币","slug":"区块链与比特币","permalink":"https://xnerv.wang/categories/%E5%8C%BA%E5%9D%97%E9%93%BE%E4%B8%8E%E6%AF%94%E7%89%B9%E5%B8%81/"}],"tags":[{"name":"区块链与比特币","slug":"区块链与比特币","permalink":"https://xnerv.wang/tags/%E5%8C%BA%E5%9D%97%E9%93%BE%E4%B8%8E%E6%AF%94%E7%89%B9%E5%B8%81/"},{"name":"区块链","slug":"区块链","permalink":"https://xnerv.wang/tags/%E5%8C%BA%E5%9D%97%E9%93%BE/"},{"name":"比特币","slug":"比特币","permalink":"https://xnerv.wang/tags/%E6%AF%94%E7%89%B9%E5%B8%81/"},{"name":"SPV","slug":"SPV","permalink":"https://xnerv.wang/tags/SPV/"}]},{"title":"比特币白皮书：一种点对点的电子现金系统（转载）","slug":"bitcoin-a-peer-to-peer-electronic-cash-system-cn","date":"2018-01-20T23:01:00.000Z","updated":"2023-08-21T02:24:20.825Z","comments":true,"path":"bitcoin-a-peer-to-peer-electronic-cash-system-cn/","link":"","permalink":"https://xnerv.wang/bitcoin-a-peer-to-peer-electronic-cash-system-cn/","excerpt":"原文作者：中本聪（Satoshi Nakamoto） 作者邮箱：Satoshin@gmx.com 执行翻译：8btc.com 巴比特 QQagent [摘要]：本文提出了一种完全通过点对点技术实现的电子现金系统，它使得在线支付能够直接由一方发起并支付给另外一方，中间不需要通过任何的金融机构。虽然数字签名（Digital signatures）部分解决了这个问题，但是如果仍然需要第三方的支持才能防止双重支付（double-spending）的话，那么这种系统也就失去了存在的价值。我们(we)在此提出一种解决方案，使现金系统在点对点的环境下运行，并防止双重支付问题。该网络通过随机散列（hashing）对全部交易加上时间戳（timestamps），将它们合并入一个不断延伸的基于随机散列的工作量证明（proof-of-work）的链条作为交易记录，除非重新完成全部的工作量证明，形成的交易记录将不可更改。最长的链条不仅将作为被观察到的事件序列（sequence）的证明，而且被看做是来自CPU计算能力最大的池（pool）。只要大多数的CPU计算能力都没有打算合作起来对全网进行攻击，那么诚实的节点将会生成最长的、超过攻击者的链条。这个系统本身需要的基础设施非常少。信息尽最大努力在全网传播即可，节点(nodes)可以随时离开和重新加入网络，并将最长的工作量证明链条作为在该节点离线期间发生的交易的证明。","text":"原文作者：中本聪（Satoshi Nakamoto） 作者邮箱：Satoshin@gmx.com 执行翻译：8btc.com 巴比特 QQagent [摘要]：本文提出了一种完全通过点对点技术实现的电子现金系统，它使得在线支付能够直接由一方发起并支付给另外一方，中间不需要通过任何的金融机构。虽然数字签名（Digital signatures）部分解决了这个问题，但是如果仍然需要第三方的支持才能防止双重支付（double-spending）的话，那么这种系统也就失去了存在的价值。我们(we)在此提出一种解决方案，使现金系统在点对点的环境下运行，并防止双重支付问题。该网络通过随机散列（hashing）对全部交易加上时间戳（timestamps），将它们合并入一个不断延伸的基于随机散列的工作量证明（proof-of-work）的链条作为交易记录，除非重新完成全部的工作量证明，形成的交易记录将不可更改。最长的链条不仅将作为被观察到的事件序列（sequence）的证明，而且被看做是来自CPU计算能力最大的池（pool）。只要大多数的CPU计算能力都没有打算合作起来对全网进行攻击，那么诚实的节点将会生成最长的、超过攻击者的链条。这个系统本身需要的基础设施非常少。信息尽最大努力在全网传播即可，节点(nodes)可以随时离开和重新加入网络，并将最长的工作量证明链条作为在该节点离线期间发生的交易的证明。 1. 简介 互联网上的贸易，几乎都需要借助金融机构作为可资信赖的第三方来处理电子支付信息。虽然这类系统在绝大多数情况下都运作良好，但是这类系统仍然内生性地受制于“基于信用的模式”(trust based model)的弱点。我们无法实现完全不可逆的交易，因为金融机构总是不可避免地会出面协调争端。而金融中介的存在，也会增加交易的成本，并且限制了实际可行的最小交易规模，也限制了日常的小额支付交易。并且潜在的损失还在于，很多商品和服务本身是无法退货的，如果缺乏不可逆的支付手段，互联网的贸易就大大受限。因为有潜在的退款的可能，就需要交易双方拥有信任。而商家也必须提防自己的客户，因此会向客户索取完全不必要的个人信息。而实际的商业行为中，一定比例的欺诈性客户也被认为是不可避免的，相关损失视作销售费用处理。而在使用物理现金的情况下，这些销售费用和支付问题上的不确定性却是可以避免的，因为此时没有第三方信用中介的存在。 所以，我们非常需要这样一种电子支付系统，它基于密码学原理而不基于信用，使得任何达成一致的双方，能够直接进行支付，从而不需要第三方中介的参与。杜绝回滚(reverse)支付交易的可能，这就可以保护特定的卖家免于欺诈；而对于想要保护买家的人来说，在此环境下设立通常的第三方担保机制也可谓轻松加愉快。在这篇论文中，我们(we)将提出一种通过点对点分布式的时间戳服务器来生成依照时间前后排列并加以记录的电子交易证明，从而解决双重支付问题。只要诚实的节点所控制的计算能力的总和，大于有合作关系的(cooperating)攻击者的计算能力的总和，该系统就是安全的。 2. 交易(Transactions) 我们定义，一枚电子货币（an electronic coin）是这样的一串数字签名：每一位所有者通过对前一次交易和下一位拥有者的公钥(Public key) 签署一个随机散列的数字签名，并将这个签名附加在这枚电子货币的末尾，电子货币就发送给了下一位所有者。而收款人通过对签名进行检验，就能够验证该链条的所有者。 该过程的问题在于，收款人将难以检验，之前的某位所有者，是否对这枚电子货币进行了双重支付。通常的解决方案，就是引入信得过的第三方权威，或者类似于造币厂(mint)的机构，来对每一笔交易进行检验，以防止双重支付。在每一笔交易结束后，这枚电子货币就要被造币厂回收，而造币厂将发行一枚新的电子货币；而只有造币厂直接发行的电子货币，才算作有效，这样就能够防止双重支付。可是该解决方案的问题在于，整个货币系统的命运完全依赖于运作造币厂的公司，因为每一笔交易都要经过该造币厂的确认，而该造币厂就好比是一家银行。 我们需要收款人有某种方法，能够确保之前的所有者没有对更早发生的交易实施签名。从逻辑上看，为了达到目的，实际上我们需要关注的只是于本交易之前发生的交易，而不需要关注这笔交易发生之后是否会有双重支付的尝试。为了确保某一次交易是不存在的，那么唯一的方法就是获悉之前发生过的所有交易。在造币厂模型里面，造币厂获悉所有的交易，并且决定了交易完成的先后顺序。如果想要在电子系统中排除第三方中介机构，那么交易信息就应当被公开宣布（publicly announced）[1] ，我们需要整个系统内的所有参与者，都有唯一公认的历史交易序列。收款人需要确保在交易期间绝大多数的节点都认同该交易是首次出现。 3. 时间戳服务器(Timestamp server) 本解决方案首先提出一个“时间戳服务器”。时间戳服务器通过对以区块(block)形式存在的一组数据实施随机散列而加上时间戳，并将该随机散列进行广播，就像在新闻或世界性新闻组网络（Usenet）的发帖一样[2][3][4][5] 。显然，该时间戳能够证实特定数据必然于某特定时间是的确存在的，因为只有在该时刻存在了才能获取相应的随机散列值。每个时间戳应当将前一个时间戳纳入其随机散列值中，每一个随后的时间戳都对之前的一个时间戳进行增强(reinforcing)，这样就形成了一个链条（Chain）。 4. 工作量证明（Proof-of-Work） 为了在点对点的基础上构建一组分散化的时间戳服务器，仅仅像报纸或世界性新闻网络组一样工作是不够的，我们还需要一个类似于亚当•柏克（Adam Back）提出的哈希现金（Hashcash）[6] 。在进行随机散列运算时，工作量证明机制引入了对某一个特定值的扫描工作，比方说SHA-256下，随机散列值以一个或多个0开始。那么随着0的数目的上升, 找到这个解所需要的工作量将呈指数增长，而对结果进行检验则仅需要一次随机散列运算。 我们在区块中补增一个随机数(Nonce)，这个随机数要使得该给定区块的随机散列值出现了所需的那么多个0。我们通过反复尝试来找到这个随机数，直到找到为止，这样我们就构建了一个工作量证明机制。只要该CPU耗费的工作量能够满足该工作量证明机制，那么除非重新完成相当的工作量，该区块的信息就不可更改。由于之后的区块是链接在该区块之后的，所以想要更改该区块中的信息，就还需要重新完成之后所有区块的全部工作量。 同时，该工作量证明机制还解决了在集体投票表决时，谁是大多数的问题。如果决定大多数的方式是基于IP地址的，一IP地址一票，那么如果有人拥有分配大量IP地址的权力，则该机制就被破坏了。而工作量证明机制的本质则是一CPU一票。“大多数”的决定表达为最长的链，因为最长的链包含了最大的工作量。如果大多数的CPU为诚实的节点控制，那么诚实的链条将以最快的速度延长，并超越其他的竞争链条。如果想要对业已出现的区块进行修改，攻击者必须重新完成该区块的工作量外加该区块之后所有区块的工作量，并最终赶上和超越诚实节点的工作量。我们将在后文证明，设想一个较慢的攻击者试图赶上随后的区块，那么其成功概率将呈指数化递减。 另一个问题是，硬件的运算速度在高速增长，而节点参与网络的程度则会有所起伏。为了解决这个问题，工作量证明的难度(the proof-of-work difficulty)将采用移动平均目标的方法来确定，即令难度指向令每小时生成区块的速度为某一个预定的平均数。如果区块生成的速度过快，那么难度就会提高。 5. 网络 运行该网络的步骤如下： 新的交易向全网进行广播； 每一个节点都将收到的交易信息纳入一个区块中； 每个节点都尝试在自己的区块中找到一个具有足够难度的工作量证明； 当一个节点找到了一个工作量证明，它就向全网进行广播； 当且仅当包含在该区块中的所有交易都是有效的且之前未存在过的，其他节点才认同该区块的有效性； 其他节点表示他们接受该区块，而表示接受的方法，则是在跟随该区块的末尾，制造新的区块以延长该链条，而将被接受区块的随机散列值视为先于新区快的随机散列值。 节点始终都将最长的链条视为正确的链条，并持续工作和延长它。如果有两个节点同时广播不同版本的新区块，那么其他节点在接收到该区块的时间上将存在先后差别。当此情形，他们将在率先收到的区块基础上进行工作，但也会保留另外一个链条，以防后者变成最长的链条。该僵局（tie）的打破要等到下一个工作量证明被发现，而其中的一条链条被证实为是较长的一条，那么在另一条分支链条上工作的节点将转换阵营，开始在较长的链条上工作。 所谓“新的交易要广播”，实际上不需要抵达全部的节点。只要交易信息能够抵达足够多的节点，那么他们将很快被整合进一个区块中。而区块的广播对被丢弃的信息是具有容错能力的。如果一个节点没有收到某特定区块，那么该节点将会发现自己缺失了某个区块，也就可以提出自己下载该区块的请求。 6. 激励 我们约定如此：每个区块的第一笔交易进行特殊化处理，该交易产生一枚由该区块创造者拥有的新的电子货币。这样就增加了节点支持该网络的激励，并在没有中央集权机构发行货币的情况下，提供了一种将电子货币分配到流通领域的一种方法。这种将一定数量新货币持续增添到货币系统中的方法，非常类似于耗费资源去挖掘金矿并将黄金注入到流通领域。此时，CPU的时间和电力消耗就是消耗的资源。 另外一个激励的来源则是交易费（transaction fees）。如果某笔交易的输出值小于输入值，那么差额就是交易费，该交易费将被增加到该区块的激励中。只要既定数量的电子货币已经进入流通，那么激励机制就可以逐渐转换为完全依靠交易费，那么本货币系统就能够免于通货膨胀。 激励系统也有助于鼓励节点保持诚实。如果有一个贪婪的攻击者能够调集比所有诚实节点加起来还要多的CPU计算力，那么他就面临一个选择：要么将其用于诚实工作产生新的电子货币，或者将其用于进行二次支付攻击。那么他就会发现，按照规则行事、诚实工作是更有利可图的。因为该等规则使得他能够拥有更多的电子货币，而不是破坏这个系统使得其自身财富的有效性受损。 7. 回收硬盘空间 如果最近的交易已经被纳入了足够多的区块之中，那么就可以丢弃该交易之前的数据，以回收硬盘空间。为了同时确保不损害区块的随机散列值，交易信息被随机散列时，被构建成一种Merkle树（Merkle tree）[7] 的形态，使得只有根(root)被纳入了区块的随机散列值。通过将该树（tree）的分支拔除（stubbing）的方法，老区块就能被压缩。而内部的随机散列值是不必保存的。 不含交易信息的区块头（Block header）大小仅有80字节。如果我们设定区块生成的速率为每10分钟一个，那么每一年产生的数据位4.2MB。（80 bytes * 6 * 24 * 365 = 4.2MB）。2008年，PC系统通常的内存容量为2GB，按照摩尔定律的预言，即使将全部的区块头存储于内存之中都不是问题。 8. 简化的支付确认（Simplified Payment Verification） 在不运行完整网络节点的情况下，也能够对支付进行检验。一个用户需要保留最长的工作量证明链条的区块头的拷贝，它可以不断向网络发起询问，直到它确信自己拥有最长的链条，并能够通过merkle的分支通向它被加上时间戳并纳入区块的那次交易。节点想要自行检验该交易的有效性原本是不可能的，但通过追溯到链条的某个位置，它就能看到某个节点曾经接受过它，并且于其后追加的区块也进一步证明全网曾经接受了它。 当此情形，只要诚实的节点控制了网络，检验机制就是可靠的。但是，当全网被一个计算力占优的攻击者攻击时，将变得较为脆弱。因为网络节点能够自行确认交易的有效性，只要攻击者能够持续地保持计算力优势，简化的机制会被攻击者焊接的（fabricated）交易欺骗。那么一个可行的策略就是，只要他们发现了一个无效的区块，就立刻发出警报，收到警报的用户将立刻开始下载被警告有问题的区块或交易的完整信息，以便对信息的不一致进行判定。对于日常会发生大量收付的商业机构，可能仍会希望运行他们自己的完整节点，以保持较大的独立完全性和检验的快速性。 9. 价值的组合与分割（Combining and Splitting Value） 虽然可以单个单个地对电子货币进行处理，但是对于每一枚电子货币单独发起一次交易将是一种笨拙的办法。为了使得价值易于组合与分割，交易被设计为可以纳入多个输入和输出。一般而言是某次价值较大的前次交易构成的单一输入，或者由某几个价值较小的前次交易共同构成的并行输入，但是输出最多只有两个：一个用于支付，另一个用于找零（如有）。 需要指出的是，当一笔交易依赖于之前的多笔交易时，这些交易又各自依赖于多笔交易，但这并不存在任何问题。因为这个工作机制并不需要展开检验之前发生的所有交易历史。 10. 隐私（Privacy） 传统的造币厂模型为交易的参与者提供了一定程度的隐私保护，因为试图向可信任的第三方索取交易信息是严格受限的。但是如果将交易信息向全网进行广播，就意味着这样的方法失效了。但是隐私依然可以得到保护：将公钥保持为匿名。公众得知的信息仅仅是有某个人将一定数量的货币发所给了另外一个人，但是难以将该交易同特定的人联系在一起，也就是说，公众难以确信，这些人究竟是谁。这同股票交易所发布的信息是类似的，股票交易发生的时间、交易量是记录在案且可供查询的，但是交易双方的身份信息却不予透露。 作为额外的预防措施，使用者可以让每次交易都生成一个新的地址，以确保这些交易不被追溯到一个共同的所有者。但是由于并行输入的存在，一定程度上的追溯还是不可避免的，因为并行输入表明这些货币都属于同一个所有者。此时的风险在于，如果某个人的某一个公钥被确认属于他，那么就可以追溯出此人的其它很多交易。 11. 计算 设想如下场景：一个攻击者试图比诚实节点产生链条更快地制造替代性区块链。即便它达到了这一目的，但是整个系统也并非就此完全受制于攻击者的独断意志了，比方说凭空创造价值，或者掠夺本不属于攻击者的货币。这是因为节点将不会接受无效的交易，而诚实的节点永远不会接受一个包含了无效信息的区块。一个攻击者能做的，最多是更改他自己的交易信息，并试图拿回他刚刚付给别人的钱。 诚实链条和攻击者链条之间的竞赛，可以用二叉树随机漫步（Binomial Random Walk)来描述。成功事件定义为诚实链条延长了一个区块，使其领先性+1，而失败事件则是攻击者的链条被延长了一个区块，使得差距-1。 攻击者成功填补某一既定差距的可能性，可以近似地看做赌徒破产问题（Gambler’s Ruin problem）。假定一个赌徒拥有无限的透支信用，然后开始进行潜在次数为无穷的赌博，试图填补上自己的亏空。那么我们可以计算他填补上亏空的概率，也就是该攻击者赶上诚实链条，如下所示[8] ： 假定p&gt;q，那么攻击成功的概率就因为区块数的增长而呈现指数化下降。由于概率是攻击者的敌人，如果他不能幸运且快速地获得成功，那么他获得成功的机会随着时间的流逝就变得愈发渺茫。那么我们考虑一个收款人需要等待多长时间，才能足够确信付款人已经难以更改交易了。我们假设付款人是一个支付攻击者，希望让收款人在一段时间内相信他已经付过款了，然后立即将支付的款项重新支付给自己。虽然收款人届时会发现这一点，但为时已晚。 收款人生成了新的一对密钥组合，然后只预留一个较短的时间将公钥发送给付款人。这将可以防止以下情况：付款人预先准备好一个区块链然后持续地对此区块进行运算，直到运气让他的区块链超越了诚实链条，方才立即执行支付。当此情形，只要交易一旦发出，攻击者就开始秘密地准备一条包含了该交易替代版本的平行链条。 然后收款人将等待交易出现在首个区块中，然后在等到z个区块链接其后。此时，他仍然不能确切知道攻击者已经进展了多少个区块，但是假设诚实区块将耗费平均预期时间以产生一个区块，那么攻击者的潜在进展就是一个泊松分布，分布的期望值为： 当此情形，为了计算攻击者追赶上的概率，我们将攻击者取得进展区块数量的泊松分布的概率密度，乘以在该数量下攻击者依然能够追赶上的概率。 化为如下形式，避免对无限数列求和： 写为如下C语言代码： 123456789101112131415#include double AttackerSuccessProbability(double q, int z)&#123;double p = 1.0 - q;double lambda = z * (q / p);double sum = 1.0;int i, k;for (k = 0; k &lt;= z; k++)&#123;double poisson = exp(-lambda);for (i = 1; i &lt;= k; i++)poisson *= lambda / i;sum -= poisson * (1 - pow(q / p, z - k));&#125;return sum;&#125; 对其进行运算，我们可以得到如下的概率结果，发现概率对z值呈指数下降。 当q=0.1时 1234567891011z&#x3D;0 P&#x3D;1.0000000z&#x3D;1 P&#x3D;0.2045873z&#x3D;2 P&#x3D;0.0509779z&#x3D;3 P&#x3D;0.0131722z&#x3D;4 P&#x3D;0.0034552z&#x3D;5 P&#x3D;0.0009137z&#x3D;6 P&#x3D;0.0002428z&#x3D;7 P&#x3D;0.0000647z&#x3D;8 P&#x3D;0.0000173z&#x3D;9 P&#x3D;0.0000046z&#x3D;10 P&#x3D;0.0000012 当q=0.3时 1234567891011z&#x3D;0 P&#x3D;1.0000000z&#x3D;5 P&#x3D;0.1773523z&#x3D;10 P&#x3D;0.0416605z&#x3D;15 P&#x3D;0.0101008z&#x3D;20 P&#x3D;0.0024804z&#x3D;25 P&#x3D;0.0006132z&#x3D;30 P&#x3D;0.0001522z&#x3D;35 P&#x3D;0.0000379z&#x3D;40 P&#x3D;0.0000095z&#x3D;45 P&#x3D;0.0000024z&#x3D;50 P&#x3D;0.0000006 求解令P&lt;0.1%的z值： 为使P&lt;0.001，则 12345678q&#x3D;0.10 z&#x3D;5q&#x3D;0.15 z&#x3D;8q&#x3D;0.20 z&#x3D;11q&#x3D;0.25 z&#x3D;15q&#x3D;0.30 z&#x3D;24q&#x3D;0.35 z&#x3D;41q&#x3D;0.40 z&#x3D;89q&#x3D;0.45 z&#x3D;340 12.结论 我们在此提出了一种不需要信用中介的电子支付系统。我们首先讨论了通常的电子货币的电子签名原理，虽然这种系统为所有权提供了强有力的控制，但是不足以防止双重支付。为了解决这个问题，我们提出了一种采用工作量证明机制的点对点网络来记录交易的公开信息，只要诚实的节点能够控制绝大多数的CPU计算能力，就能使得攻击者事实上难以改变交易记录。该网络的强健之处在于它结构上的简洁性。节点之间的工作大部分是彼此独立的，只需要很少的协同。每个节点都不需要明确自己的身份，由于交易信息的流动路径并无任何要求，所以只需要尽其最大努力传播即可。节点可以随时离开网络，而想重新加入网络也非常容易，因为只需要补充接收离开期间的工作量证明链条即可。节点通过自己的CPU计算力进行投票，表决他们对有效区块的确认，他们不断延长有效的区块链来表达自己的确认，并拒绝在无效的区块之后延长区块以表示拒绝。本框架包含了一个P2P电子货币系统所需要的全部规则和激励措施。 注释 W Dai（戴伟）,a scheme for a group of untraceable digital pseudonyms to pay each other with money and to enforce contracts amongst themselves without outside help（一种能够借助电子假名在群体内部相互支付并迫使个体遵守规则且不需要外界协助的电子现金机制）, “B-money”, http://www.weidai.com/bmoney.txt, 1998(#refmark-1) H. Massias, X.S. Avila, and J.-J. Quisquater, “Design of a secure timestamping service with minimal trust requirements,”（在最小化信任的基础上设计一种时间戳服务器） In 20th Symposium on Information Theory in the Benelux, May 1999.(#refmark-2) S. Haber, W.S. Stornetta, “How to time-stamp a digital document,” （怎样为电子文件添加时间戳）In Journal of Cryptology, vol 3, No.2, pages 99-111, 1991.(#refmark-3) D. Bayer, S. Haber, W.S. Stornetta, “Improving the efficiency and reliability of digital time-stamping,”（提升电子时间戳的效率和可靠性） In Sequences II: Methods in Communication, Security and Computer Science, pages 329-334, 1993.(#refmark-4) S. Haber, W.S. Stornetta, “Secure names for bit-strings,”（比特字串的安全命名） In Proceedings of the 4th ACM Conference on Computer and Communications Security, pages 28-35, April 1997. on Computer and Communications Security, pages 28-35, April 1997.(#refmark-5) A. Back, “Hashcash – a denial of service counter-measure,”（哈希现金——拒绝服务式攻击的克制方法）http://www.hashcash.org/papers/hashcash.pdf, 2002.(#refmark-6) R.C. Merkle, “Protocols for public key cryptosystems,” （公钥密码系统的协议）In Proc. 1980 Symposium on Security and Privacy, IEEE Computer Society, pages 122-133, April 1980. S. Haber, W.S. Stornetta, “Secure names for bit-strings,”（比特字串安全命名） In Proceedings of the 4th ACM Conference on Computer and Communications Security, pages 28-35, April 1997. on Computer and Communications Security, pages 28-35, April 1997. H. Massias, X.S. Avila, and J.-J. Quisquater, “Design of a secure timestamping service with minimal trust requirements,”（在最小化信任的条件下设计一种时间戳服务器） In 20th Symposium on Information Theory in the Benelux, May 1999.(#refmark-7) W. Feller, “An introduction to probability theory and its applications,” （概率学理论与应用导论）1957(#refmark-8) 本文地址：http://xnerv.wang/bitcoin-a-peer-to-peer-electronic-cash-system-cn/ 转载自：比特币白皮书：一种点对点的电子现金系统","categories":[{"name":"区块链与比特币","slug":"区块链与比特币","permalink":"https://xnerv.wang/categories/%E5%8C%BA%E5%9D%97%E9%93%BE%E4%B8%8E%E6%AF%94%E7%89%B9%E5%B8%81/"}],"tags":[{"name":"区块链与比特币","slug":"区块链与比特币","permalink":"https://xnerv.wang/tags/%E5%8C%BA%E5%9D%97%E9%93%BE%E4%B8%8E%E6%AF%94%E7%89%B9%E5%B8%81/"},{"name":"区块链","slug":"区块链","permalink":"https://xnerv.wang/tags/%E5%8C%BA%E5%9D%97%E9%93%BE/"},{"name":"比特币","slug":"比特币","permalink":"https://xnerv.wang/tags/%E6%AF%94%E7%89%B9%E5%B8%81/"}]},{"title":"HOW TO: Find the Problem Exception Stack When You Receive an UnhandledExceptionFilter Call in the Stack Trace（转载）","slug":"how-to-find-the-problem-exception-stack-when-you-receive-an-unhandlede","date":"2017-12-10T00:43:00.000Z","updated":"2023-08-21T02:24:19.970Z","comments":true,"path":"how-to-find-the-problem-exception-stack-when-you-receive-an-unhandlede/","link":"","permalink":"https://xnerv.wang/how-to-find-the-problem-exception-stack-when-you-receive-an-unhandlede/","excerpt":"Summary The UnhandledExceptionFilter function is called when no exception handler is defined to handle the exception that is raised. The function typically passes the exception up to the Ntdll.dll file, which catches and tries to handle it. In some scenarios in which a memory snapshot of the process exists, you can see that the thread that holds the lock points to a thread that calls the UnhandledExceptionFilter function. In those cases, you can follow the steps in this article to identify the DLL that caused the exception.","text":"Summary The UnhandledExceptionFilter function is called when no exception handler is defined to handle the exception that is raised. The function typically passes the exception up to the Ntdll.dll file, which catches and tries to handle it. In some scenarios in which a memory snapshot of the process exists, you can see that the thread that holds the lock points to a thread that calls the UnhandledExceptionFilter function. In those cases, you can follow the steps in this article to identify the DLL that caused the exception. Open a Dump File by Using Windbg.exe Download and install the debuggers. To download the debuggers, visit the following Microsoft Web site: Microsoft Debugging Tools http://www.microsoft.com/whdc/devtools/ddk/default.mspx Open the folder where you installed the debuggers, and then double-click Windbg.exe to start the debugger. On the File menu, click Open Crash Dump (or press CTRL+D), and then select the dump file that you want to view. Use Windbg.exe to Identify the Exception Stack. In Windbg.exe, open the .dmp file of the process. Make sure that you are pointing the symbol path to a correct location. For more information about how to do this, visit the following Microsoft Web site: How to Get Symbols http://www.microsoft.com/whdc/devtools/ddk/default.mspx At a command prompt, type ~*kb to list all of the threads in the process. Identify the thread that makes the call to the function Kernel32!UnhandledExceptionFilter. It looks similar to the following: 123456120 id: f0f0f0f0.a1c Suspend: 1 Teb 7ff72000 UnfrozenChildEBP RetAddr Args to Child09a8f334 77eb9b46 0000244c 00000001 00000000 ntdll!ZwWaitForSingleObject+0xb [i386\\usrstubs.asm @ 2004]09a8f644 77ea7e7a 09a8f66c 77e861ae 09a8f674 KERNEL32!UnhandledExceptionFilter+0x2b5[D:\\nt\\private\\windows\\base\\client\\thread.c @ 1753]09a8ffec 00000000 787bf0b8 0216fe94 00000000 KERNEL32!BaseThreadStart+0x65 [D:\\nt\\private\\windows\\base\\client\\support.c @ 453] Switch to that thread (in this example, the thread is “~120s”). Display the memory contents at the location specified by the first parameter of Kernel32!UnhandledExceptionFilter by using dd First Param. This points to the EXCEPTION_POINTERS structure. ··· 0:120&gt; dd 09a8f66c 09a8f66c 09a8f738 09a8f754 09a8f698 77f8f45c 09a8f67c 09a8f738 09a8ffdc 09a8f754 09a8f710 09a8f68c 09a8ffdc 77f8f5b5 09a8ffdc 09a8f720 09a8f69c 77f8f3fa 09a8f738 09a8ffdc 09a8f754 09a8f6ac 09a8f710 77e8615b 09a8fad4 00000000 09a8f6bc 09a8f738 74a25336 09a8f6e0 09a8f910 09a8f6cc 01dc8ad8 0d788918 00000001 018d1f28 09a8f6dc 00000001 61746164 7073612e 09a8f71c ··· The first DWORD represents the exception record. To obtain information about the type of exception, run the following at a command prompt: .exr first DWORD from step 6 ··· 0:120&gt; .exr 09a8f738 ExceptionAddress: 78011f32 (MSVCRT!strnicmp+0x00000092) ExceptionCode: c0000005 ExceptionFlags: 00000000 NumberParameters: 2 Parameter[0]: 00000000 Parameter[1]: 00000000 Attempt to read from address 00000000 ··· The second DWORD is the context record. To obtain contextual information, run the following at a command prompt: .cxr second DWORD from step 6 ··· 0:120&gt; .cxr 09a8f754 eax=027470ff ebx=7803cb28 ecx=00000000 edx=00000000 esi=00000000 edi=09a8fad4 eip=78011f32 esp=09a8fa20 ebp=09a8fa2c iopl=0 nv up ei ng nz na po nc cs=001b ss=0023 ds=0023 es=0023 fs=003b gs=0000 efl=00010286 MSVCRT!strnicmp+92: 78011f32 8a06 mov al,[esi] ··· Run a kv command to get the call stack of the actual exception. This helps you to identify the actual problem in the process that might not have been handled correctly. 1234567891011121314151617181920212223242526272829303132333435363738390:120&gt; kvChildEBP RetAddr Args to ChildWARNING: Stack unwind information not available. Following frames may be wrong.09a8fa2c 780119ab 09a8fad4 00000000 09a8faa8 MSVCRT!strnicmp+0x9209a8fa40 7801197c 09a8fad4 00000000 6d7044fd MSVCRT!stricmp+0x3c09a8fa80 6e5a6ef6 09a8fad4 2193d68d 00e5e298 MSVCRT!stricmp+0xd09a8fa94 6d7043bf 09a8fad4 09a8faa8 0000001c IisRTL!CLKRHashTable::FindKey+0x59 (FPO: [2,0,1])09a8faac 749fc22d 09a8fad4 01d553b0 0000001c ISATQ!CDirMonitor::FindEntry+0x1e(FPO: [Non-Fpo]) [D:\\nt\\private\\inet\\iis\\svcs\\infocomm\\atq\\dirmon.cpp @ 884]09a8fac4 749fd1cb 09a8fad4 09a8fb10 525c3a46 asp!RegisterASPDirMonitorEntry+0x6e(FPO: [EBP 0x09a8fb08] [2,0,4]) [D:\\nt\\private\\inet\\iis\\svcs\\cmp\\asp\\aspdmon.cpp @ 534]09a8fb08 749fcdd6 00000000 09a8fcbc 018d1f28 asp!CTemplateCacheManager::RegisterTemplateForChangeNotification+0x8a(FPO: [Non-Fpo]) [D:\\nt\\private\\inet\\iis\\svcs\\cmp\\asp\\cachemgr.cpp @ 621]09a8fb3c 74a08bfe 00000000 000000fa 74a30958 asp!CTemplateCacheManager::Load+0x382(FPO: [Non-Fpo]) [D:\\nt\\private\\inet\\iis\\svcs\\cmp\\asp\\cachemgr.cpp @ 364]09a8fc68 74a0d4c9 04c12518 018d1f28 09a8fcbc asp!LoadTemplate+0x42(FPO: [Non-Fpo]) [D:\\nt\\private\\inet\\iis\\svcs\\cmp\\asp\\exec.cpp @ 1037]09a8fcc0 74a2c3e5 00000000 0637ee38 09a8fd58 asp!CHitObj::ViperAsyncCallback+0x3e8(FPO: [Non-Fpo]) [D:\\nt\\private\\inet\\iis\\svcs\\cmp\\asp\\hitobj.cpp @ 2414]09a8fcd8 787c048a 00000000 77aa1b03 01e91ed8 asp!CViperAsyncRequest::OnCall+0x3f(FPO: [Non-Fpo]) [D:\\nt\\private\\inet\\iis\\svcs\\cmp\\asp\\viperint.cpp @ 194]09a8fce0 77aa1b03 01e91ed8 77a536d8 00000000 COMSVCS!STAActivityWorkHelper+0xa(FPO: [1,0,0])09a8fd24 77aa1927 000752f8 000864dc 787c0480 ole32!EnterForCallback+0x6a(FPO: [Non-Fpo]) [D:\\nt\\private\\ole32\\com\\dcomrem\\crossctx.cxx @ 1759]09a8fe50 77aa17ea 000864dc 787c0480 01e91ed8 ole32!SwitchForCallback+0x12b(FPO: [Non-Fpo]) [D:\\nt\\private\\ole32\\com\\dcomrem\\crossctx.cxx @ 1644]09a8fe78 77aa60c1 000864dc 787c0480 01e91ed8 ole32!PerformCallback+0x50(FPO: [Non-Fpo]) [D:\\nt\\private\\ole32\\com\\dcomrem\\crossctx.cxx @ 1559]09a8fed4 77aa5fa6 04f2b4c0 787c0480 01e91ed8 ole32!CObjectContext::InternalContextCallback+0xf5(FPO: [Non-Fpo]) [D:\\nt\\private\\ole32\\com\\dcomrem\\context.cxx @ 3866]09a8fef4 787bd3c3 04f2b4c0 787c0480 01e91ed8 ole32!CObjectContext::DoCallback+0x1a(FPO: [Non-Fpo]) [D:\\nt\\private\\ole32\\com\\dcomrem\\context.cxx @ 3746]09a8ff24 787bf373 0216fb3c 00000007 09a8ffec COMSVCS!STAActivityWork::DoWork+0x73(FPO: [0,4,2])09a8ffb4 77e8758a 0216fe94 0216fb3c 00000007 COMSVCS!STAThread::STAThreadWorker+0x2bb(FPO: [EBP 0x09a8ffec] [1,31,4])09a8ffec 00000000 787bf0b8 0216fe94 00000000 KERNEL32!BaseThreadStart+0x52(FPO: [Non-Fpo]) [D:\\nt\\private\\windows\\base\\client\\support.c @ 451] References For more information, see the following books: Solomon, David A., and Mark Russinovich. Inside Microsoft Windows 2000, Third Edition (http://www.microsoft.com/mspress/books/4354.asp). Redmond: Microsoft Press, 2000. Solomon, David A. Inside Windows NT - Second Edition (Microsoft Programming Series). Redmond: Microsoft Press, 1998. Richter, Jeffrey. Programming Applications with Microsoft Windows (http://www.microsoft.com/mspress/books/2345.asp). Redmond: Microsoft Press, 1999. 本文地址：http://xnerv.wang/how-to-find-the-problem-exception-stack-when-you-receive-an-unhandlede/ 转载自：HOW TO: Find the Problem Exception Stack When You Receive an UnhandledExceptionFilter Call in the Stack Trace","categories":[{"name":"WinDbg","slug":"WinDbg","permalink":"https://xnerv.wang/categories/WinDbg/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/tags/Windows/"},{"name":"Heap","slug":"Heap","permalink":"https://xnerv.wang/tags/Heap/"},{"name":"MSDN","slug":"MSDN","permalink":"https://xnerv.wang/tags/MSDN/"},{"name":"WinDbg","slug":"WinDbg","permalink":"https://xnerv.wang/tags/WinDbg/"}]},{"title":"分布式理论及架构的一些整理","slug":"summary-of-distributed-theory-and-architecture","date":"2017-11-26T23:19:00.000Z","updated":"2023-08-21T02:24:20.458Z","comments":true,"path":"summary-of-distributed-theory-and-architecture/","link":"","permalink":"https://xnerv.wang/summary-of-distributed-theory-and-architecture/","excerpt":"CAP理论及BASE理论 分布式系统的CAP理论 一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三项中的两项。 麻省理工学院的Seth Gilbert和Nancy Lynch从理论上证明了CAP","text":"CAP理论及BASE理论 分布式系统的CAP理论 一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三项中的两项。 麻省理工学院的Seth Gilbert和Nancy Lynch从理论上证明了CAP 分布式系统的BASE理论 eBay的架构师Dan Pritchett源于对大规模分布式系统的实践总结，在ACM上发表文章提出BASE理论，BASE理论是对CAP理论的延伸，核心思想是即使无法做到强一致性（Strong Consistency，CAP的一致性就是强一致性），但应用可以采用适合的方式达到最终一致性（Eventual Consitency）。 BASE是指基本可用（Basically Available）、软状态（ Soft State）、最终一致性（ Eventual Consistency）。 基本可用（Basically Available） 基本可用是指分布式系统在出现故障的时候，允许损失部分可用性，即保证核心可用。 电商大促时，为了应对访问量激增，部分用户可能会被引导到降级页面，服务层也可能只提供降级服务。这就是损失部分可用性的体现。 软状态（ Soft State） 软状态是指允许系统存在中间状态，而该中间状态不会影响系统整体可用性。分布式存储中一般一份数据至少会有三个副本，允许不同节点间副本同步的延时就是软状态的体现。mysql replication的异步复制也是一种体现。 最终一致性（ Eventual Consistency） 最终一致性是指系统中的所有数据副本经过一定时间后，最终能够达到一致的状态。弱一致性和强一致性相反，最终一致性是弱一致性的一种特殊情况。 两阶段提交主要保证了分布式事务的原子性：即所有结点要么全做要么全不做。 一个协调者，多个参与者。协调者收集参与者的投票，全通过就commit，否则abort。 将提交分成两阶段进行的目的很明确，就是尽可能晚地提交事务，让事务在提交前尽可能地完成所有能完成的工作，这样，最后的提交阶段将是一个耗时极短的微小操作，这种操作在一个分布式系统中失败的概率是非常小的，也就是所谓的“网络通讯危险期”非常的短暂，这是两阶段提交确保分布式事务原子性的关键所在。（唯一理论上两阶段提交出现问题的情况是当协调者发出提交指令后当机并出现磁盘故障等永久性错误，导致事务不可追踪和恢复）。 （注意，二阶段提交相对的就是一阶段提交，就是普通的本地事务模式） 关于分布式事务、两阶段提交协议、三阶提交协议 无论是二阶段提交还是三阶段提交都无法彻底解决分布式的一致性问题。Google Chubby的作者Mike Burrows说过， there is only one consensus protocol, and that’s Paxos” – all other approaches are just broken versions of Paxos. 意即世上只有一种一致性算法，那就是Paxos，所有其他一致性算法都是Paxos算法的不完整版。 3PC在2PC的基础上，加上了第一个CanCommit阶段。在第三个doCommit阶段，如果超时会自动commit，其它阶段超时会自动rollback。 深入理解分布式系统的2PC和3PC 2PC协议中，如果出现协调者和参与者都挂了的情况，有可能导致数据不一致。（这个应该指的是一个参加者节点在一段很短的时间内处于数据不一致状态，这也是不允许的，即使之后能够逐步恢复） 简单概括一下就是，如果挂掉的那台机器已经执行了commit，那么协调者可以从所有未挂掉的参与者的状态中分析出来，并执行commit。如果挂掉的那个参与者执行了rollback，那么协调者和其他的参与者执行的肯定也是rollback操作。 所以，再多引入一个阶段之后，3PC解决了2PC中存在的那种由于协调者和参与者同时挂掉有可能导致的数据一致性问题。 3PC存在的问题 在doCommit阶段，如果参与者无法及时接收到来自协调者的doCommit或者rebort请求时，会在等待超时之后，会继续进行事务的提交。 所以，由于网络原因，协调者发送的abort响应没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操作。这样就和其他接到abort命令并执行回滚的参与者之间存在数据不一致的情况。 XA事务 MySQL的XA事务，估计就是对XA事务的支持。这样如果有一个XA事务管理器，就可以在MySQL和其它支持XA事务的数据库如SQL Server之间，进行分布式事务处理了。 X/Open 组织（即现在的 Open Group ）定义了分布式事务处理模型。 X/Open DTP 模型（ 1994 ）包括应用程序（ AP ）、事务管理器（ TM ）、资源管理器（ RM ）、通信资源管理器（ CRM ）四部分。一般，常见的事务管理器（ TM ）是交易中间件，常见的资源管理器（ RM ）是数据库，常见的通信资源管理器（ CRM ）是消息中间件。 通常把一个数据库内部的事务处理，如对多个表的操作，作为本地事务看待。数据库的事务处理对象是本地事务，而分布式事务处理的对象是全局事务。 XA 就是 X/Open DTP 定义的交易中间件与数据库之间的接口规范（即接口函数），交易中间件用它来通知数据库事务的开始、结束以及提交、回滚等。 XA 接口函数由数据库厂商提供。 拜占庭将军问题 拜占庭将军问题（Byzantine Generals Problem），是由莱斯利兰伯特提出的点对点通信中的基本问题。 在分布式计算上，不同的计算机透过讯息交换，尝试达成共识；但有时候，系统上协调计算机（Coordinator / Commander）或成员计算机 （Member / Lieutanent）可能因系统错误并交换错的讯息，导致影响最终的系统一致性。拜占庭将军问题就根据错误计算机的数量，寻找可能的解决办法 ，这无法找到一个绝对的答案，但只可以用来验证一个机制的有效程度）。 开源分布式系统 HBase可以看作对应Google BigTable的开源实现。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。另一个不同的是HBase基于列的而不是基于行的模式。 解析全球级分布式数据库Google Spanner OceanBase 一个高性能的分布式表格系统，提供类似BigTable的性能和扩展性，但表格中保存的是强类型的数据，比如integer, string， datetime等。 它使用C++编写，运行于64位Linux环境下。生产环境下需要使用多台机器搭建OceanBase集群以提供高可用和高性能，但是你也完全可以使用一台机器运行OceanBase。 TFS（Taobao !FileSystem） 淘宝针对海量非结构化数据存储设计的分布式系统，构筑在普通的Linux机器集群上，可为外部提供高可靠和高并发的存储访问。高可扩展、高可用、高性能、面向互联网服务。 Tair Tair是一个高性能，分布式，可扩展，高可靠的key/value结构存储系统。 tair的总体结构 tair的负载均衡算法 tair的分布采用的是一致性哈希算法，对于所有的key，分到Q个桶中，桶是负载均衡和数据迁移的基本单位。config server 根据一定的策略把每个桶指派到不同的data server上。因为数据按照key做hash算法，所以可以认为每个桶中的数据基本是平衡的。保证了桶分布的均衡性，就保证了数据分布的均衡性。 本文地址：http://xnerv.wang/summary-of-distributed-theory-and-architecture/","categories":[{"name":"分布式及存储","slug":"分布式及存储","permalink":"https://xnerv.wang/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8F%8A%E5%AD%98%E5%82%A8/"}],"tags":[{"name":"原创","slug":"原创","permalink":"https://xnerv.wang/tags/%E5%8E%9F%E5%88%9B/"},{"name":"分布式及存储","slug":"分布式及存储","permalink":"https://xnerv.wang/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8F%8A%E5%AD%98%E5%82%A8/"},{"name":"2PC","slug":"2PC","permalink":"https://xnerv.wang/tags/2PC/"},{"name":"3PC","slug":"3PC","permalink":"https://xnerv.wang/tags/3PC/"}]},{"title":"一些大学常用算法及知识点的回顾","slug":"some-knowledge-points-learned-in-university","date":"2017-11-21T05:13:00.000Z","updated":"2023-08-21T02:24:21.225Z","comments":true,"path":"some-knowledge-points-learned-in-university/","link":"","permalink":"https://xnerv.wang/some-knowledge-points-learned-in-university/","excerpt":"本文是关于大学时一些常见的数据结构、算法和知识点的总结。其实与其说工作中会用到，不如说面试时被面到的可能性更大一些。不过其中一些特定领域的算法，像银行家算法，BAT的面试中我也还没有遇到过。适当地回顾大学时学到的一些知识点，也许能给自己带来一些快乐吧，一种仅存在于回忆中的快乐。","text":"本文是关于大学时一些常见的数据结构、算法和知识点的总结。其实与其说工作中会用到，不如说面试时被面到的可能性更大一些。不过其中一些特定领域的算法，像银行家算法，BAT的面试中我也还没有遇到过。适当地回顾大学时学到的一些知识点，也许能给自己带来一些快乐吧，一种仅存在于回忆中的快乐。 排序算法 冒泡排序 通过相邻元素的两两比较和swap，每次都将当前子序列中的最大/最小元素交换到当前子序列的最后面。 快速排序 选定第一个元素作为中轴，然后当i &lt; j的前提下，指针j从右至左寻找一个比中轴小的元素，指针i从左至右寻找一个比中轴大的元素，然后swap两者。最终i=j，则将i所在元素和中轴再swap。 这里的重点是j必须先走。举个例子：3、1、2、5、4这个序列，如果i先移动，则最终的结构是5、1、2、3、4，显然是错误的。原因是：中轴选的是左边第一个元素（3），如果左边的指针先移动，极端案例下会使得左边指针停留在一个比中轴大的元素上（这里的5），而右边指针无法在i &lt; j的情况下找到一个比中轴小的元素，导致最终5和3交换而错误。 的确无法很好地进行证明，只能举个反例来说明。 12345678910111213141516171819202122232425262728293031323334#include &lt;stdio.h&gt;int a[101],n;//定义全局变量，这两个变量需要在子函数中使用void quicksort(int left,int right)&#123; int i,j,t,temp; if(left&gt;right) return; temp=a[left]; //temp中存的就是基准数 i=left; j=right; while(i!=j) &#123; //顺序很重要，要先从右边开始找 while(a[j]&gt;=temp &amp;&amp; i&lt;j) j--; //再找右边的 while(a[i]&lt;=temp &amp;&amp; i&lt;j) i++; //交换两个数在数组中的位置 if(i&lt;j) &#123; t=a[i]; a[i]=a[j]; a[j]=t; &#125; &#125; //最终将基准数归位 a[left]=a[i]; a[i]=temp; quicksort(left,i-1);//继续处理左边的，这里是一个递归的过程 quicksort(i+1,right);//继续处理右边的 ，这里是一个递归的过程&#125; 归并排序 1234567891011121314151617181920212223242526272829303132333435363738394041424344//将有二个有序数列a[first...mid]和a[mid...last]合并。void mergearray(int a[], int first, int mid, int last, int temp[])&#123; int i = first, j = mid + 1; int m = mid, n = last; int k = 0; while (i &lt;= m &amp;&amp; j &lt;= n) &#123; if (a[i] &lt;= a[j]) temp[k++] = a[i++]; else temp[k++] = a[j++]; &#125; while (i &lt;= m) temp[k++] = a[i++]; while (j &lt;= n) temp[k++] = a[j++]; for (i = 0; i &lt; k; i++) a[first + i] = temp[i];&#125;void mergesort(int a[], int first, int last, int temp[])&#123; if (first &lt; last) &#123; int mid = (first + last) / 2; mergesort(a, first, mid, temp); //左边有序 mergesort(a, mid + 1, last, temp); //右边有序 mergearray(a, first, mid, last, temp); //再将二个有序数列合并 &#125;&#125;bool MergeSort(int a[], int n)&#123; int *p = new int[n]; if (p == NULL) return false; mergesort(a, 0, n - 1, p); delete[] p; return true;&#125; 注：有的书上是在mergearray()合并有序数列时分配临时数组，但是过多的new操作会非常费时。因此作了下小小的变化。只在MergeSort()中new一个临时数组。后面的操作都共用这一个临时数组。 鸡尾酒排序 冒泡排序的变种，也被称作定向冒泡排序，鸡尾酒搅拌排序，搅拌排序（也可以视作选择排序的一种变形），涟漪排序，来回排序或快乐小时排序。 先找到最小的数字，把他放到第一位，然后找到最大的数字放到最后一位。然后再找到第二小的数字放到第二位，再找到第二大的数字放到倒数第二位。以此类推，直到完成排序。 二分查找 递归版本： 12345678910111213int BSearch(elemtype a[],elemtype x,int low,int high)/*在下届为low，上界为high的数组a中折半查找数据元素x*/&#123; int mid; if(low &gt; high) return -1; mid=(low + high) / 2; if(x == a[mid]) return mid; if(x &lt; a[mid]) return(BSearch(a, x, low, mid-1)); else return(BSearch(a, x, mid+1, high));&#125; 非递归版本 12345678910111213141516171819int binary_search(int* a, int len, int goal)&#123; int low &#x3D; 0; int high &#x3D; len - 1; while(low &lt;&#x3D; high) &#123; int middle &#x3D; (low + high)&#x2F;2; if(a[middle] &#x3D;&#x3D; goal) return middle; &#x2F;&#x2F;在左半边 else if(a[middle] &gt; goal) high &#x3D; middle - 1; &#x2F;&#x2F;在右半边 else low &#x3D; middle + 1; &#125; &#x2F;&#x2F;没找到 return -1;&#125; 图的遍历 DFS深度遍历（邻接图） 首先申请一个visted[n]数组，标记每个节点是否已经访问。 循环这个数组，如果一个节点没有被访问。就对其进行DFS遍历。 DFS遍历：标记当前节点为已访问。然后从邻接图中获取该节点的第一个未访问的可连通邻居，递归对该邻居进行DFS。然后再获取第二个未访问的可连通邻居，重复这个循环直到所有可连通邻居都处理完。 BFS广度遍历（邻接图） 首先申请一个visted[n]数组，标记每个节点是否已经访问。 循环这个数组，如果一个节点没有被访问。就对其进行BFS遍历。 BFS遍历：标记当前节点为已访问。然后创建一个queue，将当前节点放入queue。然后类似于二叉树的层次遍历，以queue中的当前节点作为种子，在while循环中，每次弹出queue中的一个节点，获取其所有未访问的邻居节点加入到queue中。直到最后queue为空。 最短路径 Dijkstra最短路径算法（单源最短路径） 这个算法既可以被划入到贪心法的范畴，也可以划入到动态规划的范畴。（排序的操作涉及贪心法，而之后的推进过程可看作是动态规划） 首先算出V0到各点的直连路径，然后从小到大排列，例如是V1、V2、V3…… 建立一个大小为n的数组distances（下标从1开始），表示V0分别到这个n个点的最短距离，初始值就是直连距离。 首先V0到V1的最短路径一定就是V0到V1的直连路径，因为V1是距离V0最近的点，因此不可能有一条经过第三个点的绕路可以比V0到V1的直连路径更短。 然后V2则是比较V0-V1和V0-V1-V2，选取最短的那条。更新distances[2]。 而对于V3，则考察V0-V3直连距离，以及经过V1或V2到达V3的间接距离，选取最短的那条。更新distances[3]。 …… 这里有个问题，为什么V0到V3的最短距离，一定是从已知点集合U中选择路径，而不是从未知点集合V中选择，例如V0-V4-V3这样的路径? 这就是一开始对n个直连路径进行排序的原因，V0-V4一定 &gt;= V0-V3，因此V0-V4-V3一定 &gt;= V0-V3。 另外本算法不能处理负权重边，用上面的例子说明，如果V0-V4是负权重，则有可能V0-V4-V3一定 &lt; V0-V3，因此这种算法就不再使用。（此时有没有其它好的方法？除了各边都加上同一个偏移量，将负权重都转正。） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125/**************************************** About: 有向图的Dijkstra算法实现* Author: Tanky Woo* Blog: www.WuTianQi.com***************************************/#include &lt;iostream&gt;using namespace std;const int maxnum = 100;const int maxint = 999999;void Dijkstra(int n, int v, int *dist, int *prev, int c[maxnum][maxnum])&#123; bool s[maxnum]; // 判断是否已存入该点到S集合中 for(int i=1; i&lt;=n; ++i) &#123; dist[i] = c[v][i]; s[i] = 0; // 初始都未用过该点 if(dist[i] == maxint) prev[i] = 0; else prev[i] = v; &#125; dist[v] = 0; s[v] = 1; // 依次将未放入S集合的结点中，取dist[]最小值的结点，放入结合S中 // 一旦S包含了所有V中顶点，dist就记录了从源点到所有其他顶点之间的最短路径长度 for(int i=2; i&lt;=n; ++i) &#123; int tmp = maxint; int u = v; // 找出当前未使用的点j的dist[j]最小值 for(int j=1; j&lt;=n; ++j) if((!s[j]) &amp;&amp; dist[j]&lt;tmp) &#123; u = j; // u保存当前邻接点中距离最小的点的号码 tmp = dist[j]; &#125; s[u] = 1; // 表示u点已存入S集合中 // 更新dist for(int j=1; j&lt;=n; ++j) if((!s[j]) &amp;&amp; c[u][j]&lt;maxint) &#123; int newdist = dist[u] + c[u][j]; if(newdist &lt; dist[j]) &#123; dist[j] = newdist; prev[j] = u; &#125; &#125; &#125;&#125;void searchPath(int *prev,int v, int u)&#123; int que[maxnum]; int tot = 1; que[tot] = u; tot++; int tmp = prev[u]; while(tmp != v) &#123; que[tot] = tmp; tot++; tmp = prev[tmp]; &#125; que[tot] = v; for(int i=tot; i&gt;=1; --i) if(i != 1) cout &lt;&lt; que[i] &lt;&lt; &quot; -&gt; &quot;; else cout &lt;&lt; que[i] &lt;&lt; endl;&#125;int main()&#123; freopen(&quot;input.txt&quot;, &quot;r&quot;, stdin); // 各数组都从下标1开始 int dist[maxnum]; // 表示当前点到源点的最短路径长度 int prev[maxnum]; // 记录当前点的前一个结点 int c[maxnum][maxnum]; // 记录图的两点间路径长度 int n, line; // 图的结点数和路径数 // 输入结点数 cin &gt;&gt; n; // 输入路径数 cin &gt;&gt; line; int p, q, len; // 输入p, q两点及其路径长度 // 初始化c[][]为maxint for(int i=1; i&lt;=n; ++i) for(int j=1; j&lt;=n; ++j) c[i][j] = maxint; for(int i=1; i&lt;=line; ++i) &#123; cin &gt;&gt; p &gt;&gt; q &gt;&gt; len; if(len &lt; c[p][q]) // 有重边 &#123; c[p][q] = len; // p指向q c[q][p] = len; // q指向p，这样表示无向图 &#125; &#125; for(int i=1; i&lt;=n; ++i) dist[i] = maxint; for(int i=1; i&lt;=n; ++i) &#123; for(int j=1; j&lt;=n; ++j) printf(&quot;%8d&quot;, c[i][j]); printf(&quot;\\n&quot;); &#125; Dijkstra(n, 1, dist, prev, c); // 最短路径长度 cout &lt;&lt; &quot;源点到最后一个顶点的最短路径长度: &quot; &lt;&lt; dist[n] &lt;&lt; endl; // 路径 cout &lt;&lt; &quot;源点到最后一个顶点的路径为: &quot;; searchPath(prev, 1, n);&#125; Floyd最短路径算法（多源最短路径） Dijkstra算法是用于计算单点到其它点的最短距离，复杂度是O(n2)，则计算完n个点的数据的复杂度就是O(n3)。而Floyd的算法复杂度也是O(n3). 虽然两者的算法复杂度一样，但是如果依次对某个顶点运用Dijkstra算法,则与Floyd算法相比,很多路径和结果计算是重复的,虽然复杂度相同,但是运算量差了很多。 此外，Dijkstra算法使用的前提是图中路径长度必须大于等于0，但是Floyd算法则仅仅要求没有总和小于0的环路就可以了。因此Floyd 算法应用范围比Dijkstra算法要广。 Floyd算法的思路：首先求出各点之间的直连距离矩阵，然后逐步引入1号点、2号点。。。 12345678// 经过1号顶点for(i=1;i&lt;=n;i++) for(j=1;j&lt;=n;j++) if (e[i][j] &gt; e[i][1]+e[1][j]) e[i][j]=e[i][1]+e[1][j];// 经过2号顶点for(i=1;i&lt;=n;i++) for(j=1;j&lt;=n;j++) if (e[i][j] &gt; e[i][2]+e[2][j]) e[i][j]=e[i][2]+e[2][j]; 从而推导出一个三重循环 12345for(k=1;k&lt;=n;k++) for(i=1;i&lt;=n;i++) for(j=1;j&lt;=n;j++) if(e[i][j]&gt;e[i][k]+e[k][j]) e[i][j]=e[i][k]+e[k][j]; Floyd算法另一种理解DP，为理论爱好者准备的，上面这个形式的算法其实是Floyd算法的精简版，而真正的Floyd算法是一种基于DP(Dynamic Programming)的最短路径算法。设图G中n 个顶点的编号为1到n。令c [i, j, k]表示从i 到j 的最短路径的长度，其中k 表示该路径中的最大顶点，也就是说c[i,j,k]这条最短路径所通过的中间顶点最大不超过k。因此，如果G中包含边&lt; i, j &gt;，则c[i, j, 0] =边&lt; i, j &gt; 的长度；若i= j ，则c[i,j,0]=0；如果G中不包含边&lt; i, j &gt;，则c (i, j, 0)= +∞。c[i, j, n] 则是从i 到j 的最短路径的长度。对于任意的k&gt;0，通过分析可以得到：中间顶点不超过k 的i到j的最短路径有两种可能：该路径含或不含中间顶点k。若不含，则该路径长度应为c[i, j, k-1]，否则长度为 c[i, k, k-1] +c [k, j, k-1]。c[i, j, k]可取两者中的最小值。状态转移方程：c[i, j, k]=min{c[i, j, k-1], c [i, k, k-1]+c [k, j, k-1]}，k＞0。这样，问题便具有了最优子结构性质，可以用动态规划方法来求解。 最小生成树：Prim算法和Kruskal算法 网上的Prim实现方法的时间复杂度是O(N2)，但实际上先用快速排序对各边进行排序，然后用两个hashset，U和V记录两种点。然后遍历一次边的有序列表进行了，时间复杂度O(logN)。 Kruskal算法的实现思想是：也是先对所有边排序，然后一开始时将每条边当作一个连通分量。然后循环地遍历边的有序列表，如果边的两个顶点不在同一个连通分量中，就将这两个连通分量合并为同一个（新的连通分量标识选择两者中较小的那个）。 （描述不太清楚，还是不知道两个算法的区别） KMP算法 （待补完） 汉诺塔 12345678910111213141516class Solution: def __init__(self): self.i = 0 def move(self, n, src, dst): self.i += 1 print &quot;the %d step: move dish %d %s to %s.&quot; % (self.i, n, src, dst) def hanoi(self, n, src, dst, dep): if 1 == n: self.move(1, src, dst) else: self.hanoi(n - 1, src, dep, dst) self.move(n, src, dst) self.hanoi(n - 1, dep, dst, src) Bloom Filter Bloom Filter的这种高效是有一定代价的：在判断一个元素是否属于某个集合时，有可能会把不属于这个集合的元素误认为属于这个集合（false positive）。因此，Bloom Filter不适合那些“零错误”的应用场合。而在能容忍低错误率的应用场合下，Bloom Filter通过极少的错误换取了存储空间的极大节省。 无法从Bloom Filter集合中删除一个元素。因为该元素对应的位会牵动到其他的元素。所以一个简单的改进就是 counting Bloom filter，用一个counter数组代替位数组，就可以支持删除了。 此外，Bloom Filter的hash函数选择会影响算法的效果。 银行家算法 银行家算法：每个资源申请者要预先填写最大申请额度，然后以后每次能提出额度内的申请，银行家评估如果通过这次申请，那剩余的资源是否能构成安全序列。这里有一个假设是，每一名资源申请者只有在获取到最大额度的申请时，才会释放掉手中的资源。因此关键在于如何判断是否能够构成安全序列。 安全序列的判定：检查所有申请者，看当前剩余资源能够使得其中至少一个申请者可以满足最大额度申请而释放掉其资源。因为这是一个使得可用资源增长的过程，因此检查的顺序是不会影响最终判定结果的。 CA工作原理 SSL(Secure Socket Layer) 是一种加密技术，可以提供对称加密和非对称加密。由于它在协议层里正好是在传输层与应用层之间，这就决定了上层应用必须经过它，这就是它广泛流行和易于实现的原因。 对称加密有md5，sha1。由于md5已被学者证明可以计算出加密冲突，即它有一定的不安全性，所以建议用sha1加密。 非对称性加密有RSA，即密码有一对，一个私钥，一个公钥，公钥可以让所有人知道，私钥只有自己知道。 这样理解，服务器产生一对密钥，公钥给别人即客户端，客户端用它来加密，加密后发给服务端，服务端用自己的私钥解密后得到数据。 数字签名就和上面的过程相反，即数据由服务端用私钥加密，客户端用服务端的公钥解密，解得出来就说明这数据包的确是出服务端发过来的。 数字签名是由服务端自己签的，但没人去验证这个服务端是不是你所要访问的真实的，所以需要第三方来帮忙检验，就和支付宝处于第三方来协调的位置一样。这个第三方就叫CA。 所以服务器产生的公钥就交给CA，CA用CA自己的私钥加密，即数字签名，加密生会生成证书，证书还是要交给服务端，放在服务端那边。当客户端访问服务端时，服务端就会把这个证书安装到客户端上。 客户端就会用CA提供的CA自己的公钥来解密这个证书，（当然这个CA是浏览器预装时嵌入的可信的CA，如果不是预装时嵌入的CA，此时就没有CA的公钥，就解不了，就会弹出告警。）解得开就说明这个证书是某个CA认证过了的，是可信的，解开后就会得到数据，而这个数据就是服务端的公钥，此时用这个公钥与服务端进行数据传输。 数据传输过程中，由于RSA方式加解密速度非常慢，所以会把对称与非对称两者结合起来用，即用RSA把对称加密的密码进行加密传输，再用对称密码进行加解密，这样就可以提高效率，且是安全的。 B树 B树：分支节点和叶节点均保存记录的关键码和记录的指针 B+树：分支节点只保存记录关键码的复制，无记录指针。 所有记录都集中在叶节点一层，并且叶节点可以构成一维线性表，便于连续访问和范围查询。 两者的插入、删除基本一致。 对于同样阶的B树和B+树，B+树的树高和平均检索长度均大于B树（因为B+树必须检索到叶节点一层） 但实际上检索过程中，最耗时间的是…………IO，也就是访盘次数越少越好。 B+树的分支节点无记录指针，同样一个盘块可以存放的关键码数就更多，所以虽然平均检索长度大，但访盘次数反而少，速度也就比B树快。 deque、stack、queue、priority_queue，heap deque和list/vector一样是基本的容器类型。但stack/queue/priority_queue/heap则是抽象容器类型，必须基于一种基本容器类型。 stack和queue默认是使用deque，而priority_queue默认是使用vector。 queue是一种单向队列，push类似push_back，pop类似pop_front，其实就是用adapter模式将基本容器类型的接口变化了一下，所以目前看来实际应用的不多。stack也类似。 priority_queue/heap则是基于基本容器类型，实现了一种高层的抽象数据结构。 堆算法 堆中每一个节点的父节点的下标是(i-1)/2, i!=0。 第一个非叶子结点是size/2。 堆的建立 从最后一个非叶子结点开始，从后向前推，每次都比较一个非叶子结点和其两个子节点，将最大者交换到该非叶子结点的位置。时间复杂度是O(N)。 堆的插入 将元素放到最后面，然后逐步与父节点进行调整。 父节点的下标是(i-1)/2,i!=0，对于大顶堆，如果当前节点比其父节点大，则swap，然后重复这个过程。 时间复杂度是o(logN)。 堆的删除 将堆顶元素弹出，然后将最后一个元素填充到堆顶，然后逐步与子节点进行调整。 子节点的下标是2i+1和2i+2，比较这两个节点与当前节点，将（可能的）最大者与当前节点进行swap，重复这个过程。 时间复杂度是o(logN)。 堆排序 首先需要建立堆。 然后每次将堆顶的元素与当前子序列中的最后一个元素交换，这样就同时完成了一个元素的直接选择排序，又完成了堆删除中的一步：将最后一个元素填充到堆顶。重复这个过程。 时间复杂度是O(N)+Nlog(N)=Nlog(N) 另外堆排序是不稳定的排序。（有没有可能规定相等元素在比较、替换等时的一套规则，使得堆排序能够稳定？） 路由协议 LS/DV 路由协议分成链路状态协议（LS）、和距离矢量协议（DV）两大类。 链路状态协议：例如OSPF协议。使用链路状态路由协议时，每台路由创建自己的LSA（链路状态通告），并在路由更新中泛洪（将网络的所有细节通告给其他的所有路由器）LSA给其他的所有路由器。泛洪LSA就是路由器将LSA发给邻居，邻居再将它转发给他的邻居，知道所有的路由器都收到这个LSA，路由器相连的子网也会创建并泛洪链路LSA，最后每台路由器都有所有路由器的LSA和所有链路LSA。 距离矢量协议：例如RIP协议。它们发送的全部的周期性（默认每隔30秒）的路由更新。更新中只包括子网和各自的距离（即到达目的子网的度量值）。除了邻居路由之外，路由器不了解网络拓扑的细节（因为它之和邻居路由交换数据）。如果到相同的子网有多条路由时，路由器选择最低度量值的路由，如果度量值相同时，就都选择（如rip协议中，有时有2个最佳路由）。 自治系统AS 自治系统AS：每一个AS分配一个16位的ASN号码，中国貌似至少有几十个ASN号码，上海电信、广东电信啥的都有自己独立的ASN号码。 在互联网中，一个自治系统(AS)是一个有权自主地决定在本系统中应采用何种路由协议的小型单位。这个网络单位可以是一个简单的网络也可以是一个由一个或多个普通的网络管理员来控制的网络群体，它是一个单独的可管理的网络单元（例如一所大学，一个企业或者一个公司个体）。一个自治系统有时也被称为是一个路由选择域（routing domain）。一个自治系统将会分配一个全局的唯一的16位号码，有时我们把这个号码叫做自治系统号（ASN）。 IGP/EGP IGP是内部网关协议，是一类协议的统称，其本身并不是协议。例如OSPF、RIP就属于IGP协议。 EGP同样，是外部网关协议的统称。而BGP是EGP中一个具体的协议。 RIP协议 RIP协议被设计用于使用同种技术的中型网络，因此适应于大多数的校园网和使用速率变化不是很大的连续线的地区性网络。对于更复杂的环境，一般不使用RIP协议。 RIP作为一个系统长驻进程（daemon）而存在于路由器中，负责从网络系统的其它路由器接收路由信息，从而对本地IP层路由表作动态的维护，保证IP层发送报文时选择正确的路由。同时负责广播本路由器的路由信息，通知相邻路由器作相应的修改。RIP协议处于UDP协议的上层，RIP所接收的路由信息都封装在UDP协议的数据报中，RIP在520号UDP端口上接收来自远程路由器的路由修改信息，并对本地的路由表做相应的修改，同时通知其它路由器。 RIP路由协议用“更新（UNPDATES）”和“请求（REQUESTS）”这两种分组来传输信息的。每个具有RIP协议功能的路由器每隔30秒用UDP520端口给与之直接相连的机器广播更新信息。更新信息反映了该路由器所有的路由选择信息数据库。路由选择信息数据库的每个条目由“局域网上能达到的IP地址”和“与该网络的距离”两部分组成。请求信息用于寻找网络上能发出RIP报文的其他设备。 由于RIP的路由条目中并不保存完整路径，因此一旦网络波动，以及通讯延迟，可能会产生技术到无穷的问题。RIP设置最大条数15跳，也是为了避免这个问题。 水平分割：记录每条最佳路由是从哪个邻居的哪个端口收到的，将不会再通过该端口向该邻居广播这条最佳路由。但是局限是只能在两个路由器时有效，三个及更多的路由器，仍有可能形成环。 毒化逆转：与水平分割不同，还是会通过该端口向该邻居广播这条路由，但是会设置成距离16：不可达。有可能立刻解决路由选择环路。否则，不正确的路径将在路由表中驻留到超时为止。破坏逆转的缺点是它增加了路由更新的的数据大小，且还是有可能形成环。 保持定时器法：当一条路由被删除后，一定时间内（如180s）不会再更新该路由。 触发更新法：毒化逆转将任何两个路由器构成的环路打破，但三个或更多个路由器构成的环路仍会发生，直到无穷（16）时为止。触发式更新法可加速收敛时间，它的工作原理是当某个路径的跳数改变了，路由器立即发出更新信息，不管路由器是否到达常规信息更新时间都发出更新信息。 OSPF算法 OSPF算法真的不会成环么？例如A和B，两者交替地到目的IP的延时较短，那packet不就有可能在这两台路由器之间循环么？ 海明码 （待补完） 欧拉回路/哈密顿回路 欧拉回路是经过所有边一次然后回到原点，哈密顿回路是经过所有节点一次然后回到原点，TSP旅行商问题就是哈密顿回路。 可见欧拉回路是有可能重复经过一个点的，但哈密顿回路不能重复经过一条边。 本文地址：http://xnerv.wang/some-knowledge-points-learned-in-university/","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://xnerv.wang/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"原创","slug":"原创","permalink":"https://xnerv.wang/tags/%E5%8E%9F%E5%88%9B/"},{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://xnerv.wang/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"},{"name":"排序算法","slug":"排序算法","permalink":"https://xnerv.wang/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"name":"查找算法","slug":"查找算法","permalink":"https://xnerv.wang/tags/%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%95/"},{"name":"路由协议","slug":"路由协议","permalink":"https://xnerv.wang/tags/%E8%B7%AF%E7%94%B1%E5%8D%8F%E8%AE%AE/"}]},{"title":"(MSDN) 格式规范语法：printf 和 wprintf 函数","slug":"format-specification-fields-printf-and-wprintf-functions","date":"2017-11-19T05:33:00.000Z","updated":"2023-08-21T02:24:21.525Z","comments":true,"path":"format-specification-fields-printf-and-wprintf-functions/","link":"","permalink":"https://xnerv.wang/format-specification-fields-printf-and-wprintf-functions/","excerpt":"各种 printf 函数采用格式字符串和可选参数，并生成用于输出的格式化的字符序列。 格式字符串包含零个或多个指令，这些指令是用于输出的文本字符或描述如何在输出中设置参数格式的已编码的转换规范。 本主题介绍用于对格式字符串中的转换规范进行编码的语法。 有关这些函数的列表，请参阅流 I/O。 一个转换规范由以下形式的可选和必需字段组成： %[标志][宽度][.精度][大小]类型 转换规范的每个字段都是一个用于指示特定的格式选项或转换说明符的字符或数字。 必填的类型字段指定要应用于参数的转换类型。 可选的标志、宽度和精度字段控制格式的其他方面（如前导空格或前导零、对齐方式和显示的精度）。 大小字段指定使用和转换的参数的大小。 一个基本的转换规范仅包含百分号和一个类型字符。 例如，%s 指定一个字符串转换。 若要打印百分号字符，请使用 %%。 如果百分号后跟一个没有任何意义的字符作为格式字段，则将调用无效的参数处理程序。 有关详细信息，请参阅参数验证。","text":"各种 printf 函数采用格式字符串和可选参数，并生成用于输出的格式化的字符序列。 格式字符串包含零个或多个指令，这些指令是用于输出的文本字符或描述如何在输出中设置参数格式的已编码的转换规范。 本主题介绍用于对格式字符串中的转换规范进行编码的语法。 有关这些函数的列表，请参阅流 I/O。 一个转换规范由以下形式的可选和必需字段组成： %[标志][宽度][.精度][大小]类型 转换规范的每个字段都是一个用于指示特定的格式选项或转换说明符的字符或数字。 必填的类型字段指定要应用于参数的转换类型。 可选的标志、宽度和精度字段控制格式的其他方面（如前导空格或前导零、对齐方式和显示的精度）。 大小字段指定使用和转换的参数的大小。 一个基本的转换规范仅包含百分号和一个类型字符。 例如，%s 指定一个字符串转换。 若要打印百分号字符，请使用 %%。 如果百分号后跟一个没有任何意义的字符作为格式字段，则将调用无效的参数处理程序。 有关详细信息，请参阅参数验证。 重要 为了实现安全性和稳定性，请确保转换规范字符串不是用户定义的。 例如，考虑这样一个程序，它提示用户输入名称并将输入存储在一个名为 user_name 的字符串变量中。 若要打印 user_name，请勿执行下列操作： printf( user_name ); /* Danger! If user_name contains “%s”, program will crash */` 而应执行以下操作： printf( &quot;%s&quot;, user_name ); 类型转换说明符 类型转换说明符字符指定是否要将相应的参数解释为字符、字符串、指针、整数或浮点数。 类型字符是唯一必填的转换规范字段，它出现在任何可选字段之后。 将根据相应的类型字符和可选的大小前缀对紧跟格式字符串的参数进行解释。 将通过使用 c 或 C指定字符类型 char 和 wchar_t 的转换，将通过使用 s 或 S 指定单字节和多字节或宽字符字符串，具体取决于正在使用的格式设置函数。 通过使用 c 和 s 指定的字符和字符串参数将被 printf 系列函数解释为 char 和 char*，或被 wprintf 系列函数解释为 wchar_t 和 wchar_t*。 通过使用 C 和 S 指定的字符和字符串参数将被 printf 系列函数解释为 wchar_t 和 wchar_t*，或被 wprintf 系列函数解释为 char 和 char*。 此行为是 Microsoft 专用的。 将通过使用 d、i、o、u、x 和 X 指定整数类型（如 short、int、long、long long）及其 unsigned 变体。将通过使用 a、A、e、E、f、F、g 和 G 指定浮点类型（如 float、double 和 long double）。默认情况下，除非由大小前缀进行修改，否则整数参数将被强制为 int 类型，浮点参数将被强制为 double。 在 64 位系统上，int 是 32 位的值；因此，确定 64 位整数的输出格式时，将把它截断，除非使用 ll 或 I64 的大小前缀。 由 p 指定的指针类型使用平台的默认指针大小。 备注 与 printf 和 wprintf 函数一起使用时，Z 类型字符以及 c、C、s 和 S 类型字符的行为是 Microsoft 扩展。 在所有的格式设置函数中，ISO C 标准始终对窄字符和字符串使用 c 和 s，而对宽字符和字符串使用 C 和 S。 类型字段字符 类型字符 参数 输出格式 c 字符 与 printf 函数一起使用时，指定单字节字符；与 wprintf 函数一起使用时，指定宽字符。 C 字符 与 printf 函数一起使用时，指定宽字符；与 wprintf 函数一起使用时，指定单字节字符。 d 整数 带符号十进制整数。 i 整数 带符号十进制整数。 o 整数 无符号八进制整数。 u 整数 无符号十进制整数。 x 整数 无符号十六进制整数；使用“abcdef.” X 整数 无符号十六进制整数；使用“ABCDEF.” e 浮点 有符号的值，形式为 [-]d.dddde±dd[d]，其中 d 是一个十进制数，dddd 是一个或多个十进制数（具体取决于指定的精度），或为默认的六个数，dd[d] 是两个或三个十进制数（具体取决于输出格式和指数大小）。 E 浮点 与 e 格式相同，只不过指数由 E 引入，而不是由 e 引入。 f 浮点 有符号的值，形式为 [-]dddd.dddd，其中 dddd 是一个或多个十进制数。 小数点前的数字位数取决于数字的度量值，小数点后的数字位数取决于请求的精度，或为默认的六位数。 F 浮点 与 f 格式相同，只不过 infinity 和 nan 输出为大写形式。 g 浮点 有符号的值将显示为 f 或 e 格式，取其中对于给定的值和精度更为精简一个。 仅当值的指数小于 -4 或大于等于 precision 参数时，才使用 e 格式。 截去尾随零，仅当后跟一个或多个数字时，才会显示小数点。 G 浮点 与 g 格式相同，只不过指数由 E 引入，而不是由 e 引入（如果适用）。 a 浮点 有符号的十六进制双精度浮点值，形式为 [-]0x_h.hhhh_p±dd，其中 h.hhhh 是尾数的十六进制数（使用小写字母），dd 是一位指数或多位指数。 精度指定此点后的数字位数。 A 浮点 有符号的十六进制双精度浮点值，形式为 [-]0X_h.hhhh_P±dd，其中 h.hhhh 是尾数的十六进制数（使用大写字母），dd 是一位指数或多位指数。 精度指定此点后的数字位数。 n 指向整数的指针 目前成功写入流或缓冲区的字符数。 此值存储在地址作为自变量的整数中。 可通过参数大小规范前缀控制指向的整数的大小。 n 说明符默认为禁用；请参阅重要的安全说明了解相关信息。 p 指针类型 将自变量显示为十六进制数中的地址。 s 字符串 与 printf 函数一起使用时，指定单字节或多字节字符串；与 wprintf 函数一起使用时，指定宽字符字符串。 将于第一个空字符之前或达到精度值时显示字符。 S 字符串 与 printf 函数一起使用时，指定宽字符字符串；与 wprintf 函数一起使用时，指定单字节或多字节字符串。 将于第一个空字符之前或达到精度值时显示字符。 Z ANSI_STRING 或 UNICODE_STRING 结构 将 ANSI_STRING 或 UNICODE_STRING 结构的地址作为参数传递时，会显示包含在由结构的 Buffer 字段指向的缓冲区中的字符串。 使用 w 的大小修饰符前缀指定 UNICODE_STRING 参数，例如 %wZ。 结构的 Length 字段必须设置为字符串的长度（以字节为单位）。 结构的 MaximumLength 字段必须设置为缓冲区的长度（以字节为单位）。 通常情况下，Z 类型字符仅在使用转换规范的驱动程序调试函数（如 dbgPrint 和 kdPrint）中使用。 | 从 Visual Studio 2015 开始，如果对应浮点转换说明符（a、A、e、E、f、F、g、G）的参数为无穷大、不定或 NaN，格式化的输出则符合 C99 标准。 下表列出了格式化的输出： 值 输出 infinity inf 静默 NaN nan 信号 NaN nan(snan) 不定 NaN nan(ind) 可能以符号作为其中任何一个值的前缀。 如果浮点类型转换说明符字符是一个大写字母，则输出也将使用大写字母格式。 例如，如果格式说明符是 %F而不是 %f，则 infinity 的格式将被设置为 INF，而不是 inf。 scanf 函数也可以分析这些字符串，使这些值可以通过 printf 和 scanf 函数进行往返。 在 Visual Studio 2015 之前，CRT 使用一种不同的非标准格式作为无穷大、不定和 NaN 值的输出： 值 输出 + 无穷 1.#INF 随机数字 - 无穷 -1.#INF 随机数字 不定（与静默 NaN 相同） 数字 .#IND 随机数字 NaN 数字 .#NAN 随机数字 其中任何一种都可能已采用符号作为前缀并且格式设置也可能略有不同，具体取决于字段宽度和精度，有时会起到不寻常的作用。 例如，printf(&quot;%.2f\\n&quot;, INFINITY) 可以打印 1.#J，因为 #INF 会“四舍五入”到 2 位数的精度。 备注 如果与 %s 或 %S 对应的参数，或与 %Z 对应的参数的 Buffer 字段为空指针，则将显示“(null)”。 备注 在所有的指数格式中，要显示的指数的位数最少为两位，仅在必要时使用三位。 通过使用 _set_output_format 函数，可以将显示的数字位数设置为三位，以确保与为 Visual Studio 2013 及更早版本编写的代码的后向兼容性。 重要 %n 格式在本质上是不安全的，因此它默认处于禁用状态。 如果在格式字符串中遇到 %n，则将调用无效的参数处理程序，如参数验证中所述。 若要启用 %n 支持，请参阅 _set_printf_count_output。 标志指令 转换规范中的第一个可选字段包含标志指令、零个或多个标志字符，用于指定输出对齐方式以及控制符号、空白、前导零、小数点以及八进制和十六进制前缀的输出。 转换规范中可能会出现多个标志指令，并且标志字符可能会按任意顺序出现。 标志字符 Flag 含义 默认 - 在给定的字段宽度内左对齐结果。 右对齐。 + 如果输出值为有符号的类型，则在该值前使用符号（+ 或 -）作为前缀。 只对有符号的负值 (-) 显示符号。 0 如果将 0 作为宽度的前缀，则会在达到最小宽度前添加前导零。 如果 0 和 - 同时出现，0 则将被忽略。 如果为整数格式（i、u、x、X、o、d）指定了 0，并且还存在精度规范（例如 %04.d），0 则将被忽略。 如果为 a 或 A 浮点格式指定了 0，则会在 0x 或 0X 前缀后，在尾数前追加前导零。 不填充。 空白 (’ ') 如果输出值为有符号的正值，则使用空白作为其前缀。 如果空白和 + 标志同时出现，空白则将被忽略。 没有显示空白。 # 与 o、x 或 X 格式一起使用时，# 标志将分别使用 0、0x 或 0X 作为任何非零输出值的前缀。 没有显示空白。 与e、E、f、F、a 或 A 格式一起使用时，# 标志将强制输出值包含小数点。 仅当小数点后紧跟数字时，才会显示小数点。 与 g 或 G 格式一起使用时，# 标志将强制输出值包含小数点，并阻止截断尾随零。 与 c、d、i、u 或 s 一起使用时，则将被忽略。 | 仅当小数点后紧跟数字时，才会显示小数点。 尾随零将被截断。 | 宽度规范 在转换规范中，可选宽度规范字段出现在任何标志字符之后。 宽度参数是控制输出的最小字符数量的非负十进制整数。 如果输出值中的字符数小于指定宽度，则将在值的左侧或右侧添加空白（具体取决于是否指定了左对齐标志 (-)），直到达到最小宽度为止。 如果 0 作为宽度的前缀，则将向整数或浮点转换添加前导零，直到达到最小宽度为止，但转换到 infinity 或 NaN 时除外。 宽度规范永远不会导致值被截断。 如果输出值中的字符数大于指定宽度，或如果未提供宽度，则将根据精度规范输出值中的所有字符。 如果宽度规范是一个星号 (*)，则参数列表中的 int 参数将提供此值。 宽度参数必须先于在参数列表中要设置其格式的值，如以下示例中所示： 1printf(&quot;%0*f&quot;, 5, 3); /* 00003 is output */ 转换规范中缺少宽度值或此值较小将不会导致截断输出值。 如果转换结果的宽度大于宽度值，则字段将扩展以包含转换结果。 精度规范 在转换规范中，第三个可选字段是精度规范。 它包含一个句点 (.)，后跟一个非负十进制整数，指定字符串字符数、小数位数或要输出的有效数字位数，具体取决于转换类型. 与宽度规范不同的是，精度规范可能导致输出值截断或浮点值舍入。 如果将精度指定为 0 并且要转换的值为 0，则结果为无字符输出，如以下示例中所示： 1printf( &quot;%.0d&quot;, 0 ); /* No characters output */ 如果精度规范是一个星号 (*)，则参数列表中的某个 int 参数将提供此值。 在参数列表中，精度参数前必须先于要设置其格式的值，如以下示例中所示： 1printf( &quot;%.*f&quot;, 3, 3.14159265 ); /* 3.142 output */ 如果省略精度，则类型字符将决定精度的解释或默认精度，如下表中所示。 精度值如何影响类型 类型 含义 默认 a、A 精度指定此点后的数字位数。 默认精度为 13。 如果精度为 0，除非使用了 # 标志，否则不会打印小数点。 c、C 精度不产生任何影响。 打印字符。 d、i、o、u、x、X 精度指定要打印的最小数字位数。 如果参数中的数字位数小于精度，则将在输出值的左侧使用零进行填充。 数字位数超过精度时，值将不会被截断。 默认精度为 1。 e、E 精度指定此小数点后要打印的数字位数。 打印的最后一位数舍入。 默认精度为 6。 如果精度为 0，或者如果句点 (.) 后面不跟数字，则不会打印小数点。 f、F 精度值指定此小数点后的数字位数。 如果出现小数点，则在它之前至少会显示一个数字。 该值舍入为适当数量的数字。 默认精度为 6。 如果精度为 0，或者如果句点 (.) 后面不跟数字，则不会打印小数点。 g、G 精度指定打印的最大有效位数。 打印六个有效位数，并且任何尾随零都会被截断。 s、S 精度指定要打印的最大字符数。 不会打印超过精度的字符。 在遇到 null 字符之前不会打印字符。 参数大小规范 在转换规范中，大小字段是类型转换说明符的参数长度修饰符。 大小字段作为类型字段（hh、h、j、l（小写的 L）、L、ll、t、w、z、I（大写的 i）、I32 和 I64）的前缀，根据它们修饰的转换说明符，指定对应参数的“大小”（长型或短型、32 位或 64 位、单字节字符或宽字符）。 这些大小前缀在 printf 和 wprintf 系列函数中与 类型 字符一起使用，以指定参数大小的解释（如下表中所示）。 大小字段对于某些参数类型是可选的。 未指定任何大小前缀时，格式化程序使用整数参数（例如，有符号或无符号的 char、short、int、long 和枚举类型）作为 32 位 int 类型，而使用 float、double 和 long double 浮点参数作为 64 位 double 类型。 这与变量自变量列表的默认自变量提升规则相匹配。 有关自变量提升的详细信息，请参阅后缀表达式中的“省略号和默认自变量”。 在 32 位和 64 位系统上，64 位整数参数的转换规范必须包含 ll 或 I64 大小前缀。 否则，格式化程序的行为是不明确的。 某些类型在 32 位和 64 位代码中具有不同大小。 例如，size_t 在针对 x86 编译的代码中是 32 位长，而在针对 x64 编译的代码中是 64 位。 若要为宽度可变的类型创建与平台无关的格式设置代码，可以使用宽度可变的参数大小修饰符。 或者，使用 64 位参数大小修饰符，并将宽度可变的参数类型显式提升为 64 位。 特定于 Microsoft 的 I（大写的 i）参数大小修饰符可处理宽度可变的整数参数，但我们建议使用特定于类型的 j、t 和 z 修饰符以确保可移植性。 printf 和 wprintf 格式类型说明符的大小前缀 若要指定 使用前缀 及类型说明符 char unsigned char hh d、i、o、u、x 或 X short int short unsigned int h d、i、o、u、x 或 X __int32 unsigned __int32 I32 d、i、o、u、x 或 X __int64 unsigned __int64 I64 d、i、o、u、x 或 X intmax_t uintmax_t j 或 I（大写的 i） d、i、o、u、x 或 X long double l（小写的 L）或 L a、A、e、E、f、F、g 或 G long int long unsigned int l（小写的 L） d、i、o、u、x 或 X long long int unsigned long long int ll（小写的 LL） d、i、o、u、x 或 X ptrdiff_t t 或 I（大写的 i） d、i、o、u、x 或 X size_t z 或 I（大写的 i） d、i、o、u、x 或 X 单字节字符 h c 或 C 宽字符 l（小写的 L）或 w c 或 C 单字节字符串 h s、S 或 Z 宽字符字符串 l（小写的 L）或 w s、S 或 Z intmax_t、uintmax_t、ptrdiff_t 和 size_t 类型在 32 位平台上为 __int32 或 unsigned __int32，在 64 位平台上为 __int64 或 unsigned __int64。 I（大写的 i）、j、t 和 z 大小前缀采用平台的正确参数宽度。 在 Visual C++ 中，虽然 long double 是互异的类型，但是它具有与 double 相同的内部表示形式。 hc 或 hC 类型说明符与 printf 函数中的 c 以及 wprintf 函数中的 C 是同义的。 lc、lC、wc 或 wC 类型说明符与 printf 函数中的 C 以及 wprintf 函数中的 c 是同义的。 hs 或 hS 类型说明符与 printf 函数中的 s 以及 wprintf 函数中的 S 是同义的。 ls、lS、ws 或 wS 类型说明符与 printf 函数中的 S 以及 wprintf 函数中的 s 是同义的。 备注 I（大写的 i）、I32、I64 和 w 参数大小修饰符前缀是 Microsoft 扩展，且不符合 ISO C。 h 前缀（在与 char 类型的数据一起使用时）和 l 前缀（在与 double 类型的数据一起使用时）是 Microsoft 扩展。 另请参阅 printf、_printf_l、wprintf、_wprintf_l printf_s、_printf_s_l、wprintf_s、_wprintf_s_l printf_p 位置参数 本文地址：http://xnerv.wang/format-specification-fields-printf-and-wprintf-functions/ 转载自：(MSDN) Format Specification Syntax: printf and wprintf Functions","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://xnerv.wang/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"C++","slug":"C","permalink":"https://xnerv.wang/tags/C/"},{"name":"MSDN","slug":"MSDN","permalink":"https://xnerv.wang/tags/MSDN/"},{"name":"Programing","slug":"Programing","permalink":"https://xnerv.wang/tags/Programing/"}]},{"title":"(MSDN) 格式规范字段：scanf 和 wscanf 函数","slug":"format-specification-fields-scanf-and-wscanf-functions","date":"2017-11-19T05:30:00.000Z","updated":"2023-08-21T02:24:21.500Z","comments":true,"path":"format-specification-fields-scanf-and-wscanf-functions/","link":"","permalink":"https://xnerv.wang/format-specification-fields-scanf-and-wscanf-functions/","excerpt":"此处的信息适用于整个 scanf 函数系列（包括安全版本），并描述了用于告诉 scanf 函数如何将输入流（如 stdin 的输入流 scanf）分析为插入到程序变量中的值。 格式规范具有以下形式： %[*] [width] [{h | l | ll | I64 | L}]type","text":"此处的信息适用于整个 scanf 函数系列（包括安全版本），并描述了用于告诉 scanf 函数如何将输入流（如 stdin 的输入流 scanf）分析为插入到程序变量中的值。 格式规范具有以下形式： %[*] [width] [{h | l | ll | I64 | L}]type format 参数指定输入的解释，并且可以包含以下一项或多项： 空白字符：空白 (’ ‘)；制表符 (’\\t’)；或换行符 (’\\n’)。 空白字符将使得 scanf 读取（但不存储）输入中所有连续的空白字符，直至下一个非空白字符。 格式中的一个空白字符与输入中的任何数字（包括 0）和空白字符的组合相匹配。 百分号 (%) 除外的非空白字符。 非空白字符将使得 scanf 读取（但不存储）匹配的非空白字符。 如果输入流中的下一个字符不匹配，则 scanf 将会终止。 由百分号 (%) 引入的格式规范。 格式规范将使得 scanf 读取输入中的字符并将其转换为指定类型的值。 该值将分配给参数列表中的一个参数。 格式为从左向右读取。 不符合格式规范的字符应匹配输入流中的字符序列；将扫描但不存储输入流中的匹配字符。 如果输入流中的字符与格式规范冲突，则 scanf 将终止，并且将该字符留在输入流中，就像没有读取过它一样。 遇到第一个格式规范时，第一个输入字段的值将根据此规范进行转换并储存在第一个 argument 指定的位置中。 第二个格式规范将使得第二个输入字段进行转换并储存在第二个 argument 中，依此类推，直至格式字符串的末尾。 将输入字段定义为第一个空白字符（空格、制表符或换行符）之前的所有字符，或第一个无法根据格式规范转换的字符之前的所有字符，或在达到最大字段宽度（如果已指定）之前的所有字符。 如果给定的规范有太多参数，则将计算但忽略多余的参数。 如果格式规范没有足够参数，则结果不可预知。 格式规范的每个字段是一个用于指定特定格式选项的字符或数字。 位于最后一个可选格式字段之后的 type 字符决定将输入字段解释为字符、字符串还是数字。 最简单的格式规范仅包含百分号和一个 type 字符（例如，%s）。 如果百分号 (%) 后跟一个没有意义的字符作为格式控制字符，则该字符及其后面的字符（直至下一个百分号）将被视为普通字符序列，即必须与输入匹配的字符序列。 例如，若要指定要输入的百分号字符，请使用 %%。 百分号后面的星号 (*) 将取消下一个输入字段的分配（将被解释为指定类型的字段）。 将扫描但不储存该字段。 _s 函数系列的安全版本（具有 scanf 后缀的版本）需要在每个 c、C、s、S 或 [ 类型的参数之后立即传入一个缓冲区大小参数。 有关 scanf 系列函数的安全版本的更多信息，请参阅 scanf_s、_scanf_s_l、wscanf_s、_wscanf_s_l。 另请参阅 scanf 宽度规范 scanf 类型字段字符 scanf、_scanf_l、wscanf、_wscanf_l scanf_s、_scanf_s_l、wscanf_s、_wscanf_s_l 本文地址：http://xnerv.wang/format-specification-fields-scanf-and-wscanf-functions/ 转载自：(MSDN) Format Specification Fields: scanf and wscanf Functions","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://xnerv.wang/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"C++","slug":"C","permalink":"https://xnerv.wang/tags/C/"},{"name":"MSDN","slug":"MSDN","permalink":"https://xnerv.wang/tags/MSDN/"},{"name":"Programing","slug":"Programing","permalink":"https://xnerv.wang/tags/Programing/"}]},{"title":"(MSDN) Give Me a Handle, and I'll Show You an Object","slug":"msdn-give-me-a-handle-and-i-ll-show-you-an-object","date":"2017-11-14T22:09:00.000Z","updated":"2023-08-21T02:24:20.104Z","comments":true,"path":"msdn-give-me-a-handle-and-i-ll-show-you-an-object/","link":"","permalink":"https://xnerv.wang/msdn-give-me-a-handle-and-i-ll-show-you-an-object/","excerpt":"Ruediger Asche Microsoft Developer Network Technology Group Created: July 15, 1993 Abstract This article discusses objects and handles under Microsoft® Windows™ version 3.1 and Windows NT™. The terms objects and handles have different meanings in each operating system and imply a variety of relationships between objects and their corresponding handles. This article elaborates on the differences between the way components of Windows NT treat objects as opposed to the way Windows 3.1 treats them, with particular emphasis on shareability issues.","text":"Ruediger Asche Microsoft Developer Network Technology Group Created: July 15, 1993 Abstract This article discusses objects and handles under Microsoft® Windows™ version 3.1 and Windows NT™. The terms objects and handles have different meanings in each operating system and imply a variety of relationships between objects and their corresponding handles. This article elaborates on the differences between the way components of Windows NT treat objects as opposed to the way Windows 3.1 treats them, with particular emphasis on shareability issues. Introduction You got a smile so bright, You know you could have been a candle. I’m holding you so tight, You know you could have been a handle. Smokey Robinson, “The Way You Do the Things You Do” One of the masterpieces of contemporary American literature, Webster’s New World Dictionary (Paperback Edition), defines a handle as “that part of a tool, etc., by which it is held or lifted.” Now, is this a precise definition or what? What on earth is etc.? By intuitive knowledge, we know that etc. can cover anything from toothbrushes to tennis rackets to cookware. While I leave it up to the imagination of the reader to figure out what Smokey Robinson wanted to hold or lift, I will tell you right away what we will be handling in this article: objects. Object is one of the major buzzwords of today’s computer reality. You find objects everywhere, from object-oriented languages to object linking and embedding. Microsoft® Windows™ has used the term Object for a long time to describe several very different things. To further confuse the issue, things that are not explicitly labeled as objects behave the same way as some objects do. Windows NT™ is essentially an object-centered operating system and has a very definite notion of what an object is. Unfortunately, Windows NT’s idea of an object is not related at all to the concept that 16-bit Windows has of an object; as a result, when writing a Win32®-based application for Windows NT, you will need to deal with even more “object types,” which will make the confusion about objects potentially even greater. This article untangles the different meanings of the term Object and the related term handle, and makes you aware of potential incompatibilities between applications written for Windows version 3.1 and for Win32. We display the entire object and handle layout of Windows NT in the Appendix at the end of this article. Please refer to it as necessary as you read this article. One more note: For the sake of this discussion, we will entirely leave out C++ and Microsoft Foundation Class Library objects. Although some Foundation class objects are directly mapped to objects provided by the operating system, the way Foundation class objects are referenced, maintained, and protected is exclusively a compiler issue. In this article we will focus on objects that are provided and maintained on the operating system level. This article is not meant to be a supplement to the documentation of Windows NT. All that is described herein applies to version 3.1 of Windows NT and may change in future versions of the operating system. Please be aware that applications written for the Win32 API are meant to be portable between all platforms that support this API, and any assumptions about a particular implementation (such as the relationships between handles and objects as described in this article) will break applications that operate under such assumptions. Objects in 16-Bit Windows The idea of an object that Windows 3.1 introduced was roughly “something that is accessed through a handle.” In Windows 3.1, you create the object—a process that returns a magic cookie called a handle—and subsequently use the handle to access the object. This ensures that the actual implementation of the object can be changed without affecting the application. If you have used objects and handles exactly like this, you are well off, and you will find that you will need to change almost nothing when porting your application from Windows 3.1 to 32-bit Windows NT. However, many developers for Windows 3.1 made assumptions about the relationship between handles and their objects—sometimes they had to because application programming interfaces (APIs) were missing for certain operations. Such assumptions will break applications under Windows NT. Maybe you have used GlobalHandle to retrieve the handle of any given address and subsequently use the handle to reallocate the object? Works like a charm under 16-bit Windows. Maybe you have extracted the HIWORD of a given pointer and converted it into a handle, or peeked into memory without doing a GlobalLock using that handle? Maybe you were bold, retrieving the base of the local descriptor table and looking at descriptors by using the handle as an offset into the table? Beware, it’s these kinds of things that will leave you with major redesign work when you try to port your application. Such things used to work under 16-bit Windows, mainly due to two widely known facts: First, objects under 16-bit Windows generally fall into two categories: objects that are stored in GDI’s and USER’s local heaps, and objects that are allocated from the global heap (that is, Windows internally calls GlobalAlloc to allocate memory for that object). Second, the handles that belong to global heap objects are, in fact, selectors to those global memory segments, which implies an immediate relationship between the handle and its memory location. For example, there is no API under Windows 3.1 that returns the size of a resource, given its handle. No problem; because resources are allocated from the global heap, you can eventually use all of the global heap functions to party on the resources. You can call GlobalPageLock if, for whatever reason, you wish to permanently lock the resource; you can call GlobalSize to figure out its current size; you can GlobalRealloc it if you wish to dynamically change its size; and you can even GlobalLock it to write to or read from the resource data directly. The entities that are explicitly labeled as objects under Windows 3.1 are GDI and USER objects. GDI objects are brushes, pens, bitmaps, fonts, palettes, and regions; USER objects are window classes, atoms, menus, and windows. Handles for those objects are, in fact, pointers into the default data segments of the GDI and USER modules, respectively. A relationship between the handles and the objects is implied in that the handles, when converted to pointers into USER’s and GDI’s default data segment, point to data structures that describe the objects internally. Unfortunately, the API sets that treat those objects are fairly inconsistent. Although the Create_xxx_ and corresponding DeleteObject APIs were originally designed to provide a uniform Create/Use/Delete sandwich for all GDI object types, not even palettes could be incorporated into the paradigm when they were introduced with Windows 3.0; SelectObject will not work on them. On the other hand, walking GDI’s local heap will reveal that palettes are stored in there the same way as brushes, pens, bitmaps, fonts, device contexts, and regions are; thus, although they are stored internally the same way, they have different operations that work on them. One of the corollaries of the design of Windows 3.1 is that all objects are globally accessible. Because only one local descriptor table (LDT) is being used for all Windows-based applications, all global handles (that is, selectors) can be used by all applications and DLLs; and because all GDI and USER objects are references into the globally accessible GDI and USER default data segments, they can be seen and used by all applications as well. This implementation detail is widely used by many applications, be it explicitly (for example, in order to share memory) or implicitly (for example, in order to enumerate windows or fonts). One of the unfortunate consequences of this design is that any application can (deliberately or inadvertently) modify or possibly destroy objects that belong to other applications. There is no protection mechanism against these kinds of practices. (“The Way We Used to Do the Things We Did.”) Object Management Under Windows NT Windows NT has a rather precise and orthogonal idea of what an object is. Windows NT has an API set that works on all kinds of objects; handles are maintained by the executive via a consistent mechanism that allows for secure access to objects, supports synchronization, and rejects attempts to delete objects that are still in use. The objects that Windows NT knows about are processes, threads, files (including devices that look like files, such as communication devices, mail slots, or pipes), file mappings (or sections, as they are labeled on the executive level), events, semaphores, and mutexes. All of those object types have certain properties in common that we will elaborate on a bit later. You can see that none of the things that Windows 3.1 calls objects (hereafter called “old objects”) are covered by the term objects in Windows NT. The equivalents of Windows 3.1 “old objects” are not implemented on the executive level under Windows NT, so next we’ll look a little bit more closely at how the Win32 subsystem implements “old objects.” In other words, a window or a brush or a pen does not have anything to do with a Windows NT object, and the Windows NT executive would be totally clueless if it were passed a handle to one of those objects. How the Subsystem Realizes Windows GDI Objects Let us begin with GDI objects. Under Windows 3.1, GDI objects are stored in GDI’s default data segment. They are designed to be global; that is, one application can create an object and pass it on to another application. There is no ownership of GDI objects in 16-bit Windows; thus, one application can destroy a GDI object that you created in another application. Under Windows NT, GDI objects are stored in the client part of the Win32 subsystem’s GDI module. That means that any GDI object is valid only in the context of the application that created it. However, consider this: Create an object and pass its handle to another application. Let this other application call SelectObject on that newly created object, and the call may succeed! However, nothing might happen; or instead of a brush, a pen might be selected into the device context! What is happening? Well, what happens here is that handles to GDI objects are internally implemented as offsets into a handle table that resides on the client side of the Win32 subsystem. (Remember that the Win32 client is a DLL that resides in a Win32-based application’s address space and is called by the application.) In other words, handle tables are kept on a per-process basis, but they are not process-tagged. That means that a handle to an object that belongs to process A might coincidentally look like a valid handle in process B’s context. Thus, a call to SelectObject from B might succeed, but B will actually have selected a totally different object into its device context—or worse, the right one. Selecting the right object may be worse because the objects might coincidentally be the same, so you think it works, but the application will behave weirdly later on. So, do not pass handles to GDI objects between applications; they have totally different meanings in different processes. Actually, the picture is even more complex than that. Because all applications share the same physical output device, maintaining a device context cannot be left up to the client alone. Any output call will eventually end up in the server, and thus, the server has to know about its clients’ device contexts as well as their GDI objects. The subsystem divides the representation of the GDI object (including the device context) between the client and the server. For efficiency’s sake, as much work as possible is done by the client—in most cases, for example, a call to SelectObject will not make it to the server; the client will simply update an internal data structure. As soon as the server does the drawing, it knows how to retrieve and realize the objects on the client’s side. The GDI part of the Win32 subsystem has a handle manager of its own through which all accesses to GDI objects go. This is also true for USER, as we will see later. A handle to a GDI object is, in fact, made up of two components: One part encodes the offset into an object table that resides in the client part of the subsystem, and one part is a uniqueness identifier. This identifier changes—as the name implies—whenever a handle is recycled and is stored both as part of the object and as part of the handle. Validating an object involves, among other things, matching the unique part of the handle against the value in the object. If a handle is recycled, the new handle returned is different from the one that belonged to the destroyed object with the same table index. This mechanism is used to make sure that stray pointers or previously deleted object handles that are passed into a function that works on an object do not reference wrong objects. If the validation fails, an error 6, INVALID_HANDLE, is generated. In future versions of Windows NT, it may well be that this implementation detail changes. Although a handle will most likely still be composed of two 16-bit components for compatibility reasons, the interpretation of the “uniqueness” part may change due to performance considerations. You might wonder now why you don’t see the INVALID_HANDLE error when you pass a GDI object handle between applications in error. The reason is that initially the “uniqueness” value is the same for all handles in all address spaces; objects become unique only after the handle is recycled. Thus, two valid objects in different processes may very well have the same “uniqueness” value. “The Way We Do the Things We Do?” You betcha. The downside of this implementation is that it cuts down on the number of available handles. A handle has the size of a DWORD—that is, it’s 32 bits. Thus, a maximum of 232 or 4 GB of objects can be theoretically addressed by a handle. Currently, the two components of the handle—the table index and the uniqueness identifier—take up 16 bits of the handle each, such that theoretically a maximum of 64K (216) of GDI objects could be accessed per process, but the maximum size of the client’s handle table is currently hardcoded to 16K of entries. And Now for Something Completely Different: USER Objects The story of USER objects is different from GDI’s story. Under Windows 3.1, the definition of Object covers both objects internal to USER’s default data segment (windows, window classes, menus, and so on) and resources that are actually allocated from the global heap. Windows NT still maintains both object types in the USER module, but because there is no global heap anymore as there was in Windows 3.1, resources behave a little bit differently. One of the main goals in the implementation of the USER part of Windows NT’s Win32 subsystem was compatibility. Under Windows 3.1, an application has a high degree of control over USER-maintained objects. For example, through the EnumWindows function, an application for Windows 3.1 can get access to all top-level windows in the system; with the window handle, the menu of an application can be modified, messages can be monitored, device contexts obtained, and so on. In order for Windows NT to maintain this degree of control, USER objects are maintained on the server side of the Win32 subsystem, and the functions that access the objects (such as DrawIcon) need to do more work. For example, if the object were located on the client side, accessing it would mainly be a task of translating the handle into a pointer, but in the USER implementation, memory in the server process must be referenced. Just like its GDI counterpart, the Windows NT USER component keeps a handle manager that works pretty much like the one described in the GDI discussion, but the object tables are maintained on the server side of the subsystem, and therefore the object limit is global. Currently, it is 64K entries. Resources The good news is that under Windows NT, resources are not handled differently from the other USER objects at all. Under Windows 3.x, the local heaps of USER and GDI turned out to be severe system bottlenecks for object-intensive applications. If you have worked with 16-bit Windows intensively, chances are that at more than one point you have seen the free system resources decreasing severely when you worked with large menus or many windows. Windows 3.1 somewhat widened that bottleneck by making menus resources instead of objects within USER’s default data segment. Windows NT implements all USER objects orthogonally. From the user’s point of view, the major differences between resources and other objects is that at times resources need to be accessed directly, without using the API functions that operate on them. (This holds especially true for user-defined “raw” resources.) There are actually two different ways to access resources under Windows NT—one in which the resource resides in the server process and one in which it resides in the client. To exemplify the first case, let’s assume your application calls LoadIcon and passes the returned handle on to application B, which then calls DrawIcon on that handle. Guess what? Works fine. Just like under Windows 3.1. This seems to be strange at first glance because the two applications have disjoint address spaces, so how can the target application see something that the source application loaded? Note that you cannot do much more with that handle. In particular, you cannot get your hands on the memory that describes the icon because the handle has no meaning whatsoever to either your application or application B. It is a handle local to the server part of the Win32 subsystem. DrawIcon eventually executes in the server part as well, and thus it knows how to interpret the handle. In this case, the icon is a real object in the strict sense of the word: The handle is a true magic cookie without any meaning to anyone except for the operations that are allowed on it (such as DrawIcon). As a side effect of this modular approach, using this technique to access resources is fairly limited. You cannot modify the resource or look at its memory. (We will look at ways to work around this in a second.) Also, there is no way for the subsystem to determine if the object is in use by anybody but the process that owns it. In our example, the subsystem cannot know if somebody is using the icon, so as soon as your application terminates, the icon is gone from the server subsystem address space, and a subsequent attempt to call DrawIcon on it will return with an error code of ERROR_INVALID_CURSOR_HANDLE. Do not be confused about “CURSOR” in that error message—as far as USER32 is concerned, icons and cursors are basically the same; internally, they are represented as the same data structures. So how does one share resources among processes? Well, the bad news is that there is no easy way for applications to share resources dynamically at run time so that a change in the resource that one application makes appears in the other application. Aside from using the CopyIcon, CopyAcceleratorTable, and CopyCursor functions, which are especially designed to create local copies of resources, the best one can do about this is to have one application map an image of the other application’s executable into its own memory space, locate the resource in the image, and party on it. To aid this process, the LoadResource API family has been modified for Windows NT. If application HAND wants to access a resource in application FOOT, it needs to use the LoadLibrary, FindResource, and LoadResource APIs as described in the Win32 API Help file under the documentation for FindResource. If you want to permanently change resources in the target executable file, you should use the UpdateResource, BeginUpdateResource, and EndUpdateResource APIs. The trick here is that LoadLibrary will map the executable file into the address space of the process that calls it and return a pointer to that image. All that FindResource and LoadResource do is examine the executable header in that image, figure out where within the image the resource is located, and return a pointer to the resource image. The part that will probably confuse you here is that LoadLibrary also works on .EXE files. Under Windows 3.1, if you load an executable file, it will execute right away, whereas a DLL will linger in memory until it is called. LoadLibrary, LoadModule, and WinExec are very closely interrelated under Windows 3.1, but they are totally different under Windows NT, where LoadLibrary basically maps the executable file into the process’s address space. LoadLibrary also executes the DLL entry point routine when a library is loaded, but it does not execute anything when an application is loaded. The return value from WinExec, which is a stripped-down version of CreateProcess, is a handle to a Windows NT executive object, whereas an instance handle, the value returned from LoadLibrary, is a virtual pointer. Win32 Kernel Objects The preceding discussion on virtual pointers leads us right into the remaining category of objects in the Win32 subsystem—kernel objects. Now, here we run into a terminology problem: Under Windows 3.1, the component that is responsible for memory management and system services is called the “kernel.” Unfortunately, the same term is used for the component in Windows NT that executes in privileged mode at the lowest level. Windows NT renames the part of the Win32 subsystem that corresponds to the kernel in Windows 3.1 as the “base.” Thus, this section does not really deal with kernel objects, but with base objects. Most of the objects that you access through the base are, in fact, native Windows NT objects, which we will discuss in the following section. The only entity that the kernel component of Windows 3.1 dubs an object is a memory object from which a number of other object types derive, such as modules or instances; in the rest of this section, we will look at these in more detail and see how those are implemented in Windows NT. Let us elaborate on the last statement of the preceding section a little bit more. Windows 3.1 knows instance handles, task handles, and module handles. Instance handles are, in fact, selectors of the default data segments of the application instance. (This fact is sometimes used to change the data segment so that an operation can work on the default data segment of a different application—an ugly practice, but it works.) The task handle is the selector of a global memory block that contains task-specific parameters (such as the input queue that is associated with the task). The module handle is the selector of a global memory block that contains a modified version of the executable header associated with the module. (This memory block is also known as the module database.) For applications in Windows 3.1, there is only one module database in the system for all instances of the application, whereas both the task database and the default data segment exist separately for each instance of the application. The GetModuleHandle API returns the handle (selector) of the module database, whereas a call to GetWindowWord with GWW_HINSTANCE passed as the second parameter returns the instance handle, and the GetCurrentTask API returns the task database handle. For other applications, the TaskFirst/TaskNext Toolhelp API functions can be employed to retrieve the task database handles. For more information on modules, tasks, and instances under Windows 3.1, please consult the Knowledge Base articles Q76676, “Differences Between Task Handles and Instance Handles,” and Q78327, “HANDLEs Returned by GetModuleHandle and LoadLibrary,” as well as Bob Gunderson’s technical article “Modules, Instances, and Tasks.”!Alink(Knowledge Base, MSDN™ Library) Windows NT does not use task databases or module databases anymore, and the default data segment is built on top of an application’s virtual address space. The hInstance value that is being passed to an application upon startup is basically the same as the value returned from LoadLibrary: a virtual address in the application’s address space that contains the memory-mapped image of the executable file. Incidentally, GetModuleHandle(NULL) returns the same value. It is fairly easy to confuse this value with a process handle, which is something totally different (namely, a native Windows NT object) and cannot be used interchangeably with those virtual pointers. The remaining kernel object type is the heap object type, which we’ll look at now. As Randy Kath’s technical article “Managing Heap Memory in Win32” outlines, the Windows NT heap memory manager resides on top of the virtual memory manager; that is, a heap is essentially a chunk of virtual memory that consists of the heap itself and some administrative information that is used to maintain it. The handle returned from the HeapCreate API is essentially the virtual address of this memory chunk, and a call to HeapAlloc will look into the administration header and return a pointer to the memory chunk. You should never attempt to write to a memory location that belongs to the heap header because it would seriously confuse the heap manager. Also, due to the very nature of disjoint address spaces, one application’s local heap is of no relevance to other applications. Because heap handles are virtual pointers, and no record is kept of the local heaps allocated by the base part of the Win32 subsystem, there is really no limit on the number of heaps you can theoretically allocate, except for the limits that the executive imposes on the number of virtual memory chunks that an application can use. For details on virtual memory management, please refer to Randy Kath’s technical articles “The Virtual-Memory Manager in Windows NT” and “Managing Virtual Memory in Win32.” It is worthwhile mentioning here, however, that you cannot allocate more than 32K of virtual memory blocks in any process because the virtual address space of a process is limited to 2 GB, and the minimum size for each allocation is 64K; thus, the maximum number of allocations that can be made is 2G/64K, or 32K of virtual memory blocks. Note that this is different from the number of allocations that can be drawn from heaps. Because heaps themselves are virtual memory blocks, you cannot create more than 32K-worth of heaps per application, but from a given heap, you can allocate as many memory blocks as you want to. Native Windows NT Objects We will now look at native objects in Windows NT. As Helen Custer describes in depth in Inside Windows NT (Microsoft Press, 1993), the object manager is an integral and central part of Windows NT. It is responsible for accepting and processing requests to create, access, and destroy objects—both from a process and from the executive. Some of the objects that Windows NT knows about are not directly visible to applications, such as driver objects or symbolic link objects, but internally they work in exactly the same way as the aforementioned objects. In Inside Windows NT, these invisible objects are called kernel objects, as opposed to executive objects. We have already enumerated the native Windows NT objects that are visible to Win32 applications: processes, threads, files, file mappings, events, semaphores, and mutexes. We will not elaborate too much on these here because Chapter Three of Inside Windows NT already gives a comprehensive overview of these objects and how to use them. The worthwhile aspects to mention about them are the following: They can attain a signaled state so that you can call WaitForSingleObject or WaitForMultipleObjects on them. You can use DuplicateHandle to access the object from inside other processes. (Some restrictions apply! Refer to the Win32 API Help file for details.) The Windows NT executive keeps a reference count for each object and removes the object only after the last handle is closed. Access to the objects is monitored by the security subsystem. (I will cover “security and how you can make use of it” in a future article.) Most objects can be optionally named so that other processes can access them by name if security allows. Handles to executive objects are always associated with a particular process—this statement does not always hold true for kernel objects; for example, objects that describe the drivers for bootable hard drives must exist before the system has started up. The association of handles to processes is the reason you cannot use an existing handle to access an object from another process; you need to either obtain another handle using DuplicateHandle to create a handle table entry for the same object in another process or open an object by name in the other process. Unlike the handles that are maintained by the Win32 USER and GDI subsystem components, handles to native objects under Windows NT are not unique; that is, upon destruction of an object, the corresponding handle may be recycled and will look exactly like the handle to the destroyed object. There is one unintuitive consequence of this implementation: Process IDs under Windows NT are actually handles into a dummy system table—that is, a table that does not associate its handles with any objects. If you create a process and store its ID away, then after termination of the process, the ID may be recycled for other types of global objects. You therefore cannot use process IDs to uniquely identify processes. Finally, there is no hardcoded limit on the number of entries a handle table can have. A handle table maintained by the executive may grow dynamically if necessary; thus, the number of handles that can be allocated per process depends only on the memory available on your machine. (Note that Windows NT may further limit this number on a per-user basis by reducing the user’s quota on nonpageable memory.) Conclusion We have discussed several types of objects and their handles under Windows NT, and how they relate to corresponding object types under Windows 3.1. The most important result from this discussion is that any assumption about the implementation of objects and the meaning of handles is very likely to break code compatibility between platforms and should, therefore, be avoided. Another important point to mention is that the shareability of an object between processes is a matter of the implementation of the operating system and varies from object type to object type and from operating system to operating system. The table below lists the object types discussed in this article, as well as their locations and their shareability. From the point of view of implementation, a handle under Windows NT is always realized in one of two categories: An offset into a table that is being maintained by either the executive or a subsystem component. A virtual address in the address space of a process. The concept that makes this implementation possible is the memory-mapped file (or section object). Whereas Windows 3.1 needs to copy data from an executable file into main memory to build code segments or load resources, Windows NT only needs to create a memory-mapped file object that is backed by the executable file instead of the system pagefile. Thus, a resource can be retrieved by calculating a pointer into the memory-mapped file object that represents the executable. The USER part of the Win32 subsystem uses this technique to implement both methods of resource access: A call to LoadIcon, for example, will map the executable into the server’s address space so that a subsequent call to DrawIcon will retrieve the icon data from the memory-mapped image, whereas LoadLibrary maps the executable into the client’s address space so that FindResource and LoadResource will retrieve the data from there (which is why resources retrieved this way will not have any meaning to other processes). Also, it is important to emphasize the disjoint API sets that are available for the different types of objects. You cannot call DeleteObject on a resource or an executive object; conversely, CloseHandle will fail on GDI objects and resources. A good way to look at it is to classify each type of object as a member of something one could call an “object type class,” a category of objects that are roughly characterized by being maintained by the same component of Windows NT. Following this approach, there are the following object type classes in Windows NT: Executive objects (processes, threads, sections, file objects, events, semaphores, and mutexes) Win32 GDI objects (pens, brushes, fonts, palettes, regions, device contexts, bitmap headers) Win32 USER objects WIN32 resources (accelerator tables, bitmap resources, dialog box templates, font resources, menu resources, raw data resources, string table entries, message table entries, cursors/icons) Other USER objects (windows, menus) Win32 base objects (heaps) Summary of Object Properties Executive objects Win32 base objects Win32 GDI objects Win32 USER objects Shareable Via APIs (DuplicateHandle or Open_xxx_) No No Yes Validated Yes No Yes, by uniqueness Yes, by uniqueness Limit Only by physical memory 32K of heaps 16K per process 64K systemwide (including resources and internal types NoteShareable indicates whether an object of that type can be accessed by several processes. Validated indicates whether the handle protects the object against invalid access to it, possibly due to an invalid handle. Appendix In this diagram, three processes are running in a particular Windows NT session—the Win32 subsystem server process and two clients (that is, Win32-based applications). Note that by design of Windows NT, all processes’ virtual addresses over 2 GB are mapped globally (that is, are mapped to the same physical addresses in all processes). Use these notes to interpret the numbers in the diagram: An instance handle is a pointer to the image of an executable file in a client process. A resource handle as obtained by FindResource and LoadResource is a pointer to that process within the image. A handle returned from VirtualAlloc or HeapCreate is a pointer to the beginning of the memory block in the client’s address space. A handle returned from HeapAlloc is a pointer into the chunk of memory allocated by 3. A GDI handle is a relative offset into a table located in the client’s address space. A USER handle is a relative offset into a table located in the server’s address space. A handle to a native Windows NT object is a relative offset into a table located in system space. There are several of those tables—one per process and a few tables maintained by the system. A USER object itself is located in the server’s address space. In the case of resources, 8 still holds true, but just as in 2, the resource is referenced through a memory-mapped image of the file that holds the executable, only the image resides in the server’s address space this time. This is the scenario you encounter, for example, when calling LoadIcon. The data structures that describe native Windows NT objects reside in system address space. Depending on the object type, part of the object may also be located in a process’s address space. (This holds true, for example, for section objects.) The data that describes a GDI object resides in the client’s address space. Please also observe the restriction mentioned earlier in this article under “How the Subsystem Realizes Window GDI Objects.” Article link: http://xnerv.wang/msdn-give-me-a-handle-and-i-ll-show-you-an-object/ Reprinted from: (MSDN) Give Me a Handle, and I’ll Show You an Object","categories":[{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/categories/Windows/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/tags/Windows/"},{"name":"MSDN","slug":"MSDN","permalink":"https://xnerv.wang/tags/MSDN/"},{"name":"Memory Management","slug":"Memory-Management","permalink":"https://xnerv.wang/tags/Memory-Management/"}]},{"title":"(MSDN) Managing Virtual Memory","slug":"msdn-managing-virtual-memory","date":"2017-11-14T21:58:00.000Z","updated":"2023-08-21T02:24:20.221Z","comments":true,"path":"msdn-managing-virtual-memory/","link":"","permalink":"https://xnerv.wang/msdn-managing-virtual-memory/","excerpt":"Randy Kath Microsoft Developer Network Technology Group Created: January 20, 1993 Abstract Determining which function or set of functions to use for managing memory in your application is difficult without a solid understanding of how each group of functions works and the overall impact they each have on the operating system. In an effort to simplify these decisions, this technical article focuses on the virtual memory management functions: which ones are available, how they are used, and how their use affects the operating system. The following topics are discussed in this article: Reserving, committing, and freeing virtual memory Changing protection on pages of virtual memory Locking pages of virtual memory Querying a process’s virtual memory A sample application called ProcessWalker accompanies this technical article on the Microsoft Developer Network CD. This sample application is useful for exploring the virtual address space of a process. It also employs the use of virtual memory functions for implementing a linked list structure.","text":"Randy Kath Microsoft Developer Network Technology Group Created: January 20, 1993 Abstract Determining which function or set of functions to use for managing memory in your application is difficult without a solid understanding of how each group of functions works and the overall impact they each have on the operating system. In an effort to simplify these decisions, this technical article focuses on the virtual memory management functions: which ones are available, how they are used, and how their use affects the operating system. The following topics are discussed in this article: Reserving, committing, and freeing virtual memory Changing protection on pages of virtual memory Locking pages of virtual memory Querying a process’s virtual memory A sample application called ProcessWalker accompanies this technical article on the Microsoft Developer Network CD. This sample application is useful for exploring the virtual address space of a process. It also employs the use of virtual memory functions for implementing a linked list structure. Introduction This is one of three related technical articles—“Managing Virtual Memory,” “Managing Memory-Mapped Files,” and “Managing Heap Memory”—that explain how to manage memory in applications for Windows. In each article, this introduction identifies the basic memory components in the Windows programming model and indicates which article to reference for specific areas of interest. The first version of the Microsoft Windows operating system introduced a method of managing dynamic memory based on a single global heap, which all applications and the system share, and multiple, private local heaps, one for each application. Local and global memory management functions were also provided, offering extended features for this new memory management system. More recently, the Microsoft C run-time (CRT) libraries were modified to include capabilities for managing these heaps in Windows using native CRT functions such as malloc and free. Consequently, developers are now left with a choice—learn the new application programming interface (API) provided as part of Windows or stick to the portable, and typically familiar, CRT functions for managing memory in applications written for Windows. The Windows API offers three groups of functions for managing memory in applications: memory-mapped file functions, heap memory functions, and virtual memory functions. Figure 1. The Windows API provides different levels of memory management for versatility in application programming. In all, six sets of memory management functions exist in Windows, as shown in Figure 1, all of which were designed to be used independently of one another. So, which set of functions should you use? The answer to this question depends greatly on two things: the type of memory management you want and how the functions relevant to it are implemented in the operating system. In other words, are you building a large database application where you plan to manipulate subsets of a large memory structure? Or maybe you’re planning some simple dynamic memory structures, such as linked lists or binary trees? In both cases, you need to know which functions offer the features best suited to your intention and exactly how much of a resource hit occurs when using each function. Table 1 categorizes the memory management function groups and indicates which of the three technical articles in this series describes each group’s behavior. Each technical article emphasizes the impact these functions have on the system by describing the behavior of the system in response to using the functions. Table 1. Memory Management Functions Memory set System resource affected Related technical article Virtual memory functions A process’ virtual address spaceSystem pagefileSystem memoryHard disk space “Managing Virtual Memory” Memory-mapped file functions A process’s virtual address spaceSystem pagefileStandard file I/OSystem memoryHard disk space “Managing Memory-Mapped Files” Heap memory functions A process’s virtual address spaceSystem memoryProcess heap resource structure “Managing Heap Memory” Global heap memory functions A process’s heap resource structure “Managing Heap Memory” Local heap memory functions A process’s heap resource structure “Managing Heap Memory” C run-time reference library A process’s heap resource structure “Managing Heap Memory” Windows Memory System Overview Windows employs a page-based virtual memory system that uses linear addressing. Internally, the system manages all memory in segments called pages. Each page of physical memory is backed by either a pagefile for volatile pages of memory or a disk file for read-only memory pages. There can be as many as 16 separate pagefiles at a time. Code, resources, and other read-only data are backed directly by the files from which they originated. Windows NT provides an independent, 2 gigabyte (GB) user address space for each application (process) in the system. To the application, it appears that there is 2 GB of memory available, regardless of the amount of physical memory that is actually available. When an application requests more memory than is available, Windows NT satisfies the request by paging noncritical pages of memory—from this and/or other processes—to a pagefile and freeing those physical pages of memory. Conceptually, the global heap no longer exists in Windows NT. Instead, each process has a private 32-bit address space from which all of the memory for the process is allocated—including code, resources, data, DLLs (dynamic-link libraries), and dynamic memory. Realistically, the system is still limited by whatever hardware resources are available, but the management of available resources is performed independently of the applications in the system. Virtual Memory Windows NT makes a distinction between memory and address space. Each process is attributed 2 GB of user address space no matter how much physical memory is actually available for the process. Also, all processes use the same range of linear 32-bit addresses ranging from 0000000016-7FFFFFFF16, regardless of what memory is available. Windows NT takes care of paging memory to and from disk at appropriate times so that each process is sure to be able to address the memory it needs. Although two processes may attempt to access memory at the same virtual address simultaneously, the Windows NT virtual memory manager actually represents these two memory locations at different physical locations where neither is likely to coincide with the original virtual address. This is virtual memory. Because of virtual memory, an application is able to manage its own address space without having to consider the impact on other processes in the system. The memory manager in Windows NT is responsible for seeing that all applications have enough physical memory to operate effectively at any given moment. Applications for the Windows NT operating system do not have to be concerned with sharing system memory with other applications as they did in Windows version 3.1 or earlier. Yet even with their own address space, applications still have the ability to share memory with other applications. One benefit of distinguishing between memory and address space is the capability it provides to applications for loading extremely large files into memory. Instead of having to read a large file into memory, Windows NT provides support for the application to reserve the range of addresses that the file needs. Then, sections of the file can be viewed (physically read into memory) as needed. The same can be done for large allocations of dynamic memory through virtual memory support. In previous versions of Windows, an application had to allocate memory before being able to manipulate the addresses in that memory. In Windows NT, the address space of each process is already allocated; whether there is any memory associated with the addresses in the address space is a different issue. The virtual memory management functions provide low-level support for independently managing both the addresses and memory of a process. The key virtual memory functions are: VirtualAlloc and VirtualFree VirtualLock and VirtualUnlock VirtualQuery or VirtualQueryEx VirtualProtect or VirtualProtectEx Each function is grouped with its counterpart if it has one. Memory is allocated using VirtualAlloc and, once allocated, must be freed with VirtualFree. Similarly, pages that have been locked with VirtualLock must be unlocked with VirtualUnlock when no longer needed. VirtualQuery and VirtualProtect have no counterparts, but they both have complementary functions (indicated by the Ex extension on the function names) that allow them to be used on processes other than the calling process, if the calling process has the appropriate privilege to do so. These functions are explained below in their appropriate context. Free, Reserved, and Committed Virtual Memory Every address in a process can be thought of as either free, reserved, or committed at any given time. A process begins with all addresses free, meaning they are free to be committed to memory or reserved for future use. Before any free address may be used, it must first be allocated as reserved or committed. Attempting to access an address that is either reserved or free generates an access violation exception. The entire 2 GB of addresses in a process are either free for use, reserved for future use, or committed to specific memory (in use). Figure 2 represents a hypothetical process consisting of free, reserved, and committed addresses. Figure 2. A process’s 2 GB of virtual address space is divided into regions of free, reserved, and committed memory locations. Reserved Addresses When reserving addresses in a process, no pages of physical memory are committed, and perhaps more importantly, no space is reserved in the pagefile for backing the memory. Also, reserving a range of addresses is no guarantee that at a later time there will be physical memory available to commit to those addresses. Rather, it is simply saving a specific free address range until needed, protecting the addresses from other allocation requests. Without this type of protection, routine operations such as loading a DLL or resource could occupy specific addresses and jeopardize their availability for later use. Reserving addresses is a quick operation, completely independent of the size of the address range being reserved. Whether reserving a 1 GB or a 4K range of addresses, the function is relatively speedy. This is not surprising considering that no resources are allocated during the operation. The function merely makes an entry into the process’s virtual address descriptor (VAD) tree. To reserve a range of addresses, invoke the VirtualAlloc function as shown in the following code fragment: 12345/* Reserve a 10 MB range of addresses */lpBase = VirtualAlloc (NULL, 10485760, MEM_RESERVE, PAGE_NOACCESS); As shown here, a value of NULL used for the first parameter, lpAddress, directs the function to reserve the range of addresses at whichever location is most convenient. Alternatively, a specific address could have been passed indicating a precise starting address for the reserved range. Either way, the return value to this function indicates the address at the beginning of the reserved range of addresses, unless the function is unable to complete the request. Then, the return value for the VirtualAlloc function is an error-status value. The second parameter indicates the range of addresses the function should allocate. This value can be anywhere from one page to 2 GB in size, but VirtualAlloc is actually constrained to a smaller range than that. The minimum size that can be reserved is 64K, and the maximum that can be reserved is the largest contiguous range of free addresses in the process. Requesting one page of reserved addresses results in a 64K address range. Conversely, requesting 2 GB will certainly fail because it is not possible to have that much address space free at any given time. (Remember that the act of loading an application consumes part of the initial 2 GB address space.) Note Windows NT builds a safeguard into every process’s address space. Both the upper and lower 65,536 bytes of each process are permanently reserved by the system. These portions of the address space are reserved to trap stray pointers—pointers that attempt to address memory in the range 0000000016-0000FFFF16 or 7FFF000016-7FFFFFFF16. Not coincidentally, it is easy to detect pointers in this range by simply ignoring the lower four nibbles (the rightmost two bytes) in these addresses. Essentially, a pointer is invalid if the upper four nibbles are 000016 or 7FFF16; all other values represent valid addresses. The final two parameters in the VirtualAlloc function, dwAllocationType and dwProtect, are used to determine how to allocate the addresses and the protection to associate with them. Addresses can be allocated as either type MEM_COMMIT or MEM_RESERVE. PAGE_READONLY, PAGE_READWRITE, and PAGE_NOACCESS are the three protections that can be applied to virtual memory. Reserved addresses are always PAGE_NOACCESS, a default enforced by the system no matter what value is passed to the function. Committed pages can be either read-only, read-write, or no-access. Committed Memory To use reserved addresses, memory must first be committed to the addresses. Committing memory to addresses is similar to reserving it—call VirtualAlloc with the dwAllocation parameter equal to MEM_COMMIT. At this point, resources become committed to addresses. Memory can be committed as little as one page at a time. The maximum amount of memory that can be committed is based solely on the maximum range of contiguous free or reserved addresses (but not a combination of both), regardless of the amount of physical memory available to the system. When memory is committed, physical pages of memory are allocated and space is reserved in a pagefile. That is, pages of committed memory always exist as either physical pages of memory or as pages that have been paged to the pagefile on disk. It is also possible that, while committing a chunk of memory, part or all of that memory will not reside in physical memory initially. Some pages of memory reside initially in the pagefile until accessed. Once pages of memory are committed, the virtual memory manager treats them like all other pages of memory in the system. In the Windows NT virtual memory system, page tables are used to access physical pages of memory. Each page table is itself a page of memory, like committed pages. Occasionally, when committing memory, additional pages must be allocated for page tables at the same time. So a request to commit a page of memory can require one page commitment for a page table, one page for the requested page, and two pages of space in the pagefile to back each of these pages. Consequently, the time it takes VirtualAlloc to complete a memory-commit request varies widely, depending on the state of the system and the size of the request. The following example demonstrates how to commit a specific page of reserved addresses from the previous example to a page of memory. 12345/* Commit memory for 3rd page of addresses. */lpPage3 = VirtualAlloc (lpBase + (2 * 4096), 4096, MEM_COMMIT, PAGE_READWRITE); Notice that instead of specifying NULL for lpAddress, a specific address is given to indicate exactly which page of reserved addresses becomes committed to memory. Also, this page of memory is initially given PAGE_READWRITE protection instead of PAGE_NOACCESS as in the previous example. The return address from the function is the virtual address of the first pages of committed addresses. Freeing Virtual Memory Once addresses have been allocated as either reserved or committed, VirtualFree is the only way to release them—that is, return them to free addresses. VirtualFree can also be used to decommit committed pages and, at the same time, return the addresses to reserved status. When decommitting addresses, all physical memory and pagefile space associated with the addresses is released. The following example demonstrates how to decommit the page of memory committed in the previous example. 12345/* Decommit memory for 3rd page of addresses. */VirtualFree (lpBase + (2 * 4096), 4096, MEM_DECOMMIT, PAGE_NOACCESS); Only addresses that are committed can be decommitted. This is important to remember when you need to decommit a large range of addresses. Say, for example, you have a range of addresses where several subsets of the addresses are committed and others are reserved. The only way to make the entire range reserved is to independently decommit each subset of committed addresses one by one. Attempting to decommit the entire range of addresses will fail because reserved addresses cannot be decommitted. Conversely, the same range of addresses can be freed in one fell swoop. It doesn’t matter what the state of an address is when the address is freed. The following example demonstrates freeing the 10 MB range of addresses reserved in the first example. 12345/* Free entire 10 MB range of addresses. */VirtualFree (lpBase, 10485760, MEM_RELEASE, PAGE_NOACCESS); Changing Protection on Pages of Virtual Memory Use the VirtualProtect function as a method for changing the protection on committed pages of memory. An application can, for example, commit a page of addresses as PAGE_READWRITE and immediately fill the page with data. Then, the protection on the page could be changed to PAGE_READONLY, effectively protecting the data from being overwritten by any thread in the process. The following example uses the VirtualProtect function to make an inaccessible page available. 12345/* Change page protection to read/write. */VirtualProtect (lpStack + 4096, 4096, PAGE_READWRITE, lpdwOldProt); Consider the following as a context for using this function. A data-buffering application receives a varying flow of data. Depending on specific hardware configurations and other software applications competing for CPU time, the flow of data may at times exceed the capability of the process. To prevent this from happening, the application designs a memory system that initially commits some pages of memory for a buffer. The application then protects the upper page of memory with PAGE_NOACCESS protection so that any attempt to access this memory generates an exception. The application also surrounds this code with an exception handler to handle access violations. When an access violation exception occurs, the application is able to determine that the buffer is approaching its upper limit. It responds by changing the protection on the page to PAGE_READWRITE, allowing the buffer to receive any additional data and continue uninterrupted. At the same time, the application spawns another thread to slow the data flow until the buffer is back down to a reasonable operating range. When things are back to normal, the upper page is returned to PAGE_NOACCESS and the additional thread goes away. This scenario describes how combining page protection and exception handling can be used to provide unique memory management opportunities. Locking Pages of Virtual Memory Processes in Windows NT have a minimal set of pages called a working set that, in order for the process to run properly, must be present in memory when running. Windows NT assigns a default number of pages to a process at startup and gradually tunes that number to achieve a balanced optimum performance among all active processes in the system. When a process is running (actually, when the threads of a process are running), Windows NT works hard at making sure that the process has its working set of pages resident in physical memory at all times. Processes in Windows NT are granted subtle influence into this system behavior with the VirtualLock and VirtualUnlock functions. Essentially, a process can establish specific pages to lock into its working set. However, this does not give the process free reign over its working set. It cannot affect the number of pages that make up its working set (the system adjusts the working set for each process routinely), and it cannot control when the working set is in memory and when it is not. The maximum number of pages that can be locked into a process’s working set at one time is limited to 32. An application could do more harm than good by locking pages of committed memory into the working set because doing so may force other critical pages in the process to become replaced. In that case, the pages could become paged to disk, causing page faults to occur whenever they were accessed. Then the process would spend much of its CPU allotment just paging critical pages in and out of memory. Below is an example that locks a range of addresses into memory when the process is running. 12/* Lock critical addresses into memory. */VirtualLock (lpCriticalData, 1024); Notice the range of addresses being locked into memory in this example is less than one page. It is not necessary for the entire range to be in a single page of memory. The net result is that the entire page of memory containing the data for the addresses, not just the data for the addresses indicated, is locked into memory. If the data straddles a page boundary, both pages are locked. Querying a Process’s Virtual Memory Given a process’s 2 GB of address space, managing the entire range of addresses would be difficult without the ability to query address information. Because the addresses themselves are represented independent of the memory that may or may not be committed to them, querying them is simply a matter of accessing the data structure that maintains their state. In Windows NT, this structure is the virtual address descriptor tree mentioned earlier. Windows exposes the capability of “walking the VAD structure” in the VirtualQuery and VirtualQueryEx functions. Again, the Ex suffix indicates which function can be called from one process to query another—if the calling process has the security privilege necessary to perform this function. The following example is extracted from the ProcessWalker sample: 12345/* Query next region of memory in child process. */VirtualQueryEx (hChildProcess, lpMem, lpList, sizeof (MEMORY_BASIC_INFORMATION)); The ProcessWalker application’s primary function is to walk a process’s address space, identifying each of its distinct address regions and representing specific state information about each region. It does this by enumerating each region one at a time from the bottom of the process to the top. lpMem is used to indicate the location of each region. Initially it is set to 0, and after returning from each query of a new region, it is incremented by the size of the region it queried. This process is repeated until lpMem reaches the upper system reserved area. lpList is a pointer to a MEMORY_BASIC_INFORMATION structure to be filled in by the VirtualQueryEx function. When the function returns, this structure represents information about the region queried. The structure has the following members: 123456789typedef struct _MEMORY_BASIC_INFORMATION &#123; /* mbi */ PVOID BaseAddress; /* Base address of region */ PVOID AllocationBase; /* Allocation base address */ DWORD AllocationProtect; /* Initial access protection */ DWORD RegionSize; /* Size in bytes of region */ DWORD State; /* Committed, reserved, free */ DWORD Protect; /* Current access protection */ DWORD Type; /* Type of pages */&#125; MEMORY_BASIC_INFORMATION; The VirtualQuery function returns this state information for any contiguous address region. The function determines the lower bound of the region and the size of the region, along with the exact state of the addresses in the region. The address it uses to determine the region can be any address in the region. So, if you wish to determine how much stack space has been committed at any given time, follow these steps: Get the thread context for the thread in question. Call the VirtualQuery function, supplying the address of the stack pointer in the thread context information as the lpMem parameter in the function. The query returns the size of the committed memory and the address of the base of the stack in the MEMORY_BASIC_INFORMATION structure in the form of the RegionSize and BaseAddress, respectively. Regions of memory, as defined by VirtualQuery, are a contiguous range of addresses whose protection, type, and base allocation are the same. The type and protection values are described earlier in this technical article. The base allocation is the lpAddress parameter value that is used when the entire region of memory was first allocated via the VirtualAlloc function. It is represented in the MEMORY_BASIC_INFORMATION structure as the AllocationBase field. When free addresses become either reserved or committed, their base allocation is determined at that time. A region of memory is not static by any means. Once a single page in a region of reserved addresses becomes committed, the region is broken into one or more reserved regions and one committed region. This continues as pages of memory change state. Similarly, when one of several PAGE_READWRITE committed pages is changed to PAGE_READONLY protection, the region is broken into multiple, smaller regions. Conclusion The virtual memory management functions in Windows offer direct management of virtual memory in Windows NT. Each process’s 2 GB user address space is divided into regions of memory that are either reserved, committed, or free virtual addresses. A region is defined as a contiguous range of addresses in which the protection, type, and base allocation of each address is the same. Within each region are one or more pages of addresses that also carry protection and pagelock flag status bits. The virtual memory management functions provide capabilities for applications to alter the state of pages in the virtual address space. An application can change the type of memory from committed to reserved or change the protection from PAGE_READWRITE to PAGE_READONLY to prevent access to a region of addresses. An application can lock a page into the working set for a process to minimize paging for a critical page of memory. The virtual memory functions are considered low-level functions, meaning they are relatively fast but they lack many high-level features. Article link: http://xnerv.wang/msdn-managing-virtual-memory/ Reprinted from: (MSDN) Managing Virtual Memory","categories":[{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/categories/Windows/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/tags/Windows/"},{"name":"MSDN","slug":"MSDN","permalink":"https://xnerv.wang/tags/MSDN/"},{"name":"Memory Management","slug":"Memory-Management","permalink":"https://xnerv.wang/tags/Memory-Management/"},{"name":"Virtual Memory","slug":"Virtual-Memory","permalink":"https://xnerv.wang/tags/Virtual-Memory/"}]},{"title":"(MSDN) Managing Memory-Mapped Files","slug":"msdn-managing-memory-mapped-files","date":"2017-11-14T06:33:00.000Z","updated":"2023-08-21T02:24:20.193Z","comments":true,"path":"msdn-managing-memory-mapped-files/","link":"","permalink":"https://xnerv.wang/msdn-managing-memory-mapped-files/","excerpt":"Randy KathMicrosoft Developer Network Technology Group Created: February 9, 1993 Abstract Determining which function or set of functions to use for managing memory in your application is difficult without a solid understanding of how each group of functions works and the overall impact they each have on the operating system. In an effort to simplify these decisions, this technical article focuses on the use of the memory-mapped file functions in Windows: the functions that are available, the way they are used, and the impact their use has on operating system resources. The following topics are discussed in this article: Introduction to managing memory in Windows operating systems What are memory-mapped files? How are memory-mapped files implemented? Sharing memory with memory-mapped files Using memory-mapped file functions In addition to this technical article, a sample application called ProcessWalker is included on the Microsoft Developer Network CD. This sample application is useful for exploring the behavior of memory-mapped files in a process, and it provides several useful implementation examples.","text":"Randy KathMicrosoft Developer Network Technology Group Created: February 9, 1993 Abstract Determining which function or set of functions to use for managing memory in your application is difficult without a solid understanding of how each group of functions works and the overall impact they each have on the operating system. In an effort to simplify these decisions, this technical article focuses on the use of the memory-mapped file functions in Windows: the functions that are available, the way they are used, and the impact their use has on operating system resources. The following topics are discussed in this article: Introduction to managing memory in Windows operating systems What are memory-mapped files? How are memory-mapped files implemented? Sharing memory with memory-mapped files Using memory-mapped file functions In addition to this technical article, a sample application called ProcessWalker is included on the Microsoft Developer Network CD. This sample application is useful for exploring the behavior of memory-mapped files in a process, and it provides several useful implementation examples. Introduction This is one of three related technical articles—“Managing Virtual Memory,” “Managing Memory-Mapped Files,” and “Managing Heap Memory”—that explain how to manage memory in applications for Windows. In each article, this introduction identifies the basic memory components in the Windows API programming model and indicates which article to reference for specific areas of interest. The first version of the Windows operating system introduced a method of managing dynamic memory based on a single global heap, which all applications and the system share, and multiple, private local heaps, one for each application. Local and global memory management functions were also provided, offering extended features for this new memory management system. More recently, the Microsoft C run-time (CRT) libraries were modified to include capabilities for managing these heaps in Windows using native CRT functions such as malloc and free. Consequently, developers are now left with a choice—learn the new application programming interface (API) provided as part of Windows or stick to the portable, and typically familiar, CRT functions for managing memory in applications written for Windows. Window offers three groups of functions for managing memory in applications: memory-mapped file functions, heap memory functions, and virtual-memory functions. Figure 1. The Windows API provides different levels of memory management for versatility in application programming. In all, six sets of memory management functions exist in Windows, as shown in Figure 1, all of which were designed to be used independently of one another. So which set of functions should you use? The answer to this question depends greatly on two things: the type of memory management you want and how the functions relevant to it are implemented in the operating system. In other words, are you building a large database application where you plan to manipulate subsets of a large memory structure? Or maybe you’re planning some simple dynamic memory structures, such as linked lists or binary trees? In both cases, you need to know which functions offer the features best suited to your intention and exactly how much of a resource hit occurs when using each function. Table 1 categorizes the memory management function groups and indicates which of the three technical articles in this series describes each group’s behavior. Each technical article emphasizes the impact these functions have on the system by describing the behavior of the system in response to using the functions. Table 1. Various Memory Management Functions Available Memory set System resource affected Related technical article Virtual memory functions A process’s virtual address spaceSystem pagefileSystem memoryHard disk space “Managing Virtual Memory” Memory-mapped file functions A process’s virtual address spaceSystem pagefileStandard file I/OSystem memoryHard disk space “Managing Memory-Mapped Files” Heap memory functions A process’s virtual address spaceSystem memoryProcess heap resource structure “Managing Heap Memory” Global heap memory functions A process’s heap resource structure “Managing Heap Memory” Local heap memory functions A process’s heap resource structure “Managing Heap Memory” C run-time reference library A process’s heap resource structure “Managing Heap Memory” Each technical article discusses issues surrounding the use of Windows-specific functions. What Are Memory-Mapped Files? Memory-mapped files (MMFs) offer a unique memory management feature that allows applications to access files on disk in the same way they access dynamic memory—through pointers. With this capability you can map a view of all or part of a file on disk to a specific range of addresses within your process’s address space. And once that is done, accessing the content of a memory-mapped file is as simple as dereferencing a pointer in the designated range of addresses. So, writing data to a file can be as simple as assigning a value to a dereferenced pointer as in: 1*pMem = 23; Similarly, reading from a specific location within the file is simply: 1nTokenLen = *pMem; In the above examples, the pointer pMem represents an arbitrary address in the range of addresses that have been mapped to a view of a file. Each time the address is referenced (that is, each time the pointer is dereferenced), the memory-mapped file is the actual memory being addressed. NoteWhile memory-mapped files offer a way to read and write directly to a file at specific locations, the actual action of reading/writing to the disk is handled at a lower level. Consequently, data is not actually transferred at the time the above instructions are executed. Instead, much of the file input/output (I/O) is cached to improve general system performance. You can override this behavior and force the system to perform disk transactions immediately by using the memory-mapped file function FlushViewOfFile explained later. What Do Memory-Mapped Files Have to Offer? One advantage to using MMF I/O is that the system performs all data transfers for it in 4K pages of data. Internally all pages of memory are managed by the virtual-memory manager (VMM). It decides when a page should be paged to disk, which pages are to be freed for use by other applications, and how many pages each application can have out of the entire allotment of physical memory. Since the VMM performs all disk I/O in the same manner—reading or writing memory one page at a time—it has been optimized to make it as fast as possible. Limiting the disk read and write instructions to sequences of 4K pages means that several smaller reads or writes are effectively cached into one larger operation, reducing the number of times the hard disk read/write head moves. Reading and writing pages of memory at a time is sometimes referred to as paging and is common to virtual-memory management operating systems. Another advantage to using MMF I/O is that all of the actual I/O interaction now occurs in RAM in the form of standard memory addressing. Meanwhile, disk paging occurs periodically in the background, transparent to the application. While no gain in performance is observed when using MMFs for simply reading a file into RAM, other disk transactions can benefit immensely. Say, for example, an application implements a flat-file database file structure, where the database consists of hundreds of sequential records. Accessing a record within the file is simply a matter of determining the record’s location (a byte offset within the file) and reading the data from the file. Then, for every update, the record must be written to the file in order to save the change. For larger records, it may be advantageous to read only part of the record into memory at a time as needed. Unfortunately, though, each time a new part of the record is needed, another file read is required. The MMF approach works a little differently. When the record is first accessed, the entire 4K page(s) of memory containing the record is read into memory. All subsequent accesses to that record deal directly with the page(s) of memory in RAM. No disk I/O is required or enforced until the file is later closed or flushed. NoteDuring normal system paging operations, memory-mapped files can be updated periodically. If the system needs a page of memory that is occupied by a page representing a memory-mapped file, it may free the page for use by another application. If the page was dirty at the time it was needed, the act of writing the data to disk will automatically update the file at that time. (A dirty page is a page of data that has been written to, but not saved to, disk; for more information on types of virtual-memory pages, see “The Virtual-Memory Manager in Windows NT” on the Developer Network CD.) The flat-file database application example is useful in pointing out another advantage of using memory-mapped files. MMFs provide a mechanism to map portions of a file into memory as needed. This means that applications now have a way of getting to a small segment of data in an extremely large file without having to read the entire file into memory first. Using the above example of a large flat-file database, consider a database file housing 1,000,000 records of 125 bytes each. The file size necessary to store this database would be 1,000,000 * 125 = 125,000,000 bytes. To read a file that large would require an extremely large amount of memory. With MMFs, the entire file can be opened (but at this point no memory is required for reading the file) and a view (portion) of the file can be mapped to a range of addresses. Then, as mentioned above, each page in the view is read into memory only when addresses within the page are accessed. How Are They Implemented? Since Windows NT is a page-based virtual-memory system, memory-mapped files represent little more than an extension of an existing, internal memory management component. Essentially all applications in Windows NT are represented in their entirety by one or more files on disk and a subset of those files resident in random access memory (RAM) at any given time. For example, each application has an executable file that represents pages of executable code and resources for the application. These pages are swapped into and out of RAM, as they are needed, by the operating system. When a page of memory is no longer needed, the operating system relinquishes control over the page on behalf of the application that owns it and frees it for use by another. When that page becomes needed again, it is re-read from the executable file on disk. This is called backing the memory with a file, in this case, the executable file. Similarly, when a process starts, pages of memory are used to store static and dynamic data for that application. Once committed, these pages are backed by the system pagefile, similar to the way the executable file is used to back the pages of code. Figure 2 is a graphical representation of how pages of code and data are backed on the hard disk. Figure 2. Memory used to represent pages of code in processes for Windows NT are backed directly by the application’s executable module while memory used for pages of data are backed by the system pagefile. Treating both code and data in the same manner paves the way for propagating this functionality to a level where applications can use it, too—which is what Windows does through memory-mapped files. Shared Memory in Windows NT Both code and data are treated the same way in Windows NT—both are represented by pages of memory and both have their pages backed by a file on disk. The only real difference is the file by which they are backed—code by the executable image and data by the system pagefile. Because of this, memory-mapped files are also able to provide a mechanism for sharing data between processes. By extending the memory-mapped file capability to include portions of the system pagefile, applications are able to share data that is backed by the pagefile. Shown in Figure 3, each application simply maps a view of the same portion of the pagefile, making the same pages of memory available to each application. Figure 3. Processes share memory by mapping independent views of a common region in the system pagefile. Windows NT’s tight security system prevents processes from directly sharing information among each other, but MMFs provide a mechanism that works with the security system. In order for one process to share data with another via MMFs, each process must have common access to the file. This is achieved by giving the MMF object a name that both processes use to open the file. Internally, a shared section of the pagefile translates into pages of memory that are addressable by more than one process. To do this, Windows NT uses an internal resource called a prototype page-table entry (PPTE). PPTEs enable more than one process to address the same physical page of memory. A PPTE is a system resource, so their availability and security is controlled by the system alone. This way processes can share data and still exist on a secure operating system. Figure 4 indicates how PPTEs are used in Windows NT’s virtual addressing scheme. Figure 4. Prototype page-table entries are the mechanism that permits pages of memory to be shared among processes. One of the best ways to use an MMF for sharing data is to use it in a DLL (dynamic-link library). The PortTool application serves as a useful illustration. PortTool uses a DLL to provide its porting functionality and relies on the main application for the user interface. The reason for this is simple: Other applications can then also use the DLL functionality. That is, other editors that are programmable can import the porting functionality. Because it is entirely feasible for PortTool to be running while another editor that imports the PortTool DLL is also running, it is best to economize system resources as much as possible between the applications. PortTool does this by using an MMF for sharing the porting information with both processes. Otherwise, both applications would be required to load their own set of porting information while running at the same time, a waste of system resources. The PortTool code demonstrates sharing memory via an MMF in a DLL. Using Memory-Mapped File Functions Memory-mapped file functions can be thought of as second cousins to the virtual-memory management functions in Windows. Like the virtual-memory functions, these functions directly affect a process’s address space and pages of physical memory. No overhead is required to manage the file views, other than the basic virtual-memory management that exists for all processes. These functions deal in reserved pages of memory and committed addresses in a process. The entire set of memory-mapped file functions are: CreateFileMapping OpenFileMapping MapViewOfFile MapViewOfFileEx UnmapViewOfFile FlushViewOfFile CloseHandle Each of these functions is individually discussed below, along with code examples that demonstrate their use. Creating a File Mapping To use a memory-mapped file, you start by creating a memory-mapped file object. The act of creating an MMF object has very little impact on system resources. It does not affect your process’s address space, and no virtual memory is allocated for the object (other than for the internal resources that are necessary in representing the object). One exception, however, is that, if the MMF object represents shared memory, an adequate portion of the system pagefile is reserved for use by the MMF during the creation of the object. The CreateFileMapping function is used to create the file-mapping object as demonstrated in the example listed below, a portion of PMEM.C, the source module from the ProcessWalker sample application. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364case IDM_MMFCREATENEW: &#123; char szTmpFile[256]; /* Create temporary file for mapping. */ GetTempPath (256, szTmpFile); GetTempFileName (szTmpFile, &quot;PW&quot;, 0, MMFiles[wParam-IDM_MMFCREATE].szMMFile); /* If file created, continue to map file. */ if ((MMFiles[wParam-IDM_MMFCREATE].hFile = CreateFile (MMFiles[wParam-IDM_MMFCREATE].szMMFile, GENERIC_WRITE | GENERIC_READ, FILE_SHARE_WRITE, NULL, CREATE_ALWAYS, FILE_ATTRIBUTE_TEMPORARY, NULL)) != (HANDLE)INVALID_HANDLE_VALUE) goto MAP_FILE; &#125; break;case IDM_MMFCREATEEXIST: &#123; char szFilePath[MAX_PATH]; OFSTRUCT of; /* Get existing filename for mapfile. */ *szFilePath = 0; if (!GetFileName (hWnd, szFilePath, &quot;*&quot;)) break; /* If file opened, continue to map file. */ if ((MMFiles[wParam-IDM_MMFCREATE].hFile = (HANDLE)OpenFile (szFilePath, &amp;of, OF_READWRITE)) != (HANDLE)HFILE_ERROR) goto MAP_FILE; &#125; break;case IDM_MMFCREATE: /* Associate shared memory file handle value. */ MMFiles[wParam-IDM_MMFCREATE].hFile = (HANDLE)0xffffffff;MAP_FILE: /* Create 20MB file mapping. */ if (!(MMFiles[wParam-IDM_MMFCREATE].hMMFile = CreateFileMapping (MMFiles[wParam-IDM_MMFCREATE].hFile, NULL, PAGE_READWRITE, 0, 0x01400000, NULL))) &#123; ReportError (hWnd); if (MMFiles[wParam-IDM_MMFCREATE].hFile) &#123; CloseHandle (MMFiles[wParam-IDM_MMFCREATE].hFile); MMFiles[wParam-IDM_MMFCREATE].hFile = NULL; &#125; &#125; break; /* from IDM_MMFCREATE */ In the sample code above, three cases are demonstrated. They represent creating a memory-mapped file by first creating a temporary disk file, creating a memory-mapped file from an existing file, and creating a memory-mapped file out of part of the system pagefile. In case IDM_MMFCREATENEW, a temporary file is created first, before the memory-mapped file. For case IDM_MMFCREATEEXIST, the File Open dialog is used to retrieve a filename, and that file is then opened before the memory-mapped file is created. In the third case, IDM_MMFCREATE, the memory-mapped file is created either using the system pagefile or using one of the standard files created in the two earlier cases. Notice that the CreateFileMapping function need only be called once for all three different cases. The first parameter to the CreateFileMapping function, hFile, is used to supply the handle to the file that is to be memory-mapped. If the system pagefile is to be used, the value 0xFFFFFFFF must be specified instead. In the above examples, a structure is used to represent both the standard file and memory-mapped file information. In the example above, the hMMFile field in the structure MMFiles[wParam-IDM_MMFCREATE] is either 0xFFFFFFFF (its default value), or it is the value of the file handle retrieved in either of the earlier cases. In all three cases, the memory-mapped file is specified to be 20 MB (0x01400000) in size, regardless of the size of any files created or opened for mapping. The fourth and fifth parameters, dwMaximumSizeHigh and dwMaximumSizeLow, are used to indicate the size of the file mapping. If these parameters indicate a specific size for the memory-mapped file when memory mapping a file other than the pagefile, the file on disk is fitted to this new size—whether larger or smaller makes no difference. As an alternative, when memory mapping a file on disk, you can set the size parameters to 0. In this case, the memory-mapped file will be the same size as the original disk file. When mapping a section of the pagefile, you must specify the size of the memory-mapped file. The second parameter to the CreateFileMapping function, lpsa, is used to supply a pointer to a SECURITY_ATTRIBUTES structure. Since memory-mapped files are an object, they have the same security attributes that can be applied to every other object. A NULL value indicates that no security attributes are relevant to your use of the memory-mapped file. The third parameter, fdwProtect, is used to indicate the type of protection to place on the entire memory-mapped file. You can use this parameter to protect the memory-mapped file from writes by specifying PAGE_READONLY or to permit read and write access with PAGE_READWRITE. One other parameter of interest is the lpszMapName parameter, which can be used to give the MMF object a name. In order to open a handle to an existing file-mapping object, the object must be named. All that is required of the name is a simple string that is not already being used to identify another object in the system. Obtaining a File-Mapping Object Handle In order to map a view of a memory-mapped file, all you need is a valid handle to the MMF object. You can obtain a valid handle in one of several ways: by creating the object as described above, by opening the object with the OpenFileMapping function, by inheriting the object handle, or by duplicating the handle. Opening a memory-mapped file object To open a file-mapping object, the object must have been given a name during the creation of the object. A name uniquely identifies the object to this and other processes that wish to share the MMF object. The following portion of code from PORT.C shows how to open a file-mapping object by name. 123456789101112131415161718192021222324252627282930313233343536/* Load name for file-mapping object. */LoadString (hDLL, IDS_MAPFILENAME, szMapFileName, MAX_PATH);/* After first process initializes, port data. */if ((hMMFile = OpenFileMapping (FILE_MAP_WRITE, FALSE, szMapFileName))) /* Exit now since initialization was already performed by another process. */ return TRUE;/* Retrieve path and file for ini file. */if (!GetIniFile (hDLL, szIniFilePath)) return FALSE;/* Test for ini file existence and get length of file. */if ((int)(hFile = (HANDLE)OpenFile (szIniFilePath, &amp;of, OF_READ)) == -1) return FALSE;else &#123; nFileSize = GetFileSize (hFile, NULL); CloseHandle (hFile); &#125;/* Allocate a segment of the swap file for shared memory 2*Size of ini file. */if (!(hMMFile = CreateFileMapping ((HANDLE)0xFFFFFFFF, NULL, PAGE_READWRITE, 0, nFileSize * 2, szMapFileName))) return FALSE; The OpenFileMapping function requires only three arguments, the most important of these being the name of the object. As shown in the example, the name is simply a unique string. If the string is not unique to the system, the MMF object will not be created. Once the object exists, however, the name is guaranteed for the life of the object. Also, note in the above example that the MMF object is opened first, possibly before the object has been created. This logic relies on the fact that, if the object does not already exist, the OpenFileMapping function will fail. This is useful in a DLL where the DLL’s initialization code is called repeatedly, once for every process that attaches to it. The sample from PORT.C above occurs in the DLL’s initialization code that is called every time a DLL gets attached to another process. The first time it is called, the OpenFileMapping function fails because the object does not already exist. The logic, then, continues execution until it reaches the CreateFileMapping function, and it is there that the object is first created. Immediately after initially creating the object, the PortTool code initializes the data in the file mapping by writing porting-specific information to the memory-mapped file. To do this, the memory-mapped file is created with PAGE_READWRITE protection. All subsequent calls to the DLL’s initialization function result in the OpenFileMapping function successfully returning a valid object handle. This way the DLL does not need to keep track of which process is the first to attach to the DLL. Note that for every process that attaches to the DLL, the object name is retrieved from the same source—a string from the DLL’s resource string table. Since the DLL is able to retrieve the object name from its own resource string table, the name is global to all processes, yet no process is actually aware of the name used. The DLL is able to effectively encapsulate this functionality while at the same time providing the benefit of shared memory to each process that attaches to the DLL. The PortTool example presents a useful context for sharing memory. Yet, keep in mind that any file on disk could have been used in the same way. If an application were to implement some database services to several other applications, it could set up memory-mapped files using basic disk files, instead of the pagefile, and share that information in the same way. And as the first code listing illustrates, a temporary file could be used to share data instead of the pagefile. Inheriting and duplicating memory-mapped file object handles Ordinarily, for two processes to share a memory-mapped file, they must both be able to identify it by name. An exception to this is child processes, which can inherit their parent’s handles. Most objects in Windows can be explicitly targeted for inheritance or not. (Some objects are not inheritable, such as GDI object handles.) When creating an MMF object, a Boolean field in the optional SECURITY_ATTRIBUTES structure can be used to designate whether the handle is to be inheritable or not. If the MMF object handle is designated as inheritable, any child processes of the process that created the object can access the object through the same handle as their parent. Literally, this means the child process can access the object by supplying the same handle value as the parent. Communicating that handle to the child process is another concern. The child process is still another process after all, having its own address space, so the handle variable itself is not transferable. Either some interprocess communication (IPC) mechanism or the command line can be used to communicate handle values to child processes. Further, the DuplicateHandle function is provided to offer more control as to when handles can be inherited and not. This function can be used to create a duplicate handle of the original and can be used to change the inheritance state of the handle. An application can invoke this function to change an MMF object handle state to inheritable before passing the handle along to a child process, or it can do the opposite—it can take an inheritable handle and preserve it from being inherited. Viewing Part of a Memory-Mapped File Once obtained, the handle to the memory-mapped file object is used to map views of the file to your process’s address space. Views can be mapped and unmapped at will while the MMF object exists. When a view of the file is mapped, system resources are finally allocated. A contiguous range of addresses, large enough to span the size of the file view, are now committed in your process’s address space. Yet, even though the addresses have been committed for the file view, physical pages of memory are still only committed on a demand basis when using the memory. So, the only way to allocate a page of physical memory for a committed page of addresses in your memory-mapped file view is to generate a page fault for that page. This is done automatically the first time you read or write to any address in the page of memory. To map a view of a memory-mapped file, use either the MapViewOfFile or the MapViewOfFileEx function. With both of these functions, a handle to a memory-mapped file object is a required parameter. The following example shows how the PortTool sample application implements this function. 123456/* Map a view of this file for writing. */lpMMFile = (char *)MapViewOfFile (hMMFile, FILE_MAP_WRITE, 0, 0, 0); In this example, the entire file is mapped, so the final three parameters are less meaningful. The first parameter specifies the file-mapping object handle. The second parameter indicates the access mode for the view of the file. This can be FILE_MAP_READ, FILE_MAP_WRITE, or FILE_MAP_ALL_ACCESS, provided the protection on the file-mapping object permits it. If the object is created with PAGE_READWRITE protection, all of these access types are available. If, on the other hand, the file is created with PAGE_READONLY protection, the only access type available is FILE_MAP_READ. This allows the object creator control over how the object can be viewed. The second and third parameters are used to indicate the low and high halves, respectively, of a 64-bit offset into the memory-mapped file. This offset from the start of the memory-mapped file is where the view is to begin. The final parameter indicates how much of the file is to be viewed. This parameter can be set to 0 to indicate that the entire file is to be mapped. In that case, the 64-bit offset value is ignored. The function returns a pointer to the location in the process’s address space where the file view has been mapped. This is an arbitrary location in your process, depending on where the contiguous range of addresses are available. If you want to map the file view to a specific set of addresses in your process, the MapViewOfFileEx function provides this capability. This function simply adds an additional parameter, lpvBase, to indicate the location in your process to map the view. The return value to MapViewOfFileEx is the same value as lpvBase if the function is successful; otherwise, it is NULL. Similarly, for MapViewOfFile the return value is NULL if the function fails. Multiple views of the same file-mapping object can coexist and overlap each other as shown in Figure 5. Figure 5. Memory-mapped file objects permit multiple, overlapped views of the file from one or more processes at the same time. Notice that multiple views of a memory-mapped file can overlap, regardless of what process maps them. In a single process with overlapping views, you simply end up with two or more virtual addresses in a process that refer to the same location in physical memory. So, it’s possible to have several PTEs referencing the same page frame. Remember, each page of a shared memory-mapped file is represented by only one physical page of memory. To view that page of memory, a process needs a page directory entry and page-table entry to reference the page frame. There are two ways in which needing only one physical page of memory for a shared page benefits applications in the system. First, there is an obvious savings of resources because both processes share both the physical page of memory and the page of hard disk storage used to back the memory-mapped file. Second, there is only one set of data, so all views are always coherent with one another. This means that changes made to a page in the memory-mapped file via one process’s view are automatically reflected in a common view of the memory-mapped file in another process. Essentially, Windows NT is not required to do any special bookkeeping to ensure the integrity of data to both applications. Unmapping a View of a Memory-Mapped File Once a view of the memory-mapped file has been mapped, the view can be unmapped at any time by calling the UnmapViewOfFile function. As you can see below, there is nothing tricky about this function. Simply supply the one parameter that indicates the base address, where the view of the file begins in your process 12345678910111213/* Load tokens for APIS section. */LoadString (hDLL, IDS_PORTAPIS, szSection, MAX_PATH);if (!LoadSection (szIniFilePath, szSection, PT_APIS, &amp;nOffset, lpMMFile)) &#123; /* Clean up memory-mapped file. */ UnmapViewOfFile (lpMMFile); CloseHandle (hMMFile); return FALSE; &#125; As mentioned above, you can have multiple views of the same memory-mapped file, and they can overlap. But what about mapping two identical views of the same memory-mapped file? After learning how to unmap a view of a file, you could come to the conclusion that it would not be possible to have two identical views in a single process because their base address would be the same, and you wouldn’t be able to distinguish between them. This is not true. Remember that the base address returned by either the MapViewOfFile or the MapViewOfFileEx function is not the base address of the file view. Rather, it is the base address in your process where the view begins. So mapping two identical views of the same memory-mapped file will produce two views having different base addresses, but nonetheless identical views of the same portion of the memory-mapped file. The point of this little exercise is to emphasize that every view of a single memory-mapped file object is always mapped to a unique range of addresses in the process. The base address will be different for each view. For that reason the base address of a mapped view is all that is required to unmap the view. Flushing Views of Files An important feature for memory-mapped files is the ability to write any changes to disk immediately if necessary. This feature is provided through the FlushViewOfFile function. Changes made to a memory-mapped file through a view of the file, other than the system pagefile, are automatically written to disk when the view is unmapped or when the file-mapping object is deleted. Yet, if an application needs to force the changes to be written immediately, FlushViewOfFile can be used for that purpose. 12/* Force changes to disk immediately. */FlushViewOfFile (lpMMFile, nMMFileSize); The example listed above flushes an entire file view to disk. In doing so, the system only writes the dirty pages to disk. Since the Windows NT virtual-memory manager automatically tracks changes made to pages, it is a simple matter for it to enumerate all dirty pages in a range of addresses, writing them to disk. The range of addresses is formed by taking the base address of the file view supplied by the first parameter to the FlushViewOfFile function as the starting point and extending to the size supplied by the second parameter, cbFlush. The only requirement is that the range be within the bounds of a single file view. Releasing a Memory-Mapped File Like most other objects, a memory-mapped file object is closed by calling the CloseHandle function. It is not necessary to unmap all views of the memory-mapped file before closing the object. As mentioned above, dirty pages are written to disk before the object is freed. To close a memory-mapped file, call the CloseHandle function, which supplies the memory-mapped file object handle for the function parameter. 12/* Close memory-mapped file. */CloseHandle (hMMFile); It is worth noting that closing a memory-mapped file does nothing more than free the object. If the memory-mapped file represents a file on disk, the file must still be closed using standard file I/O functions. Also, if you create a temporary file explicitly for use as a memory-mapped file as in the initial ProcessWalker example, you are responsible for removing the temporary file yourself. To illustrate what the entire cleanup process may look like, consider the following example from the ProcessWalker sample application. 1234567891011121314151617181920212223242526272829303132333435case IDM_MMFFREE:case IDM_MMFFREENEW:case IDM_MMFFREEEXIST: &#123; HCURSOR hOldCursor; OFSTRUCT of; /* Put hourglass cursor up. */ hOldCursor = (HCURSOR)SetClassLong (hWnd, GCL_HCURSOR, 0); SetCursor (LoadCursor (0, IDC_WAIT)); /* Release memory-mapped file and associated file if any. */ CloseHandle (MMFiles[wParam-IDM_MMFFREE].hMMFile); MMFiles[wParam-IDM_MMFFREE].hMMFile = NULL; if (MMFiles[wParam-IDM_MMFFREE].hFile) &#123; CloseHandle (MMFiles[wParam-IDM_MMFFREE].hFile); MMFiles[wParam-IDM_MMFFREE].hFile = NULL; &#125; /* If temporary file, delete here. */ if (wParam == IDM_MMFFREENEW) &#123; OpenFile (MMFiles[wParam-IDM_MMFFREE].szMMFile, &amp;of, OF_DELETE); *(MMFiles[wParam-IDM_MMFFREE].szMMFile) = 0; &#125; /* Replace wait cursor with old cursor. */ SetClassLong (hWnd, GCL_HCURSOR, (LONG)hOldCursor); SetCursor (hOldCursor); &#125; break; In this example, the memory-mapped file can be one of three types: the system pagefile, a temporary file, or an existing file on disk. If the file is the system pagefile, the memory-mapped file object is simply closed, and no additional cleanup is necessary. If the memory-mapped file is mapped from an existing file, that file is closed right after closing the memory-mapped file. If the memory-mapped file is a mapping of a temporary file, it is no longer needed and is deleted using standard file I/O immediately after closing the temporary file handle, which cannot occur until after closing the memory-mapped file object handle. Conclusion Memory-mapped files provide unique methods for managing memory in the Windows application programming interface. They permit an application to map its virtual address space directly to a file on disk. Once a file has been memory-mapped, accessing its content is reduced to dereferencing a pointer. A memory-mapped file can also be mapped by more than one application simultaneously. This represents the only mechanism for two or more processes to directly share data in Windows NT. With memory-mapped files, processes can map a common file or portion of a file to unique locations in their own address space. This technique preserves the integrity of private address spaces for all processes in Windows NT. Memory-mapped files are also useful for manipulating large files. Since creating a memory mapping file consumes few physical resources, extremely large files can be opened by a process and have little impact on the system. Then, smaller portions of the file called “views” can be mapped into the process’s address space just before performing I/O. There are many techniques for managing memory in applications for Windows. Whether you need the benefits of memory sharing or simply wish to manage virtual memory backed by a file on disk, memory-mapped file functions offer the support you need. Article link: http://xnerv.wang/msdn-managing-memory-mapped-files/ Reprinted from: (MSDN) Managing Memory-Mapped Files","categories":[{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/categories/Windows/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/tags/Windows/"},{"name":"Heap","slug":"Heap","permalink":"https://xnerv.wang/tags/Heap/"},{"name":"MSDN","slug":"MSDN","permalink":"https://xnerv.wang/tags/MSDN/"},{"name":"Memory Management","slug":"Memory-Management","permalink":"https://xnerv.wang/tags/Memory-Management/"}]},{"title":"(MSDN) Managing Heap Memory","slug":"msdn-managing-heap-memory","date":"2017-11-14T06:18:00.000Z","updated":"2023-08-21T02:24:20.170Z","comments":true,"path":"msdn-managing-heap-memory/","link":"","permalink":"https://xnerv.wang/msdn-managing-heap-memory/","excerpt":"Randy Kath Microsoft Developer Network Technology Group Created: April 3, 1993 Abstract Determining which function or set of functions to use for managing memory in your application is difficult without a solid understanding of how each group of functions works and the overall impact they each have on the operating system. In an effort to simplify these decisions, this technical article focuses on the use of heaps in Windows: the functions that are available, the way they are used, and the impact their use has on operating system resources. The following topics are discussed: The purpose of heaps General heap behavior The two types of heaps Global and local memory functions Heap memory functions Overhead on heap memory allocations Summary and recommendations In addition to this technical article, a sample application called ProcessWalker is included on the Microsoft Developer Network CD. This sample application explores the behavior of heap memory functions in a process, and it provides several useful implementation examples.","text":"Randy Kath Microsoft Developer Network Technology Group Created: April 3, 1993 Abstract Determining which function or set of functions to use for managing memory in your application is difficult without a solid understanding of how each group of functions works and the overall impact they each have on the operating system. In an effort to simplify these decisions, this technical article focuses on the use of heaps in Windows: the functions that are available, the way they are used, and the impact their use has on operating system resources. The following topics are discussed: The purpose of heaps General heap behavior The two types of heaps Global and local memory functions Heap memory functions Overhead on heap memory allocations Summary and recommendations In addition to this technical article, a sample application called ProcessWalker is included on the Microsoft Developer Network CD. This sample application explores the behavior of heap memory functions in a process, and it provides several useful implementation examples. Introduction This is one of three related technical articles—“Managing Virtual Memory,” “Managing Memory-Mapped Files,” and “Managing Heap Memory”—that explain how to manage memory using the Windows API (application programming interface). This introduction identifies the basic memory components in the programming model and indicates which article to reference for specific areas of interest. The first version of the Microsoft® Windows™ operating system introduced a method of managing dynamic memory based on a single global heap, which all applications and the system share, and multiple, private local heaps, one for each application. Local and global memory management functions were also provided, offering extended features for this new memory management system. More recently, the Microsoft C run-time (CRT) libraries were modified to include capabilities for managing these heaps in Windows using native CRT functions such as malloc and free. Consequently, developers are now left with a choice—learn the new API provided as part of Windows version 3.1 or stick to the portable, and typically familiar, CRT functions for managing memory in applications written for 16-bit Windows. Windows offers three groups of functions for managing memory in applications: memory-mapped file functions, heap memory functions, and virtual memory functions. Figure 1. The Windows API provides different levels of memory management for versatility in application programming. Six sets of memory management functions exist in Windows, as shown in Figure 1, all of which were designed to be used independently of one another. So which set of functions should you use? The answer to this question depends greatly on two things: the type of memory management you want, and how the functions relevant to it are implemented in the operating system. In other words, are you building a large database application where you plan to manipulate subsets of a large memory structure? Or maybe you’re planning some simple dynamic memory structures, such as linked lists or binary trees? In both cases, you need to know which functions offer the features best suited to your intention and exactly how much of a resource hit occurs when using each function. The following table categorizes the memory management function groups and indicates which of the three technical articles in this series describes each group’s behavior. Each technical article emphasizes the impact these functions have on the system by describing the behavior of the system in response to using the functions. Memory set System resource affected Related technical article Virtual memory functions A process’s virtual address spaceSystem pagefileSystem memoryHard disk space “Managing Virtual Memory” Memory-mapped file functions A process’s virtual address spaceSystem pagefileStandard file I/OSystem memoryHard disk space “Managing Memory-Mapped Files” Heap memory functions A process’s virtual address spaceSystem memoryProcess heap resource structure “Managing Heap Memory” Global heap memory functions A process’s heap resource structure “Managing Heap Memory” Local heap memory functions A process’s heap resource structure “Managing Heap Memory” C run-time reference library A process’s heap resource structure “Managing Heap Memory” Each technical article discusses issues surrounding the use of Windows-specific functions. The Purpose of Heaps The Windows subsystem on Windows NT provides high-level memory management functions that make it easy for applications to build dynamic data structures, provide compatibility with previous versions of Windows, and create buffers and temporary placeholders for system functions. These memory management functions return handles and pointers to blocks of memory that are allocated at run time and managed by an entity called the heap. The heap’s primary function is to efficiently manage the memory and address space of a process for an application. A tall order, considering the heap has absolutely no idea what the application intends to do with its memory and address space. Yet the heap manages to provide a robust set of functions that allow developers to overlook some of the finer details of system resources (such as the difference between reserved, free, and committed memory) so that they can turn their attention to the more important task at hand, that of implementing their applications. In Windows NT, heaps provide a smaller granularity for the size of the smallest allocatable chunk of memory than the virtual memory management functions do. Applications typically need to allocate a specific number of bytes to fulfill a parameter request or to act as a temporary buffer. For example, when loading a string resource with the LoadString function, an application passes a pointer to a buffer that receives the string resource. The size of the buffer must be large enough to hold the string and a null terminator. Without the heap manager, applications would be forced to use the virtual memory management functions, which allocate a minimum of one page of memory at a time. General Heap Behavior While the heap does provide support for managing smaller chunks of memory, it is itself nothing more than a chunk of memory implemented in the Windows NT virtual memory system. Consequently, the techniques that the heap uses to manage memory are based on the virtual memory management functions available to the heap. Evidence of this is found in the way heaps manifest themselves in a process, if only we could observe this behavior… The ProcessWalker (PW) sample application explores each of the components within a process, including all of its heaps. PW identifies the heaps in a process and shows the amount of reserved and committed memory associated with a particular heap. As with all other regions of memory in a process, the smallest region of committed memory within a heap is one page (4096 bytes). This does not mean that the smallest amount of memory that can be allocated in a heap is 4096 bytes; rather, the heap manager commits pages of memory as needed to satisfy specific allocation requests. If, for example, an application allocates 100 bytes via a call to GlobalAlloc, the heap manager allocates a 100-byte chunk of memory within its committed region for this request. If there is not enough committed memory available at the time of the request, the heap manager simply commits another page to make the memory available. Ideally, then, if an application repetitively allocates 100-byte chunks of memory, the heap will commit an additional page of memory every forty-first request (40 * 100 bytes = 4000 bytes). Upon the forty-first request for a chunk of 100 bytes, the heap manager realizes there is not enough committed memory to satisfy the request, so it commits another page of memory and then completes the requested allocation. In this way, the heap manager is responsible for managing the virtual memory environment completely transparent of the application. In reality, though, the heap manager requires additional memory for managing the memory in the heap. So instead of allocating only 100 bytes as requested, it also allocates some space for managing each particular chunk of memory. The type of memory and the size of the allocation determine the size of this additional memory. I’ll discuss these issues later in this article. The two Types of Heaps Every process in Windows has one heap called the default heap. Processes can also have as many other dynamic heaps as they wish, simply by creating and destroying them on the fly. The system uses the default heap for all global and local memory management functions, and the C run-time library uses the default heap for supporting malloc functions. The heap memory functions, which indicate a specific heap by its handle, use dynamic heaps. The behavior of dynamic heaps is discussed in the “Heap Memory API” section later in this article. The default and dynamic heaps are basically the same thing, but the default heap has the special characteristic of being identifiable as the default. This is how the C run-time library and the system identify which heap to allocate from. The GetProcessHeap function returns a handle to the default heap for a process. Since functions such as GlobalAlloc or malloc are executed within the context of the thread that called them, they can simply call GetProcessHeap to retrieve a handle to the default heap, and then manage memory accordingly. Managing the Default Heap Both default and dynamic heaps have a specific amount of reserved and committed memory regions associated with them, but they behave differently with respect to these limits. The default heap’s reserved and committed memory region sizes are designated when the application is linked. Each application carries this information about itself within its executable image information. You can view this information by dumping header information for the executable image. For example, type the following command at a Windows NT command prompt (PWALK.EXE is used here to complete the example; you will need to substitute your own path and executable file): 12345678910111213141516171819202122232425262728link32 -dump -headers d:\\samples\\walker\\pwalk.exe...OPTIONAL HEADER VALUES 10B magic # 2.29 linker version B000 size of code 1E800 size of initialized data 600 size of uninitialized data 18470 address of entry point 10000 base of code 20000 base of data ----- new ----- 400000 image base 10000 section alignment 200 file alignment 2 subsystem (Windows GUI) 0.B operating system version 1.0 image version 3.A subsystem version C0000 size of image 400 size of headers 0 checksum 10000 size of stack reserve 1000 size of stack commit 10000 size of heap reserve 1000 size of heap commit ... The last two entries are hexadecimal values specifying the amount of reserved and committed space initially needed by the application. There are two ways to tell the linker what to use for these values. You can link your application with a module definition file and include a statement in the file like the following: 1HEAPSIZE 0x10000 0x1000 Or you can directly inform the linker by using the /HEAP linker switch, as in: 1/HEAP: 0x10000, 0x1000 In both examples, the heap is specified to initially have 0x10000 (64K) bytes reserved address space and 0x1000 (4K) bytes committed memory. If you fail to indicate the heap size in either method, the linker uses the default values of 0x100000 (1 MB) reserved address space and 0x1000 (4K) committed memory. The linker accepts almost any value for heap reserve space, because the application loader ensures that the application will meet certain minimum requirements during the load process. In other words, you can link an application with an initial heap value of 1 page reserved address space. The linker doesn’t perform any data validation; it simply marks the executable with the given value. Yet, since the minimum range of address space that can be reserved is 16 pages (64K), the loader compensates by reserving 16 pages for the application heap at load time. As indicated above, options exist that indicate how much memory should initially be committed for an application’s default heap. The problem is they don’t seem to work yet. The linker for the second beta release of Windows NT marks all executable applications with 0x1000 (4K) initial committed memory for the default heap size, regardless of the value indicated as an option. Yet this is not that big a deal because it actually may be better for an application to commit as it needs to, rather than to commit memory that is not being used. A Default Heap That Grows and Spreads In its simplest form, the default heap spans a range of addresses. Some ranges are reserved, while others are committed and have pages of memory associated with them. In this case the addresses are contiguous, and they all originated from the same base allocation address. In some cases the default heap needs to allocate more memory than is available in its current reserved address space. For these cases the heap can either fail the call that requests the memory, or reserve an additional address range elsewhere in the process. The default heap manager opts for the second choice. When the default heap needs more memory than is currently available, it reserves another 1-MB address range in the process. It also initially commits as much memory as it needs from this reserved address range to satisfy the allocation request. The default heap manager is then responsible for managing this new memory region as well as the original heap space. If necessary, it will repeat this throughout the application until the process runs out of memory and address space. You could end up with a default heap that manifests itself in your process in a manner similar to the heap represented in Figure 2. Figure 2. The default heap for each process can expand into free address regions within the process. So the default heap is not really a static heap at all. In fact, it may seem like a waste of energy to bother with managing the size of the initial heap since the heap always behaves in the same way after initialization. Managing the size of the default heap offers one advantage, though. It takes time to locate and reserve a new range of addresses in a process; you can save time by simply reserving a large enough address range initially. If you expect your application to require more heap space than the 1-MB default, reserve more address space initially to avoid allocating another region of addresses in your process later. Remember, each application has 2 gigabytes (GB) of address space to work with, and requires very little physical memory to support it. Global and Local Memory Functions At first glance it appears that the local and global memory management functions exist in Windows purely for backward compatibility with Windows version 3.1. This may be true, but the functions are managed as efficiently as the new heap functions discussed below. In fact, porting an application from 16-bit Windows does not necessarily include migrating from global and local memory functions to heap memory functions. The global and local functions offer the same basic capabilities (and then some) and are just as fast to work with. If anything, they are probably more convenient to work with because you do not have to keep track of a heap handle. Nonetheless, the implementation of these functions is not the same as it was for 16-bit Windows. 16-bit Windows had a global heap, and each application had a local heap. Those two heap managers implemented the global and local functions. Allocating memory via GlobalAlloc meant retrieving a chunk of memory from the global heap, while LocalAlloc allocated memory from the local heap. Windows now has a single heap for both types of functions—the default heap described above. Now you’re probably wondering if there is any difference between the local and global functions themselves. Well, the answer is no, they are now the same. In fact, they are interchangeable. Memory allocated via a call to LocalAlloc can be reallocated with GlobalReAlloc and then locked by LocalLock. The following table lists the global and local functions now available. Global memory functions Local memory functions GlobalAlloc LocalAlloc GlobalDiscard LocalDiscard GlobalFlags LocalFlags GlobalFree LocalFree GlobalHandle LocalHandle GlobalLock LocalLock GlobalReAlloc LocalReAlloc GlobalSize LocalSize GlobalUnlock LocalUnlock It seems redundant to have two sets of functions that perform exactly the same, but that’s where the backward compatibility comes in. Whether you used the global or local functions in a 16-bit Windows application before doesn’t matter now—they are equally efficient. Types of Global and Local Memory In the Windows API, the global and local memory management functions provide two types of memory, MOVEABLE and FIXED. MOVEABLE memory can be further qualified as DISCARDABLE memory. When you allocate memory with either GlobalAlloc or LocalAlloc, you designate the type of memory you want by supplying the appropriate memory flag. The following table lists and describes each memory flag for global and local memory. | Global memory flag | Local memory flag | Allocation meaning | | — | — | | GMEM_FIXED | LMEM_FIXED | Allocate fixed memory. | | GMEM_MOVEABLE | LMEM_MOVEABLE | Allocate movable memory. | | GMEM_DISCARDABLE | LMEM_DISCARDABLE | Allocate discardable, movable memory. | | GMEM_ZEROINIT | LMEM_ZEROINIT | Initialize memory to zeros during allocation. | | GMEM_NODISCARD | LMEM_NODISCARD | Do not discard other memory to meet the needs of this allocation; instead, fail this request. | | GMEM_NOCOMPACT | LMEM_NOCOMPACT | Do not discard or move other memory to meet the needs of this allocation; instead, fail this request. | | GMEM_SHARE, GMEM_DDESHARE | N/A | Allocate global memory that is more efficient to use with DDE. | It is surprising that the distinction between FIXED and MOVEABLE memory still exists in these functions. In 16-bit Windows, MOVEABLE memory compacted the local and global heaps to reduce fragmentation and make more memory available to all applications. Yet the Windows NT virtual memory system does not rely on these techniques for efficient memory management and has little to gain by applications using them. In any case, they still exist and could actually be used in some circumstances. When allocating FIXED memory in Windows, the GlobalAlloc and LocalAlloc functions return a 32-bit pointer to the memory block rather than a handle as they do for MOVEABLE memory. The pointer can directly access the memory without having to lock it first. This pointer can also be passed to the GlobalFree and LocalFree functions to release the memory without having to first retrieve the handle by calling the GlobalHandle function. With FIXED memory, allocating and freeing memory is similar to using the C run-time functions _malloc and _free. MOVEABLE memory, on the other hand, cannot provide this luxury. Because MOVEABLE memory can be moved (and DISCARDABLE memory can be discarded), the heap manager needs a handle to identify the chunk of memory to move or discard. To access the memory, the handle must first be locked by calling either GlobalLock or LocalLock. As in 16-bit Windows, the memory cannot be moved or discarded while the handle is locked. The MOVEABLE Memory Handle Table As it turns out, each handle to MOVEABLE memory is actually a pointer into the MOVEABLE memory handle table. The handle table exists outside the heap, elsewhere in the process’s address space. To learn more about this behavior, we created a test application that allocates several MOVEABLE memory blocks from the default heap. The handle table was created only upon allocation of the first MOVEABLE chunk of memory. This is nice, because if you never allocate any MOVEABLE memory, the table is never created and does not waste memory. ProcessWalker reveals that when this handle table is created, 1 page of memory is committed for the initial table and 512K of address space is reserved (see Figure 3). Figure 3. ProcessWalker highlights changes in the address space of a process. The heap memory handle table is shown as it looks immediately after it is created. If you work out the math you can see that the number of memory handles available for MOVEABLE memory is limited. 512K bytes / 8 bytes per handle = 65,535 handles. In fact, the system imposes this limitation on each process. Fortunately, this only applies to MOVEABLE memory. You can allocate as many chunks of FIXED memory as you like, provided the memory and address space are available in your process. Each handle requires 8 bytes of committed memory in the handle table to represent information, including the virtual address of the memory, the lock count on the handle, and the type of memory. Figure 4 shows how the handle table looks when viewed in ProcessWalker. Figure 4. Each MOVEABLE memory handle is 8 bytes and represents the location of the chunk of memory. Note In the ProcessWalker view window, each line is 16 bytes (two memory handles). The far-left column represents the address of each line within the process. The addresses in white indicate the lines in which data changed since the last refresh. Red text shows exactly which bytes changed. This feature allows you to easily track changes in memory in response to certain events within the child process. A very useful debugging tool, to say the least. The first 8 bytes in the view window—at address 0x000f0000—show the first handle allocated in the table. The handle value for this example would actually be 0x000f0004, which is a 4-byte offset into the 8-byte handle table entry. At the 4-byte offset in the table entry is the current 32-bit address representing the location of the chunk of memory. In this example, the 32-bit address is 0x00042A70, reading 4 bytes from right to left starting at 0x000f0008 and ending at 0x000f0004. If the memory is moved to a new location, this address changes to reflect the new location. If you back up 4 bytes from the handle to the first byte in this handle table entry, you’ll find the lock count for the handle. Remember, a block of memory can be moved or discarded only if its handle’s lock count is zero. Since there is only 1 byte to record the lock count, it stands to reason that a handle can be locked only 256 (0xFF) times. Unfortunately, the system currently does not warn you when you reach that limit. If you lock a handle a 257th time, you receive a valid pointer to the memory, but the lock count remains at 256. It is possible, then, that you could unlock the handle 256 times and expect the memory to be locked. Yet the lock count is decremented to 0, and the memory could be moved or discarded. So what happens if you try to access the pointer you believe to be valid? Well, it depends on what is now at the address where you believe the memory to be. The memory could still be there, or some other memory could have been moved there. This is not the desired behavior, and we hope the system will be fixed to prevent this from happening. Incidentally, how should the system be fixed? The easiest thing to do would be to fail the call to GlobalLock or LocalLock on any attempt beyond 256, so check the return value to make sure the function succeeds. The third and fourth bytes of the handle represent the memory type, that is, MOVEABLE, DDESHARE, or DISCARDABLE. Presumably because of compatibility with 16-bit Windows, local and global memory flags that have the same name and meaning do not have the same value. (The WINBASE.H header file defines each flag.) Yet once the memory is allocated, the handle table entry records no difference. Whether you use LMEM_DISCARDABLE or GMEM_DISCARDABLE does not matter—the handle table identifies all DISCARDABLE memory the same way. A little exploring in ProcessWalker shows the following type value identifiers. Memory type Byte 3 Byte 4 MOVEABLE 02 – DISCARDABLE 06 – DDESHARE – 08 Keep in mind that these values as represented in the handle table are not documented, so they could change with a new release of Windows NT. But it would be easy to use ProcessWalker to view the handle table to see the changes. Returning to the test application example… After allocating the first MOVEABLE memory block and thereby creating the handle table, we used the test application to allocate 15 more handles from the table. A quick view of the memory window showed the first eight lines had data associated with them, while the rest of the committed page was filled with zeros. Then we allocated one more handle, making a total of 17 handles. The view window was refreshed to indicate what changes occurred in the table as a result of allocating the seventeenth handle only. Figure 4 (above) shows the results. As you can see, more than 8 bytes changed during the last allocation. In fact, 8 lines were updated for a total of 128 bytes. The first 8 bytes represent the seventeenth handle entry information as expected. The following 120 bytes indicate that the heap manager initializes the handle table every sixteenth handle. Examining this initialization data shows that the next 15 available handle table entries are identified in the table. When allocating the eighteenth handle, the location of the nineteenth handle is identified by the address location portion of the eighteenth handle table entry. As expected, then, if a handle is removed from the table (the memory is freed), the address location in that entry is replaced with the location of the next available handle after it. This behavior indicates that the heap manager keeps the location of the next available handle table entry in a static variable and uses that entry itself to store the location of the subsequent entry. Notice also that the last initialized entry has a null location for the next address. The heap manager uses this as an indicator to initialize another 16 handle table entries during the next handle allocation. Another efficiency in the heap manager is its ability to commit pages of memory for the handle table as it needs them, not all 128 pages (512K) at once. Since the heap manager initializes its handle table in 16-handle (128-byte) increments, it is easy to determine when to commit a new page of memory. Every thirty-second (4096 / 128 = 32) initialization requires a new page of committed memory. Also, the entries do not straddle page boundaries, so their management is easier and potentially more efficient. CRT Library Managing memory in 16-bit Windows involved a great deal of uncertainty about using the C run-time (CRT) library. Now, there should be little hesitation. The current CRT library is implemented in a manner similar to FIXED memory allocated via the local and global memory management functions. The CRT library is also implemented using the same default heap manager as the global and local memory management functions. Subsequent memory allocations via malloc, GlobalAlloc, and LocalAlloc return pointers to memory allocated from the same heap. The heap manager does not divide its space among the CRT and global/local memory functions, and it does not maintain separate heaps for these functions. Instead, it treats them the same, promoting consistent behavior across the types of functions. As a result, you can now write code using the functions you’re most comfortable with. And, if you’re interested in portability, you can safely use the CRT functions exclusively for managing heap memory. Heap Memory API As mentioned earlier, you can create as many dynamic heaps as you like by simply calling HeapCreate. You must specify the maximum size of the heap so that the function knows how much address space to reserve. You can also indicate how much memory to commit initially. In the following example, a heap is created in your process that reserves 1 MB of address space and initially commits two pages of memory: 1hHeap = HeapCreate (0, 0x2000, 0x100000); Since you can have many dynamic heaps in your process, you need a handle to identify each heap when you access it. The HeapCreate function returns this handle. Each heap you create returns a unique handle so that several dynamic heaps may coexist in your process. Having to identify your heap by handle also makes managing dynamic heaps more difficult than managing the default heap, since you have to keep each heap handle around for the life of the heap. If you’re bothered by having to keep this handle around, there is an alternative. You can use the heap memory functions on the default heap instead of creating a dynamic heap explicitly for them. To do this, simply use the GetProcessHeap function to get the handle of the default heap. For simple allocations of short-term memory, the following example taken from the ProcessWalker sample application illustrates how easy this is to do: 12345678char *szCaption;int nCaption = GetWindowTextLength (hWnd);/* retrieve view memory range */szCaption = HeapAlloc (GetProcessHeap (), HEAP_ZERO_MEMORY, nCaption+1);GetWindowText (hViewWnd, szCaption, nCaption); In this example the default heap is chosen because the allocation is independent of other memory management needs, and there is no reason to create a dynamic heap just for this one allocation. Note that you could achieve the same result by using the global, local, or CRT functions since they allocate only from the default heap. As shown earlier, the first parameter to HeapCreate allows you to specify an optional HEAP_NO_SERIALIZE flag. A serialized heap does not allow two threads to access it simultaneously. The default behavior is to serialize access to the heap. If you plan to use a heap strictly within one thread, specifying the HEAP_NO_SERIALIZE flag improves overall heap performance slightly. Like all of the heap memory functions, HeapAlloc requires a heap handle as its first argument. The example uses the GetProcessHeap function instead of a dynamic heap handle. The second parameter to HeapAlloc is an optional flag that indicates whether the memory should be zeroed first and whether to generate exceptions on error. To get zeroed memory, specify the HEAP_ZERO_MEMORY flag as shown above. The generate exceptions flag (HEAP_GENERATE_EXCEPTIONS) is a useful feature if you have exception handling built into your application. When using this flag, the function raises an exception on failure rather than just returning NULL. Depending on your use, exception handling can be an effective way of triggering special events—such as low memory situations—in your application. HeapAlloc has a cousin called HeapReAlloc that works in much the same way as the standard global and local memory management functions described earlier. Use HeapReAlloc primarily to resize a previous allocation. This function has four parameters, three of which are the same as for HeapAlloc. The new parameter, lpMem, is a pointer to the chunk of memory being resized. It is important to note that although heap memory is not movable as in global and local memory, it may be moved during the HeapReAlloc function. This function returns a pointer to the resized chunk of memory, which may or may not be at the same location as initially indicated by the pointer passed to the function. This is the only time memory can be moved in dynamic heaps, and the only chunk of memory affected is the one identified by lpMem in the function. You can also override this behavior by specifying the HEAP_REALLOC_IN_PLACE_ONLY flag. With this flag, if there is not enough room to reallocate the memory in place, the function returns with failure status rather than move the memory. Memory allocated with HeapAlloc or reallocated with HeapReAlloc can be freed by calling HeapFree. This function is easy to use: simply indicate the heap handle and a pointer to the chunk of memory to free. Completing the example above, here is how you can use HeapFree with the default heap: 12/* free default heap memory */HeapFree (GetProcessHeap (), szCaption); This example shows how easy it can be to work with heaps in Windows. However, you may still want to create a dynamic heap specifically to manage complex dynamic data structures in your application. In fact, one nice benefit of heap memory is how well it caters to the needs of traditional data structures such as binary trees, linked lists, and dynamic arrays. Having the heap handle provides a way of uniquely identifying these structures independently. One example that comes to mind is when you’re building a multiwindow application—perhaps a multiple document interface (MDI) application. Simply create and store a heap handle in the window extra bytes or window property list for each window during the WM_CREATE message. Then it is easy to write a window procedure that manages data structures from its own heap by retrieving the heap handle when necessary. When the window goes away, simply have it destroy the heap in the WM_DESTROY message. You can easily destroy dynamic heaps by calling HeapDestroy on a specific heap handle. This powerful function will remove a heap regardless of its state. The HeapDestroy function doesn’t care whether you have outstanding allocations in the heap or not. To make heap memory management most efficient, you would only create a heap of the size you need. In some cases it is easy to determine the heap size you need—if you’re allocating a buffer to read a file into, simply check the size of the file. In other cases, heap size is more difficult to figure—if you’re creating a dynamic data structure that grows according to user interaction, it is difficult to predict what the user will do. Dynamic heaps have a provision for this latter circumstance. By specifying a maximum heap size of zero, you make the heap assume a behavior like the default heap. That is, it will grow and spread in your process as much as necessary, limited only by available memory. In this case, available memory means available address space in your process and available pagefile space on your hard disk. Overhead on Heap Memory Allocations With all types of heap memory, an overhead is associated with each memory allocation. This overhead is due to: Granularity on memory allocations within the heap. The overhead necessary to manage each memory segment within the heap. The granularity of heap allocations in 32-bit Windows is 16 bytes. So if you request a global memory allocation of 1 byte, the heap returns a pointer to a chunk of memory, guaranteeing that the 1 byte is available. Chances are, 16 bytes will actually be available because the heap cannot allocate less than 16 bytes at a time. Global and Local Memory Functions For global and local memory functions, the documentation suggests that you use the GlobalSize and LocalSize functions to determine the exact size of the allocation, but in my tests this function consistently returned the size I requested and not the size actually allocated. To confirm this finding, turn to ProcessWalker again and view the committed memory in your default heap. For the sake of observation, perform consecutive allocations of 1, 2, 3, 14, 15, and 16 bytes using either GlobalAlloc or LocalAlloc. In this particular example, we used GlobalAlloc with the GMEM_MOVEABLE flag, but the result is the same for memory allocated as GMEM_FIXED. Then, refresh your view of the committed memory in the heap. Finally, scroll the view window so that the addresses of the allocated blocks of memory are all in view. You should see something similar to the window in Figure 5. Figure 5. A ProcessWalker view of the default heap after making consecutive allocations of 1, 2, 3, 14, 15, and 16 bytes using the GlobalAlloc function. In Figure 5, the first line represents the header for the 1-byte allocation, followed immediately by the minimum 16 bytes allocated for the 1-byte request. Just to add visual clarity, the letter m appears in each allocated byte beginning at the address of the allocation. The m’s are visible in the ASCII representation of memory to the right of each line. At the end of all heap allocations is a heap tail, as indicated by the last line of new information in Figure 5. So, although you request only 1 byte and GlobalSize returns a size of 1 byte, there are actually 16 bytes allocated and available. Similarly, for the 3-byte allocation, 13 additional bytes are available. Even more interesting is the apparent waste of memory for allocations of 14, 15, and 16 bytes. In these allocations an additional 16 bytes were allocated for no reason. These are the lines immediately following the lines with 14, 15, and 16 m characters. Presumably you could also use these extra 16 bytes, but again GlobalSize does not indicate their existence. So what is the cost of allocating from a heap? Well, it depends on the size of the allocation. Every 1-byte allocation uses a total of 32 bytes, and a 16-byte allocation uses a total of 48 bytes. This is a considerable amount of overhead on smaller allocations. Therefore, it would not be a good idea to accumulate a number of small heap allocations because they would cost a great deal in actual memory used. On the other hand, there is nothing wrong with allocating a large chunk of memory from the heap and dividing it into smaller pieces. Heap Memory Functions Similar to the global and local memory functions, the heap memory functions have a minimum 16-byte granularity and the same overhead on memory allocations. Figure 6 presents a ProcessWalker view of a dynamic heap using the same allocations of 1, 2, 3, 14, 15, and 16 bytes, but this time using the HeapAlloc function. Figure 6. A ProcessWalker view of a dynamic heap after making consecutive allocations of 1, 2, 3, 14, 15, and 16 bytes using the HeapAlloc function. For this example, the letter h appears in the allocated memory for comparison with the global memory allocations shown in Figure 5. A comparison of Figures 5 and 6 shows that the two heaps behave in an identical manner for the test allocations. Also, as in the global and local memory functions, a HeapSize function determines the exact amount of memory allocated for a given request. Unfortunately, this function seems to be implemented exactly like the GlobalSize and LocalSize functions. In my tests, HeapSize always returned the amount of the allocation request, while actually allocating with the same 16-byte granularity. C Run-Time Memory Functions Can you guess how the C run-time library memory functions will behave on this same test? Yes, the C run-time functions exhibit the same heap memory behavior. Figure 7 represents the same allocations using the malloc function. Figure 7. A ProcessWalker view of the default heap after making consecutive allocations of 1, 2, 3, 14, 15, and 16 bytes using the C run-time malloc function. Unlike the global/local and heap memory functions, the C run-time library does not provide a means for retrieving the exact number of bytes allocated. Then again, how useful is that information anyway, since the GlobalSize, LocalSize, and HeapSize functions fail to perform accurately? Summary and Recommendations Heap memory management in Windows is greatly improved over 16-bit Windows. Instead of a systemwide global heap and application-specific local heaps, each application has a default heap and as many dynamic heaps as the application wants to create. Both types of heaps can grow dynamically and use as much of the address space as they need to satisfy an allocation request. The default heap provides all dynamic memory allocations for the C run-time library malloc functions as well as the global and local memory functions. The heap memory functions can also allocate from the default heap by using its handle, which they retrieve by calling the GetProcessHeap function. The dynamic heap provides serialization to avoid conflict among multiple threads accessing the same heap. To support the management of multiple dynamic heaps, each heap is identified by a unique handle returned by the HeapCreate function. Heaps do at least one thing well—they allocate smaller chunks of memory rather quickly. So whenever you are reluctant to create an automatic variable in a window procedure simply for an occasional LoadString buffer, why not use GlobalAlloc or LocalAlloc, malloc or HeapAlloc? Heaps are also very good at allocating storage for dynamic data structures. Dynamic heaps lend themselves particularly well to managing several distinct dynamic data structures in an application. Finally, what is the cost of using a heap? Well, if you never use any MOVEABLE (including DISCARDABLE) memory, the cost is considerably lower, and MOVEABLE memory probably doesn’t buy you much in a 32-bit linear address space. Not to mention that at 8 bytes per MOVEABLE memory handle, the system limits processes to 65,535 handles. Each chunk of heap memory allocated in either the default heap or a dynamic heap is subject to a 16-byte granularity and is charged a 16-byte header. In total, then, allocating 1 byte of MOVEABLE memory costs 40 bytes (8 bytes for the handle table entry, 16 bytes for granularity, and 16 bytes for the header). Happy heaping. Article link: http://xnerv.wang/msdn-managing-heap-memory/ Reprinted from: (MSDN) Managing Heap Memory","categories":[{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/categories/Windows/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/tags/Windows/"},{"name":"Heap","slug":"Heap","permalink":"https://xnerv.wang/tags/Heap/"},{"name":"MSDN","slug":"MSDN","permalink":"https://xnerv.wang/tags/MSDN/"},{"name":"Memory Management","slug":"Memory-Management","permalink":"https://xnerv.wang/tags/Memory-Management/"}]},{"title":"(MSDN) Heap: Pleasures and Pains","slug":"msdn-heap-pleasures-and-pains","date":"2017-11-14T06:06:00.000Z","updated":"2023-08-21T02:24:20.150Z","comments":true,"path":"msdn-heap-pleasures-and-pains/","link":"","permalink":"https://xnerv.wang/msdn-heap-pleasures-and-pains/","excerpt":"Murali R. Krishnan Microsoft Corporation February 1999 Summary: Discusses common heap performance problems and how to protect against them. (9 printed pages) Introduction Are you a happy-go-lucky user of dynamically allocated C/C++ objects? Do you use Automation extensively for communicating back and forth between modules? Is it possible that your program is slow because of heap allocation? You are not alone. Almost all projects run into heap issues sooner or later. The common tendency is to say, “It’s the heap that’s slow and my code is really good.” Well, that is partially right. It pays to understand more about heap, its usage, and what can happen.","text":"Murali R. Krishnan Microsoft Corporation February 1999 Summary: Discusses common heap performance problems and how to protect against them. (9 printed pages) Introduction Are you a happy-go-lucky user of dynamically allocated C/C++ objects? Do you use Automation extensively for communicating back and forth between modules? Is it possible that your program is slow because of heap allocation? You are not alone. Almost all projects run into heap issues sooner or later. The common tendency is to say, “It’s the heap that’s slow and my code is really good.” Well, that is partially right. It pays to understand more about heap, its usage, and what can happen. What Is a Heap? (If you already know what a heap is, you can jump ahead to the section “What Are Common Heap Performance Problems?”) A heap is used for allocating and freeing objects dynamically for use by the program. Heap operations are called for when: The number and size of objects needed by the program are not known ahead of time. An object is too large to fit into a stack allocator. A heap uses parts of memory outside of what is allocated for the code and stack during run time. The following graph shows the different layers of heap allocators. GlobalAlloc/GlobalFree: Heap calls that talk directly to the per-process default heap. LocalAlloc/LocalFree: Heap calls that talk directly to the per-process default heap. COM’s IMalloc allocator (or CoTaskMemAlloc / CoTaskMemFree): Functions use the default per-process heap. Automation uses the Component Object Model (COM)'s allocator, and the requests use the per-process heap. C/C++ Run-time (CRT) allocator: Provides malloc() and free() as well as new and delete operators. Languages like Microsoft Visual Basic® and Java also offer new operators and use garbage collection instead of heaps. CRT creates its own private heap, which resides on top of the Windows heap. The Windows heap is a thin layer surrounding the Windows run-time allocator. All APIs forward their requests to the NTDLL. The Windows run-time allocator provides the core heap allocator within Windows. It consists of a front-end allocator with 128 free lists for sizes ranging from 8 to 1,024 bytes. The back-end allocator uses virtual memory to reserve and commit pages. At the bottom of the chart is the Virtual Memory Allocator, which reserves and commits pages used by the OS. All allocators use the virtual memory facility for accessing the data. Shouldn’t it be simple to allocate and free blocks? Why would this take a long time? Notes on Heap Implementation Traditionally, the operating system and run-time libraries come with an implementation of the heap. At the beginning of a process, the OS creates a default heap called Process heap.The Process heap is used for allocating blocks if no other heap is used. Language run times also can create separate heaps within a process. (For example, C run time creates a heap of its own.) Besides these dedicated heaps, the application program or one of the many loaded dynamic-link libraries (DLLs) may create and use separate heaps. Windows offers a rich set API for creating and using private heaps. When applications or DLLs create private heaps, these live in the process space and are accessible process-wide. Any allocation of data made from a given heap should be freed for the same heap. (Allocation from one heap and free to another makes no sense.) The heap sits on top of the operating system’s Virtual Memory Manager in all virtual memory systems. The language run-time heaps reside on top of the virtual memory, as well. In some cases, these are layered on OS heaps, but the language run-time heap performs its own memory management by allocating large blocks. Bypassing the OS heap to use the virtual memory functions may enable a heap to do a better job of allocating and using blocks. Typical heap implementations consist of front-end and back-end allocators. The front-end allocator maintains a free list of fixed-sized blocks. On an allocation call, the heap attempts to find a free block from the front-end list. If this fails, the heap is forced to allocate a large block from the back end (reserving and committing virtual memory) to satisfy the request. Common implementations have per-block allocation overhead, which costs execution cycles and also reduces available storage. The Knowledge Base article Q10758, “Managing Memory with calloc() and malloc()” (search on article ID number), contains more background on these topics. Also, a detailed discussion on the heap implementations and designs can be found in “Dynamic Storage Allocation: A Survey and Critical Review” by Paul R. Wilson, Mark S. Johnstone, Michael Neely, and David Boles. In International Workshop on Memory Management,Kinross, Scotland, UK, September 1995 (http://www.cs.utexas.edu/users/oops/papers.html). The implementation (Windows NT version 4.0 and later) uses 127 free lists of 8-byte aligned blocks ranging from 8 to 1,024 bytes and a grab-bag list. The grab-bag list (free list[0]) holds blocks greater than 1,024 bytes in size. The free list contains objects linked together in a doubly linked list. By default, the Process heap performs coalescing operations. (Coalescing is the act of combining adjacent free blocks to build a larger block.) Coalescing costs additional cycles but reduces internal fragmentation of heap blocks. A single global lock protects the heap against multithreaded usage. (See the first commandment in Server Performance and Scalability Killers by George Reilly.) This lock is essential to protecting the heap data structures from random access across multiple threads. This lock can have an adverse impact on performance when heap operations are too frequent. What Are Common Heap Performance Problems? Here are the most common obstacles you will encounter when working with the heap: Slowdown as a result of allocation operations. It simply takes a long time to allocate. The most likely cause of the slowdown is that the free lists do not have the blocks, and hence the run-time allocator code spends cycles hunting for a larger free block or allocating a fresh block from the back-end allocator. Slowdown as a result of free operations. Free operations consume more cycles, mainly if coalescing is enabled. During coalescing, each free operation should “find” its neighbors, pull them out to construct a larger block, and reinsert the larger block into the free list. During that find, memory may be touched in a random order, causing cache misses and performance slowdown. Slowdown as a result of heap contention. Contention occurs when two or more threads try to access data at the same time and one must wait for the other to complete before it can proceed. Contention always causes trouble; it’s by far the biggest problem that one encounters on multiprocessor systems. An application or DLL with heavy use of memory blocks will slow down when run with multiple threads (and on multiprocessor systems). The use of single lock—the common solution—means that all operations using the heap are serialized. The serialization causes threads to switch context while waiting for the lock. Imagine the slowdown caused by stop-and-go at a flashing red stoplight. Contention usually leads to context switch of the threads and processes. Context switches are very costly, but even more costly is the loss of data from the processor cache and the rebuilding of that data when the thread is brought to life afterwards. Slowdown as a result of heap corruption. Corruption occurs when the application does not use the heap blocks properly. Common scenarios include double free or use of a block after a free, and the obvious problems of overwriting beyond block boundaries. Slowdown as a result of frequent allocs and reallocs. This is a very common phenomenon when you use scripting languages. The strings are repeatedly allocated, grown with reallocation, and freed up. Don’t do this. Try to allocate large strings, if possible, and use the buffer. An alternative is to minimize concatenation operations. Contention is the problem that introduces slowdown in the allocation as well as free operations. Ideally we would like to have a heap with no contention and fast alloc/free. Alas, such a general-purpose heap does not exist yet, though it might sometime in the future. In all the server systems (such as IIS, MSProxy, DatabaseStacks, Network servers, Exchange, and others), the heap lock is a BIG bottleneck. The larger the number of processors, the worse the contention. Protecting Yourself from the Heap Now that you understand the problems with heap, don’t you want the magic wand that can eliminate these problems? I wish there were one. But there is no magic to make the heap go faster—so don’t expect to make things faster in the last week before shipping the product. Plan your heap strategy earlier, and you will be far better off. Altering the way you use the heap, and reducing the number of heap operations, is a solid strategy for improving performance. How do you reduce the use of heap operations? You can reduce the number of heap operations by exploiting locality inside data structures. Consider the following example: 12345678910111213141516171819202122232425262728293031323334struct ObjectA &#123; // data for objectA&#125;struct ObjectB &#123; // data for objectB&#125;// Use of ObjectA and ObjectB together.//// Use pointers//struct ObjectB &#123; struct ObjectA * pObjA; // data for objectB&#125;//// Use embedding//struct ObjectB &#123; struct ObjectA pObjA; // data for objectB&#125;//// Aggregation – use ObjectA and ObjectB inside another object//struct ObjectX &#123; struct ObjectA objA; struct ObjectB objB;&#125; Avoid using pointers for associating two data structures. If you use pointers to associate two data structures, the objects A and B from the preceding example will be allocated and freed separately. This is an additional cost—and is the practice we want to avoid. Embed pointed child objects into the parent object. Anytime we have a pointer in an object, it means we have a dynamic element (80 percent) and a new location to be dereferenced. Embedding increases locality and reduces the need for further allocation/free. This will improve the performance of your application. Combine smaller objects to form bigger objects (aggregation). Aggregation reduces the number of blocks allocated and freed up. If you have multiple developers working on various parts of a design, you may end up with many small objects that can be combined. The challenge of this integration is to find the right aggregation boundaries. Inline a buffer that can satisfy 80 percent of your needs (aka the 80-20 rule). In several situations, memory buffers are required for storing string/binary data and the total number of bytes is not known ahead of time. Take measurements and inline a buffer of a size that can satisfy 80 percent of your needs. For the remaining 20 percent, you can allocate a new buffer and have a pointer for that buffer. This way, you reduce the allocation and free calls as well as increase spatial locality of data, which ultimately will improve the performance of your code. Allocate objects in chunks (chunking). Chunking is a way of allocating objects in groups of more than one object at a time. If you have to keep track of a list of items, for example a list of {name, value} pairs, you have two options: Option 1 is to allocate one node per name-value pair. Option 2 is to allocate a structure that can hold, say, five name-value pairs. And if, for example, storing four pairs is a common scenario, you would reduce the number of nodes and the amount of extra space needed for additional linked-list pointers. Chunking is processor cache-friendly, especially for the L1-cache, because of the increased locality it offers—not to mention that some of the data blocks are located in the same virtual page for chunked allocations. Use _amblksiz appropriately. The C run time (CRT) has its custom front-end allocator that allocates blocks in sizes of _amblksiz from the back end (Windows heap). Setting _amblksiz to a higher value can potentially reduce the number of calls made to the back end. This is applicable only to programs using the CRT extensively. The savings you will gain by using the preceding techniques will vary with object types, sizes, and workload. But you will always gain in performance and scalability. On the downside, the code will be a bit specialized, but it can be manageable if well-thought-out. More Performance Boosters The following are a few more techniques for enhancing speed: Use the Windows heap Thanks to the efforts and hard work of several folks, a few significant improvements went into Microsoft Windows 2000: Improved locking inside the heap code. The heap code uses one lock per heap. This global lock is used for protecting the heap data structure for multithreaded usage. Unfortunately, in high-traffic scenarios, a heap can still get bogged down in this global lock, leading to high contention and poor performance. On Windows 2000, the critical region of code inside locks is reduced to minimize the probability of contention, thus improving scalability. Use of Lookaside lists. The heap data structure uses a fast cache for all free items of blocks sized between 8 and 1,024 bytes (in 8-byte increments). The fast cache was originally protected within the global lock. Now lookaside lists are used to access the fast cache free list. These lists do not require locking, and instead use 64-bit interlocked operations, thus improving performance. Internal data structure algorithms are improved as well. These improvements eliminate the need for allocation caches, but do not preclude other optimizations. Evaluate your code with the Windows heap; it should be optimal for blocks of less than 1,024 bytes (1 KB) (blocks from front-end allocator). GlobalAlloc() and LocalAlloc() build on the same heap and are common mechanisms to access the per-process heaps. Use Heap* APIs to access the per-process heap, or to create your own heaps for allocation operations if high localized performance is desired. You can also use VirtualAlloc() / VirtualFree() operations directly if needed for large block operations. The described improvements made their way into Windows 2000 beta 2 and Windows NT 4.0 SP4. After the changes, the rate of heap lock contention fell significantly. This benefits all direct users of Windows heaps. The CRT heap is built on top of the Windows heap, but uses its own small-block heap and hence does not benefit from Windows changes. (Visual C++ version 6.0 also has an improved heap allocator.) Use allocation caches An allocation cache allows you to cache allocated blocks for future reuse. This can reduce the number of alloc/free calls to the process heap (or global heap), as well as allow maximum reuse of the blocks once allocated. In addition, allocation caches permit you to gather statistics to gain a better understanding of object usage at the higher level. Typically, a custom heap allocator is implemented on top of the process heap. The custom heap allocator behaves much like the system heap. The major difference is that it provides a cache on top of the process heap for the allocated objects. The caches are designed for a fixed set of sizes (for example, 32 bytes, 64 bytes, 128 bytes, and so on). This is a good strategy, but this sort of custom heap allocator misses the semantic information related to the objects that are allocated and freed. In contrast to the custom heap allocators, “Alloc-caches” are implemented as a per-class allocation cache. They can retain a lot of semantic information_,_ in addition to providing all the goodies of the custom heap allocator. Each allocation cache handler is associated with one object in the target binary. It can be initialized with a set of parameters indicating the concurrency level, size of the object, number of elements to keep in the free list, and so on. The allocation cache handler object maintains its own private pool of freed entities (not exceeding the specified threshold) and uses private lock for protection. Together, the allocation cache and private locks reduce the traffic to the main system heap, thus providing increased concurrency, maximum reuse, and higher scalability. A scavenger is required periodically to check the activity of all the allocation cache handlers and reclaim unused resources. If and when no activity is found, the pool of allocated objects can be freed, thus improving performance. Each alloc/free activity can be audited. The first level of information includes a total count of objects, allocations, and free calls made. You can derive the semantic relationship between various objects by looking at their statistics. The relationship can be used to reduce memory allocation using one of the many techniques just explained. Allocation caches also act as a debugging aid in helping you track down the number of objects that are not properly cleaned up. You can even find exact callers in fault by looking at dynamic stack back traces and signatures in addition to the objects that were not cleaned up. MP heap MP heap is a package for multiprocessor-friendly distributed allocation. Originally implemented by JVert, the heap abstraction here is built on top of the Windows heap package. MP heap creates multiple heaps and attempts to distribute allocation calls to different heaps with the goal of reducing the contention on any single lock. This package is a good step—sort of an improved MP-friendly custom heap allocator. However, it offers no semantic information and lacks in statistics. A common way to use the MP heap is as an SDK library. You can benefit greatly if you create a reusable component with this SDK. If you build this SDK library into every DLL, however, you will increase your working set. Rethink algorithms and data structures To scale on multiprocessor machines, algorithms, implementation, data structures, and hardware have to scale dynamically. Look at the data structures most commonly allocated and freed. Ask yourself, “Can I get the job done with a different data structure?” For example, if you have a list of read-only items that is loaded at application initialization time, this list does not need to be a linear-linked list. It can very well be a dynamically allocated array. A dynamically allocated array would reduce heap blocks in memory, reduce fragmentation, and therefore give you a performance boost. Reducing the number of small objects needed reduces the load on the heap allocator. For example, we used five different objects in our server’s critical processing path, each separately allocated and freed. Caching the objects together reduced heap calls from five to one and dramatically reduced the load on the heap, especially when we were processing more than 1,000 requests per second. If you make extensive use of Automation structures, consider factoring out Automation BSTRs from your mainline code, or at least avoid repeated operations on BSTR. (BSTR concatenation leads to excessive reallocs and alloc/free operations.) Summary Heap implementations tend to stay general for all platforms, and hence have heavy overhead. Each individual’s code has specific requirements, but design can accommodate the principles discussed in this article to reduce heap interaction. Evaluate the use of heap in your code. Streamline your code to use fewer heap calls: Analyze the critical paths and fix data structures. Make measurements to quantify the costs of heap calls before implementing custom wrappers. If you are unhappy about performance, ask the OS group to improve the heap. More requests of this sort mean more attention toward improving the heap. Ask the C run-time group to make the allocators thin wrappers on heaps provided by the OS. As a result, the cost of C run-time heap calls is reduced as the OS heap is improved. Heap improvements are continuously made in the operating system. Stay tuned and take advantage of the same. Murali Krishnan is a lead software design engineer with the Internet Information Server (IIS) team. He has worked on IIS since version 1.0 and has successfully shipped versions 1.0 through 4.0. Murali formed and led the IIS performance team for three years (1995-1998), and has influenced IIS performance from day one. He holds an M.S. from the University of Wisconsin-Madison and a B.S. from Anna University, India, both in Computer Science. Outside work, he reads, plays volleyball, and cooks at home. Article link: http://xnerv.wang/msdn-heap-pleasures-and-pains/ Reprinted from: (MSDN) Heap: Pleasures and Pains","categories":[{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/categories/Windows/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/tags/Windows/"},{"name":"Heap","slug":"Heap","permalink":"https://xnerv.wang/tags/Heap/"},{"name":"MSDN","slug":"MSDN","permalink":"https://xnerv.wang/tags/MSDN/"},{"name":"Memory Management","slug":"Memory-Management","permalink":"https://xnerv.wang/tags/Memory-Management/"}]},{"title":"UNIX/LINUX 平台可执行文件格式分析（转载）","slug":"analysis-of-unix-linux-platform-executable-file-format","date":"2017-11-14T02:21:00.000Z","updated":"2023-08-21T02:24:19.863Z","comments":true,"path":"analysis-of-unix-linux-platform-executable-file-format/","link":"","permalink":"https://xnerv.wang/analysis-of-unix-linux-platform-executable-file-format/","excerpt":"可执行文件格式综述 相对于其它文件类型，可执行文件可能是一个操作系统中最重要的文件类型，因为它们是完成操作的真正执行者。可执行文件的大小、运行速度、资源占用情况以及可扩展性、可移植性等与文件格式的定义和文件加载过程紧密相关。研究可执行文件的格式对编写高性能程序和一些黑客技术的运用都是非常有意义的。 不管何种可执行文件格式，一些基本的要素是必须的，显而易见的，文件中应包含代码和数据。因为文件可能引用外部文件定义的符号（变量和函数），因此重定位信息和符号信息也是需要的。一些辅助信息是可选的，如调试信息、硬件信息等。基本上任意一种可执行文件格式都是按区间保存上述信息，称为段（Segment）或节（Section）。不同的文件格式中段和节的含义可能有细微区别，但根据上下文关系可以很清楚的理解，这不是关键问题。最后，可执行文件通常都有一个文件头部以描述本文件的总体结构。 相对可执行文件有三个重要的概念：编译（compile）、连接（link，也可称为链接、联接）、加载（load）。源程序文件被编译成目标文件，多个目标文件被连接成一个最终的可执行文件，可执行文件被加载到内存中运行。因为本文重点是讨论可执行文件格式，因此加载过程也相对重点讨论。下面是LINUX平台下ELF文件加载过程的一个简单描述。","text":"可执行文件格式综述 相对于其它文件类型，可执行文件可能是一个操作系统中最重要的文件类型，因为它们是完成操作的真正执行者。可执行文件的大小、运行速度、资源占用情况以及可扩展性、可移植性等与文件格式的定义和文件加载过程紧密相关。研究可执行文件的格式对编写高性能程序和一些黑客技术的运用都是非常有意义的。 不管何种可执行文件格式，一些基本的要素是必须的，显而易见的，文件中应包含代码和数据。因为文件可能引用外部文件定义的符号（变量和函数），因此重定位信息和符号信息也是需要的。一些辅助信息是可选的，如调试信息、硬件信息等。基本上任意一种可执行文件格式都是按区间保存上述信息，称为段（Segment）或节（Section）。不同的文件格式中段和节的含义可能有细微区别，但根据上下文关系可以很清楚的理解，这不是关键问题。最后，可执行文件通常都有一个文件头部以描述本文件的总体结构。 相对可执行文件有三个重要的概念：编译（compile）、连接（link，也可称为链接、联接）、加载（load）。源程序文件被编译成目标文件，多个目标文件被连接成一个最终的可执行文件，可执行文件被加载到内存中运行。因为本文重点是讨论可执行文件格式，因此加载过程也相对重点讨论。下面是LINUX平台下ELF文件加载过程的一个简单描述。 内核首先读ELF文件的头部，然后根据头部的数据指示分别读入各种数据结构，找到标记为可加载（loadable）的段，并调用函数 mmap()把段内容加载到内存中。在加载之前，内核把段的标记直接传递给 mmap()，段的标记指示该段在内存中是否可读、可写，可执行。显然，文本段是只读可执行，而数据段是可读可写。这种方式是利用了现代操作系统和处理器对内存的保护功能。著名的Shellcode（ 参考资料 17）的编写技巧则是突破此保护功能的一个实际例子。 内核分析出ELF文件标记为 PT_INTERP 的段中所对应的动态连接器名称，并加载动态连接器。现代 LINUX 系统的动态连接器通常是 /lib/ld-linux.so.2，相关细节在后面有详细描述。 内核在新进程的堆栈中设置一些标记-值对，以指示动态连接器的相关操作。 内核把控制传递给动态连接器。 动态连接器检查程序对外部文件（共享库）的依赖性，并在需要时对其进行加载。 动态连接器对程序的外部引用进行重定位，通俗的讲，就是告诉程序其引用的外部变量/函数的地址，此地址位于共享库被加载在内存的区间内。动态连接还有一个延迟（Lazy）定位的特性，即只在&quot;真正&quot;需要引用符号时才重定位，这对提高程序运行效率有极大帮助。 动态连接器执行在ELF文件中标记为 .init 的节的代码，进行程序运行的初始化。在早期系统中，初始化代码对应函数 _init(void)(函数名强制固定)，在现代系统中，则对应形式为 123456void__attribute((constructor))init_function(void)&#123;……&#125; 其中函数名为任意。 8：动态连接器把控制传递给程序，从 ELF 文件头部中定义的程序进入点开始执行。在 a.out 格式和ELF格式中，程序进入点的值是显式存在的，在 COFF 格式中则是由规范隐含定义。 从上面的描述可以看出，加载文件最重要的是完成两件事情：加载程序段和数据段到内存；进行外部定义符号的重定位。重定位是程序连接中一个重要概念。我们知道，一个可执行程序通常是由一个含有 main() 的主程序文件、若干目标文件、若干共享库（Shared Libraries）组成。（注：采用一些特别的技巧，也可编写没有 main 函数的程序，请参阅 参考资料 2）一个 C 程序可能引用共享库定义的变量或函数，换句话说就是程序运行时必须知道这些变量/函数的地址。在静态连接中，程序所有需要使用的外部定义都完全包含在可执行程序中，而动态连接则只在可执行文件中设置相关外部定义的一些引用信息，真正的重定位是在程序运行之时。静态连接方式有两个大问题：如果库中变量或函数有任何变化都必须重新编译连接程序；如果多个程序引用同样的变量/函数，则此变量/函数会在文件/内存中出现多次，浪费硬盘/内存空间。比较两种连接方式生成的可执行文件的大小，可以看出有明显的区别。 a.out 文件格式分析 a.out 格式在不同的机器平台和不同的 UNIX 操作系统上有轻微的不同，例如在 MC680x0 平台上有 6 个 section。下面我们讨论的是最&quot;标准&quot;的格式。 a.out 文件包含 7 个 section，格式如下： exec header（执行头部，也可理解为文件头部） text segment（文本段） data segment(数据段) text relocations(文本重定位段) data relocations（数据重定位段） symbol table（符号表） string table（字符串表） 执行头部的数据结构： 12345678910struct exec &#123; unsigned long a_midmag; /* 魔数和其它信息 */ unsigned long a_text; /* 文本段的长度 */ unsigned long a_data; /* 数据段的长度 */ unsigned long a_bss; /* BSS段的长度 */ unsigned long a_syms; /* 符号表的长度 */ unsigned long a_entry; /* 程序进入点 */ unsigned long a_trsize; /* 文本重定位表的长度 */ unsigned long a_drsize; /* 数据重定位表的长度 */&#125;; 文件头部主要描述了各个 section 的长度，比较重要的字段是 a_entry（程序进入点），代表了系统在加载程序并初试化各种环境后开始执行程序代码的入口。这个字段在后面讨论的 ELF 文件头部中也有出现。由 a.out 格式和头部数据结构我们可以看出，a.out 的格式非常紧凑，只包含了程序运行所必须的信息（文本、数据、BSS），而且每个 section 的顺序是固定的。这种结构缺乏扩展性，如不能包含&quot;现代&quot;可执行文件中常见的调试信息，最初的 UNIX 黑客对 a.out 文件调试使用的工具是 adb，而 adb 是一种机器语言调试器！ a.out 文件中包含符号表和两个重定位表，这三个表的内容在连接目标文件以生成可执行文件时起作用。在最终可执行的 a.out 文件中，这三个表的长度都为 0。a.out 文件在连接时就把所有外部定义包含在可执行程序中，如果从程序设计的角度来看，这是一种硬编码方式，或者可称为模块之间是强藕和的。在后面的讨论中，我们将会具体看到ELF格式和动态连接机制是如何对此进行改进的。 a.out 是早期UNIX系统使用的可执行文件格式，由AT&amp;T设计，现在基本上已被 ELF 文件格式代替。a.out 的设计比较简单，但其设计思想明显的被后续的可执行文件格式所继承和发扬。可以参阅 参考资料 16 和阅读 参考资料 15 源代码加深对 a.out 格式的理解。 参考资料 12 讨论了如何在&quot;现代&quot;的红帽LINUX运行 a.out 格式文件。 COFF 文件格式分析 COFF 格式比 a.out 格式要复杂一些，最重要的是包含一个节段表(section table)，因此除了 .text，.data，和 .bss 区段以外，还可以包含其它的区段。另外也多了一个可选的头部，不同的操作系统可一对此头部做特定的定义。 COFF 文件格式如下： File Header(文件头部) Optional Header(可选文件头部) Section 1 Header(节头部) ……… Section n Header(节头部) Raw Data for Section 1(节数据) Raw Data for Section n(节数据) Relocation Info for Sect. 1(节重定位数据) Relocation Info for Sect. n(节重定位数据) Line Numbers for Sect. 1(节行号数据) Line Numbers for Sect. n(节行号数据) Symbol table(符号表) String table(字符串表) 文件头部的数据结构： 12345678910struct filehdr &#123; unsigned short f_magic; /* 魔数 */ unsigned short f_nscns; /* 节个数 */ long f_timdat; /* 文件建立时间 */ long f_symptr; /* 符号表相对文件的偏移量 */ long f_nsyms; /* 符号表条目个数 */ unsigned short f_opthdr; /* 可选头部长度 */ unsigned short f_flags; /* 标志 */ &#125;; COFF 文件头部中魔数与其它两种格式的意义不太一样，它是表示针对的机器类型，例如 0x014c 相对于 I386 平台，而 0x268 相对于 Motorola 68000系列等。当 COFF 文件为可执行文件时，字段 f_flags 的值为 F_EXEC（0X00002），同时也表示此文件没有未解析的符号，换句话说，也就是重定位在连接时就已经完成。由此也可以看出，原始的 COFF 格式不支持动态连接。为了解决这个问题以及增加一些新的特性，一些操作系统对 COFF 格式进行了扩展。Microsoft 设计了名为 PE（Portable Executable）的文件格式，主要扩展是在 COFF 文件头部之上增加了一些专用头部，具体细节请参阅 参考资料 18，某些 UNIX 系统也对 COFF 格式进行了扩展，如 XCOFF（extended common object file format）格式，支持动态连接，请参阅 参考资料 5。 紧接文件头部的是可选头部，COFF 文件格式规范中规定可选头部的长度可以为 0，但在 LINUX 系统下可选头部是必须存在的。下面是 LINUX 下可选头部的数据结构： 123456789101112typedef struct&#123; char magic[2]; /* 魔数 */ char vstamp[2]; /* 版本号 */ char tsize[4]; /* 文本段长度 */ char dsize[4]; /* 已初始化数据段长度 */ char bsize[4]; /* 未初始化数据段长度 */ char entry[4]; /* 程序进入点 */ char text_start[4]; /* 文本段基地址 */ char data_start[4]; /* 数据段基地址 */&#125;COFF_AOUTHDR; 字段 magic 为 0413 时表示 COFF 文件是可执行的，注意到可选头部中显式定义了程序进入点，标准的 COFF 文件没有明确的定义程序进入点的值，通常是从 .text 节开始执行，但这种设计并不好。 前面我们提到，COFF 格式比 a.out 格式多了一个节段表，一个节头条目描述一个节数据的细节，因此 COFF 格式能包含更多的节，或者说可以根据实际需要，增加特定的节，具体表现在 COFF 格式本身的定义以及稍早提及的 COFF 格式扩展。我个人认为，节段表的出现可能是 COFF 格式相对 a.out 格式最大的进步。下面我们将简单描述 COFF 文件中节的数据结构，因为节的意义更多体现在程序的编译和连接上，所以本文不对其做更多的描述。此外，ELF 格式和 COFF格式对节的定义非常相似，在随后的 ELF 格式分析中，我们将省略相关讨论。 12345678910111213struct COFF_scnhdr&#123; char s_name[8]; /* 节名称 */ char s_paddr[4]; /* 物理地址 */ char s_vaddr[4]; /* 虚拟地址 */ char s_size[4]; /* 节长度 */ char s_scnptr[4]; /* 节数据相对文件的偏移量 */ char s_relptr[4]; /* 节重定位信息偏移量 */ char s_lnnoptr[4]; /* 节行信息偏移量 */ char s_nreloc[2]; /* 节重定位条目数 */ char s_nlnno[2]; /* 节行信息条目数 */ char s_flags[4]; /* 段标记 */&#125;; 有一点需要注意：LINUX系统中头文件coff.h中对字段s_paddr的注释是&quot;physical address&quot;，但似乎应该理解为&quot;节被加载到内存中所占用的空间长度&quot;。字段s_flags标记该节的类型，如文本段、数据段、BSS段等。在COFF的节中也出现了行信息，行信息描述了二进制代码与源代码的行号之间的对映关系，在调试时很有用。 参考资料 19是一份对COFF格式详细描述的中文资料，更详细的内容请参阅 参考资料 20。 ELF文件格式分析 ELF文件有三种类型： 可重定位文件：也就是通常称的目标文件，后缀为.o。 共享文件：也就是通常称的库文件，后缀为.so。 可执行文件：本文主要讨论的文件格式，总的来说，可执行文件的格式与上述两种文件的格式之间的区别主要在于观察的角度不同：一种称为连接视图（Linking View），一种称为执行视图（Execution View）。 首先看看ELF文件的总体布局： ELF header(ELF头部) Program header table(程序头表) Segment1（段1） Segment2（段2） ……… Sengmentn（段n） Setion header table(节头表，可选) 段由若干个节(Section)构成,节头表对每一个节的信息有相关描述。对可执行程序而言，节头表是可选的。 参考资料 1中作者谈到把节头表的所有数据全部设置为0，程序也能正确运行！ELF头部是一个关于本文件的路线图（road map），从总体上描述文件的结构。下面是ELF头部的数据结构： 1234567891011121314151617typedef struct&#123; unsigned char e_ident[EI_NIDENT]; /* 魔数和相关信息 */ Elf32_Half e_type; /* 目标文件类型 */ Elf32_Half e_machine; /* 硬件体系 */ Elf32_Word e_version; /* 目标文件版本 */ Elf32_Addr e_entry; /* 程序进入点 */ Elf32_Off e_phoff; /* 程序头部偏移量 */ Elf32_Off e_shoff; /* 节头部偏移量 */ Elf32_Word e_flags; /* 处理器特定标志 */ Elf32_Half e_ehsize; /* ELF头部长度 */ Elf32_Half e_phentsize; /* 程序头部中一个条目的长度 */ Elf32_Half e_phnum; /* 程序头部条目个数 */ Elf32_Half e_shentsize; /* 节头部中一个条目的长度 */ Elf32_Half e_shnum; /* 节头部条目个数 */ Elf32_Half e_shstrndx; /* 节头部字符表索引 */&#125; Elf32_Ehdr; 下面我们对ELF头表中一些重要的字段作出相关说明，完整的ELF定义请参阅 参考资料6和 参考资料 7。 e_ident[0]-e_ident[3]包含了ELF文件的魔数，依次是0x7f、‘E’、‘L’、‘F’。注意，任何一个ELF文件必须包含此魔数。 参考资料 3中讨论了利用程序、工具、/Proc文件系统等多种查看ELF魔数的方法。e_ident[4]表示硬件系统的位数，1代表32位，2代表64位。e_ident[5]表示数据编码方式，1代表小印第安排序（最大有意义的字节占有最低的地址），2代表大印第安排序（最大有意义的字节占有最高的地址）。e_ident[6]指定ELF头部的版本，当前必须为1。e_ident[7]到e_ident[14]是填充符，通常是0。ELF格式规范中定义这几个字节是被忽略的，但实际上是这几个字节完全可以可被利用。如病毒Lin/Glaurung.676/666（ 参考资料 1）设置e_ident[7]为0x21,表示本文件已被感染；或者存放可执行代码（ 参考资料 2）。ELF头部中大多数字段都是对子头部数据的描述，其意义相对比较简单。值得注意的是某些病毒可能修改字段e_entry（程序进入点）的值，以指向病毒代码，例如上面提到的病毒Lin/Glaurung.676/666。 一个实际可执行文件的文件头部形式如下：（利用命令readelf） 1234567891011121314151617181920ELF Header:Magic: 7f 45 4c 46 01 01 01 00 00 00 00 00 00 00 00 00Class: ELF32Data: 2&#x27;s complement, little endianVersion: 1 (current)OS/ABI: UNIX - System VABI Version: 0Type: EXEC (Executable file)Machine: Intel 80386Version: 0x1Entry point address: 0x80483ccStart of program headers: 52 (bytes into file)Start of section headers: 14936 (bytes into file)Flags: 0x0Size of this header: 52 (bytes)Size of program headers: 32 (bytes)Number of program headers: 6Size of section headers: 40 (bytes)Number of section headers: 34Section header string table index: 31 紧接ELF头部的是程序头表，它是一个结构数组，包含了ELF头表中字段e_phnum定义的条目，结构描述一个段或其他系统准备执行该程序所需要的信息。 12345678910typedef struct &#123; Elf32_Word p_type; /* 段类型 */ Elf32_Off p_offset; /* 段位置相对于文件开始处的偏移量 */ Elf32_Addr p_vaddr; /* 段在内存中的地址 */ Elf32_Addr p_paddr; /* 段的物理地址 */ Elf32_Word p_filesz; /* 段在文件中的长度 */ Elf32_Word p_memsz; /* 段在内存中的长度 */ Elf32_Word p_flags; /* 段的标记 */ Elf32_Word p_align; /* 段在内存中对齐标记 */ &#125; Elf32_Phdr; 在详细讨论可执行文件程序头表之前，首先查看一个实际文件的输出： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354Program Headers:Type Offset VirtAddr PhysAddr FileSiz MemSiz Flg AlignPHDR 0x000034 0x08048034 0x08048034 0x000c0 0x000c0 R E 0x4INTERP 0x0000f4 0x080480f4 0x080480f4 0x00013 0x00013 R 0x1 [Requesting program interpreter: /lib/ld-linux.so.2] LOAD 0x000000 0x08048000 0x08048000 0x00684 0x00684 R E 0x1000 LOAD 0x000684 0x08049684 0x08049684 0x00118 0x00130 RW 0x1000 DYNAMIC 0x000690 0x08049690 0x08049690 0x000c8 0x000c8 RW 0x4 NOTE 0x000108 0x08048108 0x08048108 0x00020 0x00020 R 0x4 Section to Segment mapping: Segment Sections... 00 01 .interp 02 .interp .note.ABI-tag .hash .dynsym .dynstr .gnu.version .gnu.version_r .rel.dyn .rel.plt .init .plt .text .fini .rodata .eh_frame 03 .data .dynamic .ctors .dtors .jcr .got .bss 04 .dynamic05 .note.ABI-tagSection Headers: [Nr] Name Type Addr Off Size ES Flg Lk Inf Al [ 0] NULL 00000000 000000 000000 00 0 0 0 [ 1] .interp PROGBITS 080480f4 0000f4 000013 00 A 0 0 1 [ 2] .note.ABI-tag NOTE 08048108 000108 000020 00 A 0 0 4 [ 3] .hash HASH 08048128 000128 000040 04 A 4 0 4 [ 4] .dynsym DYNSYM 08048168 000168 0000b0 10 A 5 1 4 [ 5] .dynstr STRTAB 08048218 000218 00007b 00 A 0 0 1 [ 6] .gnu.version VERSYM 08048294 000294 000016 02 A 4 0 2 [ 7] .gnu.version_r VERNEED 080482ac 0002ac 000030 00 A 5 1 4 [ 8] .rel.dyn REL 080482dc 0002dc 000008 08 A 4 0 4 [ 9] .rel.plt REL 080482e4 0002e4 000040 08 A 4 b 4 [10] .init PROGBITS 08048324 000324 000017 00 AX 0 0 4 [11] .plt PROGBITS 0804833c 00033c 000090 04 AX 0 0 4 [12] .text PROGBITS 080483cc 0003cc 0001f8 00 AX 0 0 4 [13] .fini PROGBITS 080485c4 0005c4 00001b 00 AX 0 0 4 [14] .rodata PROGBITS 080485e0 0005e0 00009f 00 A 0 0 32 [15] .eh_frame PROGBITS 08048680 000680 000004 00 A 0 0 4 [16] .data PROGBITS 08049684 000684 00000c 00 WA 0 0 4 [17] .dynamic DYNAMIC 08049690 000690 0000c8 08 WA 5 0 4 [18] .ctors PROGBITS 08049758 000758 000008 00 WA 0 0 4 [19] .dtors PROGBITS 08049760 000760 000008 00 WA 0 0 4 [20] .jcr PROGBITS 08049768 000768 000004 00 WA 0 0 4 [21] .got PROGBITS 0804976c 00076c 000030 04 WA 0 0 4 [22] .bss NOBITS 0804979c 00079c 000018 00 WA 0 0 4 [23] .comment PROGBITS 00000000 00079c 000132 00 0 0 1 [24] .debug_aranges PROGBITS 00000000 0008d0 000098 00 0 0 8 [25] .debug_pubnames PROGBITS 00000000 000968 000040 00 0 0 1 [26] .debug_info PROGBITS 00000000 0009a8 001cc6 00 0 0 1 [27] .debug_abbrev PROGBITS 00000000 00266e 0002cc 00 0 0 1 [28] .debug_line PROGBITS 00000000 00293a 0003dc 00 0 0 1 [29] .debug_frame PROGBITS 00000000 002d18 000048 00 0 0 4 [30] .debug_str PROGBITS 00000000 002d60 000bcd 01 MS 0 0 1 [31] .shstrtab STRTAB 00000000 00392d 00012b 00 0 0 1 [32] .symtab SYMTAB 00000000 003fa8 000740 10 33 56 4 [33] .strtab STRTAB 00000000 0046e8 000467 00 0 0 1 对一个ELF可执行程序而言，一个基本的段是标记p_type为PT_INTERP的段，它表明了运行此程序所需要的程序解释器（/lib/ld-linux.so.2），实际上也就是动态连接器（dynamic linker）。最重要的段是标记p_type为PT_LOAD的段，它表明了为运行程序而需要加载到内存的数据。查看上面实际输入，可以看见有两个可LOAD段，第一个为只读可执行（FLg为R E）,第二个为可读可写（Flg为RW）。段1包含了文本节.text，注意到ELF文件头部中程序进入点的值为0x80483cc，正好是指向节.text在内存中的地址。段二包含了数据节.data，此数据节中数据是可读可写的，相对的只读数据节.rodata包含在段1中。ELF格式可以比COFF格式包含更多的调试信息，如上面所列出的形式为.debug_xxx的节。在I386平台LINUX系统下，用命令file查看一个ELF可执行程序的可能输出是：a.out: ELF 32-bit LSB executable, Intel 80386, version 1 (SYSV), for GNU/Linux 2.2.5, dynamically linked (uses shared libs), not stripped。 ELF文件中包含了动态连接器的全路径，内核定位&quot;正确&quot;的动态连接器在内存中的地址是&quot;正确&quot;运行可执行文件的保证， 参考资料 13讨论了如何通过查找动态连接器在内存中的地址以达到颠覆（Subversiver）动态连接机制的方法。 最后我们讨论ELF文件的动态连接机制。每一个外部定义的符号在全局偏移表(Global Offset Table GOT)中有相应的条目,如果符号是函数则在过程连接表(Procedure Linkage Table PLT)中也有相应的条目，且一个PLT条目对应一个GOT条目。对外部定义函数解析可能是整个ELF文件规范中最复杂的，下面是函数符号解析过程的一个描述。 代码中调用外部函数func,语句形式为call 0xaabbccdd,地址0xaabbccdd实际上就是符号func在PLT表中对应的条目地址（假设地址为标号.PLT2）。 PLT表的形式如下 12345678910.PLT0: pushl 4(%ebx) /* GOT表的地址保存在寄存器ebx中 */jmp *8(%ebx)nop; nopnop; nop.PLT1: jmp *name1@GOT(%ebx)pushl $offsetjmp .PLT0@PC.PLT2: jmp *func@GOT(%ebx)pushl $offsetjmp .PLT0@PC 查看标号.PLT2的语句,实际上是跳转到符号func在GOT表中对应的条目。 在符号没有重定位前，GOT表中此符号对应的地址为标号.PLT2的下一条语句，即是pushl $offset，其中$offset是符号func的重定位偏移量。注意到这是一个二次跳转。 在符号func的重定位偏移量压栈后,控制跳到PLT表的第一条目，把GOT[1]的内容压栈，并跳转到GOT[2]对应的地址。 GOT[2]对应的实际上是动态符号解析函数的代码，在对符号func的地址解析后，会把func在内存中的地址设置到GOT表中此符号对应的条目中。 当第二次调用此符号时，GOT表中对应的条目已经包含了此符号的地址，就可直接调用而不需要利用PLT表进行跳转。 动态连接是比较复杂的，但为了获得灵活性的代价通常就是复杂性。其最终目的是把GOT表中条目的值修改为符号的真实地址，这也可解释节.got包含在可读可写段中。 动态连接是一个非常重要的进步，这意味着库文件可以被升级、移动到其他目录等等而不需要重新编译程序（当然，这不意味库可以任意修改，如函数入参的个数、数据类型应保持兼容性）。从很大程度上说，动态连接机制是ELF格式代替a.out格式的决定性原因。如果说面对对象的编程本质是面对接口（interface）的编程，那么动态连接机制则是这种思想的地一个非常典型的应用，具体的讲，动态连接机制与设计模式中的桥接（BRIDGE）方法比较类似，而它的LAZY特性则与代理（PROXY）方法非常相似。动态连接操作的细节描述请参阅 参考资料 8，9，10，11。通过阅读命令readelf、objdump 的源代码以及 参考资料 14中所提及的相关软件源代码，可以对ELF文件的格式有更彻底的了解。 总结 不同时期的可执行文件格式深刻的反映了技术进步的过程，技术进步通常是针对解决存在的问题和适应新的环境。早期的UNIX系统使用a.out格式，随着操作系统和硬件系统的进步，a.out格式的局限性越来越明显。新的可执行文件格式COFF在UNIX System VR3中出现，COFF格式相对a.out格式最大变化是多了一个节头表（section head table），能够在包含基础的文本段、数据段、BSS段之外包含更多的段，但是COFF对动态连接和C++程序的支持仍然比较困难。为了解决上述问题，UNIX系统实验室(UNIX SYSTEM Laboratories USL) 开发出ELF文件格式，它被作为应用程序二进制接口（Application binary Interface ABI）的一部分，其目的是替代传统的a.out格式。例如，ELF文件格式中引入初始化段.init和结束段.fini（分别对应构造函数和析构函数）则主要是为了支持C++程序。1994年6月ELF格式出现在LINUX系统上，现在ELF格式作为UNIX/LINUX最主要的可执行文件格式。当然我们完全有理由相信，在将来还会有新的可执行文件格式出现。 上述三种可执行文件格式都很好的体现了设计思想中分层的概念，由一个总的头部刻画了文件的基本要素，再由若干子头部/条目刻画了文件的若干细节。比较一下可执行文件格式和以太数据包中以太头、IP头、TCP头的设计，我想我们能很好的感受分层这一重要的设计思想。 参考资料 21从全局的角度讨论了各种文件的格式，并提出一个比较夸张的结论：Everything Is Byte! 最后的题外话：大多数资料中对a.out格式的评价较低，常见的词语有黑暗年代（dark ages）、丑陋（ugly）等等，当然，从现代的观点来看，的确是比较简单，但是如果没有曾经的简单何来今天的精巧？正如我们今天可以评价石器时代的技术是ugly,那么将来的人们也可以嘲讽今天的技术是非常ugly。我想我们也许应该用更平和的心态来对曾经的技术有一个公正的评价。 参考资料 （原文中未提供参考资料的链接） What’s the difference of section and segment in ELF file format Section is static, segment is dynamic The quote is correct, but to actually understand it the difference, you should try to understand the fields of the section header and program header (segment) entries, and how they are be used by the linker (sections) and operating system (segment). Particularly important informations are (besides lengths): section: tell the linker if a section is either: raw data to be loaded into memory, e.g. .data, .text, etc. or formatted metadata about other sections, that will be used by the linker, but disappear at runtime e.g. .symtab, .srttab, .rela.text segment: tells the operating system: where should a segment be loaded into virtual memory what permissions the segments have (read, write, execute). Remember that this can be efficiently enforced by the processor: How does x86 paging work? I have written a tutorial that covers that in more detail at: http://www.cirosantilli.com/elf-hello-world/ Does a segment contain one or more sections? Yes, and it is the linker that puts sections into segments. In Binutils, how sections are put into segments by ld is determined by a text file called a linker script. Docs: https://sourceware.org/binutils/docs/ld/Scripts.html You can get the default one with ld --verbose, and set a custom one with -T. For example, my default Ubuntu 17.04 linker script contains: 12345678.text :&#123; *(.text.unlikely .text.*_unlikely .text.unlikely.*) *(.text.exit .text.exit.*) *(.text.startup .text.startup.*) *(.text.hot .text.hot.*) *(.text .stub .text.* .gnu.linkonce.t.*)&#125; which tells the linker to put sections named .text.unlikely, .text.*_unlikely, .text.exit, etc. in the .text segment. OS development is a case where custom scripts are useful, minimal example: https://github.com/cirosantilli/x86-bare-metal-examples/blob/d217b180be4220a0b4a453f31275d38e697a99e0/linker.ld Once the executable is linked, it is only possible to know which section went to which segment if the linker stores the optional section header in the executable: Where is the “Section to segment mapping” stored in ELF files? 本文地址：http://xnerv.wang/analysis-of-unix-linux-platform-executable-file-format/ 转载自：UNIX/LINUX 平台可执行文件格式分析","categories":[{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/categories/Linux/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/tags/Linux/"},{"name":"COFF","slug":"COFF","permalink":"https://xnerv.wang/tags/COFF/"},{"name":"linker","slug":"linker","permalink":"https://xnerv.wang/tags/linker/"}]},{"title":"彻底研究Python的编码问题","slug":"all-truths-about-python-encoding-problem","date":"2017-11-06T01:14:00.000Z","updated":"2023-08-21T02:24:21.618Z","comments":true,"path":"all-truths-about-python-encoding-problem/","link":"","permalink":"https://xnerv.wang/all-truths-about-python-encoding-problem/","excerpt":"Python的初学者（以及很多熟练工）相信都遇到过下面的运行时错误信息： UnicodeDecodeError: ‘ASCII’ codec can’t decode byte 0xe4 in position 0: ordinal not in range(128) 然后百度一下发现说明这个问题的网页有一大把，基本无非是下面两种解决方案： 在Python文件的开头加一句#coding=utf-8。 在代码中加入： 123import sysreload(sys)sys.setdefaultencoding(&#x27;utf8&#x27;) 一般大家的做法是把两者都加上，然后问题一般也会得到解决。但很少有人会去深究过Python会什么会产生这样的编码问题，以及为什么通过加上面的代码可以解决这个问题。作为一个合格的程序员，遇到这种问题就应该追根究底，否则就只是以码谋生的码畜了 :)","text":"Python的初学者（以及很多熟练工）相信都遇到过下面的运行时错误信息： UnicodeDecodeError: ‘ASCII’ codec can’t decode byte 0xe4 in position 0: ordinal not in range(128) 然后百度一下发现说明这个问题的网页有一大把，基本无非是下面两种解决方案： 在Python文件的开头加一句#coding=utf-8。 在代码中加入： 123import sysreload(sys)sys.setdefaultencoding(&#x27;utf8&#x27;) 一般大家的做法是把两者都加上，然后问题一般也会得到解决。但很少有人会去深究过Python会什么会产生这样的编码问题，以及为什么通过加上面的代码可以解决这个问题。作为一个合格的程序员，遇到这种问题就应该追根究底，否则就只是以码谋生的码畜了 :) string与Unicode string Python的编码问题一般只在Python2.x中存在，在Python3.x中则基本不会遇到这样的问题。根本原因在于Python2.x中存在两种字符串：string和Unicode string，两者都继承自basic string。 string类似于C语言中的char*，或者C++中的std::string，本质上都是byte数组，本身不具备特定的encoding类型，而是由string的使用者决定的。例如向string中写入一个汉字，如果是UTF-8编码，则可能是三个char（或者更多），如果是GBK编码，则可能是两个char，但对于Python解释器，它并不清楚string用的是encoding，一切都是bytes。。。 而Unicode string则明确地存储了Unicode编码的字符串。在Python2.x中，a1=&quot;abc&quot;;，a1就是一个string（是什么编码还不确定，等下再谈）。而a2=u&quot;abc&quot;;，a2就是一个Unicode string。 C/C++中的文件编码和字符串编码 C/C++不仅仅是一种编程语言，更是一个深入了解计算机底层原理的窗口和机会，一个只会堆砌代码而浮于表面的程序员，是不能称之为一个真正意义上的程序员的（这跟月薪有没有上3w没有关系）。 我们先回过头来看看C/C++中是如何处理文件编码和字符串编码的问题的。以VC为例，VC下有两个编码的选项，分为Source encoding和Execution encoding。前者指定了源文件的编码（这比BOM更准确），后者是字符串常量等的编码。简单说，如果指定Source encoding为GBK，同时指定Execution encoding为UTF-8，则首先所有的源文件必须用GBK编码保存，否则VC在读入源文件时可能会无法解码其中的一些字符。同时，像const char* a3=&quot;中文&quot;;这样的常量字符串赋值语句，a3就是使用了UTF-8编码保存的字符串。在中文Windows系统下，如果不特意设置这两种编码，VC则一般使用的是system default encoding。 （参考 Char * encoding, /execution-charset (Set Execution Character Set), /source-charset (Set Source Character Set)。） Python中的文件编码和字符串编码 对比C/C++处理编码的方法，我们再看看Python是怎么做的。 首先，类似于Source encoding，Python可以通过在文件开头加上#coding=utf-8这句来向Python解析器指明本py源文件的编码。然后，麻烦事情来了，与C/C++中的Execution encoding有很大的不同，对于Python2.x的string而言，对其用字符串常量赋值时，string中存储的encoding，是跟#coding=utf-8挂钩的，而不是像C++中的Execution encoding一样有一个单独的选项来指定。也就是说，如果指定了#coding=utf-8，那么在运行时，a1=&quot;中文&quot;就会用UTF-8编码来存储和读取变量a1，如果没有设置#coding=utf-8及其它任何#coding=xxx，那么就是用默认的ASCII编码，但显然ASCII是无法编码&quot;中文&quot;的，于是在运行时你就会看到类似下面的报错： 1SyntaxError: Non-ASCII character &#x27;\\xe4&#x27; in file D:\\test.py on line 1, but no encoding declared; see http://python.org/dev/peps/pep-0263/ for details 而关于sys.defaultencoding，这个在没有明确指明解码方式的时候使用。比如有如下代码： 1234#! /usr/bin/env python# -*- coding: utf-8 -*-s = &#x27;中文&#x27;s.encode(&#x27;gb18030&#x27;) 这句代码将s（string类型）重新编码为gb18030的格式（仍然是string类型）， Python会自动的先将s解码为Unicode string类型，然后再编码成gb18030。因为解码是Python自动进行的，我们没有指明解码方式，Python就会使用sys.defaultencoding指明的方式来解码。很多情况下sys.defaultencoding是ASCII，如果s不是这个类型就会出错。 拿上面的情况来说，我的sys.defaultencoding是ASCII，而s的编码方式和文件的编码方式一致，是UTF-8的，所以出错了: 1UnicodeDecodeError: &#x27;ascii&#x27; codec can&#x27;t decode byte 0xe4 in position 0: ordinal not in range(128) 但是sys.setdefaultencoding('utf8')有的时候也会导致一些意想不到的问题，所以目前网上主流意见是不推荐使用 。例如这篇文章中提到使用sys.setdefaultencoding可能会遇到的一些问题：Why sys.setdefaultencoding() will break code 在Python3.x中，编码的问题基本不存在了，也没有了sys.setsystemencoding。因为Python3.x取消了string，只有Unicode string，也避免了很多编码转换的问题。 关于#coding:utf-8 #coding:utf-8指引Python解释器如何读取整个py文件，有些IDE也会根据这个comment来自动决定保存格式及显示格式，应该与py文件的保存格式一致。 根据PEP 263 – Defining Python Source Code Encodings中的说明，其实写成#coding=utf-8，#coding:utf-8或者#-*- coding:utf-8 -*-都是可以的，正则定义为coding[:=]\\s*([-\\w.]+)。 encode/decode encode是将Unicode string转化为指定encoding编码的string，而decode是将某种encoding编码的string转化为Unicode string。 如果decode指定错了encoding，例如，假设a1已经是Unicode编码的字符串变量，执行： 1print a1.decode(&#x27;utf-8&#x27;) 则可能报错： 1234567Traceback (most recent call last):... File &quot;C:\\Python27\\lib\\encodings\\utf_8.py&quot;, line 16, in decode return codecs.utf_8_decode(input, errors, True)UnicodeEncodeError: &#x27;ascii&#x27; codec can&#x27;t encode character u&#x27;\\u3010&#x27; in position 0: ordinal not in range(128) print/sys.xxx.write print和sys.xxx.write在输出unicode-object string时会自动转换编码为sys.xxx.encoding，无需自己来转。 但是，如果是utf8 string，而当前环境是Windows，则输出显然会乱码。如果是中文版Windows，用的是GB编码（code page 936）。如果是英文版Windows，用的可能是ANSI编码（code page 1252）。 最佳实践 本文地址：http://xnerv.wang/all-truths-about-python-encoding-problem/","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://xnerv.wang/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"编程语言","slug":"编程语言","permalink":"https://xnerv.wang/tags/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"},{"name":"Python","slug":"Python","permalink":"https://xnerv.wang/tags/Python/"},{"name":"原创","slug":"原创","permalink":"https://xnerv.wang/tags/%E5%8E%9F%E5%88%9B/"},{"name":"未完成","slug":"未完成","permalink":"https://xnerv.wang/tags/%E6%9C%AA%E5%AE%8C%E6%88%90/"}]},{"title":"23种设计模式全解析（转载）","slug":"23-design-patterns-analysis","date":"2017-11-04T19:31:00.000Z","updated":"2023-08-21T02:24:18.536Z","comments":true,"path":"23-design-patterns-analysis/","link":"","permalink":"https://xnerv.wang/23-design-patterns-analysis/","excerpt":"一、设计模式的分类 总体来说设计模式分为三大类： 创建型模式，共五种：工厂方法模式、抽象工厂模式、单例模式、建造者模式、原型模式。 结构型模式，共七种：适配器模式、装饰器模式、代理模式、外观模式、桥接模式、组合模式、享元模式。 行为型模式，共十一种：策略模式、模板方法模式、观察者模式、迭代子模式、责任链模式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式。 其实还有两类：并发型模式和线程池模式。用一个图片来整体描述一下：","text":"一、设计模式的分类 总体来说设计模式分为三大类： 创建型模式，共五种：工厂方法模式、抽象工厂模式、单例模式、建造者模式、原型模式。 结构型模式，共七种：适配器模式、装饰器模式、代理模式、外观模式、桥接模式、组合模式、享元模式。 行为型模式，共十一种：策略模式、模板方法模式、观察者模式、迭代子模式、责任链模式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式。 其实还有两类：并发型模式和线程池模式。用一个图片来整体描述一下： 二、设计模式的六大原则 总原则：开闭原则（Open Close Principle） 开闭原则就是说对扩展开放，对修改关闭。在程序需要进行拓展的时候，不能去修改原有的代码，而是要扩展原有代码，实现一个热插拔的效果。所以一句话概括就是：为了使程序的扩展性好，易于维护和升级。想要达到这样的效果，我们需要使用接口和抽象类等，后面的具体设计中我们会提到这点。 1、单一职责原则 不要存在多于一个导致类变更的原因，也就是说每个类应该实现单一的职责，如若不然，就应该把类拆分。 2、里氏替换原则（Liskov Substitution Principle） 里氏代换原则(Liskov Substitution Principle LSP)面向对象设计的基本原则之一。 里氏代换原则中说，任何基类可以出现的地方，子类一定可以出现。 LSP是继承复用的基石，只有当衍生类可以替换掉基类，软件单位的功能不受到影响时，基类才能真正被复用，而衍生类也能够在基类的基础上增加新的行为。里氏代换原则是对“开-闭”原则的补充。实现“开-闭”原则的关键步骤就是抽象化。而基类与子类的继承关系就是抽象化的具体实现，所以里氏代换原则是对实现抽象化的具体步骤的规范。—— From Baidu 百科 历史替换原则中，子类对父类的方法尽量不要重写和重载。因为父类代表了定义好的结构，通过这个规范的接口与外界交互，子类不应该随便破坏它。 3、依赖倒转原则（Dependence Inversion Principle） 这个是开闭原则的基础，具体内容：面向接口编程，依赖于抽象而不依赖于具体。写代码时用到具体类时，不与具体类交互，而与具体类的上层接口交互。 4、接口隔离原则（Interface Segregation Principle） 这个原则的意思是：每个接口中不存在子类用不到却必须实现的方法，如果不然，就要将接口拆分。使用多个隔离的接口，比使用单个接口（多个接口方法集合到一个的接口）要好。 5、迪米特法则（最少知道原则）（Demeter Principle） 就是说：一个类对自己依赖的类知道的越少越好。也就是说无论被依赖的类多么复杂，都应该将逻辑封装在方法的内部，通过public方法提供给外部。这样当被依赖的类变化时，才能最小的影响该类。 最少知道原则的另一个表达方式是：只与直接的朋友通信。类之间只要有耦合关系，就叫朋友关系。耦合分为依赖、关联、聚合、组合等。我们称出现为成员变量、方法参数、方法返回值中的类为直接朋友。局部变量、临时变量则不是直接的朋友。我们要求陌生的类不要作为局部变量出现在类中。 6、合成复用原则（Composite Reuse Principle） 原则是尽量首先使用合成/聚合的方式，而不是使用继承。 三、Java的23中设计模式 A、创建模式 从这一块开始，我们详细介绍Java中23种设计模式的概念，应用场景等情况，并结合他们的特点及设计模式的原则进行分析。 首先，简单工厂模式不属于23中涉及模式，简单工厂一般分为：普通简单工厂、多方法简单工厂、静态方法简单工厂。 0、简单工厂模式 简单工厂模式模式分为三种： 01、普通 就是建立一个工厂类，对实现了同一接口的一些类进行实例的创建。首先看下关系图： 举例如下：（我们举一个发送邮件和短信的例子） 首先，创建二者的共同接口： 123public interface Sender &#123; public void Send();&#125; 其次，创建实现类： 123456public class MailSender implements Sender &#123; @Override public void Send() &#123; System.out.println(“this is mailsender!”); &#125;&#125; 123456public class SmsSender implements Sender &#123; @Override public void Send() &#123; System.out.println(“this is sms sender!”); &#125;&#125; 最后，建工厂类： 123456789101112public class SendFactory &#123; public Sender produce(String type) &#123; if (“mail”.equals(type)) &#123; return new MailSender(); &#125; else if (“sms”.equals(type)) &#123; return new SmsSender(); &#125; else &#123; System.out.println(“请输入正确的类型!”); return null; &#125; &#125;&#125; 我们来测试下： 1234567public class FactoryTest &#123; public static void main(String[] args) &#123; SendFactory factory = new SendFactory(); Sender sender = factory.produce(“sms”); sender.Send(); &#125;&#125; 输出：this is sms sender! 02、多个方法 是对普通工厂方法模式的改进，在普通工厂方法模式中，如果传递的字符串出错，则不能正确创建对象，而多个工厂方法模式是提供多个工厂方法，分别创建对象。关系图： 将上面的代码做下修改，改动下SendFactory类就行，如下： 12345678public class SendFactory &#123; public Sender produceMail()&#123; return new MailSender(); &#125; public Sender produceSms()&#123; return new SmsSender(); &#125;&#125; 测试类如下： 1234567public class FactoryTest &#123; public static void main(String[] args) &#123; SendFactory factory = new SendFactory(); Sender sender = factory.produceMail(); sender.Send(); &#125;&#125; 输出：this is mailsender! 03、多个静态方法 将上面的多个工厂方法模式里的方法置为静态的，不需要创建实例，直接调用即可。 12345678public class SendFactory &#123; public static Sender produceMail()&#123; return new MailSender(); &#125; public static Sender produceSms()&#123; return new SmsSender(); &#125;&#125; 123456public class FactoryTest &#123; public static void main(String[] args) &#123; Sender sender = SendFactory.produceMail(); sender.Send(); &#125;&#125; 输出：this is mailsender! 总体来说，工厂模式适合：凡是出现了大量的产品需要创建，并且具有共同的接口时，可以通过工厂方法模式进行创建。在以上的三种模式中，第一种如果传入的字符串有误，不能正确创建对象，第三种相对于第二种，不需要实例化工厂类，所以，大多数情况下，我们会选用第三种——静态工厂方法模式。 1、工厂方法模式（Factory Method） 简单工厂模式有一个问题就是，类的创建依赖工厂类，也就是说，如果想要拓展程序，必须对工厂类进行修改，这违背了闭包原则，所以，从设计角度考虑，有一定的问题，如何解决？就用到工厂方法模式，创建一个工厂接口和创建多个工厂实现类，这样一旦需要增加新的功能，直接增加新的工厂类就可以了，不需要修改之前的代码。 请看例子： 123public interface Sender &#123; public void Send();&#125; 两个实现类： 123456public class MailSender implements Sender &#123; @Override public void Send() &#123; System.out.println(“this is mailsender!”); &#125;&#125; 123456public class SmsSender implements Sender &#123; @Override public void Send() &#123; System.out.println(“this is sms sender!”); &#125;&#125; 两个工厂类： 12345public class SendMailFactory implements Provider &#123; @Override public Sender produce()&#123; return new MailSender(); 123456public class SendSmsFactory implements Provider&#123; @Override public Sender produce() &#123; return new SmsSender(); &#125;&#125; 再提供一个接口： 123public interface Provider &#123; public Sender produce();&#125; 测试类： 1234567public class Test &#123; public static void main(String[] args) &#123; Provider provider = new SendMailFactory(); Sender sender = provider.produce(); sender.Send(); &#125;&#125; 其实这个模式的好处就是，如果你现在想增加一个功能：发及时信息，则只需做一个实现类，实现Sender接口，同时做一个工厂类，实现Provider接口，就OK了，无需去改动现成的代码。这样做，拓展性较好！ 2、抽象工厂模式 工厂方法模式和抽象工厂模式不好分清楚，他们的区别如下： 工厂方法模式： 一个抽象产品类，可以派生出多个具体产品类。 一个抽象工厂类，可以派生出多个具体工厂类。 每个具体工厂类只能创建一个具体产品类的实例。 抽象工厂模式： 多个抽象产品类，每个抽象产品类可以派生出多个具体产品类。 一个抽象工厂类，可以派生出多个具体工厂类。 每个具体工厂类可以创建多个具体产品类的实例，也就是创建的是一个产品线下的多个产品。 区别： 工厂方法模式只有一个抽象产品类，而抽象工厂模式有多个。 工厂方法模式的具体工厂类只能创建一个具体产品类的实例，而抽象工厂模式可以创建多个。 工厂方法创建 “一种” 产品，他的着重点在于&quot;怎么创建&quot;，也就是说如果你开发，你的大量代码很可能围绕着这种产品的构造，初始化这些细节上面。也因为如此，类似的产品之间有很多可以复用的特征，所以会和模版方法相随。 抽象工厂需要创建一些列产品，着重点在于&quot;创建哪些&quot;产品上，也就是说，如果你开发，你的主要任务是划分不同差异的产品线，并且尽量保持每条产品线接口一致，从而可以从同一个抽象工厂继承。 对于java来说，你能见到的大部分抽象工厂模式都是这样的： —它的里面是一堆工厂方法，每个工厂方法返回某种类型的对象。 比如说工厂可以生产鼠标和键盘。那么抽象工厂的实现类（它的某个具体子类）的对象都可以生产鼠标和键盘，但可能工厂A生产的是罗技的键盘和鼠标，工厂B是微软的。 这样A和B就是工厂，对应于抽象工厂； 每个工厂生产的鼠标和键盘就是产品，对应于工厂方法； 用了工厂方法模式，你替换生成键盘的工厂方法，就可以把键盘从罗技换到微软。但是用了抽象工厂模式，你只要换家工厂，就可以同时替换鼠标和键盘一套。如果你要的产品有几十个，当然用抽象工厂模式一次替换全部最方便（这个工厂会替你用相应的工厂方法） 所以说抽象工厂就像工厂，而工厂方法则像是工厂的一种产品生产线 3、单例模式（Singleton） 单例对象（Singleton）是一种常用的设计模式。在Java应用中，单例对象能保证在一个JVM中，该对象只有一个实例存在。这样的模式有几个好处： 某些类创建比较频繁，对于一些大型的对象，这是一笔很大的系统开销。 省去了new操作符，降低了系统内存的使用频率，减轻GC压力。 有些类如交易所的核心交易引擎，控制着交易流程，如果该类可以创建多个的话，系统完全乱了。（比如一个军队出现了多个司令员同时指挥，肯定会乱成一团），所以只有使用单例模式，才能保证核心交易服务器独立控制整个流程。 首先我们写一个简单的单例类： 123456789101112131415161718public class Singleton &#123; /* 持有私有静态实例，防止被引用，此处赋值为null，目的是实现延迟加载 */ private static Singleton instance = null; /* 私有构造方法，防止被实例化 */ private Singleton() &#123; &#125; /* 静态工程方法，创建实例 */ public static Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; /* 如果该对象被用于序列化，可以保证对象在序列化前后保持一致 */ public Object readResolve() &#123; return instance; &#125;&#125; 这个类可以满足基本要求，但是，像这样毫无线程安全保护的类，如果我们把它放入多线程的环境下，肯定就会出现问题了，如何解决？我们首先会想到对getInstance方法加synchronized关键字，如下： 123456public static synchronized Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; 但是，synchronized关键字锁住的是这个对象，这样的用法，在性能上会有所下降，因为每次调用getInstance()，都要对对象上锁，事实上，只有在第一次创建对象的时候需要加锁，之后就不需要了，所以，这个地方需要改进。我们改成下面这个： 12345678910public static Singleton getInstance() &#123; if (instance == null) &#123; synchronized (instance) &#123; if (instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125; 似乎解决了之前提到的问题，将synchronized关键字加在了内部，也就是说当调用的时候是不需要加锁的，只有在instance为null，并创建对象的时候才需要加锁，性能有一定的提升。但是，这样的情况，还是有可能有问题的，看下面的情况：在Java指令中创建对象和赋值操作是分开进行的，也就是说instance = new Singleton();语句是分两步执行的。但是JVM并不保证这两个操作的先后顺序，也就是说有可能JVM会为新的Singleton实例分配空间，然后直接赋值给instance成员，然后再去初始化这个Singleton实例。这样就可能出错了，我们以A、B两个线程为例： a&gt; A、B线程同时进入了第一个if判断 b&gt;A首先进入synchronized块，由于instance为null，所以它执行instance = new Singleton(); c&gt;由于JVM内部的优化机制，JVM先画出了一些分配给Singleton实例的空白内存，并赋值给instance成员（注意此时JVM没有开始初始化这个实例），然后A离开了synchronized块。 d&gt;B进入synchronized块，由于instance此时不是null，因此它马上离开了synchronized块并将结果返回给调用该方法的程序。 e&gt;此时B线程打算使用Singleton实例，却发现它没有被初始化，于是错误发生了。 所以程序还是有可能发生错误，其实程序在运行过程是很复杂的，从这点我们就可以看出，尤其是在写多线程环境下的程序更有难度，有挑战性。我们对该程序做进一步优化： 123456private static class SingletonFactory&#123; private static Singleton instance = new Singleton(); &#125; public static Singleton getInstance()&#123; return SingletonFactory.instance; &#125; 实际情况是，单例模式使用内部类来维护单例的实现，JVM内部的机制能够保证当一个类被加载的时候，这个类的加载过程是线程互斥的。这样当我们第一次调用getInstance的时候，JVM能够帮我们保证instance只被创建一次，并且会保证把赋值给instance的内存初始化完毕，这样我们就不用担心上面的问题。同时该方法也只会在第一次调用的时候使用互斥机制，这样就解决了低性能问题。这样我们暂时总结一个完美的单例模式： 1234567891011121314151617public class Singleton &#123; /* 私有构造方法，防止被实例化 */ private Singleton() &#123; &#125; /* 此处使用一个内部类来维护单例 */ private static class SingletonFactory &#123; private static Singleton instance = new Singleton(); &#125; /* 获取实例 */ public static Singleton getInstance() &#123; return SingletonFactory.instance; &#125; /* 如果该对象被用于序列化，可以保证对象在序列化前后保持一致 */ public Object readResolve() &#123; return getInstance(); &#125;&#125; 其实说它完美，也不一定，如果在构造函数中抛出异常，实例将永远得不到创建，也会出错。所以说，十分完美的东西是没有的，我们只能根据实际情况，选择最适合自己应用场景的实现方法。也有人这样实现：因为我们只需要在创建类的时候进行同步，所以只要将创建和getInstance()分开，单独为创建加synchronized关键字，也是可以的： 12345678910111213141516public class SingletonTest &#123; private static SingletonTest instance = null; private SingletonTest() &#123; &#125; private static synchronized void syncInit() &#123; if (instance == null) &#123; instance = new SingletonTest(); &#125; &#125; public static SingletonTest getInstance() &#123; if (instance == null) &#123; syncInit(); &#125; return instance; &#125;&#125; 考虑性能的话，整个程序只需创建一次实例，所以性能也不会有什么影响。 补充：采用”影子实例”的办法为单例对象的属性同步更新 123456789101112131415161718192021222324public class SingletonTest &#123; private static SingletonTest instance = null; private Vector properties = null; public Vector getProperties() &#123; return properties; &#125; private SingletonTest() &#123; &#125; private static synchronized void syncInit() &#123; if (instance == null) &#123; instance = new SingletonTest(); &#125; &#125; public static SingletonTest getInstance() &#123; if (instance == null) &#123; syncInit(); &#125; return instance; &#125; public void updateProperties() &#123; SingletonTest shadow = new SingletonTest(); properties = shadow.getProperties(); &#125;&#125; 通过单例模式的学习告诉我们： 单例模式理解起来简单，但是具体实现起来还是有一定的难度。 synchronized关键字锁定的是对象，在用的时候，一定要在恰当的地方使用（注意需要使用锁的对象和过程，可能有的时候并不是整个对象及整个过程都需要锁）。 到这儿，单例模式基本已经讲完了，结尾处，笔者突然想到另一个问题，就是采用类的静态方法，实现单例模式的效果，也是可行的，此处二者有什么不同？ 首先，静态类不能实现接口。（从类的角度说是可以的，但是那样就破坏了静态了。因为接口中不允许有static修饰的方法，所以即使实现了也是非静态的） 其次，单例可以被延迟初始化，静态类一般在第一次加载是初始化。之所以延迟加载，是因为有些类比较庞大，所以延迟加载有助于提升性能。 再次，单例类可以被继承，他的方法可以被覆写。但是静态类内部方法都是static，无法被覆写。 最后一点，单例类比较灵活，毕竟从实现上只是一个普通的Java类，只要满足单例的基本需求，你可以在里面随心所欲的实现一些其它功能，但是静态类不行。从上面这些概括中，基本可以看出二者的区别，但是，从另一方面讲，我们上面最后实现的那个单例模式，内部就是用一个静态类来实现的，所以，二者有很大的关联，只是我们考虑问题的层面不同罢了。两种思想的结合，才能造就出完美的解决方案，就像HashMap采用数组+链表来实现一样，其实生活中很多事情都是这样，单用不同的方法来处理问题，总是有优点也有缺点，最完美的方法是，结合各个方法的优点，才能最好的解决问题！ 4、建造者模式（Builder） 5、原型模式（Prototype） 原型模式虽然是创建型的模式，但是与工程模式没有关系，从名字即可看出，该模式的思想就是将一个对象作为原型，对其进行复制、克隆，产生一个和原对象类似的新对象。本小结会通过对象的复制，进行讲解。在Java中，复制对象是通过clone()实现的，先创建一个原型类： 123456public class Prototype implements Cloneable &#123; public Object clone() throws CloneNotSupportedException &#123; Prototype proto = (Prototype) super.clone(); return proto; &#125;&#125; 很简单，一个原型类，只需要实现Cloneable接口，覆写clone方法，此处clone方法可以改成任意的名称，因为Cloneable接口是个空接口，你可以任意定义实现类的方法名，如cloneA或者cloneB，因为此处的重点是super.clone()这句话，super.clone()调用的是Object的clone()方法，而在Object类中，clone()是native的，具体怎么实现，我会在另一篇文章中，关于解读Java中本地方法的调用，此处不再深究。在这儿，我将结合对象的浅复制和深复制来说一下，首先需要了解对象深、浅复制的概念： 浅复制：将一个对象复制后，基本数据类型的变量都会重新创建，而引用类型，指向的还是原对象所指向的。 深复制：将一个对象复制后，不论是基本数据类型还有引用类型，都是重新创建的。简单来说，就是深复制进行了完全彻底的复制，而浅复制不彻底。 此处，写一个深浅复制的例子： 123456789101112131415161718192021222324252627282930313233343536public class Prototype implements Cloneable, Serializable &#123; private static final long serialVersionUID = 1L; private String string; private SerializableObject obj; /* 浅复制 */ public Object clone() throws CloneNotSupportedException &#123; Prototype proto = (Prototype) super.clone(); return proto; &#125; /* 深复制 */ public Object deepClone() throws IOException, ClassNotFoundException &#123; /* 写入当前对象的二进制流 */ ByteArrayOutputStream bos = new ByteArrayOutputStream(); ObjectOutputStream oos = new ObjectOutputStream(bos); oos.writeObject(this); /* 读出二进制流产生的新对象 */ ByteArrayInputStream bis = new ByteArrayInputStream(bos.toByteArray()); ObjectInputStream ois = new ObjectInputStream(bis); return ois.readObject(); &#125; public String getString() &#123; return string; &#125; public void setString(String string) &#123; this.string = string; &#125; public SerializableObject getObj() &#123; return obj; &#125; public void setObj(SerializableObject obj) &#123; this.obj = obj; &#125;&#125;class SerializableObject implements Serializable &#123; private static final long serialVersionUID = 1L;&#125; 要实现深复制，需要采用流的形式读入当前对象的二进制输入，再写出二进制数据对应的对象。 B、结构模式（7种） 我们接着讨论设计模式，上篇文章我讲完了5种创建型模式，这章开始，我将讲下7种结构型模式：适配器模式、装饰模式、代理模式、外观模式、桥接模式、组合模式、享元模式。其中对象的适配器模式是各种模式的起源，我们看下面的图： 6、适配器模式 适配器模式将某个类的接口转换成客户端期望的另一个接口表示，目的是消除由于接口不匹配所造成的类的兼容性问题。主要分为三类：类的适配器模式、对象的适配器模式、接口的适配器模式。 01、类的适配器模式 核心思想就是：有一个Source类，拥有一个方法，待适配，目标接口是Targetable，通过Adapter类，将Source的功能扩展到Targetable里，看代码： 12345public class Source &#123; public void method1() &#123; System.out.println(“this is original method!”); &#125;&#125; 123456public interface Targetable &#123; /* 与原类中的方法相同 */ public void method1(); /* 新类的方法 */ public void method2();&#125; 123456public class Adapter extends Source implements Targetable &#123; @Override public void method2() &#123; System.out.println(“this is the targetable method!”); &#125;&#125; Adapter类继承Source类，实现Targetable接口，下面是测试类： 1234567public class AdapterTest &#123; public static void main(String[] args) &#123; Targetable target = new Adapter(); target.method1(); target.method2(); &#125;&#125; 输出： 12this is original method!this is the targetable method! 这样Targetable接口的实现类就具有了Source类的功能。 02、对象的适配器模式 基本思路和类的适配器模式相同，只是将Adapter类作修改，这次不继承Source类，而是持有Source类的实例，以达到解决兼容性的问题。看图： 只需要修改Adapter类的源码即可： 123456789101112131415public class Wrapper implements Targetable &#123; private Source source; public Wrapper(Source source)&#123; super(); this.source = source; &#125; @Override public void method2() &#123; System.out.println(“this is the targetable method!”); &#125; @Override public void method1() &#123; source.method1(); &#125;&#125; 测试类： 12345678public class AdapterTest &#123; public static void main(String[] args) &#123; Source source = new Source(); Targetable target = new Wrapper(source); target.method1(); target.method2(); &#125;&#125; 输出与第一种一样，只是适配的方法不同而已。 03、接口的适配器模式 第三种适配器模式是接口的适配器模式，接口的适配器是这样的：有时我们写的一个接口中有多个抽象方法，当我们写该接口的实现类时，必须实现该接口的所有方法，这明显有时比较浪费，因为并不是所有的方法都是我们需要的，有时只需要某一些，此处为了解决这个问题，我们引入了接口的适配器模式，借助于一个抽象类，该抽象类实现了该接口，实现了所有的方法，而我们不和原始的接口打交道，只和该抽象类取得联系，所以我们写一个类，继承该抽象类，重写我们需要的方法就行。看一下类图： 这个很好理解，在实际开发中，我们也常会遇到这种接口中定义了太多的方法，以致于有时我们在一些实现类中并不是都需要。看代码： 1234public interface Sourceable &#123; public void method1(); public void method2();&#125; 抽象类Wrapper2： 1234public abstract class Wrapper2 implements Sourceable&#123; public void method1()&#123;&#125; public void method2()&#123;&#125;&#125; 12345public class SourceSub1 extends Wrapper2 &#123; public void method1()&#123; System.out.println(“the sourceable interface’s first Sub1!”); &#125;&#125; 12345public class SourceSub2 extends Wrapper2 &#123; public void method2()&#123; System.out.println(“the sourceable interface’s second Sub2!”); &#125;&#125; 12345678910public class WrapperTest &#123; public static void main(String[] args) &#123; Sourceable source1 = new SourceSub1(); Sourceable source2 = new SourceSub2(); source1.method1(); source1.method2(); source2.method1(); source2.method2(); &#125;&#125; 测试输出： 12the sourceable interface’s first Sub1!the sourceable interface’s second Sub2! 达到了我们的效果！ 讲了这么多，总结一下三种适配器模式的应用场景： 类的适配器模式：当希望将一个类转换成满足另一个新接口的类时，可以使用类的适配器模式，创建一个新类，继承原有的类，实现新的接口即可。 对象的适配器模式：当希望将一个对象转换成满足另一个新接口的对象时，可以创建一个Wrapper类，持有原类的一个实例，在Wrapper类的方法中，调用实例的方法就行。 接口的适配器模式：当不希望实现一个接口中所有的方法时，可以创建一个抽象类Wrapper，实现所有方法，我们写别的类的时候，继承抽象类即可。 7. 装饰模式（Decorator） 顾名思义，装饰模式就是给一个对象增加一些新的功能，而且是动态的，要求装饰对象和被装饰对象实现同一个接口，装饰对象持有被装饰对象的实例，关系图如下： Source类是被装饰类，Decorator类是一个装饰类，可以为Source类动态的添加一些功能，代码如下： 123public interface Sourceable &#123; public void method();&#125; 123456public class Source implements Sourceable &#123; @Override public void method() &#123; System.out.println(“the original method!”); &#125;&#125; 12345678910111213public class Decorator implements Sourceable &#123; private Sourceable source; public Decorator(Sourceable source)&#123; super(); this.source = source; &#125; @Override public void method() &#123; System.out.println(“before decorator!”); source.method(); System.out.println(“after decorator!”); &#125;&#125; 测试类： 1234567public class DecoratorTest &#123; public static void main(String[] args) &#123; Sourceable source = new Source(); Sourceable obj = new Decorator(source); obj.method(); &#125;&#125; 输出： 123before decorator!the original method!after decorator! 装饰器模式的应用场景： 需要扩展一个类的功能。 动态的为一个对象增加功能，而且还能动态撤销。（继承不能做到这一点，继承的功能是静态的，不能动态增删。） 缺点：产生过多相似的对象，不易排错！ 8、代理模式（Proxy） 其实每个模式名称就表明了该模式的作用，代理模式就是多一个代理类出来，替原对象进行一些操作，比如我们在租房子的时候回去找中介，为什么呢？因为你对该地区房屋的信息掌握的不够全面，希望找一个更熟悉的人去帮你做，此处的代理就是这个意思。再如我们有的时候打官司，我们需要请律师，因为律师在法律方面有专长，可以替我们进行操作，表达我们的想法。先来看看关系图： 根据上文的阐述，代理模式就比较容易的理解了，我们看下代码： 123public interface Sourceable &#123; public void method();&#125; 123456public class Source implements Sourceable &#123; @Override public void method() &#123; System.out.println(“the original method!”); &#125;&#125; 12345678910111213141516171819public class Proxy implements Sourceable &#123; private Source source; public Proxy()&#123; super(); this.source = new Source(); &#125; @Override public void method() &#123; before(); source.method(); atfer(); &#125; private void atfer() &#123; System.out.println(“after proxy!”); &#125; private void before() &#123; System.out.println(“before proxy!”); &#125;&#125; 测试类： 123456public class ProxyTest &#123; public static void main(String[] args) &#123; Sourceable source = new Proxy(); source.method(); &#125;&#125; 输出： 123before proxy!the original method!after proxy! 代理模式的应用场景： 如果已有的方法在使用的时候需要对原有的方法进行改进，此时有两种办法： 修改原有的方法来适应。这样违反了“对扩展开放，对修改关闭”的原则。 就是采用一个代理类调用原有的方法，且对产生的结果进行控制。这种方法就是代理模式。 使用代理模式，可以将功能划分的更加清晰，有助于后期维护！ 9、外观模式（Facade） 外观模式是为了解决类与类之家的依赖关系的，像spring一样，可以将类和类之间的关系配置到配置文件中，而外观模式就是将他们的关系放在一个Facade类中，降低了类类之间的耦合度，该模式中没有涉及到接口，看下类图：（我们以一个计算机的启动过程为例） 我们先看下实现类： 12345678public class CPU &#123; public void startup()&#123; System.out.println(“cpu startup!”); &#125; public void shutdown()&#123; System.out.println(“cpu shutdown!”); &#125;&#125; 12345678public class Memory &#123; public void startup()&#123; System.out.println(“memory startup!”); &#125; public void shutdown()&#123; System.out.println(“memory shutdown!”); &#125;&#125; 12345678public class Disk &#123; public void startup()&#123; System.out.println(“disk startup!”); &#125; public void shutdown()&#123; System.out.println(“disk shutdown!”); &#125;&#125; 1234567891011121314151617181920212223242526public class Computer &#123; private CPU cpu; private Memory memory; private Disk disk; public Computer()&#123; cpu = new CPU(); memory = new Memory(); disk = new Disk(); &#125; public void startup()&#123; System.out.println(“start the computer!”); cpu.startup(); memory.startup(); disk.startup(); System.out.println(“start computer finished!”); &#125; public void shutdown()&#123; System.out.println(“begin to close the computer!”); cpu.shutdown(); memory.shutdown(); disk.shutdown(); System.out.println(“computer closed!”); &#125;&#125; User类如下： 1234567public class User &#123; public static void main(String[] args) &#123; Computer computer = new Computer(); computer.startup(); computer.shutdown(); &#125;&#125; 输出： 12345678910start the computer!cpu startup!memory startup!disk startup!start computer finished!begin to close the computer!cpu shutdown!memory shutdown!disk shutdown!computer closed! 如果我们没有Computer类，那么，CPU、Memory、Disk他们之间将会相互持有实例，产生关系，这样会造成严重的依赖，修改一个类，可能会带来其他类的修改，这不是我们想要看到的，有了Computer类，他们之间的关系被放在了Computer类里，这样就起到了解耦的作用，这，就是外观模式！ 10、桥接模式（Bridge） 桥接模式就是把事物和其具体实现分开，使他们可以各自独立的变化。桥接的用意是：将抽象化与实现化解耦，使得二者可以独立变化，像我们常用的JDBC桥DriverManager一样，JDBC进行连接数据库的时候，在各个数据库之间进行切换，基本不需要动太多的代码，甚至丝毫不用动，原因就是JDBC提供统一接口，每个数据库提供各自的实现，用一个叫做数据库驱动的程序来桥接就行了。我们来看看关系图： 实现代码： 先定义接口： 123public interface Sourceable &#123; public void method();&#125; 分别定义两个实现类： 123456public class SourceSub1 implements Sourceable &#123; @Override public void method() &#123; System.out.println(“this is the first sub!”); &#125;&#125; 123456public class SourceSub2 implements Sourceable &#123; @Override public void method() &#123; System.out.println(“this is the second sub!”); &#125;&#125; 定义一个桥，持有Sourceable的一个实例： 123456789101112public abstract class Bridge &#123; private Sourceable source; public void method()&#123; source.method(); &#125; public Sourceable getSource() &#123; return source; &#125; public void setSource(Sourceable source) &#123; this.source = source; &#125;&#125; 12345public class MyBridge extends Bridge &#123; public void method()&#123; getSource().method(); &#125;&#125; 测试类： 12345678910111213public class BridgeTest &#123; public static void main(String[] args) &#123; Bridge bridge = new MyBridge(); /*调用第一个对象*/ Sourceable source1 = new SourceSub1(); bridge.setSource(source1); bridge.method(); /*调用第二个对象*/ Sourceable source2 = new SourceSub2(); bridge.setSource(source2); bridge.method(); &#125;&#125; output： 12this is the first sub!this is the second sub! 这样，就通过对Bridge类的调用，实现了对接口Sourceable的实现类SourceSub1和SourceSub2的调用。接下来我再画个图，大家就应该明白了，因为这个图是我们JDBC连接的原理，有数据库学习基础的，一结合就都懂了。 11. 组合模式（Composite） 组合模式有时又叫部分-整体模式在处理类似树形结构的问题时比较方便，看看关系图： 直接来看代码： 1234567891011121314151617181920212223242526272829303132public class TreeNode &#123; private String name; private TreeNode parent; private Vector&lt;TreeNode&gt; children = new Vector&lt;TreeNode&gt;(); public TreeNode(String name)&#123; this.name = name; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public TreeNode getParent() &#123; return parent; &#125; public void setParent(TreeNode parent) &#123; this.parent = parent; &#125; //添加孩子节点 public void add(TreeNode node)&#123; children.add(node); &#125; //删除孩子节点 public void remove(TreeNode node)&#123; children.remove(node); &#125; //取得孩子节点 public Enumeration&lt;TreeNode&gt; getChildren()&#123; return children.elements(); &#125;&#125; 1234567891011121314public class Tree &#123; TreeNode root = null; public Tree(String name) &#123; root = new TreeNode(name); &#125; public static void main(String[] args) &#123; Tree tree = new Tree(“A”); TreeNode nodeB = new TreeNode(“B”); TreeNode nodeC = new TreeNode(“C”); nodeB.add(nodeC); tree.root.add(nodeB); System.out.println(“build the tree finished!”); &#125;&#125; 使用场景：将多个对象组合在一起进行操作，常用于表示树形结构中，例如二叉树，数等。 12、享元模式（Flyweight） 享元模式的主要目的是实现对象的共享，即共享池，当系统中对象多的时候可以减少内存的开销，通常与工厂模式一起使用。 FlyWeightFactory负责创建和管理享元单元，当一个客户端请求时，工厂需要检查当前对象池中是否有符合条件的对象，如果有，就返回已经存在的对象，如果没有，则创建一个新对象，FlyWeight是超类。一提到共享池，我们很容易联想到Java里面的JDBC连接池，想想每个连接的特点，我们不难总结出：适用于作共享的一些个对象，他们有一些共有的属性，就拿数据库连接池来说，url、driverClassName、username、password及dbname，这些属性对于每个连接来说都是一样的，所以就适合用享元模式来处理，建一个工厂类，将上述类似属性作为内部数据，其它的作为外部数据，在方法调用时，当做参数传进来，这样就节省了空间，减少了实例的数量。 看个例子： 看下数据库连接池的代码： 12345678910111213141516171819202122232425262728293031323334353637383940public class ConnectionPool &#123; private Vector&lt;Connection&gt; pool; /*公有属性*/ private String url = “jdbc:mysql://localhost:3306/test”; private String username = “root”; private String password = “root”; private String driverClassName = “com.mysql.jdbc.Driver”; private int poolSize = 100; private static ConnectionPool instance = null; Connection conn = null; /*构造方法，做一些初始化工作*/ private ConnectionPool() &#123; pool = new Vector&lt;Connection&gt;(poolSize); for (int i = 0; i &lt; poolSize; i++) &#123; try &#123; Class.forName(driverClassName); conn = DriverManager.getConnection(url, username, password); pool.add(conn); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; &#125; /* 返回连接到连接池 */ public synchronized void release() &#123; pool.add(conn); &#125; /* 返回连接池中的一个数据库连接 */ public synchronized Connection getConnection() &#123; if (pool.size() &gt; 0) &#123; Connection conn = pool.get(0); pool.remove(conn); return conn; &#125; else &#123; return null; &#125; &#125;&#125; 通过连接池的管理，实现了数据库连接的共享，不需要每一次都重新创建连接，节省了数据库重新创建的开销，提升了系统的性能！ C、关系模式（11种） 先来张图，看看这11中模式的关系： 第一类：通过父类与子类的关系进行实现。 第二类：两个类之间。 第三类：类的状态。 第四类：通过中间类 父类与子类关系 13、策略模式（strategy） 策略模式定义了一系列算法，并将每个算法封装起来，使他们可以相互替换，且算法的变化不会影响到使用算法的客户。需要设计一个接口，为一系列实现类提供统一的方法，多个实现类实现该接口，设计一个抽象类（可有可无，属于辅助类），提供辅助函数，关系图如下： 图中ICalculator提供同意的方法， AbstractCalculator是辅助类，提供辅助方法，接下来，依次实现下每个类： 首先统一接口： 123public interface ICalculator &#123; public int calculate(String exp);&#125; 辅助类： 123456789public abstract class AbstractCalculator &#123; public int[] split(String exp,String opt)&#123; String array[] = exp.split(opt); int arrayInt[] = new int[2]; arrayInt[0] = Integer.parseInt(array[0]); arrayInt[1] = Integer.parseInt(array[1]); return arrayInt; &#125;&#125; 三个实现类： 1234567public class Plus extends AbstractCalculator implements ICalculator &#123; @Override public int calculate(String exp) &#123; int arrayInt[] = split(exp,”\\\\+”); return arrayInt[0]+arrayInt[1]; &#125;&#125; 1234567public class Minus extends AbstractCalculator implements ICalculator &#123; @Override public int calculate(String exp) &#123; int arrayInt[] = split(exp,”-“); return arrayInt[0]-arrayInt[1]; &#125;&#125; 1234567public class Multiply extends AbstractCalculator implements ICalculator &#123; @Override public int calculate(String exp) &#123; int arrayInt[] = split(exp,”\\\\*”); return arrayInt[0]*arrayInt[1]; &#125;&#125; 简单的测试类： 12345678public class StrategyTest &#123; public static void main(String[] args) &#123; String exp = “2+8”; ICalculator cal = new Plus(); int result = cal.calculate(exp); System.out.println(result); &#125;&#125; 输出：10 策略模式的决定权在用户，系统本身提供不同算法的实现，新增或者删除算法，对各种算法做封装。因此，策略模式多用在算法决策系统中，外部用户只需要决定用哪个算法即可。 14、模板方法模式（Template Method） 解释一下模板方法模式，就是指：一个抽象类中，有一个主方法，再定义1…n个方法，可以是抽象的，也可以是实际的方法，定义一个类，继承该抽象类，重写抽象方法，通过调用抽象类，实现对子类的调用，先看个关系图： 就是在AbstractCalculator类中定义一个主方法calculate，calculate()调用spilt()等，Plus和Minus分别继承AbstractCalculator类，通过对AbstractCalculator的调用实现对子类的调用，看下面的例子： 12345678910111213141516public abstract class AbstractCalculator &#123; /*主方法，实现对本类其它方法的调用*/ public final int calculate(String exp,String opt)&#123; int array[] = split(exp,opt); return calculate(array[0],array[1]); &#125; /*被子类重写的方法*/ abstract public int calculate(int num1,int num2); public int[] split(String exp,String opt)&#123; String array[] = exp.split(opt); int arrayInt[] = new int[2]; arrayInt[0] = Integer.parseInt(array[0]); arrayInt[1] = Integer.parseInt(array[1]); return arrayInt; &#125;&#125; 123456public class Plus extends AbstractCalculator &#123; @Override public int calculate(int num1,int num2) &#123; return num1 + num2; &#125;&#125; 测试类： 12345678public class StrategyTest &#123; public static void main(String[] args) &#123; String exp = “8+8”; AbstractCalculator cal = new Plus(); int result = cal.calculate(exp, “\\\\+”); System.out.println(result); &#125;&#125; 我跟踪下这个小程序的执行过程：首先将exp和”\\+”做参数，调用AbstractCalculator类里的calculate(String,String)方法，在calculate(String,String)里调用同类的split()，之后再调用calculate(int ,int)方法，从这个方法进入到子类中，执行完return num1 + num2后，将值返回到AbstractCalculator类，赋给result，打印出来。正好验证了我们开头的思路。 类之间的关系 15、观察者模式（Observer） 包括这个模式在内的接下来的四个模式，都是类和类之间的关系，不涉及到继承，学的时候应该 记得归纳，记得本文最开始的那个图。观察者模式很好理解，类似于邮件订阅和RSS订阅，当我们浏览一些博客或wiki时，经常会看到RSS图标，就这的意思是，当你订阅了该文章，如果后续有更新，会及时通知你。其实，简单来讲就一句话：当一个对象变化时，其它依赖该对象的对象都会收到通知，并且随着变化！对象之间是一种一对多的关系。先来看看关系图： 我解释下这些类的作用：MySubject类就是我们的主对象，Observer1和Observer2是依赖于MySubject的对象，当MySubject变化时，Observer1和Observer2必然变化。AbstractSubject类中定义着需要监控的对象列表，可以对其进行修改：增加或删除被监控对象，且当MySubject变化时，负责通知在列表内存在的对象。我们看实现代码： 一个Observer接口： 123public interface Observer &#123; public void update();&#125; 两个实现类： 123456public class Observer1 implements Observer &#123; @Override public void update() &#123; System.out.println(“observer1 has received!”); &#125;&#125; 123456public class Observer2 implements Observer &#123; @Override public void update() &#123; System.out.println(“observer2 has received!”); &#125;&#125; Subject接口及实现类： 12345678910public interface Subject &#123; /*增加观察者*/ public void add(Observer observer); /*删除观察者*/ public void del(Observer observer); /*通知所有的观察者*/ public void notifyObservers(); /*自身的操作*/ public void operation();&#125; 123456789101112131415161718public abstract class AbstractSubject implements Subject &#123; private Vector&lt;Observer&gt; vector = new Vector&lt;Observer&gt;(); @Override public void add(Observer observer) &#123; vector.add(observer); &#125; @Override public void del(Observer observer) &#123; vector.remove(observer); &#125; @Override public void notifyObservers() &#123; Enumeration&lt;Observer&gt; enumo = vector.elements(); while(enumo.hasMoreElements())&#123; enumo.nextElement().update(); &#125; &#125;&#125; 1234567public class MySubject extends AbstractSubject &#123; @Override public void operation() &#123; System.out.println(“update self!”); notifyObservers(); &#125;&#125; 测试类： 12345678public class ObserverTest &#123; public static void main(String[] args) &#123; Subject sub = new MySubject(); sub.add(new Observer1()); sub.add(new Observer2()); sub.operation(); &#125;&#125; 输出： 123update self!observer1 has received!observer2 has received! 这些东西，其实不难，只是有些抽象，不太容易整体理解，建议读者：根据关系图，新建项目，自己写代码（或者参考我的代码）,按照总体思路走一遍，这样才能体会它的思想，理解起来容易！ 16、迭代子模式（Iterator） 顾名思义，迭代器模式就是顺序访问聚集中的对象，一般来说，集合中非常常见，如果对集合类比较熟悉的话，理解本模式会十分轻松。这句话包含两层意思：一是需要遍历的对象，即聚集对象，二是迭代器对象，用于对聚集对象进行遍历访问。我们看下关系图： 这个思路和我们常用的一模一样，MyCollection中定义了集合的一些操作，MyIterator中定义了一系列迭代操作，且持有Collection实例，我们来看看实现代码： 两个接口： 1234567public interface Collection &#123; public Iterator iterator(); /*取得集合元素*/ public Object get(int i); /*取得集合大小*/ public int size();&#125; 123456789public interface Iterator &#123; //前移 public Object previous(); //后移 public Object next(); public boolean hasNext(); //取得第一个元素 public Object first();&#125; 两个实现： 123456789101112131415public class MyCollection implements Collection &#123; public String string[] = &#123;“A”,”B”,”C”,”D”,”E”&#125;; @Override public Iterator iterator() &#123; return new MyIterator(this); &#125; @Override public Object get(int i) &#123; return string[i]; &#125; @Override public int size() &#123; return string.length; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334public class MyIterator implements Iterator &#123; private Collection collection; private int pos = -1; public MyIterator(Collection collection)&#123; this.collection = collection; &#125; @Override public Object previous() &#123; if(pos &gt; 0)&#123; pos–; &#125; return collection.get(pos); &#125; @Override public Object next() &#123; if(pos&lt;collection.size()-1)&#123; pos++; &#125; return collection.get(pos); &#125; @Override public boolean hasNext() &#123; if(pos&lt;collection.size()-1)&#123; return true; &#125;else&#123; return false; &#125; &#125; @Override public Object first() &#123; pos = 0; return collection.get(pos); &#125;&#125; 测试类： 123456789public class Test &#123; public static void main(String[] args) &#123; Collection collection = new MyCollection(); Iterator it = collection.iterator(); while(it.hasNext())&#123; System.out.println(it.next()); &#125; &#125;&#125; 输出：A B C D E 此处我们貌似模拟了一个集合类的过程，感觉是不是很爽？其实JDK中各个类也都是这些基本的东西，加一些设计模式，再加一些优化放到一起的，只要我们把这些东西学会了，掌握好了，我们也可以写出自己的集合类，甚至框架！ 17、责任链模式（Chain of Responsibility） 接下来我们将要谈谈责任链模式，有多个对象，每个对象持有对下一个对象的引用，这样就会形成一条链，请求在这条链上传递，直到某一对象决定处理该请求。但是发出者并不清楚到底最终那个对象会处理该请求，所以，责任链模式可以实现，在隐瞒客户端的情况下，对系统进行动态的调整。先看看关系图： Abstracthandler类提供了get和set方法，方便MyHandle类设置和修改引用对象，MyHandle类是核心，实例化后生成一系列相互持有的对象，构成一条链。 123public interface Handler &#123; public void operator();&#125; 123456789public abstract class AbstractHandler &#123; private Handler handler; public Handler getHandler() &#123; return handler; &#125; public void setHandler(Handler handler) &#123; this.handler = handler; &#125;&#125; 12345678910111213public class MyHandler extends AbstractHandler implements Handler &#123; private String name; public MyHandler(String name) &#123; this.name &#x3D; name; &#125; @Override public void operator() &#123; System.out.println(name+”deal!”); if(getHandler()!&#x3D;null)&#123; getHandler().operator(); &#125; &#125;&#125; 12345678910public class Test &#123; public static void main(String[] args) &#123; MyHandler h1 = new MyHandler(“h1”); MyHandler h2 = new MyHandler(“h2”); MyHandler h3 = new MyHandler(“h3”); h1.setHandler(h2); h2.setHandler(h3); h1.operator(); &#125;&#125; 输出： 123h1deal!h2deal!h3deal! 此处强调一点就是，链接上的请求可以是一条链，可以是一个树，还可以是一个环，模式本身不约束这个，需要我们自己去实现，同时，在一个时刻，命令只允许由一个对象传给另一个对象，而不允许传给多个对象。 18、命令模式（Command） 命令模式很好理解，举个例子，司令员下令让士兵去干件事情，从整个事情的角度来考虑，司令员的作用是，发出口令，口令经过传递，传到了士兵耳朵里，士兵去执行。这个过程好在，三者相互解耦，任何一方都不用去依赖其他人，只需要做好自己的事儿就行，司令员要的是结果，不会去关注到底士兵是怎么实现的。我们看看关系图： Invoker是调用者（司令员），Receiver是被调用者（士兵），MyCommand是命令，实现了Command接口，持有接收对象，看实现代码： 123public interface Command &#123; public void exe();&#125; 12345678910public class MyCommand implements Command &#123; private Receiver receiver; public MyCommand(Receiver receiver) &#123; this.receiver = receiver; &#125; @Override public void exe() &#123; receiver.action(); &#125;&#125; 12345public class Receiver &#123; public void action()&#123; System.out.println(“command received!”); &#125;&#125; 123456789public class Invoker &#123; private Command command; public Invoker(Command command) &#123; this.command = command; &#125; public void action()&#123; command.exe(); &#125;&#125; 12345678public class Test &#123; public static void main(String[] args) &#123; Receiver receiver = new Receiver(); Command cmd = new MyCommand(receiver); Invoker invoker = new Invoker(cmd); invoker.action(); &#125;&#125; 输出：command received! 这个很哈理解，命令模式的目的就是达到命令的发出者和执行者之间解耦，实现请求和执行分开，熟悉Struts的同学应该知道，Struts其实就是一种将请求和呈现分离的技术，其中必然涉及命令模式的思想！ 其实每个设计模式都是很重要的一种思想，看上去很熟，其实是因为我们在学到的东西中都有涉及，尽管有时我们并不知道，其实在Java本身的设计之中处处都有体现，像AWT、JDBC、集合类、IO管道或者是Web框架，里面设计模式无处不在。因为我们篇幅有限，很难讲每一个设计模式都讲的很详细，不过我会尽我所能，尽量在有限的空间和篇幅内，把意思写清楚了，更好让大家明白。本章不出意外的话，应该是设计模式最后一讲了，首先还是上一下上篇开头的那个图： 本章讲讲第三类和第四类。 类的状态 19、备忘录模式（Memento） 主要目的是保存一个对象的某个状态，以便在适当的时候恢复对象，个人觉得叫备份模式更形象些，通俗的讲下：假设有原始类A，A中有各种属性，A可以决定需要备份的属性，备忘录类B是用来存储A的一些内部状态，类C呢，就是一个用来存储备忘录的，且只能存储，不能修改等操作。做个图来分析一下： Original类是原始类，里面有需要保存的属性value及创建一个备忘录类，用来保存value值。Memento类是备忘录类，Storage类是存储备忘录的类，持有Memento类的实例，该模式很好理解。直接看源码： 123456789101112131415161718public class Original &#123; private String value; public String getValue() &#123; return value; &#125; public void setValue(String value) &#123; this.value = value; &#125; public Original(String value) &#123; this.value = value; &#125; public Memento createMemento()&#123; return new Memento(value); &#125; public void restoreMemento(Memento memento)&#123; this.value = memento.getValue(); &#125;&#125; 123456789101112public class Memento &#123; private String value; public Memento(String value) &#123; this.value = value; &#125; public String getValue() &#123; return value; &#125; public void setValue(String value) &#123; this.value = value; &#125;&#125; 123456789101112public class Storage &#123; private Memento memento; public Storage(Memento memento) &#123; this.memento = memento; &#125; public Memento getMemento() &#123; return memento; &#125; public void setMemento(Memento memento) &#123; this.memento = memento; &#125;&#125; 测试类： 123456789101112131415public class Test &#123; public static void main(String[] args) &#123; // 创建原始类 Original origi = new Original(“egg”); // 创建备忘录 Storage storage = new Storage(origi.createMemento()); // 修改原始类的状态 System.out.println(“初始化状态为：” + origi.getValue()); origi.setValue(“niu”); System.out.println(“修改后的状态为：” + origi.getValue()); // 回复原始类的状态 origi.restoreMemento(storage.getMemento()); System.out.println(“恢复后的状态为：” + origi.getValue()); &#125;&#125; 输出： 123初始化状态为：egg修改后的状态为：niu恢复后的状态为：egg 简单描述下：新建原始类时，value被初始化为egg，后经过修改，将value的值置为niu，最后倒数第二行进行恢复状态，结果成功恢复了。其实我觉得这个模式叫“备份-恢复”模式最形象。 20、状态模式（State） 核心思想就是：当对象的状态改变时，同时改变其行为，很好理解！就拿QQ来说，有几种状态，在线、隐身、忙碌等，每个状态对应不同的操作，而且你的好友也能看到你的状态，所以，状态模式就两点：1、可以通过改变状态来获得不同的行为。2、你的好友能同时看到你的变化。看图： State类是个状态类，Context类可以实现切换，我们来看看代码： 12345678910111213141516171819202122package com.xtfggef.dp.state;/** * 状态类的核心类 * 2012-12-1 * @author erqing * */public class State &#123; private String value; public String getValue() &#123; return value; &#125; public void setValue(String value) &#123; this.value = value; &#125; public void method1()&#123; System.out.println(“execute the first opt!”); &#125; public void method2()&#123; System.out.println(“execute the second opt!”); &#125;&#125; 12345678910111213141516171819202122232425package com.xtfggef.dp.state;/** * 状态模式的切换类 2012-12-1 * @author erqing * */public class Context &#123; private State state; public Context(State state) &#123; this.state = state; &#125; public State getState() &#123; return state; &#125; public void setState(State state) &#123; this.state = state; &#125; public void method() &#123; if (state.getValue().equals(“state1”)) &#123; state.method1(); &#125; else if (state.getValue().equals(“state2”)) &#123; state.method2(); &#125; &#125;&#125; 测试类： 123456789101112public class Test &#123; public static void main(String[] args) &#123; State state = new State(); Context context = new Context(state); //设置第一种状态 state.setValue(“state1”); context.method(); //设置第二种状态 state.setValue(“state2”); context.method(); &#125;&#125; 输出： 12execute the first opt!execute the second opt! 根据这个特性，状态模式在日常开发中用的挺多的，尤其是做网站的时候，我们有时希望根据对象的某一属性，区别开他们的一些功能，比如说简单的权限控制等。 通过中间类 21、访问者模式（Visitor） 访问者模式把数据结构和作用于结构上的操作解耦合，使得操作集合可相对自由地演化。访问者模式适用于数据结构相对稳定算法又易变化的系统。因为访问者模式使得算法操作增加变得容易。若系统数据结构对象易于变化，经常有新的数据对象增加进来，则不适合使用访问者模式。访问者模式的优点是增加操作很容易，因为增加操作意味着增加新的访问者。访问者模式将有关行为集中到一个访问者对象中，其改变不影响系统数据结构。其缺点就是增加新的数据结构很困难。—— From 百科 简单来说，访问者模式就是一种分离对象数据结构与行为的方法，通过这种分离，可达到为一个被访问者动态添加新的操作而无需做其它的修改的效果。简单关系图： 来看看原码：一个Visitor类，存放要访问的对象， 123public interface Visitor &#123; public void visit(Subject sub);&#125; 123456public class MyVisitor implements Visitor &#123; @Override public void visit(Subject sub) &#123; System.out.println(“visit the subject：”+sub.getSubject()); &#125;&#125; Subject类，accept方法，接受将要访问它的对象，getSubject()获取将要被访问的属性， 1234public interface Subject &#123; public void accept(Visitor visitor); public String getSubject();&#125; 12345678910public class MySubject implements Subject &#123; @Override public void accept(Visitor visitor) &#123; visitor.visit(this); &#125; @Override public String getSubject() &#123; return “love”; &#125;&#125; 测试： 1234567public class Test &#123; public static void main(String[] args) &#123; Visitor visitor = new MyVisitor(); Subject sub = new MySubject(); sub.accept(visitor); &#125;&#125; 输出：visit the subject：love 该模式适用场景：如果我们想为一个现有的类增加新功能，不得不考虑几个事情：1、新功能会不会与现有功能出现兼容性问题？2、以后会不会再需要添加？3、如果类不允许修改代码怎么办？面对这些问题，最好的解决方法就是使用访问者模式，访问者模式适用于数据结构相对稳定的系统，把数据结构和算法解耦， 22、中介者模式（Mediator） 中介者模式也是用来降低类类之间的耦合的，因为如果类类之间有依赖关系的话，不利于功能的拓展和维护，因为只要修改一个对象，其它关联的对象都得进行修改。如果使用中介者模式，只需关心和Mediator类的关系，具体类类之间的关系及调度交给Mediator就行，这有点像spring容器的作用。先看看图： User类统一接口，User1和User2分别是不同的对象，二者之间有关联，如果不采用中介者模式，则需要二者相互持有引用，这样二者的耦合度很高，为了解耦，引入了Mediator类，提供统一接口，MyMediator为其实现类，里面持有User1和User2的实例，用来实现对User1和User2的控制。这样User1和User2两个对象相互独立，他们只需要保持好和Mediator之间的关系就行，剩下的全由MyMediator类来维护！基本实现： ···java public interface Mediator { public void createMediator(); public void workAll(); } 12345678910111213141516171819202122&#96;&#96;&#96;javapublic class MyMediator implements Mediator &#123; private User user1; private User user2; public User getUser1() &#123; return user1; &#125; public User getUser2() &#123; return user2; &#125; @Override public void createMediator() &#123; user1 &#x3D; new User1(this); user2 &#x3D; new User2(this); &#125; @Override public void workAll() &#123; user1.work(); user2.work(); &#125;&#125; 12345678910public abstract class User &#123; private Mediator mediator; public Mediator getMediator()&#123; return mediator; &#125; public User(Mediator mediator) &#123; this.mediator = mediator; &#125; public abstract void work();&#125; 123456789public class User1 extends User &#123; public User1(Mediator mediator)&#123; super(mediator); &#125; @Override public void work() &#123; System.out.println(“user1 exe!”); &#125;&#125; 123456789public class User2 extends User &#123; public User2(Mediator mediator)&#123; super(mediator); &#125; @Override public void work() &#123; System.out.println(“user2 exe!”); &#125;&#125; 测试类： 1234567public class Test &#123; public static void main(String[] args) &#123; Mediator mediator = new MyMediator(); mediator.createMediator(); mediator.workAll(); &#125;&#125; 输出： 12user1 exe!user2 exe! 23、解释器模式（Interpreter） 解释器模式是我们暂时的最后一讲，一般主要应用在OOP开发中的编译器的开发中，所以适用面比较窄。 Context类是一个上下文环境类，Plus和Minus分别是用来计算的实现，代码如下： 123public interface Expression &#123; public int interpret(Context context);&#125; 123456public class Plus implements Expression &#123; @Override public int interpret(Context context) &#123; return context.getNum1()+context.getNum2(); &#125;&#125; 123456public class Minus implements Expression &#123; @Override public int interpret(Context context) &#123; return context.getNum1()-context.getNum2(); &#125;&#125; 1234567891011121314151617181920public class Context &#123; private int num1; private int num2; public Context(int num1, int num2) &#123; this.num1 = num1; this.num2 = num2; &#125; public int getNum1() &#123; return num1; &#125; public void setNum1(int num1) &#123; this.num1 = num1; &#125; public int getNum2() &#123; return num2; &#125; public void setNum2(int num2) &#123; this.num2 = num2; &#125;&#125; 12345678public class Test &#123; public static void main(String[] args) &#123; // 计算9+2-8的值 int result = new Minus().interpret((new Context(new Plus() .interpret(new Context(9, 2)), 8))); System.out.println(result); &#125;&#125; 最后输出正确的结果：3。基本就这样，解释器模式用来做各种各样的解释器，如正则表达式等的解释器等等！ 本文地址：http://xnerv.wang/23-design-patterns-analysis/ 转载自：23种设计模式全解析","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://xnerv.wang/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"设计模式","slug":"设计模式","permalink":"https://xnerv.wang/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"Java","slug":"Java","permalink":"https://xnerv.wang/tags/Java/"}]},{"title":"Overriding the virtual table in a C++ object（转载）","slug":"overriding-the-virtual-table-in-a-cpp-object","date":"2017-11-04T05:52:00.000Z","updated":"2023-08-21T02:24:21.700Z","comments":true,"path":"overriding-the-virtual-table-in-a-cpp-object/","link":"","permalink":"https://xnerv.wang/overriding-the-virtual-table-in-a-cpp-object/","excerpt":"Yesterday I was discussing with a friend of mine about how polymorphism is implemented in C++, and that is, using a virtual table ( remember the “virtual” keyword in method definitions? ). A virtual table is, rawly speaking, just like an array of function pointers. Each created object with virtual methods needs a virtual table. So, where does the virtual table is stored?, I really don’t know, but I do know where I can find the address of the virtual table associated to an object ( at least in g++ 4.1.1 ), the first sizeof(void*) bytes of an object are used to store a pointer to the virtual table. With this knowledge, one could think that is possible to override the virtual table pointer of the object and call arbitrary functions, and yes, we can. Let’s see some fun code.","text":"Yesterday I was discussing with a friend of mine about how polymorphism is implemented in C++, and that is, using a virtual table ( remember the “virtual” keyword in method definitions? ). A virtual table is, rawly speaking, just like an array of function pointers. Each created object with virtual methods needs a virtual table. So, where does the virtual table is stored?, I really don’t know, but I do know where I can find the address of the virtual table associated to an object ( at least in g++ 4.1.1 ), the first sizeof(void*) bytes of an object are used to store a pointer to the virtual table. With this knowledge, one could think that is possible to override the virtual table pointer of the object and call arbitrary functions, and yes, we can. Let’s see some fun code. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#include &lt;iostream&gt;using namespace std;class Parent&#123; public: virtual void VirtFunc1() &#123; cout &lt;&lt; &quot;Parent::VirtFunc1&quot; &lt;&lt; endl; &#125; virtual void VirtFunc2() &#123; cout &lt;&lt; &quot;Parent::VirtFunc2&quot; &lt;&lt; endl; &#125;&#125;;class Child : public Parent&#123; public: void VirtFunc1() &#123; cout &lt;&lt; &quot;Child::VirtFunc1&quot; &lt;&lt; endl; &#125; void VirtFunc2() &#123; cout &lt;&lt; &quot;Child::VirtFunc2&quot; &lt;&lt; endl; &#125;&#125;;typedef void (*virtual_function)();struct FakeVirtualTable &#123; virtual_function virtual_one; virtual_function virtual_two;&#125;;void fake_virtual_one()&#123; cout &lt;&lt; &quot;Faked virtual call 1&quot; &lt;&lt; endl;&#125;void fake_virtual_two()&#123; cout &lt;&lt; &quot;Faked virtual call 2&quot; &lt;&lt; endl;&#125;int main()&#123; /* declare a Child class and a base pointer to it. */ Child child_class_obj; Parent* parent_class_ptr = &amp;child_class_obj; /* create our fake virtual table with pointers to our fake methods */ FakeVirtualTable custom_table; custom_table.virtual_one = fake_virtual_one; custom_table.virtual_two = fake_virtual_two; /* take the address of our stack virtual table and override the real object pointer to the virtual table */ FakeVirtualTable* table_ptr = &amp;custom_table; memcpy(parent_class_ptr, &amp;table_ptr, sizeof(void*)); /* call the methods ( but we&#x27;re really calling the faked functions ) */ parent_class_ptr-&gt;VirtFunc1(); parent_class_ptr-&gt;VirtFunc2(); return 0;&#125; So, try to run that code and, of course, the expected result is having fake_virtual_one() and fake_virtual_two() functions called. No magic there, we just replace the first sizeof(void*) bytes of the object with our own table pointer. There is not a use I can think of right now, but it is funny …. 本文地址：http://xnerv.wang/overriding-the-virtual-table-in-a-cpp-object/ 转载自：Overriding the virtual table in a C++ object","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://xnerv.wang/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"C++","slug":"C","permalink":"https://xnerv.wang/tags/C/"},{"name":"Programing","slug":"Programing","permalink":"https://xnerv.wang/tags/Programing/"}]},{"title":"Linux进程、内核及文件系统总结","slug":"linux-process-kernel-and-file-system-summary","date":"2017-11-03T23:58:00.000Z","updated":"2023-08-21T02:24:19.579Z","comments":true,"path":"linux-process-kernel-and-file-system-summary/","link":"","permalink":"https://xnerv.wang/linux-process-kernel-and-file-system-summary/","excerpt":"fork 关于linux进程间的close-on-exec机制 一般我们会调用exec执行另一个程序，此时会用全新的程序替换子进程的正文，数据，堆和栈等。此时保存文件描述符的变量当然也不存在 了，我们就无法关闭无用的文件描述符了。所以通常我们会fork子进程后在子进程中直接执行close关掉无用的文件描述符，然后再执行exec。 但是在复杂系统中，有时我们fork子进程时已经不知道打开了多少个文件描述符（包括socket句柄等），这此时进行逐一清理确实有很大难 度。我们期望的是能在fork子进程前打开某个文件句柄时就指定好：“这个句柄我在fork子进程后执行exec时就关闭”。其实时有这样的方法的：即所 谓 的 close-on-exec。 回到我们的应用场景中来，只要我们在创建socket的时候加上 SOCK_CLOEXEC标志，就能够达到我们要求的效果，在fork子进程中执行exec的时候，会清理掉父进程创建的socket。","text":"fork 关于linux进程间的close-on-exec机制 一般我们会调用exec执行另一个程序，此时会用全新的程序替换子进程的正文，数据，堆和栈等。此时保存文件描述符的变量当然也不存在 了，我们就无法关闭无用的文件描述符了。所以通常我们会fork子进程后在子进程中直接执行close关掉无用的文件描述符，然后再执行exec。 但是在复杂系统中，有时我们fork子进程时已经不知道打开了多少个文件描述符（包括socket句柄等），这此时进行逐一清理确实有很大难 度。我们期望的是能在fork子进程前打开某个文件句柄时就指定好：“这个句柄我在fork子进程后执行exec时就关闭”。其实时有这样的方法的：即所 谓 的 close-on-exec。 回到我们的应用场景中来，只要我们在创建socket的时候加上 SOCK_CLOEXEC标志，就能够达到我们要求的效果，在fork子进程中执行exec的时候，会清理掉父进程创建的socket。 Are child processes created with fork() automatically killed when the parent is killed? No. If the parent is killed, children become children of the init process (that has the process id 1 and is launched as the first user process by the kernel). The init process checks periodically for new children, and kills them if they have exited (thus freeing resources that are allocated by their return value). How to make child process die after parent exits? Child can ask kernel to deliver SIGHUP (or other signal) when parent dies by specifying option PR_SET_PDEATHSIG in prctl() syscall like this: prctl(PR_SET_PDEATHSIG, SIGHUP); See man 2 prctl for details. Linux 技巧：让进程在后台可靠运行的几种方法 线程实现 LinuxThreads的不足 按照POSIX定义，同一进程的所有线程应该共享一个进程id和父进程id，这在目前的&quot;一对一&quot;模型下是无法实现的。 由于异步信号是内核以进程为单位分发的，而LinuxThreads的每个线程对内核来说都是一个进程，且没有实现&quot;线程组&quot;，因此，某些语义不符合POSIX标准，比如没有实现向进程中所有线程发送信号，README对此作了说明。 LinuxThreads将每个进程的线程最大数目定义为1024，但实际上这个数值还受到整个系统的总进程数限制，这又是由于线程其实是核心进程。 管理线程容易成为瓶颈，这是这种结构的通病；同时，管理线程又负责用户线程的清理工作，因此，尽管管理线程已经屏蔽了大部分的信号，但一旦管理线程死亡，用户线程就不得不手工清理了，而且用户线程并不知道管理线程的状态，之后的线程创建等请求将无人处理。 LinuxThreads中的线程同步很大程度上是建立在信号基础上的，这种通过内核复杂的信号处理机制的同步方式，效率一直是个问题。 其他的线程实现机制 LinuxThreads的问题，特别是兼容性上的问题，严重阻碍了Linux上的跨平台应用（如Apache）采用多线程设计，从而使得Linux上的线程应用一直保持在比较低的水平。在Linux社区中，已经有很多人在为改进线程性能而努力，其中既包括用户级线程库，也包括核心级和用户级配合改进的线程库。目前最为人看好的有两个项目，一个是RedHat公司牵头研发的NPTL（Native Posix Thread Library），另一个则是IBM投资开发的NGPT（Next Generation Posix Threading），二者都是围绕完全兼容POSIX 1003.1c，同时在核内和核外做工作以而实现多对多线程模型。这两种模型都在一定程度上弥补了LinuxThreads的缺点，且都是重起炉灶全新设计的。 Linux 线程模型的比较：LinuxThreads 和 NPTL LinuxThreads 的限制已经在 NPTL 以及 LinuxThreads 后期的一些版本中得到了克服。例如，最新的 LinuxThreads 实现使用了线程注册来定位线程本地数据；例如在 Intel® 处理器上，它就使用了 %fs 和 %gs 段寄存器来定位访问线程本地数据所使用的虚拟地址。尽管这个结果展示了 LinuxThreads 所采纳的一些修改的改进结果，但是它在更高负载和压力测试中，依然存在很多问题，因为它过分地依赖于一个管理线程，使用它来进行信号处理等操作。 您应该记住，在使用 LinuxThreads 构建库时，需要使用 -D_REENTRANT 编译时标志。这使得库线程是安全的。 最后，也许是最重要的事情，请记住 LinuxThreads 项目的创建者已经不再积极更新它了，他们认为 NPTL 会取代 LinuxThreads。 LinuxThreads 的缺点并不意味着 NPTL 就没有错误。作为一个面向 SMP 的设计，NPTL 也有一些缺点。我曾经看到过在最近的 Red Hat 内核上出现过这样的问题：一个简单线程在单处理器的机器上运行良好，但在 SMP 机器上却挂起了。我相信在 Linux 上还有更多工作要做才能使它具有更好的可伸缩性，从而满足高端应用程序的需求。 SIGNAL Clarification on SIGKILL, SIGTERM, SIGINT, SIGQUIT, SIGSTP and SIGHUP SIGKILL: kill -9 SIGTERM: kill SIGINT: Ctrl+C (The difference between SIGINT and SIGTERM is that the former can be sent from a terminal as input characters.) SIGQUIT Ctrl+\\ (generates a core dump of the process and also cleans up resources held up by a process.) SIGSTP Ctrl+Z (Suspends a process. The process can be resumed by sending a SIGCONT signal.) SIGHUP Ctrl+D (Hangs up a process when the controlling terminal is disconnected.) Linux内核信号处理机制介绍 如果想要进程捕获某个信号，然后作出相应的处理，就需要注册信号处理函数。同中断类似，内核也为每个进程准备了一个信号向量表,信号向量表中记录着每个信号所对应的处理机制，默认情况下是调用默认处理机制。当进程为某个信号注册了信号处理程序后，发生该信号时，内核就会调用注册的函数。 信号是异步的，一个进程不可能等待信号的到来，也不知道信号会到来，那么，进程是如何发现和接受信号呢？实际上，信号的接收不是由用户进程来完成的，而是由内核代理。当一个进程P2向另一个进程P1发送信号后，内核接受到信号，并将其放在P1的信号队列当中。当P1再次陷入内核态时，会检查信号队列，并根据相应的信号调取相应的信号处理函数。 内核态与用户态 Intel x86架构的CPU来说一共有0~3四个特权级，0级最高，3级最低，硬件上在执行每条指令时都会对指令所具有的特权级做相应的检查，相关的概念有CPL、DPL和RPL 用户态切换到内核态的3种方式：系统调用、异常、外围设备的中断。其中系统调用可以认为是用户进程主动发起的，异常和外围设备中断则是被动的。 从触发方式上看，可以认为存在前述3种不同的类型，但是从最终实际完成由用户态到内核态的切换操作上来说，涉及的关键步骤是完全一致的，没有任何区别，都相当于执行了一个中断响应的过程，因为系统调用实际上最终是中断机制实现的，而异常和中断的处理机制基本上也是一致的。 涉及到由用户态切换到内核态的步骤主要包括： 从当前进程的描述符中提取其内核栈的ss0及esp0信息。 使用ss0和esp0指向的内核栈将当前进程的cs,eip,eflags,ss,esp信息保存起来，这个过程也完成了由用户栈到内核栈的切换过程，同时保存了被暂停执行的程序的下一条指令。 将先前由中断向量检索得到的中断处理程序的cs,eip信息装入相应的寄存器，开始执行中断处理程序，这时就转到了内核态的程序执行了。 内核态程序执行完毕时如果要从内核态返回用户态，可以通过执行指令iret来完成，指令iret会将先前压栈的进入内核态前的cs,eip,eflags,ss,esp信息从栈里弹出，加载到各个对应的寄存器中，重新开始执行用户态的程序。 处理器总处于以下状态中的一种： 内核态，运行于进程上下文，内核代表进程运行于内核空间； 内核态，运行于中断上下文，内核代表硬件运行于内核空间； 用户态，运行于用户空间。 系统调用 strace常用来跟踪进程执行时的系统调用和所接收的信号。 在Linux世界，进程不能直接访问硬件设备，当进程需要访问硬件设备(比如读取磁盘文件，接收网络数据等等)时，必须由用户态模式切换至内核态模式，通过系统调用访问硬件设备。strace可以跟踪到一个进程产生的系统调用,包括参数，返回值，执行消耗的时间。 GDB则主要依赖一个系统函数ptrace。 Windows上的替代物则是WinDbg的Logger.exe和wt调试命令，以及Process Monitor则工具。 中断及IO调度 Linux 设备驱动开发 —— Tasklets 机制浅析 tasklet是I/O驱动程序中实现可延迟函数的首选方法。 中断处理程序&amp;中断服务例程 Signals and interrupts a comparison Interrupts can be viewed as a mean of communication between the CPU and the OS kernel. Signals can be viewed as a mean of communication between the OS kernel and OS processes. Interrupts may be initiated by the CPU (exceptions - e.g.: divide by zero, page fault), devices (hardware interrupts - e.g: input available), or by a CPU instruction (traps - e.g: syscalls, breakpoints). They are eventually managed by the CPU, which “interrupts” the current task, and invokes an OS-kernel provided ISR/interrupt handler. Signals may be initiated by the OS kernel (e.g: SIGFPE, SIGSEGV, SIGIO), or by a process(kill()). They are eventually managed by the OS kernel, which delivers them to the target thread/process, invoking either a generic action (ignore, terminate, terminate and dump core) or a process-provided signal handler. Hardware interrupts can also generate signals, like a keyboard interrupt generates SIGINT. Thus interrupts and signals are closely tied to each other. Linux 2.4.x内核软中断机制 硬中断、软中断 中断：通常被定义成一个事件，该事件改变处理器执行的指令顺序。这样的事件与cpu芯片外部电路产生的电信号相对应。 中断的产生：每个能够发出中断请求的硬件设备控制器都有一条称为IRQ的输出线（中断线）。所有的IRQ线都与一个中断控制器的输入引脚相连，中断控制器与cpu的intr引脚相连。 中断向量：每个中断由0-255之间的一个8位数来标识。称为中断向量。 中断描述符表：IDT是一个系统表，它与每一个中断或者异常向量相联系，每一个向量在表中有相应的中断处理程 序的入口地址。cpu的idtr寄存器执行IDT表的物理基地址。 硬中断是由硬件产生的，比如，像磁盘，网卡，键盘，时钟等。每个设备或设备集都有它自己的IRQ（中断请求）。基于IRQ，CPU可以将相应的请求分发到对应的硬件驱动上（注：硬件驱动通常是内核中的一个子程序，而不是一个独立的进程）。 中断的硬件处理：在内核被init进程初始化后，cpu运行在保护模式下。当执行一条指令后，sc和eip这对寄存器包含了下一条将要执行的指令的逻辑地址。在执行这条指令之前，cpu控制单元会检查在运行前一条指令时是否发生了一个中断。如果发生了，cpu控制单元处理中断。 中断与信号的区别：软中断通常是硬中断服务程序对内核的中断。信号则是由内核或者其他进程对某个进程的中断。 硬中断可以直接中断CPU。它会引起内核中相关的代码被触发。对于那些需要花费一些时间去处理的进程，中断代码本身也可以被其他的硬中断中断。 对于时钟中断，内核调度代码会将当前正在运行的进程挂起，从而让其他的进程来运行。它的存在是为了让调度代码（或称为调度器）可以调度多任务。 软中断的处理非常像硬中断。然而，它们仅仅是由当前正在运行的进程所产生的。 软中断并不会直接中断CPU。也只有当前正在运行的代码（或进程）才会产生软中断。这种中断是一种需要内核为正在运行的进程去做一些事情（通常为I/O）的请求。有一个特殊的软中断是Yield调用，它的作用是请求内核调度器去查看是否有一些其他的进程可以运行。 int n - 触发软中断n。相应的中断处理函数的地址为：中断向量表地址 + 4 * n。软件中断处理程序是由操作系统提供的为保证系统异步执行的机制。如在linux下由用户态向内核态转换需要调用0x80软件中断。软中断是实现系统API函数调用的手段。 中断嵌套：Linux下硬中断是可以嵌套的，但是没有优先级的概念，也就是说任何一个新的中断都可以打断正在执行的中断，但同种中断除外。软中断不能嵌套，但相同类型的软中断可以在不同CPU上并行执行。 文件系统 Linux 文件系统剖析 Linux 虚拟系统文件交换器剖析 linux文件系统中superblock,inode,dentry及file的关系 需要注意的几点如下所示： 进程每打开一个文件，就会有一个file结构与之对应。同一个进程可以多次打开同一个文件而得到多个不同的file结构，file结构描述被打开文件的属性，如文件的当前偏移量等信息。 两个不同的file结构可以对应同一个dentry结构。进程多次打开同一个文件时，对应的只有一个dentry结构。Dentry结构存储目录项和对应文件（inode）的信息。 在存储介质中，每个文件对应唯一的inode结点，但是每个文件又可以有多个文件名。即可以通过不同的文件名访问同一个文件。这里多个文件名对应一个文件的关系在数据结构中表示就是dentry和inode的关系。 Inode中不存储文件的名字，它只存储节点号；而dentry则保存有名字和与其对应的节点号，所以就可以通过不同的dentry访问同一个inode。 不同的dentry则是同个文件链接（ln命令）来实现的。 How do file permissions apply to symlinks? macos不考虑，一般的linux在chmod符号链接时其实作用于指向的文件本身，但chmod在有些情况下是会跳过涉及符号链接的情况的。 Symbolic link permissions 权限是记录在inode上的，符号链接最终指向的也是同一个inode。 EXT2 文件系统 我们将 inode 与 block 区块用图解来说明一下，如下图所示，文件系统先格式化出 inode 与 block 的区块，假设某一个文件的属性与权限数据是放置到 inode 4 号(下图较小方格内)，而这个 inode 记录了文件数据的实际放置点为 2, 7, 13, 15 这四个 block 号码，此时我们的操作系统就能够据此来排列磁盘的阅读顺序，可以一口气将四个 block 内容读出来！ 那么数据的读取就如同下图中的箭头所指定的模样了。 这种数据存取的方法我们称为索引式文件系统(indexed allocation)。那有没有其他的惯用文件系统可以比较一下啊？ 有的，那就是我们惯用的闪盘(闪存)，闪盘使用的文件系统一般为 FAT 格式。FAT 这种格式的文件系统并没有 inode 存在，所以 FAT 没有办法将这个文件的所有 block 在一开始就读取出来。每个 block 号码都记录在前一个 block 当中， 他的读取方式有点像底下这样： 上图中我们假设文件的数据依序写入1-&gt;7-&gt;4-&gt;15号这四个 block 号码中， 但这个文件系统没有办法一口气就知道四个 block 的号码，他得要一个一个的将 block 读出后，才会知道下一个 block 在何处。 如果同一个文件数据写入的 block 分散的太过厉害时，则我们的磁盘读取头将无法在磁盘转一圈就读到所有的数据， 因此磁盘就会多转好几圈才能完整的读取到这个文件的内容！ 常常会听到所谓的『碎片整理』吧？ 需要碎片整理的原因就是文件写入的 block 太过于离散了，此时文件读取的效能将会变的很差所致。 这个时候可以透过碎片整理将同一个文件所属的 blocks 汇整在一起，这样数据的读取会比较容易啊！ 想当然尔，FAT 的文件系统需要经常的碎片整理一下，那么 Ext2 是否需要磁盘重整呢？ 由于 Ext2 是索引式文件系统，基本上不太需要常常进行碎片整理的。但是如果文件系统使用太久， 常常删除/编辑/新增文件时，那么还是可能会造成文件数据太过于离散的问题，此时或许会需要进行重整一下的。 每天进步一点点——Linux中的文件描述符与打开文件之间的关系 SUID(Set UID)是让执行一个可执行程序的process拥有owner的权限，如passwd程序修改/etc/passwd文件。 SGID(Set GID)可以设置在目录上，也可以设置在文件上。设置在目录上是强制本目录下新建的（一级）文件和（一级）目录的group自动变为该目录的group。而设置在文件上，则是让执行一个可执行程序的process拥有该目录group的权限。 SBIT（Sticky Bit）设置在目录上，该目录下的（一级）文件和目录只有owner和root可以删除。 Linux的chattr和lsattr可以达到和NTFS权限一样复杂的功能。 The difference between fsync() and fdatasync() is that the later does not necessarily update the meta-data associated with a file – such as the “last modified” date – but only the file data. 本文地址：http://xnerv.wang/linux-process-kernel-and-file-system-summary/","categories":[{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/categories/Linux/"}],"tags":[{"name":"原创","slug":"原创","permalink":"https://xnerv.wang/tags/%E5%8E%9F%E5%88%9B/"},{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/tags/Linux/"},{"name":"信号","slug":"信号","permalink":"https://xnerv.wang/tags/%E4%BF%A1%E5%8F%B7/"},{"name":"中断","slug":"中断","permalink":"https://xnerv.wang/tags/%E4%B8%AD%E6%96%AD/"},{"name":"内核态与用户态","slug":"内核态与用户态","permalink":"https://xnerv.wang/tags/%E5%86%85%E6%A0%B8%E6%80%81%E4%B8%8E%E7%94%A8%E6%88%B7%E6%80%81/"},{"name":"文件系统","slug":"文件系统","permalink":"https://xnerv.wang/tags/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"}]},{"title":"(Google Developer) Protocol Buffers Encoding","slug":"google-protocol-buffers-encoding","date":"2017-10-27T05:57:00.000Z","updated":"2023-08-21T02:24:18.478Z","comments":true,"path":"google-protocol-buffers-encoding/","link":"","permalink":"https://xnerv.wang/google-protocol-buffers-encoding/","excerpt":"This document describes the binary wire format for protocol buffer messages. You don’t need to understand this to use protocol buffers in your applications, but it can be very useful to know how different protocol buffer formats affect the size of your encoded messages.","text":"This document describes the binary wire format for protocol buffer messages. You don’t need to understand this to use protocol buffers in your applications, but it can be very useful to know how different protocol buffer formats affect the size of your encoded messages. A Simple Message Let’s say you have the following very simple message definition: 123message Test1 &#123; required int32 a &#x3D; 1;&#125; In an application, you create a Test1 message and set a to 150. You then serialize the message to an output stream. If you were able to examine the encoded message, you’d see three bytes: 108 96 01 So far, so small and numeric – but what does it mean? Read on… Base 128 Varints To understand your simple protocol buffer encoding, you first need to understand varints. Varints are a method of serializing integers using one or more bytes. Smaller numbers take a smaller number of bytes. Each byte in a varint, except the last byte, has the most significant bit (msb) set – this indicates that there are further bytes to come. The lower 7 bits of each byte are used to store the two’s complement representation of the number in groups of 7 bits, least significant group first. So, for example, here is the number 1 – it’s a single byte, so the msb is not set: 10000 0001 And here is 300 – this is a bit more complicated: 11010 1100 0000 0010 How do you figure out that this is 300? First you drop the msb from each byte, as this is just there to tell us whether we’ve reached the end of the number (as you can see, it’s set in the first byte as there is more than one byte in the varint): 121010 1100 0000 0010→ 010 1100 000 0010 You reverse the two groups of 7 bits because, as you remember, varints store numbers with the least significant group first. Then you concatenate them to get your final value: 1234000 0010 010 1100→ 000 0010 ++ 010 1100→ 100101100→ 256 + 32 + 8 + 4 &#x3D; 300 Message Structure As you know, a protocol buffer message is a series of key-value pairs. The binary version of a message just uses the field’s number as the key – the name and declared type for each field can only be determined on the decoding end by referencing the message type’s definition (i.e. the .proto file). When a message is encoded, the keys and values are concatenated into a byte stream. When the message is being decoded, the parser needs to be able to skip fields that it doesn’t recognize. This way, new fields can be added to a message without breaking old programs that do not know about them. To this end, the “key” for each pair in a wire-format message is actually two values – the field number from your .proto file, plus a wire type that provides just enough information to find the length of the following value. The available wire types are as follows: Type Meaning Used For 0 Varint int32, int64, uint32, uint64, sint32, sint64, bool, enum 1 64-bit fixed64, sfixed64, double 2 Length-delimited string, bytes, embedded messages, packed repeated fields 3 Start group groups (deprecated) 4 End group groups (deprecated) 5 32-bit fixed32, sfixed32, float Each key in the streamed message is a varint with the value (field_number &lt;&lt; 3) | wire_type – in other words, the last three bits of the number store the wire type. Now let’s look at our simple example again. You now know that the first number in the stream is always a varint key, and here it’s 08, or (dropping the msb): 1000 1000 You take the last three bits to get the wire type (0) and then right-shift by three to get the field number (1). So you now know that the tag is 1 and the following value is a varint. Using your varint-decoding knowledge from the previous section, you can see that the next two bytes store the value 150. 123496 01 &#x3D; 1001 0110 0000 0001 → 000 0001 ++ 001 0110 (drop the msb and reverse the groups of 7 bits) → 10010110 → 2 + 4 + 16 + 128 &#x3D; 150 More Value Types Signed Integers As you saw in the previous section, all the protocol buffer types associated with wire type 0 are encoded as varints. However, there is an important difference between the signed int types (sint32 and sint64) and the “standard” int types (int32 and int64) when it comes to encoding negative numbers. If you use int32 or int64 as the type for a negative number, the resulting varint is always ten bytes long – it is, effectively, treated like a very large unsigned integer. If you use one of the signed types, the resulting varint uses ZigZag encoding, which is much more efficient. ZigZag encoding maps signed integers to unsigned integers so that numbers with a small absolute value (for instance, -1) have a small varint encoded value too. It does this in a way that “zig-zags” back and forth through the positive and negative integers, so that -1 is encoded as 1, 1 is encoded as 2, -2 is encoded as 3, and so on, as you can see in the following table: Signed Original Encoded As 0 0 -1 1 1 2 -2 3 2147483647 4294967294 -2147483648 4294967295 In other words, each value n is encoded using 1(n &lt;&lt; 1) ^ (n &gt;&gt; 31) for sint32s, or 1(n &lt;&lt; 1) ^ (n &gt;&gt; 63) for the 64-bit version. Note that the second shift – the (n &gt;&gt; 31) part – is an arithmetic shift. So, in other words, the result of the shift is either a number that is all zero bits (if n is positive) or all one bits (if n is negative). When the sint32 or sint64 is parsed, its value is decoded back to the original, signed version. Non-varint Numbers Non-varint numeric types are simple – double and fixed64 have wire type 1, which tells the parser to expect a fixed 64-bit lump of data; similarly float and fixed32 have wire type 5, which tells it to expect 32 bits. In both cases the values are stored in little-endian byte order. Strings A wire type of 2 (length-delimited) means that the value is a varint encoded length followed by the specified number of bytes of data. 123message Test2 &#123; required string b &#x3D; 2;&#125; Setting the value of b to “testing” gives you: 12 07 74 65 73 74 69 6e 67 The red bytes are the UTF8 of “testing”. The key here is 0x12 → tag = 2, type = 2. The length varint in the value is 7 and lo and behold, we find seven bytes following it – our string. Embedded Messages Here’s a message definition with an embedded message of our example type, Test1: 123message Test3 &#123; required Test1 c &#x3D; 3;&#125; And here’s the encoded version, again with the Test1’s a field set to 150: 1a 03 08 96 01 As you can see, the last three bytes are exactly the same as our first example (08 96 01), and they’re preceded by the number 3 – embedded messages are treated in exactly the same way as strings (wire type = 2). Optional And Repeated Elements If a proto2 message definition has repeated elements (without the [packed=true] option), the encoded message has zero or more key-value pairs with the same tag number. These repeated values do not have to appear consecutively; they may be interleaved with other fields. The order of the elements with respect to each other is preserved when parsing, though the ordering with respect to other fields is lost. In proto3, repeated fields use packed encoding, which you can read about below. For any non-repeated fields in proto3, or optional fields in proto2, the encoded message may or may not have a key-value pair with that tag number. Normally, an encoded message would never have more than one instance of a non-repeated field. However, parsers are expected to handle the case in which they do. For numeric types and strings, if the same field appears multiple times, the parser accepts the last value it sees. For embedded message fields, the parser merges multiple instances of the same field, as if with the Message::MergeFrom method – that is, all singular scalar fields in the latter instance replace those in the former, singular embedded messages are merged, and repeated fields are concatenated. The effect of these rules is that parsing the concatenation of two encoded messages produces exactly the same result as if you had parsed the two messages separately and merged the resulting objects. That is, this: 12MyMessage message;message.ParseFromString(str1 + str2); is equivalent to this: 1234MyMessage message, message2;message.ParseFromString(str1);message2.ParseFromString(str2);message.MergeFrom(message2); This property is occasionally useful, as it allows you to merge two messages even if you do not know their types. Article link: http://xnerv.wang/google-protocol-buffers-encoding/ Reprinted from: (Google Developer) Protocol Buffers: Encoding","categories":[{"name":"Protobuf","slug":"Protobuf","permalink":"https://xnerv.wang/categories/Protobuf/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"Google","slug":"Google","permalink":"https://xnerv.wang/tags/Google/"},{"name":"Protobuf","slug":"Protobuf","permalink":"https://xnerv.wang/tags/Protobuf/"},{"name":"Google Developer","slug":"Google-Developer","permalink":"https://xnerv.wang/tags/Google-Developer/"}]},{"title":"C++异常机制的实现方式和开销分析（转载）","slug":"analysis-about-cpp-exception-implementation","date":"2017-10-27T05:36:00.000Z","updated":"2023-08-21T02:24:21.657Z","comments":true,"path":"analysis-about-cpp-exception-implementation/","link":"","permalink":"https://xnerv.wang/analysis-about-cpp-exception-implementation/","excerpt":"在我几年前开始写《C++编码规范与指导》一文时，就已经规划着要加入这样一篇讨论 C++ 异常机制的文章了。没想到时隔几年以后才有机会把这个尾巴补完 :-)。 还是那句开场白：“在恰当的场合使用恰当的特性” 对每个称职的 C++ 程序员来说都是一个基本标准。想要做到这点，就必须要了解语言中每个特性的实现方式及其时空开销。异常处理由于涉及大量底层内容，向来是 C++ 各种高级机制中较难理解和透彻掌握的部分。本文将在尽量少引入底层细节的前提下，讨论 C++ 中这一崭新特性，并分析其实现开销：","text":"在我几年前开始写《C++编码规范与指导》一文时，就已经规划着要加入这样一篇讨论 C++ 异常机制的文章了。没想到时隔几年以后才有机会把这个尾巴补完 :-)。 还是那句开场白：“在恰当的场合使用恰当的特性” 对每个称职的 C++ 程序员来说都是一个基本标准。想要做到这点，就必须要了解语言中每个特性的实现方式及其时空开销。异常处理由于涉及大量底层内容，向来是 C++ 各种高级机制中较难理解和透彻掌握的部分。本文将在尽量少引入底层细节的前提下，讨论 C++ 中这一崭新特性，并分析其实现开销： 关于线程 进程和线程的概念相信各位看官早已耳熟能详。在这里，我只想带大家回忆几点重要概念： 一个进程中可以同时包含多个线程。 我们通常认为线程是操作系统可识别的最小并发执行和调度单位（不要跟俺说还有 Green Thread 或者 Fiber，OS Kernel 不认识也不参与这些物件的调度）。 同一进程中的多个线程共享代码段（代码和常量）、数据段（静态和全局变量）和扩展段（堆存储），但是每个线程有自己的栈段。栈段又叫运行时栈，用来存放所有局部变量和临时变量（参数、返回值、临时构造的变量等）。这一条对下文中的某些概念来说是非常重要的 。但是请注意，这里提到的各个“段”都是逻辑上的说法，在物理上某些硬件架构或者操作系统可能不使用段式存储。不过没关系，编译器会保证这些逻辑概念和假设的前提条件对每个 C/C++ 程序员来说始终是成立的。 由于共享了除栈以外的所有内存地址段，线程不可以有自己的“静态”或“全局”变量，为了弥补这一缺憾，操作系统通常会提供一种称为 TLS（Thread Local Storage，即：“线程本地存储”）的机制。通过该机制可以实现类似的功能。TLS 通常是线程控制块（TCB）中的某个指针所指向的一个指针数组，数组中的每个元素称为一个槽（Slot），每个槽中的指针由使用者定义，可以指向任意位置（但通常是指向堆存储中的某个偏移）。 函数的调用和返回 接着我们来回顾下一个预备知识：编译器如何实现函数的调用和返回。一般来说，编译器会为当前调用栈里的每个函数建立一个栈框架（Stack Frame）。“栈框架”担负着以下重要任务： 传递参数：通常，函数的调用参数总是在这个函数栈框架的最顶端。 传递返回地址：告诉被调用者的 return 语句应该 return 到哪里去，通常指向该函数调用的下一条语句（代码段中的偏移）。 存放调用者的当前栈指针：便于清理被调用者的所有局部变量、并恢复调用者的现场。 存放当前函数内的所有局部变量：记得吗？刚才说过所有局部和临时变量都是存储在栈上的。 最后再复习一点：栈是一种“后进先出”（LIFO）的数据结构，不过实际上大部分栈的实现都支持随机访问。 下面我们来看个具体例子： 假设有 FuncA、FuncB 和 FuncC 三个函数，每个函数均接收两个整形值作为其参数。在某线程上的某一时间段内，FuncA 调用了 FuncB，而 FuncB 又调用了 FuncC。则，它们的栈框架看起来应该像这样： 图1 函数调用栈框架示例 正如上图所示的那样，随着函数被逐级调用，编译器会为每一个函数建立自己的栈框架，栈空间逐渐消耗。随着函数的逐级返回，该函数的栈框架也将被逐级销毁，栈空间得以逐步释放。顺便说一句，递归函数的嵌套调用深度通常也是取决于运行时栈空间的剩余尺寸。 这里顺便解释另一个术语：调用约定（calling convention）。调用约定通常指：调用者将参数压入栈中（或放入寄存器中）的顺序，以及返回时由谁（调用者还是被调用者）来清理这些参数等细节规程方面的约定。 最后再说一句，这里所展示的函数调用乃是最“经典”的方式。实际情况是：在开启了优化选项后，编译器可能不会为一个内联甚至非内联的函数生成栈框架，编译器可能使用很多优化技术消除这个构造。不过对于一个 C/C++ 程序员来说，达到这样的理解程度通常就足够了。 C++ 函数的调用和返回 首先澄清一点，这里说的 “C++ 函数”是指： 该函数可能会直接或间接地抛出一个异常：即该函数的定义存放在一个 C++ 编译（而不是传统 C）单元内，并且该函数没有使用“throw()”异常过滤器。 或者该函数的定义内使用了 try 块。 以上两者满足其一即可。为了能够成功地捕获异常和正确地完成栈回退（stack unwind），编译器必须要引入一些额外的数据结构和相应的处理机制。我们首先来看看引入了异常处理机制的栈框架大概是什么样子： 图2 C++函数调用栈框架示例 由图2可见，在每个 C++ 函数的栈框架中都多了一些东西。仔细观察的话，你会发现，多出来的东西正好是一个 EXP 类型的结构体。进一步分析就会发现，这是一个典型的单向链表式结构： piPrev 成员指向链表的上一个节点，它主要用于在函数调用栈中逐级向上寻找匹配的 catch 块，并完成栈回退工作。 piHandler 成员指向完成异常捕获和栈回退所必须的数据结构（主要是两张记载着关键数据的表：“try”块表：tblTryBlocks 及“栈回退表”：tblUnwind）。 nStep 成员用来定位 try 块，以及在栈回退表中寻找正确的入口。 需要说明的是：编译器会为每一个“C++ 函数”定义一个 EHDL 结构，不过只会为包含了“try”块的函数定义 tblTryBlocks 成员。此外，异常处理器还会为每个线程维护一个指向当前异常处理框架的指针。该指针指向异常处理器链表的链尾，通常存放在某个 TLS 槽或能起到类似作用的地方。 最后，请再看一遍图2，并至少对其中的数据结构留下一个大体印象。我们会在后面多个小节中详细讨论它们。 注意：为了简化起见，本文中描述的数据结构内，大多省略了一些与话题无关的成员。 栈回退（Stack Unwind）机制 “栈回退”是伴随异常处理机制引入 C++ 中的一个新概念，主要用来确保在异常被抛出、捕获并处理后，所有生命期已结束的对象都会被正确地析构，它们所占用的空间会被正确地回收。 受益于栈回退机制的引入，以及 C++ 类所支持的“资源申请即初始化”语意，使得我们终于能够彻底告别既不优雅也不安全的 setjmp/longjmp 调用，简便又安全地实现远程跳转了。我想这也是 C++ 异常处理机制在错误处理以外唯一一种合理的应用方式了。 下面我们就来具体看看编译器是如何实现栈回退机制的： 图3 C++ 栈回退机制 图3中的“FuncUnWind”函数内，所有真实代码均以黑色和蓝色字体标示，编译器生成的代码则由灰色和橙色字体标明。此时，在图2里给出的 nStep 变量和 tblUnwind 成员作用就十分明显了。 nStep 变量用于跟踪函数内局部对象的构造、析构阶段。再配合编译器为每个函数生成的 tblUnwind 表，就可以完成退栈机制。表中的 pfnDestroyer 字段记录了对应阶段应当执行的析构操作（析构函数指针）；pObj 字段则记录了与之相对应的对象 this 指针偏移。将 pObj 所指的偏移值加上当前栈框架基址（EBP），就是要代入 pfnDestroyer 所指析构函数的 this 指针，这样即可完成对该对象的析构工作。而 nNextIdx 字段则指向下一个需要析构对象所在的行（下标）。 在发生异常时，异常处理器首先检查当前函数栈框架内的 nStep 值，并通过 piHandler 取得 tblUnwind[] 表。然后将 nStep 作为下标带入表中，执行该行定义的析构操作，然后转向由 nNextIdx 指向的下一行，直到 nNextIdx 为 -1 为止。在当前函数的栈回退工作结束后，异常处理器可沿当前函数栈框架内 piPrev的值回溯到异常处理链中的上一节点重复上述操作，直到所有回退工作完成为止。 值得一提的是，nStep 的值完全在编译时决定，运行时仅需执行若干次简单的整形立即数赋值（通常是直接赋值给CPU里的某个寄存器）。此外，对于所有内部类型以及使用了默认构造、析构方法（并且它的所有成员和基类也使用了默认方法）的类型，其创建和销毁均不影响 nStep 的值。 注意：如果在栈回退的过程中，由于析构函数的调用而再次引发了异常（异常中的异常），则被认为是一次异常处理机制的严重失败。此时进程将被强行禁止。为防止出现这种情况，应在所有可能抛出异常的析构函数中使用“std::uncaught_exception()”方法判断当前是否正在进行栈回退（即：存在一个未捕获或未完全处理完毕的异常）。如是，则应抑制异常的再次抛出。 异常捕获机制 一个异常被抛出时，就会立即引发 C++ 的异常捕获机制： 图4 C++ 异常捕获机制 在上一小节中，我们已经看到了 nStep 变量在跟踪对象构造、析构方面的作用。实际上 nStep 除了能够跟踪对象创建、销毁阶段以外，还能够标识当前执行点是否在 try 块中，以及（如果当前函数有多个 try 块的话）究竟在哪个 try 块中。这是通过在每一个 try 块的入口和出口各为 nStep 赋予一个唯一 ID 值，并确保 nStep 在对应 try 块内的变化恰在此范围之内来实现的。 在具体实现异常捕获时，首先，C++ 异常处理器检查发生异常的位置是否在当前函数的某个 try 块之内。这项工作可以通过将当前函数的 nStep 值依次在 piHandler 指向 tblTryBlocks[] 表的条目中进行范围为 [nBeginStep, nEndStep) 的比对来完成。 例如：若图4 中的 FuncB 在 nStep == 2 时发生了异常，则通过比对 FuncB 的 tblTryBlocks[] 表发现 2∈[1, 3)，故该异常发生在 FuncB 内的第一个 try 块中。 其次，如果异常发生的位置在当前函数中的某个 try 块内，则尝试匹配该 tblTryBlocks[] 相应条目中的 tblCatchBlocks[] 表。tblCatchBlocks[] 表中记录了与指定 try 块配套出现的所有 catch 块相关信息，包括这个 catch 块所能捕获的异常类型及其起始地址等信息。 若找到了一个匹配的 catch 块，则复制当前异常对象到此 catch 块，然后跳转到其入口地址执行块内代码。 否则，则说明异常发生位置不在当前函数的 try 块内，或者这个 try 块中没有与当前异常相匹配的 catch 块，此时则沿着函数栈框架中 piPrev 所指地址（即：异常处理链中的上一个节点）逐级重复以上过程，直至找到一个匹配的 catch 块或到达异常处理链的首节点。对于后者，我们称为发生了未捕获的异常，对于 C++ 异常处理器而言，未捕获的异常是一个严重错误，将导致当前进程被强制结束。 注意：虽然在图4示例中的 tblTryBlocks[] 只有一个条目，这个条目中的 tblCatchBlocks[] 也只有一行。但是在实际情况中，这两个表中都允许有多条记录。意即：一个函数中可以有多个 try 块，每个 try 块后均可跟随多个与之配套的 catch 块。 注意：按照标准意义上的理解，异常时的栈回退是伴随着异常捕获过程沿着异常处理链逐层向上进行的。但是有些编译器是在先完成异常捕获后再一次性进行栈回退的。无论具体实现使用了哪种方式，除非正在开发一个内存严格受限的嵌入式应用，通常我们按照标准语意来理解都不会产生什么问题。 备注：实际上 tblCatchBlocks 中还有一些较为关键但被故意省略的字段。比如指明该 catch 块异常对象复制方式（传值（拷贝构造）或传址（引用或指针））的字段，以及在何处存放被复制的异常对象（相对于入口地址的偏移位置）等信息。 异常的抛出 接下来讨论整个 C++ 异常处理机制中的最后一个环节，异常的抛出： 图5 C++ 异常抛出 在编译一段 C++ 代码时，编译器会将所有 throw 语句替换为其 C++ 运行时库中的某一指定函数，这里我们叫它 __CxxRTThrowExp（与本文提到的所有其它数据结构和属性名一样，在实际应用中它可以是任意名称）。该函数接收一个编译器认可的内部结构（我们叫它 EXCEPTION 结构）。这个结构中包含了待抛出异常对象的起始地址、用于销毁它的析构函数，以及它的 type_info 信息。对于没有启用 RTTI 机制（编译器禁用了 RTTI 机制或没有在类层次结构中使用虚表）的异常类层次结构，可能还要包含其所有基类的 type_info 信息，以便与相应的 catch 块进行匹配。 在图5中的深灰色框图内，我们使用 C++ 伪代码展示了函数 FuncA 中的 “throw myExp(1);” 语句将被编译器最终翻译成的样子。实际上在多数情况下，__CxxRTThrowExp 函数即我们前面曾多次提到的“异常处理器”，异常捕获和栈回退等各项重要工作都由它来完成。 __CxxRTThrowExp 首先接收（并保存）EXCEPTION 对象；然后从 TLS：Current ExpHdl 处找到与当前函数对应的 piHandler、nStep 等异常处理相关数据；并按照前文所述的机制完成异常捕获和栈回退。由此完成了包括“抛出”-&gt;“捕获”-&gt;“回退”等步骤的整套异常处理机制。 Windows 中的结构化异常处理 Microsoft Windows 带有一种名为“结构化异常处理”的机制，非常著名的“内存访问违例”出错对话框就是该机制的一种体现。Windows 结构化异常处理与前文讨论的 C++ 异常处理机制有惊人的相似之处，同样使用类似的链式结构实现。对于 Windows 下的应用程序，只需使用 SetUnhandledExceptionFilter API 注册异常处理器；用 FS:[0] 替代前文所述的 TLS: Current ExpHdl 等很少的改动，即可将此两种错误处理机制合而为一。这样做的优势十分明显： 由于可直接借助操作系统提供的机制，所以简化了 C++ 异常处理器的实现。 使“catch (…)” 块得以捕获操作系统产生的异常（如：“内存访问违例”等等）。 使操作系统的异常处理机制能够捕获所有 C++ 异常。 实际上，大多数 Windows 下的 C++ 编译器的异常机制均使用这种方式实现。 异常处理机制的开销分析 至此，我们已完整地阐述了整套 C++ 异常处理机制的实现原理。我在本文的开头曾提到，作为一名 C++ 程序员，了解其某一特性的实现原理主要是为了避免错误地使用该特性。要达到这个目的，还要在了解实现原理的基础上进行一些额外的开销分析工作： 特性 时间开销 空间开销 EHDL 无运行时开销 每“C++函数”一个 EHDL 对象，其中的 tblTryBlocks[] 成员仅在函数中包含至少一个 try 块时使用。典型情况下小于 64 字节。 C++栈框架 极高的 O(1) 效率，每次调用时进行3次额外的整形赋值和一次 TLS 访问。 每 调用两个指针和一个整形开销。典型情况下小于 16 字节。 step 跟踪 极高的 O(1) 效率每次进出 try 块或对象构造/析构一次整形立即数赋值。 无（已记入 C++ 栈框架中的相应项目）。 异常的抛出、捕获和栈回退 异常的抛出是一次 O(1) 级操作。在单个函数中进行捕获和栈回退也均为 O(1) 操作。但异常捕获的总体成本为 O(m)，其中 m 等于当前函数调用栈中，从抛出异常的位置到达匹配 catch 块之间所经过的函数调用中，包含 try 块（即：定义了有效 tblTryBlocks[]）的函数个数。栈回退的总成本为 O(n)，其中 n 等于当前函数调用栈中，从抛出异常的位置到达匹配 catch 块之间所经过的函数调用数。 在异常处理结束前，需保存异常对象及其析构函数指针和相应的 type_info 信息。具体根据对象尺寸、编译器选项（是否开启 RTTI）及异常捕获器的参数传递方式（传值或传址）等因素有较大变化。典型情况下小于 256 字节。 可以看出，在没有抛出异常时，C++ 的异常处理机制是十分有效的。在有异常被抛出后，可能会依当前函数调用栈的情形进行若干次整形比较（try块表匹配）操作，但这通常不会超过几十次。对于大多数 15 年前的 CPU 来说，整形比较也只需 1 时钟周期，所以异常捕获的效率还是很高的。栈回退的效率则与 return 语句基本相当。 考虑到即使是传统的函数调用、错误处理和逐级返回机制也不是没有代价的。这些开销在绝大多数情形下仍可以接受。空间开销方面，每“C++ 函数”一个 EHDL 结构体的引入在某些极端情形下会明显增加目标文件尺寸和内存开销。但是典型情况下，它们的影响并不大，但也没有小到可以完全忽略的程度。如果正在为一个资源严格受限的环境开发应用程序，你可能需要考虑关闭异常处理和 RTTI 机制以节约存储空间。 以上讨论的是一种典型的异常机制的实现方式，各具体编译器厂商可能有自己的优化和改进方案，但总体的出入不会很大。 小节 异常处理是 C++ 中十分有用的崭新特性之一。在绝大多数情况下，它们都有着优异的表现和令人满意的时空效率。异常处理本质上是另一种返回机制。但无论从软件工程、模块设计、编码习惯还是时空效率等角度来说，除了在有充分文档说明的前提下，偶尔可用来替代替代传统的 setjmp/longjmp 功能外，应保证只将其用于程序的错误处理机制中。 此外，由于长跳转的使用既易于出错，又难于理解和维护。在编码过程中也应当尽量避免使用。关于异常的一般性使用说明，请参考：代码风格与版式：异常。 本文地址：http://xnerv.wang/analysis-about-cpp-exception-implementation/ 转载自：C++异常机制的实现方式和开销分析","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://xnerv.wang/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"编程语言","slug":"编程语言","permalink":"https://xnerv.wang/tags/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"},{"name":"C++","slug":"C","permalink":"https://xnerv.wang/tags/C/"},{"name":"C++异常","slug":"C-异常","permalink":"https://xnerv.wang/tags/C-%E5%BC%82%E5%B8%B8/"}]},{"title":"理解 Memory barrier（内存屏障）（转载）","slug":"understand-memory-barrier","date":"2017-10-27T04:51:00.000Z","updated":"2023-08-21T02:24:21.131Z","comments":true,"path":"understand-memory-barrier/","link":"","permalink":"https://xnerv.wang/understand-memory-barrier/","excerpt":"本文例子均在 Linux（g++）下验证通过，CPU 为 X86-64 处理器架构。所有罗列的 Linux 内核代码也均在（或只在）X86-64 下有效。 本文首先通过范例（以及内核代码）来解释 Memory barrier，然后介绍一个利用 Memory barrier 实现的无锁环形缓冲区。","text":"本文例子均在 Linux（g++）下验证通过，CPU 为 X86-64 处理器架构。所有罗列的 Linux 内核代码也均在（或只在）X86-64 下有效。 本文首先通过范例（以及内核代码）来解释 Memory barrier，然后介绍一个利用 Memory barrier 实现的无锁环形缓冲区。 Memory barrier 简介 程序在运行时内存实际的访问顺序和程序代码编写的访问顺序不一定一致，这就是内存乱序访问。内存乱序访问行为出现的理由是为了提升程序运行时的性能。内存乱序访问主要发生在两个阶段： 编译时，编译器优化导致内存乱序访问（指令重排） 运行时，多 CPU 间交互引起内存乱序访问 Memory barrier 能够让 CPU 或编译器在内存访问上有序。一个 Memory barrier 之前的内存访问操作必定先于其之后的完成。Memory barrier 包括两类： 编译器 barrier CPU Memory barrier 很多时候，编译器和 CPU 引起内存乱序访问不会带来什么问题，但一些特殊情况下，程序逻辑的正确性依赖于内存访问顺序，这时候内存乱序访问会带来逻辑上的错误，例如： 12345678// thread 1while (!ok);do(x);// thread 2x = 42;ok = 1;ccc 此段代码中，ok 初始化为 0，线程 1 等待 ok 被设置为 1 后执行 do 函数。假如说，线程 2 对内存的写操作乱序执行，也就是 x 赋值后于 ok 赋值完成，那么 do 函数接受的实参就很可能出乎程序员的意料，不为 42。 编译时内存乱序访问 在编译时，编译器对代码做出优化时可能改变实际执行指令的顺序（例如 gcc 下 O2 或 O3 都会改变实际执行指令的顺序）： 1234567// test.cppint x, y, r;void f()&#123; x = r; y = 1;&#125; 编译器优化的结果可能导致 y = 1 在 x = r 之前执行完成。首先直接编译此源文件： 1g++ -S test.cpp 得到相关的汇编代码如下： 123movl r(%rip), %eaxmovl %eax, x(%rip)movl $1, y(%rip) 这里我们看到，x = r 和 y = 1 并没有乱序。现使用优化选项 O2（或 O3）编译上面的代码（g++ -O2 -S test.cpp），生成汇编代码如下： 123movl r(%rip), %eaxmovl $1, y(%rip)movl %eax, x(%rip) 我们可以清楚的看到经过编译器优化之后 movl $1, y(%rip) 先于 movl %eax, x(%rip) 执行。避免编译时内存乱序访问的办法就是使用编译器 barrier（又叫优化 barrier）。Linux 内核提供函数 barrier() 用于让编译器保证其之前的内存访问先于其之后的完成。内核实现 barrier() 如下（X86-64 架构）： 1#define barrier() __asm__ __volatile__(&quot;&quot; ::: &quot;memory&quot;) 现在把此编译器 barrier 加入代码中： 1234567int x, y, r;void f()&#123; x = r; __asm__ __volatile__(&quot;&quot; ::: &quot;memory&quot;); y = 1;&#125; 这样就避免了编译器优化带来的内存乱序访问的问题了（如果有兴趣可以再看看编译之后的汇编代码）。本例中，我们还可以使用 volatile 这个关键字来避免编译时内存乱序访问（而无法避免后面要说的运行时内存乱序访问）。volatile 关键字能够让相关的变量之间在内存访问上避免乱序，这里可以修改 x 和 y 的定义来解决问题： 1234567volatile int x, y;int r;void f()&#123; x = r; y = 1;&#125; 现加上了 volatile 关键字，这使得 x 相对于 y、y 相对于 x 在内存访问上有序。在 Linux 内核中，提供了一个宏 ACCESS_ONCE 来避免编译器对于连续的 ACCESS_ONCE 实例进行指令重排。其实 ACCESS_ONCE 实现源码如下： 1#define ACCESS_ONCE(x) (*(volatile typeof(x) *)&amp;(x)) 此代码只是将变量 x 转换为 volatile 的而已。现在我们就有了第三个修改方案： 123456int x, y, r;void f()&#123; ACCESS_ONCE(x) = r; ACCESS_ONCE(y) = 1;&#125; 到此基本上就阐述完了我们的编译时内存乱序访问的问题。下面开始介绍运行时内存乱序访问。 运行时内存乱序访问 在运行时，CPU 虽然会乱序执行指令，但是在单个 CPU 的上，硬件能够保证程序执行时所有的内存访问操作看起来像是按程序代码编写的顺序执行的，这时候 Memory barrier 没有必要使用（不考虑编译器优化的情况下）。这里我们了解一下 CPU 乱序执行的行为。在乱序执行时，一个处理器真正执行指令的顺序由可用的输入数据决定，而非程序员编写的顺序。 早期的处理器为有序处理器（In-order processors），有序处理器处理指令通常有以下几步： 指令获取 如果指令的输入操作对象（input operands）可用（例如已经在寄存器中了），则将此指令分发到适当的功能单元中。如果一个或者多个操作对象不可用（通常是由于需要从内存中获取），则处理器会等待直到它们可用 指令被适当的功能单元执行 功能单元将结果写回寄存器堆（Register file，一个 CPU 中的一组寄存器） 相比之下，乱序处理器（Out-of-order processors）处理指令通常有以下几步： 指令获取 指令被分发到指令队列 指令在指令队列中等待，直到输入操作对象可用（一旦输入操作对象可用，指令就可以离开队列，即便更早的指令未被执行） 指令被分配到适当的功能单元并执行 执行结果被放入队列（而不立即写入寄存器堆） 只有所有更早请求执行的指令的执行结果被写入寄存器堆后，指令执行的结果才被写入寄存器堆（执行结果重排序，让执行看起来是有序的） 从上面的执行过程可以看出，乱序执行相比有序执行能够避免等待不可用的操作对象（有序执行的第二步）从而提高了效率。现代的机器上，处理器运行的速度比内存快很多，有序处理器花在等待可用数据的时间里已经可以处理大量指令了。 现在思考一下乱序处理器处理指令的过程，我们能得到几个结论： 对于单个 CPU 指令获取是有序的（通过队列实现） 对于单个 CPU 指令执行结果也是有序返回寄存器堆的（通过队列实现） 由此可知，在单 CPU 上，不考虑编译器优化导致乱序的前提下，多线程执行不存在内存乱序访问的问题。我们从内核源码也可以得到类似的结论（代码不完全的摘录）： 12345#ifdef CONFIG_SMP#define smp_mb() mb()#else#define smp_mb() barrier()#endif 这里可以看到，如果是 SMP 则使用 mb，mb 被定义为 CPU Memory barrier（后面会讲到），而非 SMP 时，直接使用编译器 barrier。 在多 CPU 的机器上，问题又不一样了。每个 CPU 都存在 cache（cache 主要是为了弥补 CPU 和内存之间较慢的访问速度），当一个特定数据第一次被特定一个 CPU 获取时，此数据显然不在 CPU 的 cache 中（这就是 cache miss）。此 cache miss 意味着 CPU 需要从内存中获取数据（这个过程需要 CPU 等待数百个周期），此数据将被加载到 CPU 的 cache 中，这样后续就能直接从 cache 上快速访问。当某个 CPU 进行写操作时，它必须确保其他的 CPU 已经将此数据从它们的 cache 中移除（以便保证一致性），只有在移除操作完成后此 CPU 才能安全的修改数据。显然，存在多个 cache 时，我们必须通过一个 cache 一致性协议来避免数据不一致的问题，而这个通讯的过程就可能导致乱序访问的出现，也就是这里说的运行时内存乱序访问。这里不再深入讨论整个细节，这是一个比较复杂的问题，有兴趣可以研究http://www.rdrop.com/users/paulmck/scalability/paper/whymb.2010.06.07c.pdf一文，其详细的分析了整个过程。 现在通过一个例子来说明多 CPU 下内存乱序访问： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485// test2.cpp#include &lt;pthread.h&gt;#include &lt;assert.h&gt;// -------------------int cpu_thread1 = 0;int cpu_thread2 = 1;volatile int x, y, r1, r2;void start()&#123; x = y = r1 = r2 = 0;&#125;void end()&#123; assert(!(r1 == 0 &amp;&amp; r2 == 0));&#125;void run1()&#123; x = 1; r1 = y;&#125;void run2()&#123; y = 1; r2 = x;&#125;// -------------------static pthread_barrier_t barrier_start;static pthread_barrier_t barrier_end;static void* thread1(void*)&#123; while (1) &#123; pthread_barrier_wait(&amp;barrier_start); run1(); pthread_barrier_wait(&amp;barrier_end); &#125; return NULL;&#125;static void* thread2(void*)&#123; while (1) &#123; pthread_barrier_wait(&amp;barrier_start); run2(); pthread_barrier_wait(&amp;barrier_end); &#125; return NULL;&#125;int main()&#123; assert(pthread_barrier_init(&amp;barrier_start, NULL, 3) == 0); assert(pthread_barrier_init(&amp;barrier_end, NULL, 3) == 0); pthread_t t1; pthread_t t2; assert(pthread_create(&amp;t1, NULL, thread1, NULL) == 0); assert(pthread_create(&amp;t2, NULL, thread2, NULL) == 0); cpu_set_t cs; CPU_ZERO(&amp;cs); CPU_SET(cpu_thread1, &amp;cs); assert(pthread_setaffinity_np(t1, sizeof(cs), &amp;cs) == 0); CPU_ZERO(&amp;cs); CPU_SET(cpu_thread2, &amp;cs); assert(pthread_setaffinity_np(t2, sizeof(cs), &amp;cs) == 0); while (1) &#123; start(); pthread_barrier_wait(&amp;barrier_start); pthread_barrier_wait(&amp;barrier_end); end(); &#125; return 0;&#125; 这里创建了两个线程来运行测试代码（需要测试的代码将放置在 run 函数中）。我使用了 pthread barrier（区别于本文讨论的 Memory barrier）主要为了让两个子线程能够同时运行它们的 run 函数。此段代码不停的尝试同时运行两个线程的 run 函数，以便得出我们期望的结果。在每次运行 run 函数前会调用一次 start 函数（进行数据初始化），run 运行后会调用一次 end 函数（进行结果检查）。run1 和 run2 两个函数运行在哪个 CPU 上则通过 cpu_thread1 和 cpu_thread2 两个变量控制。 先编译此程序：g++ -lpthread -o test2 test2.cpp（这里未优化，目的是为了避免编译器优化的干扰）。需要注意的是，两个线程运行在两个不同的 CPU 上（CPU 0 和 CPU 1）。只要内存不出现乱序访问，那么 r1 和 r2 不可能同时为 0，因此断言失败表示存在内存乱序访问。编译之后运行此程序，会发现存在一定概率导致断言失败。为了进一步说明问题，我们把 cpu_thread2 的值改为 0，换而言之就是让两个线程跑在同一个 CPU 下，再运行程序发现断言不再失败。 最后，我们使用 CPU Memory barrier 来解决内存乱序访问的问题（X86-64 架构下）： 12345678910111213141516int cpu_thread1 = 0;int cpu_thread2 = 1;void run1()&#123; x = 1; __asm__ __volatile__(&quot;mfence&quot; ::: &quot;memory&quot;); r1 = y;&#125;void run2()&#123; y = 1; __asm__ __volatile__(&quot;mfence&quot; ::: &quot;memory&quot;); r2 = x;&#125; 准备使用 Memory barrier Memory barrier 常用场合包括： 实现同步原语（synchronization primitives） 实现无锁数据结构（lock-free data structures） 驱动程序 实际的应用程序开发中，开发者可能完全不知道 Memory barrier 就可以开发正确的多线程程序，这主要是因为各种同步机制中已经隐含了 Memory barrier（但和实际的 Memory barrier 有细微差别），这就使得不直接使用 Memory barrier 不会存在任何问题。但是如果你希望编写诸如无锁数据结构，那么 Memory barrier 还是很有用的。 通常来说，在单个 CPU 上，存在依赖的内存访问有序： 12Q &#x3D; P;D &#x3D; *Q; 这里内存操作有序。然而在 Alpha CPU 上，存在依赖的内存读取操作不一定有序，需要使用数据依赖 barrier（由于 Alpha 不常见，这里就不详细解释了）。 在 Linux 内核中，除了前面说到的编译器 barrier — barrier() 和 ACCESS_ONCE()，还有 CPU Memory barrier： 通用 barrier，保证读写操作有序的，mb() 和 smp_mb() 写操作 barrier，仅保证写操作有序的，wmb() 和 smp_wmb() 读操作 barrier，仅保证读操作有序的，rmb() 和 smp_rmb() 注意，所有的 CPU Memory barrier（除了数据依赖 barrier 之外）都隐含了编译器 barrier。这里的 smp 开头的 Memory barrier 会根据配置在单处理器上直接使用编译器 barrier，而在 SMP 上才使用 CPU Memory barrier（也就是 mb()、wmb()、rmb()，回忆上面相关内核代码）。 最后需要注意一点的是，CPU Memory barrier 中某些类型的 Memory barrier 需要成对使用，否则会出错，详细来说就是：一个写操作 barrier 需要和读操作（或数据依赖）barrier 一起使用（当然，通用 barrier 也是可以的），反之依然。 Memory barrier 的范例 读内核代码进一步学习 Memory barrier 的使用。 Linux 内核实现的无锁（只有一个读线程和一个写线程时）环形缓冲区 kfifo 就使用到了 Memory barrier，实现源码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197/* * A simple kernel FIFO implementation. * * Copyright (C) 2004 Stelian Pop &lt;stelian@popies.net&gt; * * This program is free software; you can redistribute it and/or modify * it under the terms of the GNU General Public License as published by * the Free Software Foundation; either version 2 of the License, or * (at your option) any later version. * * This program is distributed in the hope that it will be useful, * but WITHOUT ANY WARRANTY; without even the implied warranty of * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the * GNU General Public License for more details. * * You should have received a copy of the GNU General Public License * along with this program; if not, write to the Free Software * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA. * */#include &lt;linux/kernel.h&gt;#include &lt;linux/module.h&gt;#include &lt;linux/slab.h&gt;#include &lt;linux/err.h&gt;#include &lt;linux/kfifo.h&gt;#include &lt;linux/log2.h&gt;/** * kfifo_init - allocates a new FIFO using a preallocated buffer * @buffer: the preallocated buffer to be used. * @size: the size of the internal buffer, this have to be a power of 2. * @gfp_mask: get_free_pages mask, passed to kmalloc() * @lock: the lock to be used to protect the fifo buffer * * Do NOT pass the kfifo to kfifo_free() after use! Simply free the * &amp;struct kfifo with kfree(). */struct kfifo *kfifo_init(unsigned char *buffer, unsigned int size, gfp_t gfp_mask, spinlock_t *lock)&#123; struct kfifo *fifo; /* size must be a power of 2 */ BUG_ON(!is_power_of_2(size)); fifo = kmalloc(sizeof(struct kfifo), gfp_mask); if (!fifo) return ERR_PTR(-ENOMEM); fifo-&gt;buffer = buffer; fifo-&gt;size = size; fifo-&gt;in = fifo-&gt;out = 0; fifo-&gt;lock = lock; return fifo;&#125;EXPORT_SYMBOL(kfifo_init);/** * kfifo_alloc - allocates a new FIFO and its internal buffer * @size: the size of the internal buffer to be allocated. * @gfp_mask: get_free_pages mask, passed to kmalloc() * @lock: the lock to be used to protect the fifo buffer * * The size will be rounded-up to a power of 2. */struct kfifo *kfifo_alloc(unsigned int size, gfp_t gfp_mask, spinlock_t *lock)&#123; unsigned char *buffer; struct kfifo *ret; /* * round up to the next power of 2, since our &#x27;let the indices * wrap&#x27; technique works only in this case. */ if (!is_power_of_2(size)) &#123; BUG_ON(size &gt; 0x80000000); size = roundup_pow_of_two(size); &#125; buffer = kmalloc(size, gfp_mask); if (!buffer) return ERR_PTR(-ENOMEM); ret = kfifo_init(buffer, size, gfp_mask, lock); if (IS_ERR(ret)) kfree(buffer); return ret;&#125;EXPORT_SYMBOL(kfifo_alloc);/** * kfifo_free - frees the FIFO * @fifo: the fifo to be freed. */void kfifo_free(struct kfifo *fifo)&#123; kfree(fifo-&gt;buffer); kfree(fifo);&#125;EXPORT_SYMBOL(kfifo_free);/** * __kfifo_put - puts some data into the FIFO, no locking version * @fifo: the fifo to be used. * @buffer: the data to be added. * @len: the length of the data to be added. * * This function copies at most @len bytes from the @buffer into * the FIFO depending on the free space, and returns the number of * bytes copied. * * Note that with only one concurrent reader and one concurrent * writer, you don&#x27;t need extra locking to use these functions. */unsigned int __kfifo_put(struct kfifo *fifo, const unsigned char *buffer, unsigned int len)&#123; unsigned int l; len = min(len, fifo-&gt;size - fifo-&gt;in + fifo-&gt;out); /* * Ensure that we sample the fifo-&gt;out index -before- we * start putting bytes into the kfifo. */ smp_mb(); /* first put the data starting from fifo-&gt;in to buffer end */ l = min(len, fifo-&gt;size - (fifo-&gt;in &amp; (fifo-&gt;size - 1))); memcpy(fifo-&gt;buffer + (fifo-&gt;in &amp; (fifo-&gt;size - 1)), buffer, l); /* then put the rest (if any) at the beginning of the buffer */ memcpy(fifo-&gt;buffer, buffer + l, len - l); /* * Ensure that we add the bytes to the kfifo -before- * we update the fifo-&gt;in index. */ smp_wmb(); fifo-&gt;in += len; return len;&#125;EXPORT_SYMBOL(__kfifo_put);/** * __kfifo_get - gets some data from the FIFO, no locking version * @fifo: the fifo to be used. * @buffer: where the data must be copied. * @len: the size of the destination buffer. * * This function copies at most @len bytes from the FIFO into the * @buffer and returns the number of copied bytes. * * Note that with only one concurrent reader and one concurrent * writer, you don&#x27;t need extra locking to use these functions. */unsigned int __kfifo_get(struct kfifo *fifo, unsigned char *buffer, unsigned int len)&#123; unsigned int l; len = min(len, fifo-&gt;in - fifo-&gt;out); /* * Ensure that we sample the fifo-&gt;in index -before- we * start removing bytes from the kfifo. */ smp_rmb(); /* first get the data from fifo-&gt;out until the end of the buffer */ l = min(len, fifo-&gt;size - (fifo-&gt;out &amp; (fifo-&gt;size - 1))); memcpy(buffer, fifo-&gt;buffer + (fifo-&gt;out &amp; (fifo-&gt;size - 1)), l); /* then get the rest (if any) from the beginning of the buffer */ memcpy(buffer + l, fifo-&gt;buffer, len - l); /* * Ensure that we remove the bytes from the kfifo -before- * we update the fifo-&gt;out index. */ smp_mb(); fifo-&gt;out += len; return len;&#125;EXPORT_SYMBOL(__kfifo_get); 为了更好的理解上面的源码，这里顺带说一下此实现使用到的一些和本文主题无关的技巧： 使用与操作来求取环形缓冲区的下标，相比取余操作来求取下标的做法效率要高不少。使用与操作求取下标的前提是环形缓冲区的大小必须是 2 的 N 次方，换而言之就是说环形缓冲区的大小为一个仅有一个 1 的二进制数，那么 index &amp; (size – 1) 则为求取的下标（这不难理解） 使用了 in 和 out 两个索引且 in 和 out 是一直递增的（此做法比较巧妙），这样能够避免一些复杂的条件判断（某些实现下，in == out 时还无法区分缓冲区是空还是满） 这里，索引 in 和 out 被两个线程访问。in 和 out 指明了缓冲区中实际数据的边界，也就是 in 和 out 同缓冲区数据存在访问上的顺序关系，由于未使用同步机制，那么保证顺序关系就需要使用到 Memory barrier 了。索引 in 和 out 都分别只被一个线程修改，而被两个线程读取。__kfifo_put 先通过 in 和 out 来确定可以向缓冲区中写入数据量的多少，这时，out 索引应该先被读取后才能真正的将用户 buffer 中的数据写入缓冲区，因此这里使用到了 smp_mb()，对应的，__kfifo_get 也使用 smp_mb() 来确保修改 out 索引之前缓冲区中数据已经被成功读取并写入用户 buffer 中了。对于 in 索引，在 __kfifo_put 中，通过 smp_wmb() 保证先向缓冲区写入数据后才修改 in 索引，由于这里只需要保证写入操作有序，故选用写操作 barrier，在 __kfifo_get 中，通过 smp_rmb() 保证先读取了 in 索引（这时候 in 索引用于确定缓冲区中实际存在多少可读数据）才开始读取缓冲区中数据（并写入用户 buffer 中），由于这里只需要保证读取操作有序，故选用读操作 barrier。 到这里，Memory barrier 就介绍完毕了。 参考文献列表： http://en.wikipedia.org/wiki/Memory_barrier http://en.wikipedia.org/wiki/Out-of-order_execution https://www.kernel.org/doc/Documentation/memory-barriers.txt 本文地址：http://xnerv.wang/understand-memory-barrier/ 转载自：理解 Memory barrier（内存屏障）","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://xnerv.wang/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"操作系统","slug":"操作系统","permalink":"https://xnerv.wang/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"内存屏障","slug":"内存屏障","permalink":"https://xnerv.wang/tags/%E5%86%85%E5%AD%98%E5%B1%8F%E9%9A%9C/"}]},{"title":"(MSDN) Heap Functions","slug":"msdn-heap-functions","date":"2017-10-24T21:19:00.000Z","updated":"2023-08-21T02:24:20.125Z","comments":true,"path":"msdn-heap-functions/","link":"","permalink":"https://xnerv.wang/msdn-heap-functions/","excerpt":"Each process has a default heap provided by the system. Applications that make frequent allocations from the heap can improve performance by using private heaps. A private heap is a block of one or more pages in the address space of the calling process. After creating the private heap, the process uses functions such as HeapAlloc and HeapFree to manage the memory in that heap.","text":"Each process has a default heap provided by the system. Applications that make frequent allocations from the heap can improve performance by using private heaps. A private heap is a block of one or more pages in the address space of the calling process. After creating the private heap, the process uses functions such as HeapAlloc and HeapFree to manage the memory in that heap. The heap functions can also be used to manage memory in the process’s default heap, using the handle returned by the GetProcessHeap function. New applications should use the heap functions instead of the global and local functions for this purpose. There is no difference between memory allocated from a private heap and that allocated by using the other memory allocation functions. For a complete list of functions, see the table in Memory Management Functions. A thread should call heap functions only for the process’s default heap and private heaps that the thread creates and manages, using handles returned by the GetProcessHeap or HeapCreate function. The HeapCreate function creates a private heap object from which the calling process can allocate memory blocks by using the HeapAlloc function. HeapCreate specifies both an initial size and a maximum size for the heap. The initial size determines the number of committed, read/write pages initially allocated for the heap. The maximum size determines the total number of reserved pages. These pages create a contiguous block in the virtual address space of a process into which the heap can grow. Additional pages are automatically committed from this reserved space if requests by HeapAlloc exceed the current size of committed pages, assuming that the physical storage for it is available. Once the pages are committed, they are not decommitted until the process is terminated or until the heap is destroyed by calling the HeapDestroy function. The memory of a private heap object is accessible only to the process that created it. If a dynamic-link library (DLL) creates a private heap, it does so in the address space of the process that called the DLL. It is accessible only to that process. The HeapAlloc function allocates a specified number of bytes from a private heap and returns a pointer to the allocated block. This pointer can be used in the HeapFree, HeapReAlloc, HeapSize, and HeapValidate functions. Memory allocated by HeapAlloc is not movable. The address returned by HeapAlloc is valid until the memory block is freed or reallocated; the memory block does not need to be locked. Because the system cannot compact a private heap, it can become fragmented. Applications that allocate large amounts of memory in various allocation sizes can use the low-fragmentation heap to reduce heap fragmentation. A possible use for the heap functions is to create a private heap when a process starts up, specifying an initial size sufficient to satisfy the memory requirements of the process. If the call to the HeapCreate function fails, the process can terminate or notify the user of the memory shortage; if it succeeds, however, the process is assured of having the memory it needs. Memory requested by HeapCreate may or may not be contiguous. Memory allocated within a heap by HeapAlloc is contiguous. You should not write to or read from memory in a heap except that allocated by HeapAlloc, nor should you assume any relationship between two areas of memory allocated by HeapAlloc. You should not refer in any way to memory that has been freed by HeapFree. After the memory is freed, any information that may have been in it is gone forever. If you require information, do not free memory containing the information. Function calls that return information about memory (such as HeapSize) may not be used with freed memory, as they may return bogus data. The HeapDestroy function destroys a private heap object. It decommits and releases all the pages of the heap object, and it invalidates the handle to the heap. Related topics Comparing Memory Allocation Methods Article link: http://xnerv.wang/msdn-heap-functions/ Reprinted from: (MSDN) Heap Functions","categories":[{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/categories/Windows/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/tags/Windows/"},{"name":"Heap","slug":"Heap","permalink":"https://xnerv.wang/tags/Heap/"},{"name":"MSDN","slug":"MSDN","permalink":"https://xnerv.wang/tags/MSDN/"},{"name":"Memory Management","slug":"Memory-Management","permalink":"https://xnerv.wang/tags/Memory-Management/"}]},{"title":"(MSDN) Comparing Memory Allocation Methods","slug":"comparing-memory-allocation-methods","date":"2017-10-24T06:29:00.000Z","updated":"2023-08-21T02:24:20.082Z","comments":true,"path":"comparing-memory-allocation-methods/","link":"","permalink":"https://xnerv.wang/comparing-memory-allocation-methods/","excerpt":"The following is a brief comparison of the various memory allocation methods: CoTaskMemAlloc GlobalAlloc HeapAlloc LocalAlloc malloc new VirtualAlloc Although the GlobalAlloc, LocalAlloc, and HeapAlloc functions ultimately allocate memory from the same heap, each provides a slightly different set of functionality. For example, HeapAlloc can be instructed to raise an exception if memory could not be allocated, a capability not available with LocalAlloc. LocalAlloc supports allocation of handles which permit the underlying memory to be moved by a reallocation without changing the handle value, a capability not available with HeapAlloc.","text":"The following is a brief comparison of the various memory allocation methods: CoTaskMemAlloc GlobalAlloc HeapAlloc LocalAlloc malloc new VirtualAlloc Although the GlobalAlloc, LocalAlloc, and HeapAlloc functions ultimately allocate memory from the same heap, each provides a slightly different set of functionality. For example, HeapAlloc can be instructed to raise an exception if memory could not be allocated, a capability not available with LocalAlloc. LocalAlloc supports allocation of handles which permit the underlying memory to be moved by a reallocation without changing the handle value, a capability not available with HeapAlloc. Starting with 32-bit Windows, GlobalAlloc and LocalAlloc are implemented as wrapper functions that call HeapAlloc using a handle to the process’s default heap. Therefore, GlobalAlloc and LocalAlloc have greater overhead than HeapAlloc. Because the different heap allocators provide distinctive functionality by using different mechanisms, you must free memory with the correct function. For example, memory allocated with HeapAlloc must be freed with HeapFree and not LocalFree or GlobalFree. Memory allocated with GlobalAlloc or LocalAlloc must be queried, validated, and released with the corresponding global or local function. The VirtualAlloc function allows you to specify additional options for memory allocation. However, its allocations use a page granularity, so using VirtualAlloc can result in higher memory usage. The malloc function has the disadvantage of being run-time dependent. The new operator has the disadvantage of being compiler dependent and language dependent. The CoTaskMemAlloc function has the advantage of working well in either C, C++, or Visual Basic. It is also the only way to share memory in a COM-based application, since MIDL uses CoTaskMemAlloc and CoTaskMemFree to marshal memory. Related topics Global and Local Functions Heap Functions Virtual Memory Functions Article link: http://xnerv.wang/comparing-memory-allocation-methods/ Reprinted from: (MSDN) Comparing Memory Allocation Methods","categories":[{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/categories/Windows/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/tags/Windows/"},{"name":"Heap","slug":"Heap","permalink":"https://xnerv.wang/tags/Heap/"},{"name":"MSDN","slug":"MSDN","permalink":"https://xnerv.wang/tags/MSDN/"}]},{"title":"Allocating and freeing memory across module boundaries（转载）","slug":"allocating-and-freeing-memory-across-module-boundaries","date":"2017-10-24T05:23:00.000Z","updated":"2023-08-21T02:24:20.312Z","comments":true,"path":"allocating-and-freeing-memory-across-module-boundaries/","link":"","permalink":"https://xnerv.wang/allocating-and-freeing-memory-across-module-boundaries/","excerpt":"I’m sure it’s been drilled into your head by now that you have to free memory with the same allocator that allocated it. LocalAlloc matches LocalFree, GlobalAlloc matches GlobalFree, new[] matches delete[]. But this rule goes deeper.","text":"I’m sure it’s been drilled into your head by now that you have to free memory with the same allocator that allocated it. LocalAlloc matches LocalFree, GlobalAlloc matches GlobalFree, new[] matches delete[]. But this rule goes deeper. If you have a function that allocates and returns some data, the caller must know how to free that memory. You have a variety of ways of accomplishing this. One is to state explicitly how the memory should be freed. For example, the FormatMessage documentation explicitly states that you should use the LocalFree function to free the buffer that is allocated if you pass the FORMAT_MESSAGE_ALLOCATE_BUFFER flag. All BSTRs must be freed with SysFreeString. And all memory returned across COM interface boundaries must be allocated and freed with the COM task allocator. Note, however, that if you decide that a block of memory should be freed with the C runtime, such as with free, or with the C++ runtime via delete or delete[], you have a new problem: Which runtime? If you choose to link with the static runtime library, then your module has its own private copy of the C/C++ runtime. When your module calls new or malloc, the memory can only be freed by your module calling delete or free. If another module calls delete or free, that will use the C/C++ runtime of that other module which is not the same as yours. Indeed, even if you choose to link with the DLL version of the C/C++ runtime library, you still have to agree which version of the C/C++ runtime to use. If your DLL uses MSVCRT20.DLL to allocate memory, then anybody who wants to free that memory must also use MSVCRT20.DLL. If you’re paying close attention, you might spot a looming problem. Requiring all your clients to use a particular version of the C/C++ runtime might seem reasonable if you control all of the clients and are willing to recompile all of them each time the compiler changes. But in real life, people often don’t want to take that risk. “If it ain’t broke, don’t fix it.” Switching to a new compiler risks exposing a subtle bug, say, forgetting to declare a variable as volatile or inadvertently relying on temporaries having a particular lifetime. In practice, you may wish to convert only part of your program to a new compiler while leaving old modules alone. (For example, you may want to take advantage of new language features such as templates, which are available only in the new compiler.) But if you do that, then you lose the ability to free memory that was allocated by the old DLL, since that DLL expects you to use MSVCRT20.DLL, whereas the new compiler uses MSVCR71.DLL. The solution to this requires planning ahead. One option is to use a fixed external allocator such as LocalAlloc or CoTaskMemAlloc. These are allocators that are universally available and don’t depend on which version of the compiler you’re using. Another option is to wrap your preferred allocator inside exported functions that manage the allocation. This is the mechanism used by the NetApi family of functions. For example, the NetGroupEnum function allocates memory and returns it through the bufptr parameter. When the caller is finished with the memory, it frees it with the NetApiBufferFree function. In this manner, the memory allocation method is isolated from the caller. Internally, the NetApi functions might be using LocalAlloc or HeapAllocate or possibly even new and free. It doesn’t matter; as long as NetApiBufferFree frees the memory with the same allocator that NetGroupEnum used to allocate the memory in the first place. Although I personally prefer using a fixed external allocator, many people find it more convenient to use the wrapper technique. That way, they can use their favorite allocator throughout their module. Either way works. The point is that when memory leaves your DLL, the code you gave the memory to must know how to free it, even if it’s using a different compiler from the one that was used to build your DLL. 本文地址：http://xnerv.wang/allocating-and-freeing-memory-across-module-boundaries/ 转载自：Allocating and freeing memory across module boundaries","categories":[{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/categories/Windows/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/tags/Windows/"},{"name":"Heap","slug":"Heap","permalink":"https://xnerv.wang/tags/Heap/"},{"name":"DLL","slug":"DLL","permalink":"https://xnerv.wang/tags/DLL/"},{"name":"DLL Hell","slug":"DLL-Hell","permalink":"https://xnerv.wang/tags/DLL-Hell/"}]},{"title":"关于DLL的一些你不会想要知道的知识","slug":"everything-you-never-wanted-to-know-about-dlls-cn","date":"2017-10-23T06:16:00.000Z","updated":"2023-08-21T02:24:20.296Z","comments":true,"path":"everything-you-never-wanted-to-know-about-dlls-cn/","link":"","permalink":"https://xnerv.wang/everything-you-never-wanted-to-know-about-dlls-cn/","excerpt":"英文版本 Everything You Never Wanted To Know About DLLs. 最近因为一些原因，我需要调研动态链接在Windows平台上的实现细节。这篇文章主要是总结我在这个问题上所学到的知识，用于我将来的回顾和参考，但同时我也希望这篇文章对其他人所有帮助，因为我将要总结的这些内容，你可能需要东找西找才能找到。 废话不多说，让我们开始这趟旅程吧：","text":"英文版本 Everything You Never Wanted To Know About DLLs. 最近因为一些原因，我需要调研动态链接在Windows平台上的实现细节。这篇文章主要是总结我在这个问题上所学到的知识，用于我将来的回顾和参考，但同时我也希望这篇文章对其他人所有帮助，因为我将要总结的这些内容，你可能需要东找西找才能找到。 废话不多说，让我们开始这趟旅程吧： 导出和导入 Windows可执行文件加载器（Windows executable loader）负责在运行程序前完成所有动态加载和符号解析工作。链接器会分别计算出每一个可执行镜像（可执行镜像是一个DLL或者一个EXE文件）导出和导入了哪些函数，这个过程是通过检查可执行镜像的.edata段和.idata段来进行的。 关于.edata段和.idata段的详细信息，在PE/COFF specification这篇文档中有详细说明。 .edata段 .edata段记录了可执行镜像导出的符号（是的，EXE也可以导出符号）。主要包括： 导出地址表（export address table）：一个长度为N的数组，保存了导出的函数/变量的地址（相对于可执行镜像起始地址的相关地址）。这张表的索引称之为序号（ordinals）。 导出名称地址表（export name pointer table）：一个长度为M的并行数组（译者记：其实就是两个大小一样的数组，一个保存key，一个保存对应的value），保存的是符号地址到导出名称的映射。这个平行数组是按照导出名称的字典序排序的，从而允许进行对一个给定的导出符号名称进行二分查找。 导出序号表（export ordinal table）：也是一个长度为M的并行数组，保存了序号到对应的导出名称的映射，其中导出名称对应的是导出名称地址表中的键key。 （作为通过名称导入符号这一做法的另一种替代方法，也可以通过指定序号来导入一个符号。通过序号来导入符号的做法在运行时会稍微快一些，因为这种情况下动态链接器（dynamic linker）不需要进行查找（译者记：根据名称在导出名称地址表中找到名称对应的地址）。此外，如果导出符号的DLL并没有给某个导出项分配名称，那么通过序号来导入符号是唯一的可行之路。） 那么.edata段最初是怎样被创建的呢？主要有两种方法： 最常见的一种情况，在编译生成一些目标文件（object files）时会创建.edata段。这些目标文件对应的源码中，定义了一些带__declspec(dllimport)修饰符的函数或者变量。于是编译器就会产生一个包含了这些导出项的.edata段。 另一种比较少见的情况，开发人员会写一个.def文件，指定那些函数需要被导出。将这个.def文件提供给dll tool --output-exp，就能产生一个导出文件（export file）。导出文件是一个仅包含.edata段的目标文件，导出了在.def文件中声明的符号（导出了一些未解析的引用，通常链接器会填写这些引用到一个实际的地址）。程序员在将这些目标文件链接成DLL的时候，必须对这些导出库（export library）进行命名。 对于以上两种情况，链接器在链接时都会从所有的目标（objects）中收集.edata段，用于给整个可执行镜像文件创建一个.edata段。最后一种可能的方式是.edata段可以被链接器自身所创建，不需要将.edata段放入到任何目标文件中： 链接器可以选择在链接时导出目标文件中的所有符号。例如，这是GNU ld的默认行为（也可以通过–export-all-symbols显式地指定这种行为）。在这种情况下，由链接器来产生.edata段。（GNU ld也支持在命令行中指定一个.def文件，然后产生的.edata段就只会导出这个.def文件中声明的符号）。 .idata段 .idata段记录了可执行镜像导入的符号信息。包括： 对于导入符号涉及到的每一个可执行镜像： 可执行镜像的文件名。被动态链接器用于在磁盘上查找该文件。 导入符号查找表（import lookup table）：一个长度为N的数组，每一项要么是一个序号，要么是一个指向导入名称字符串的指针。 导入符号地址表（import address table）：一个长度为N的指针数组。动态链接器会用从导入符号查找表中对应顺序的符号的地址来填写该数组。 .idata段中的条目以如下方式被创建： 最常见的一种情况，这些条目来自目标文件中的导入库（import library）。可以对你希望导出符号的DLL或者我们之前讨论过的.def文件使用dlltool工具来创建导入库。和导出库一样，用户必须在链接这些导入库时指定名称。 或者，有一些链接器（像GNU ld）也可以让你在链接时直接指定DLL文件。对于你需要从这些DLL文件中导入的符号，链接器会自动产生相应的.idata段条目。 注意，跟导出符号不一样，__declspec(dllimport)修饰符并不会导致产生相应的.idata段。 比起第一次出现，导入库有点更复杂了。Windows动态加载器将导入符号（例如，函数Func的地址）的地址填入到导入符号地址表中。然而，当其它目标文件中的汇编代码执行call Func时，它们期待的是用Func来命名那段code的地址。但我们直到运行时的时候才知道这个地址：我们能够静态地得知的事情只有动态链接器会将这个地址存放在哪个地方。我们称这个地方为_imp_Func。 为了处理这一层额外的中间层，导入库导出的函数Func仅仅间接引用了_imp_Func（来获得实际的函数指针），然后执行jmp跳转到它。同一个工程中的所有其它的目标文件现在可以调用call Func，仿佛Func已经在其它目标文件而不是其它DLL中定义过了一样。基于这个原因，动态链接的函数的声明上的__declspec(dllimport)只是可有可无的（尽管实际上如果加上这个修饰符的话，代码的效率会有轻微的提升，我们之后会谈到这一点）。 不幸的是，如果你想要从另一个DLL中导入变量，则并没有类似的技巧。如果我们有一个导入的变量myData，并没有一种方法来定义一个导入库，使得链接到这个导入库的目标文件可以通过执行mov $eax, myData来写入到myData所在的内存位置。取而代之的是，导入库定义了一个符号__imp__myData，这个符号解析到一个可以找到myData链接地址的地方。然后编译器就会保证当你在读写用__declspec(dllimport)定义的变量时，这些读写其实是通过__imp_myData来间接进行的。因为需要在使用的时候再产生不同的代码，因此在导入变量时的__declspec声明是不可省去的。 应用实例 理论都很好，但在实践中看看所有这些的这些部分会对我们很有帮助。 构建DLL 首先，让我们来构建一个简单的DLL，同时导出了函数和变量。为了最大化地进行说明，我们将使用显式地导出库，而不是用declspec(dllexport)来修饰我们的函数，也不是提供一个.def文件给链接器。 先创建一个.def文件，library.def： 1234LIBRARY libraryEXPORTS function_export data_export DATA （DATA关键词和LIBRARY这一行仅仅影响到导入库如何被产生，之后本文会解释这一点。现在请暂时忽略这个。） 然后构建一个导出文件： 1$ dlltool --output-exp library_exports.o -d library.def 产生的目标文件基本上只包含了一个.edata段，导出了符号_data_export和_function_export，名称分别是data_export和function_export： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778$ objdump -xs library_exports.o...There is an export table in .edata at 0x0The Export Tables (interpreted .edata section contents)Export Flags 0Time&#x2F;Date stamp 4e10e5c1Major&#x2F;Minor 0&#x2F;0Name 00000028 library_exports.o.dllOrdinal Base 1Number in: Export Address Table 00000002 [Name Pointer&#x2F;Ordinal] Table 00000002Table Addresses Export Address Table 00000040 Name Pointer Table 00000048 Ordinal Table 00000050Export Address Table -- Ordinal Base 1[Ordinal&#x2F;Name Pointer] Table [ 0] data_export [ 1] function_exportSections:Idx Name Size VMA LMA File off Algn 0 .text 00000000 00000000 00000000 00000000 2**2 ALLOC, LOAD, READONLY, CODE 1 .data 00000000 00000000 00000000 00000000 2**2 ALLOC, LOAD, DATA 2 .bss 00000000 00000000 00000000 00000000 2**2 ALLOC 3 .edata 00000070 00000000 00000000 000000b4 2**2 CONTENTS, ALLOC, LOAD, RELOC, READONLY, DATASYMBOL TABLE:[ 0](sec -2)(fl 0x00)(ty 0)(scl 103) (nx 1) 0x00000000 fakeFile[ 2](sec 4)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000028 name[ 3](sec 4)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000040 afuncs[ 4](sec 4)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000048 anames[ 5](sec 4)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000050 anords[ 6](sec 4)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000054 n1[ 7](sec 4)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000060 n2[ 8](sec 1)(fl 0x00)(ty 0)(scl 3) (nx 1) 0x00000000 .textAUX scnlen 0x0 nreloc 0 nlnno 0[ 10](sec 2)(fl 0x00)(ty 0)(scl 3) (nx 1) 0x00000000 .dataAUX scnlen 0x0 nreloc 0 nlnno 0[ 12](sec 3)(fl 0x00)(ty 0)(scl 3) (nx 1) 0x00000000 .bssAUX scnlen 0x0 nreloc 0 nlnno 0[ 14](sec 4)(fl 0x00)(ty 0)(scl 3) (nx 1) 0x00000000 .edataAUX scnlen 0x70 nreloc 8 nlnno 0[ 16](sec 0)(fl 0x00)(ty 0)(scl 2) (nx 0) 0x00000000 _data_export[ 17](sec 0)(fl 0x00)(ty 0)(scl 2) (nx 0) 0x00000000 _function_exportRELOCATION RECORDS FOR [.edata]:OFFSET TYPE VALUE0000000c rva32 .edata0000001c rva32 .edata00000020 rva32 .edata00000024 rva32 .edata00000040 rva32 _data_export00000044 rva32 _function_export00000048 rva32 .edata0000004c rva32 .edataContents of section .edata: 0000 00000000 c1e5104e 00000000 28000000 .......N....(... 0010 01000000 02000000 02000000 40000000 ............@... 0020 48000000 50000000 6c696272 6172795f H...P...library_ 0030 6578706f 7274732e 6f2e646c 6c000000 exports.o.dll... 0040 00000000 00000000 54000000 60000000 ........T...&#96;... 0050 00000100 64617461 5f657870 6f727400 ....data_export. 0060 66756e63 74696f6e 5f657870 6f727400 function_export. 我们将会用一个简单的DLL实现来提供这些符号，library.c： 12345int data_export &#x3D; 42;int function_export() &#123; return 1337 + data_export;&#125; 打包到一个DLL中： 1$ gcc -shared -o library.dll library.c library_exports.o 这个DLL的导出符号表如下，可见我们已经导出了我们所需的符号信息： 12345678910111213141516171819202122The Export Tables (interpreted .edata section contents)Export Flags 0Time&#x2F;Date stamp 4e10e5c1Major&#x2F;Minor 0&#x2F;0Name 00005028 library_exports.o.dllOrdinal Base 1Number in: Export Address Table 00000002 [Name Pointer&#x2F;Ordinal] Table 00000002Table Addresses Export Address Table 00005040 Name Pointer Table 00005048 Ordinal Table 00005050Export Address Table -- Ordinal Base 1 [ 0] +base[ 1] 200c Export RVA [ 1] +base[ 2] 10f0 Export RVA[Ordinal&#x2F;Name Pointer] Table [ 0] data_export [ 1] function_export 使用DLL 当我们回过头来看看如何使用DLL时，事情变得更有有趣了。首先，我们需要一个导出库： 1$ dlltool --output-lib library.dll.a -d library.def （我们使用导入库library.dll.a而不是直接使用导出符号的对象文件library_exports.o，是因为使用库来导入允许链接器忽略.idata段中并没有被使用到的符号。而相反的是链接器无法忽略.edata段中的任何符号，因为任何一个符号都可能被这个DLL的使用者用到）。 导入库是相当复杂的。对于每一个导入符号，导入库中都包含一个对应的目标文件（disds00000.o和disds00001.o），同时也包含了其它两个目标文件（distdt.o和disdh.o），用于设立导入列表的头部和尾部。（导入列表的头部除了其它的一些东西，还包含了在运行时需要链接的DLL的名字，这是从.def文件的LIBRARY一行派生而来的。） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214$ objdump -xs library.dll.aIn archive library.dll.a:disdt.o: file format pe-i386...Sections:Idx Name Size VMA LMA File off Algn 0 .text 00000000 00000000 00000000 00000000 2**2 ALLOC, LOAD, READONLY, CODE 1 .data 00000000 00000000 00000000 00000000 2**2 ALLOC, LOAD, DATA 2 .bss 00000000 00000000 00000000 00000000 2**2 ALLOC 3 .idata$4 00000004 00000000 00000000 00000104 2**2 CONTENTS, ALLOC, LOAD, DATA 4 .idata$5 00000004 00000000 00000000 00000108 2**2 CONTENTS, ALLOC, LOAD, DATA 5 .idata$7 0000000c 00000000 00000000 0000010c 2**2 CONTENTS, ALLOC, LOAD, DATASYMBOL TABLE:[ 0](sec -2)(fl 0x00)(ty 0)(scl 103) (nx 1) 0x00000000 fakeFile[ 2](sec 1)(fl 0x00)(ty 0)(scl 3) (nx 1) 0x00000000 .textAUX scnlen 0x0 nreloc 0 nlnno 0[ 4](sec 2)(fl 0x00)(ty 0)(scl 3) (nx 1) 0x00000000 .dataAUX scnlen 0x0 nreloc 0 nlnno 0[ 6](sec 3)(fl 0x00)(ty 0)(scl 3) (nx 1) 0x00000000 .bssAUX scnlen 0x0 nreloc 0 nlnno 0[ 8](sec 4)(fl 0x00)(ty 0)(scl 3) (nx 1) 0x00000000 .idata$4AUX scnlen 0x4 nreloc 0 nlnno 0[ 10](sec 5)(fl 0x00)(ty 0)(scl 3) (nx 1) 0x00000000 .idata$5AUX scnlen 0x4 nreloc 0 nlnno 0[ 12](sec 6)(fl 0x00)(ty 0)(scl 3) (nx 1) 0x00000000 .idata$7AUX scnlen 0x7 nreloc 0 nlnno 0[ 14](sec 6)(fl 0x00)(ty 0)(scl 2) (nx 0) 0x00000000 __library_dll_a_inameContents of section .idata$4: 0000 00000000 ....Contents of section .idata$5: 0000 00000000 ....Contents of section .idata$7: 0000 6c696272 6172792e 646c6c00 library.dll.disdh.o: file format pe-i386...Sections:Idx Name Size VMA LMA File off Algn 0 .text 00000000 00000000 00000000 00000000 2**2 ALLOC, LOAD, READONLY, CODE 1 .data 00000000 00000000 00000000 00000000 2**2 ALLOC, LOAD, DATA 2 .bss 00000000 00000000 00000000 00000000 2**2 ALLOC 3 .idata$2 00000014 00000000 00000000 00000104 2**2 CONTENTS, ALLOC, LOAD, RELOC, DATA 4 .idata$5 00000000 00000000 00000000 00000000 2**2 ALLOC, LOAD, DATA 5 .idata$4 00000000 00000000 00000000 00000000 2**2 ALLOC, LOAD, DATASYMBOL TABLE:[ 0](sec -2)(fl 0x00)(ty 0)(scl 103) (nx 1) 0x00000000 fakeFile[ 2](sec 6)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 hname[ 3](sec 5)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 fthunk[ 4](sec 1)(fl 0x00)(ty 0)(scl 3) (nx 1) 0x00000000 .textAUX scnlen 0x0 nreloc 0 nlnno 0[ 6](sec 2)(fl 0x00)(ty 0)(scl 3) (nx 1) 0x00000000 .dataAUX scnlen 0x0 nreloc 0 nlnno 0[ 8](sec 3)(fl 0x00)(ty 0)(scl 3) (nx 1) 0x00000000 .bssAUX scnlen 0x0 nreloc 0 nlnno 0[ 10](sec 4)(fl 0x00)(ty 0)(scl 3) (nx 1) 0x00000000 .idata$2AUX scnlen 0x14 nreloc 3 nlnno 0[ 12](sec 6)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 .idata$4[ 13](sec 5)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 .idata$5[ 14](sec 4)(fl 0x00)(ty 0)(scl 2) (nx 0) 0x00000000 __head_library_dll_a[ 15](sec 0)(fl 0x00)(ty 0)(scl 2) (nx 0) 0x00000000 __library_dll_a_inameRELOCATION RECORDS FOR [.idata$2]:OFFSET TYPE VALUE00000000 rva32 .idata$40000000c rva32 __library_dll_a_iname00000010 rva32 .idata$5Contents of section .idata$2: 0000 00000000 00000000 00000000 00000000 ................ 0010 00000000 ....disds00001.o: file format pe-i386...Sections:Idx Name Size VMA LMA File off Algn 0 .text 00000008 00000000 00000000 0000012c 2**2 CONTENTS, ALLOC, LOAD, RELOC, READONLY, CODE 1 .data 00000000 00000000 00000000 00000000 2**2 ALLOC, LOAD, DATA 2 .bss 00000000 00000000 00000000 00000000 2**2 ALLOC 3 .idata$7 00000004 00000000 00000000 00000134 2**2 CONTENTS, RELOC 4 .idata$5 00000004 00000000 00000000 00000138 2**2 CONTENTS, RELOC 5 .idata$4 00000004 00000000 00000000 0000013c 2**2 CONTENTS, RELOC 6 .idata$6 00000012 00000000 00000000 00000140 2**1 CONTENTSSYMBOL TABLE:[ 0](sec 1)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 .text[ 1](sec 2)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 .data[ 2](sec 3)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 .bss[ 3](sec 4)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 .idata$7[ 4](sec 5)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 .idata$5[ 5](sec 6)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 .idata$4[ 6](sec 7)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 .idata$6[ 7](sec 1)(fl 0x00)(ty 0)(scl 2) (nx 0) 0x00000000 _function_export[ 8](sec 5)(fl 0x00)(ty 0)(scl 2) (nx 0) 0x00000000 __imp__function_export[ 9](sec 0)(fl 0x00)(ty 0)(scl 2) (nx 0) 0x00000000 __head_library_dll_aRELOCATION RECORDS FOR [.text]:OFFSET TYPE VALUE00000002 dir32 .idata$5RELOCATION RECORDS FOR [.idata$7]:OFFSET TYPE VALUE00000000 rva32 __head_library_dll_aRELOCATION RECORDS FOR [.idata$5]:OFFSET TYPE VALUE00000000 rva32 .idata$6RELOCATION RECORDS FOR [.idata$4]:OFFSET TYPE VALUE00000000 rva32 .idata$6Contents of section .text: 0000 ff250000 00009090 .%......Contents of section .idata$7: 0000 00000000 ....Contents of section .idata$5: 0000 00000000 ....Contents of section .idata$4: 0000 00000000 ....Contents of section .idata$6: 0000 01006675 6e637469 6f6e5f65 78706f72 ..function_expor 0010 7400 t.disds00000.o: file format pe-i386...Sections:Idx Name Size VMA LMA File off Algn 0 .text 00000000 00000000 00000000 00000000 2**2 ALLOC, LOAD, READONLY, CODE 1 .data 00000000 00000000 00000000 00000000 2**2 ALLOC, LOAD, DATA 2 .bss 00000000 00000000 00000000 00000000 2**2 ALLOC 3 .idata$7 00000004 00000000 00000000 0000012c 2**2 CONTENTS, RELOC 4 .idata$5 00000004 00000000 00000000 00000130 2**2 CONTENTS, RELOC 5 .idata$4 00000004 00000000 00000000 00000134 2**2 CONTENTS, RELOC 6 .idata$6 0000000e 00000000 00000000 00000138 2**1 CONTENTSSYMBOL TABLE:[ 0](sec 1)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 .text[ 1](sec 2)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 .data[ 2](sec 3)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 .bss[ 3](sec 4)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 .idata$7[ 4](sec 5)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 .idata$5[ 5](sec 6)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 .idata$4[ 6](sec 7)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 .idata$6[ 7](sec 5)(fl 0x00)(ty 0)(scl 2) (nx 0) 0x00000000 __imp__data_export[ 8](sec 0)(fl 0x00)(ty 0)(scl 2) (nx 0) 0x00000000 __head_library_dll_aRELOCATION RECORDS FOR [.idata$7]:OFFSET TYPE VALUE00000000 rva32 __head_library_dll_aRELOCATION RECORDS FOR [.idata$5]:OFFSET TYPE VALUE00000000 rva32 .idata$6RELOCATION RECORDS FOR [.idata$4]:OFFSET TYPE VALUE00000000 rva32 .idata$6Contents of section .idata$7: 0000 00000000 ....Contents of section .idata$5: 0000 00000000 ....Contents of section .idata$4: 0000 00000000 ....Contents of section .idata$6: 0000 00006461 74615f65 78706f72 7400 ..data_export. 注意data_export对应的目标包含一个空的.text段，然而function_export却有定义一些代码。如果我们反汇编就会看到： 1234500000000 &lt;_function_export&gt;: 0: ff 25 00 00 00 00 jmp *0x0 2: dir32 .idata$5 6: 90 nop 7: 90 nop 类型dir32的重定位告诉链接器如何填写被jmp间接引用的地址。我们可以看到当进入_function_export时，会直接跳到从名为.idata$5的内存处读取的地址。通过彻底地检查.idata段，可以发现.idata$5对应的是导入地址表中function_export这个导入名称所对应的地址，于是就能找到加载的导入项function_export的绝对地址。 虽然只有function_export拥有一个对应的_function_export函数，但是以上的两个导入项在导入库中分别对应了一个符号，这个符号的名称带有__imp__前缀（__imp__data_export和__imp__function_export)。就像我们之前探讨过的那样，这个符号代表了一个内存地址，这个地址中存放的是的指向函数或变量的指针，这个指针值由动态链接器负责填写。 通过一个导入库，我们就可以写一个使用这些导出函数的代码，例如这个main1.c： 12345678910111213141516#include &lt;stdio.h&gt;__declspec(dllimport) extern int function_export(void);__declspec(dllimport) extern int data_export;int main(int argc, char **argv) &#123; printf(&quot;%d\\n&quot;, function_export()); printf(&quot;%d\\n&quot;, data_export); data_export++; printf(&quot;%d\\n&quot;, function_export()); printf(&quot;%d\\n&quot;, data_export); return 0;&#125; 编译这段代码并连接导入库，我们就会得到我们期待的结果： 12345$ gcc main1.c library.dll.a -o main1 &amp;&amp; .&#x2F;main1137942138043 之所以library.dll.a内没有定义data_export符号而这段代码仍能编译，是因为main.c文件中的data_export声明上的__declspec(dllimport)修饰符导致编译器生成了直接使用__imp_data_export符号的代码，反汇编的话我们就会看到： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465$ gcc -c main1.c -o main1.o &amp;&amp; objdump --disassemble -r main1.omain1.o: file format pe-i386Disassembly of section .text:00000000 &lt;_main&gt;: 0: 8d 4c 24 04 lea 0x4(%esp),%ecx 4: 83 e4 f0 and $0xfffffff0,%esp 7: ff 71 fc pushl -0x4(%ecx) a: 55 push %ebp b: 89 e5 mov %esp,%ebp d: 51 push %ecx e: 83 ec 14 sub $0x14,%esp 11: e8 00 00 00 00 call 16 &lt;_main+0x16&gt; 12: DISP32 ___main 16: a1 00 00 00 00 mov 0x0,%eax 17: dir32 __imp__function_export 1b: ff d0 call *%eax 1d: 89 44 24 04 mov %eax,0x4(%esp) 21: c7 04 24 00 00 00 00 movl $0x0,(%esp) 24: dir32 .rdata 28: e8 00 00 00 00 call 2d &lt;_main+0x2d&gt; 29: DISP32 _printf 2d: a1 00 00 00 00 mov 0x0,%eax 2e: dir32 __imp__data_export 32: 8b 00 mov (%eax),%eax 34: 89 44 24 04 mov %eax,0x4(%esp) 38: c7 04 24 00 00 00 00 movl $0x0,(%esp) 3b: dir32 .rdata 3f: e8 00 00 00 00 call 44 &lt;_main+0x44&gt; 40: DISP32 _printf 44: a1 00 00 00 00 mov 0x0,%eax 45: dir32 __imp__data_export 49: 8b 00 mov (%eax),%eax 4b: 8d 50 01 lea 0x1(%eax),%edx 4e: a1 00 00 00 00 mov 0x0,%eax 4f: dir32 __imp__data_export 53: 89 10 mov %edx,(%eax) 55: a1 00 00 00 00 mov 0x0,%eax 56: dir32 __imp__function_export 5a: ff d0 call *%eax 5c: 89 44 24 04 mov %eax,0x4(%esp) 60: c7 04 24 00 00 00 00 movl $0x0,(%esp) 63: dir32 .rdata 67: e8 00 00 00 00 call 6c &lt;_main+0x6c&gt; 68: DISP32 _printf 6c: a1 00 00 00 00 mov 0x0,%eax 6d: dir32 __imp__data_export 71: 8b 00 mov (%eax),%eax 73: 89 44 24 04 mov %eax,0x4(%esp) 77: c7 04 24 00 00 00 00 movl $0x0,(%esp) 7a: dir32 .rdata 7e: e8 00 00 00 00 call 83 &lt;_main+0x83&gt; 7f: DISP32 _printf 83: b8 00 00 00 00 mov $0x0,%eax 88: 83 c4 14 add $0x14,%esp 8b: 59 pop %ecx 8c: 5d pop %ebp 8d: 8d 61 fc lea -0x4(%ecx),%esp 90: c3 ret 91: 90 nop 92: 90 nop 93: 90 nop 实际上，我们可以看到生成的代码甚至都没有使用_function_export符号，取而代之的是使用了imp__function_export。本质上，导入库中的_function_export符号在每处使用的地方都已经被内联过了。这也就是为什么使用__declspec(dllimport)可以提高跨DLL调用的性能，不过这个修饰符在声明函数时不是必须写的。 我们也许会好奇，如果在声明时去掉__declspec(dllimport)修饰符会发生什么事情。鉴于我们之前讨论的关于导入变量和导入函数之间的差别，你也许以为会链接失败。我们的测试文件main2.c是： 12345678910111213141516#include &lt;stdio.h&gt;extern int function_export(void);extern int data_export;int main(int argc, char **argv) &#123; printf(&quot;%d\\n&quot;, function_export()); printf(&quot;%d\\n&quot;, data_export); data_export++; printf(&quot;%d\\n&quot;, function_export()); printf(&quot;%d\\n&quot;, data_export); return 0;&#125; 让我们来试一试： 12345$ gcc main2.c library.dll.a -o main2 &amp;&amp; .&#x2F;main2137942138043 见鬼了！编译居然通过了？这有点令人惊讶。之所以导入库library.dll.a没有定义_data_export符号但这仍能编译通过，是由于GNU ld的一个叫做自动导入的有趣的特性。如果没有自动导入特性，链接器就会如我们所愿地报错： 123456$ gcc main2.c library.dll.a -o main2 -Wl,--disable-auto-import &amp;&amp; .&#x2F;main2&#x2F;tmp&#x2F;ccGd8Urx.o:main2.c:(.text+0x2c): undefined reference to &#96;_data_export&#39;&#x2F;tmp&#x2F;ccGd8Urx.o:main2.c:(.text+0x41): undefined reference to &#96;_data_export&#39;&#x2F;tmp&#x2F;ccGd8Urx.o:main2.c:(.text+0x49): undefined reference to &#96;_data_export&#39;&#x2F;tmp&#x2F;ccGd8Urx.o:main2.c:(.text+0x63): undefined reference to &#96;_data_export&#39;collect2: ld returned 1 exit status 微软的链接器没有实现自动导入的特性，因此如果你用的是微软的工具链的话，你就会看到类似的错误信息。 然而，有一个方法可以使得在写代码时既不用依赖于自动导入的特定，也不用使用__declspec(dllimport)关键字。我们新的代码main3.c就是这么写的： 12345678910111213141516171819#include &lt;stdio.h&gt;extern int (*_imp__function_export)(void);extern int *_imp__data_export;#define function_export (*_imp__function_export)#define data_export (*_imp__data_export)int main(int argc, char **argv) &#123; printf(&quot;%d\\n&quot;, function_export()); printf(&quot;%d\\n&quot;, data_export); data_export++; printf(&quot;%d\\n&quot;, function_export()); printf(&quot;%d\\n&quot;, data_export); return 0;&#125; 在这段代码中，我们直接使用了源自导入库中的带__imp__前缀的符号。这些符号对应的是导入函数和导入变量的真实内存地址，就像代码中的预处理宏定义data_export和function_export所表示的那样。 即使没有自动编译特性，这段代码也能完美地编译通过： 12345$ gcc main3.c library.dll.a -o main3 -Wl,--disable-auto-import &amp;&amp; .&#x2F;main3137942138043 如果你一直阅读到了这里，你应该已经对Windows上上DLL的导入和导出有了透彻的理解。 本文地址：http://xnerv.wang/everything-you-never-wanted-to-know-about-dlls-cn/","categories":[{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/categories/Windows/"}],"tags":[{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/tags/Windows/"},{"name":"DLL","slug":"DLL","permalink":"https://xnerv.wang/tags/DLL/"},{"name":"译文","slug":"译文","permalink":"https://xnerv.wang/tags/%E8%AF%91%E6%96%87/"}]},{"title":"IOCP的使用和技术内幕","slug":"iocp-usage-and-inside","date":"2017-10-23T04:40:00.000Z","updated":"2023-08-21T02:24:20.256Z","comments":true,"path":"iocp-usage-and-inside/","link":"","permalink":"https://xnerv.wang/iocp-usage-and-inside/","excerpt":"IOCP wiki 使用CreateIoCompletionPort函数创建IOCP，还可以把socket或文件句柄与IOCP关联起来。 一个线程，第一次调用GetQueuedCompletionStatus函数时，该线程变为关联了该IOCP的线程，直道下述三种情形之一发生： 该线程退出； 该线程调用GetQueuedCompletionStatus函数关联到其他的IOCP； 该IOCP被关闭。 即，一个线程在任何时刻最多关联一个IOCP。","text":"IOCP wiki 使用CreateIoCompletionPort函数创建IOCP，还可以把socket或文件句柄与IOCP关联起来。 一个线程，第一次调用GetQueuedCompletionStatus函数时，该线程变为关联了该IOCP的线程，直道下述三种情形之一发生： 该线程退出； 该线程调用GetQueuedCompletionStatus函数关联到其他的IOCP； 该IOCP被关闭。 即，一个线程在任何时刻最多关联一个IOCP。 线程调用GetQueuedCompletionStatus函数等待放入IOCP的I/O完成包（completion packet）。IOCP拥有一个线程池。阻塞在IOCP上的线程按照后进先出（LIFO）顺序被释放（这是为了减少线程切换的代价）；而一个线程的完成包按照先进先出（FIFO）顺序从IOCP的队列中取走。IOCP有一个最大允许并发的线程数量上限，在CreateIoCompletionPort函数中制定，每次I/O完成包在从队列取走前检查关联与该IOCP且正在并发执行的线程数量是否达到该限。因其他原因（如调用SuspendThread函数）而挂起的线程不算作正在执行的线程。CompletionKey(完成键)一般作为“单句柄数据”的结构体（PER_HANDLE_DATA），用来标识是哪个设备的I/O完成操作己经完成。IO重叠结构（Overlapped）一般作为“单IO数据”的结构体（PER_IO_DATA），该结构体的第1个成员为OVERLAPPED结构体，用来标识是设备的具体哪个操作。 线程可以用PostQueuedCompletionStatus函数在IOCP上放置一个完成包。 IOCP不能跨进程使用。 关闭IOCP之前，必须先关闭关联在该IOCP之上的所有File Handle或socket。 内部结构 Windows中利用CreateIoCompletionPort命令创建完成端口对象时， 操作系统内部为该对象自动创建了5个数据结构，分别是： 设备列表（Device List）： 每当调用CreateIoCompletionPort函数时，操作系统会将该设备句柄添加到设备列表中；每当调用CloseHandle关闭了某个设备句柄时，系统会将该设句柄从设备列表中删除 IO完成请求队列（I/O Completion Queue-FIFO）：当I/O请求操作完成时，或者调用了PostQueuedCompeltionStatus函数时，操作系统会将I/O请求完成包添加到I/O完成队列中。当操作系统从完成端口对象的等待线程队列中取出一个工作线程时，操作系统会同时从I/O完成队列中取出一个元素（I/O请求完成包。 等待线程队列（WaitingThread List-LIFO）：当线程中调用GetQueuedCompletionStatus函数时，操作系统会将该线程压入到等待* 线程队列中。为了减少线程切换，该队列是LIFO。当I/O完成队列非空，且工作线程并未超出总的并发数时，系统从等待线程队列中取出线程，该线程从自身代码的GetQueuedCompletoinStatus函数调用处返回并继续运行。 释放线程队列（Released Thread List）：当操作系统从等待线程队列中激活了一个工作线程时，或者挂起的线程重新被激活时，该线程被压入释放线程队列中，也即这个队列的线程处于运行状态。这个队列中的线程有两个出队列的机会：一是当线程重新调用GetQueuedCompeltionStatus函数时，线程被添加到等待线程队列中；二是当线程调用其他函数使得线程挂起时，该线程被添加到挂起线程队列中。 暂停线程队列（Paused Thread List）：释放线程队列中的线程被挂起的时候，线程被压入到挂起线程队列中；当挂起的线程重新被唤醒时，从挂起线程队列中取出放入到释放线程队列。 IOCP 浅析 IOCP 实现的基本步骤 那么 IOCP 完成端口模型又是怎样实现的呢？首先我们创建一个完成端口 CreateIOCompletionPort，然后再创建一个或多个工作线程，并指定它们到这个完成端口上去读取数据。再将远程连接的套接字句柄关联到这个完成端口。工作线程调用 getQueuedCompletionStatus 方法在关联到这个完成端口上的所有套接字上等待 I/O 的完成，再判断完成了什么类型的 I/O，然后接着发出 WSASend 和 WSARecv，并继续下一次循环阻塞在 getQueuedCompletionStatus。 具体的说，一个完成端口大概的处理流程包括： 创建一个完成端口； 1Port port = createIoCompletionPort(INVALID_HANDLE_VALUE, 0, 0, fixedThreadCount()); 创建一个线程 ThreadA； ThreadA 线程循环调用 GetQueuedCompletionStatus 方法来得到 I/O 操作结果，这个方法是一个阻塞方法； 123While(true)&#123; getQueuedCompletionStatus(port, ioResult);&#125; 主线程循环调用 accept 等待客户端连接上来； 主线程 accept 返回新连接建立以后，把这个新的套接字句柄用 CreateIoCompletionPort 关联到完成端口，然后发出一个异步的 Read 或者 Write 调用，因为是异步函数，Read/Write 会马上返回，实际的发送或者接收数据的操作由操作系统去做。 123if (handle != 0L) &#123; createIoCompletionPort(handle, port, key, 0); &#125; 主线程继续下一次循环，阻塞在 accept 这里等待客户端连接。 操作系统完成 Read 或者 Write 的操作，把结果发到完成端口。 ThreadA 线程里的 GetQueuedCompletionStatus() 马上返回，并从完成端口取得刚完成的 Read/Write 的结果。 在 ThreadA 线程里对这些数据进行处理 ( 如果处理过程很耗时，需要新开线程处理 )，然后接着发出 Read/Write，并继续下一次循环阻塞在 GetQueuedCompletionStatus() 这里。 更多参考 《Windows.Internals.Part.2.6th.Edition》 - CHAPTER 8: I/O System - I/O Completion Ports 本文地址：http://xnerv.wang/iocp-usage-and-inside/","categories":[{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/categories/Windows/"}],"tags":[{"name":"原创","slug":"原创","permalink":"https://xnerv.wang/tags/%E5%8E%9F%E5%88%9B/"},{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/tags/Windows/"},{"name":"Windows Internals","slug":"Windows-Internals","permalink":"https://xnerv.wang/tags/Windows-Internals/"},{"name":"IOCP","slug":"IOCP","permalink":"https://xnerv.wang/tags/IOCP/"}]},{"title":"一些基础的C++豆知识","slug":"cpp-bean-knowledge","date":"2017-07-23T23:34:00.000Z","updated":"2023-08-21T02:24:21.598Z","comments":true,"path":"cpp-bean-knowledge/","link":"","permalink":"https://xnerv.wang/cpp-bean-knowledge/","excerpt":"基础语法 由于C++的枚举不像C#中的枚举，其枚举类型名并不是标识符的一部分，因此经常可能发生命名冲突的问题，解决的方法有四个：在枚举元素名称前加限定前缀（如enum EnumFruit { EnumFruit_apple = 1 };），将枚举类型放在一个同名的命名空间中，或将枚举作为类的嵌套类型，或者使用C++11的enum class（What’s an enum class and why should I care?）。 struct和class的默认类继承方式都是private，这与struct的成员默认继承方式是public是不同的。 #include_next &lt;filename.h&gt;，include位于搜索路径中位于当前文件之后的文件filename.h。 在vc中，inlucde的路径的反斜杠不需要转义，如#include &quot;..\\..\\..\\Global\\Data\\GlobalPreferencesMgr.h&quot;。 对于namespace中的函数或class的前置声明，必须同样也包括在相同的namespace中，而不能用class ::std::A这种写法。（Why can’t I forward-declare a class in a namespace like this?） 没有&amp;&amp;=，只有&amp;=。 （-1 || 0） == 1，请想想为什么。","text":"基础语法 由于C++的枚举不像C#中的枚举，其枚举类型名并不是标识符的一部分，因此经常可能发生命名冲突的问题，解决的方法有四个：在枚举元素名称前加限定前缀（如enum EnumFruit { EnumFruit_apple = 1 };），将枚举类型放在一个同名的命名空间中，或将枚举作为类的嵌套类型，或者使用C++11的enum class（What’s an enum class and why should I care?）。 struct和class的默认类继承方式都是private，这与struct的成员默认继承方式是public是不同的。 #include_next &lt;filename.h&gt;，include位于搜索路径中位于当前文件之后的文件filename.h。 在vc中，inlucde的路径的反斜杠不需要转义，如#include &quot;..\\..\\..\\Global\\Data\\GlobalPreferencesMgr.h&quot;。 对于namespace中的函数或class的前置声明，必须同样也包括在相同的namespace中，而不能用class ::std::A这种写法。（Why can’t I forward-declare a class in a namespace like this?） 没有&amp;&amp;=，只有&amp;=。 （-1 || 0） == 1，请想想为什么。 位移 在C语言中，涉及位移的运算符有2个，&gt;&gt;表示右移，&lt;&lt;则表示左移。 而汇编指令中，SHL和SHR表示逻辑左移和逻辑右移，SAR和SAL表示算术左移和算术右移。 其中，逻辑左移和算术左移都是寄存器二进制位整体向左移动，并在右边补0。 而右移则不同，逻辑右移是整体向右移，并在左边补0，而算术左移则是根据原符号位的值补与其相同的值。 那么如何在C语言中分别实现逻辑和算术位移呢？根据C标准，如果在位移运算符左边的变量是有符号数，如int,char,short等，编译产生的汇编指令是算术位移指令，如果该变量是无符号数，如unsigned int,unsigned char等，编译产生的汇编指令则是逻辑位移指令。 虽然intel平台上都是little-endian字节序，如果看作左边是低端地址右边是高端地址，则左移似乎丢弃低端bits。其实，对于C语言，在移位逻辑上要看作是big-endian，例如0x1001，左边是高位，左移是丢弃高位bits。 变量 如果反码范围是-127~127，则0有00000000和10000000两种表示方法。补码由于负数是在反码的基础上+1，因此-128占用了10000000，因此补码的负数能多表示一个。 64位系统vc的long仍然只有4 bytes，64位gcc则是8 bytes。主要是由于64位Linux用的是LP64位数据模型，而64位Windows用的是LLP64位数据模型。（What is the bit size of long on 64-bit Windows?） 注意1.f也是一种合法的写法，与1.0f是等价的。（What is the difference between “1.0f” and “1.f”?） 当一个struct定义了构造函数，或者用新的C++11语法直接赋予成员默认值后（C++11 member initializer list vs in-class initializer?），就不能用new A{1, 2, “a”}这样的“集合初始化”语法了，必须提供构造函数。（Why can I not brace initialize a struct derived from another struct?） 全局变量可以用函数进行初始化，但注意static成员的初始化顺序不一定是按照定义的顺序进行的。（May I initialize a global variable with the result of a function call?） 字面常量（literal constant）根据平台的不同，有可能存储在text段，也可能存储在data段或其它地方，但具有static生命周期。 123456char *b;&#123; char *a &#x3D; &quot;abc&quot;; b &#x3D; a;&#125;&#x2F;&#x2F; b仍然是有效指针 string类采用了Copy-On-Write，将str b赋值给str a之后，如果a不修改，则a和b其实用的是同样的char*内存。所以这也是一个警示，string的c_str()地址是可能变化的，不应该去依赖这个地址。 string本身是没有encoding的，取决于输入的字符串的encoding（What encoding does std::string.c_str() use?）。而字符串encoding一般由 const char * value lifetime 12345678const char **p &#x3D; nullptr;&#123; const char *t &#x3D; &quot;test&quot;; p &#x3D; &amp;t;&#125;cout &lt;&lt; *p; &quot;test&quot;是字面常量，和global或static变量具有类似的生命周期。 变量修饰关键字 const是限制指针还是限制指向的变量，关键看const是在星号*的左边还是右边。。如const int *cptr和int const *cptr是限制int，说明指向的是一个常亮。int *const cptr是一个常量指针，不能再指向其它int变量。（更简单直观的看法是，看const的右边是什么，*cptr是原变量本身，而cptr是指针） const int *指针不能赋值给int *指针，因为一个是指向const int类型，一个是指向int类型。而int * const指针可以赋值给int *指针，因为两者都是指向int变量，指向同类型变量的指针之间的相互赋值，是不受指针本身是否为const的影响的。 对于func(const char*)，正如上面一条所说的，可以将char*实参传递过来。但是对于fun(const char*&amp; p)这种加了引用的函数，不能将char*的指针传给它，而必须传const char*指针，因为引用必须引用相同的类型，一个const char*的引用不能去引用一个char*的变量。 auto这个关键字用于声明变量的生存期为自动，即将不在任何类、结构、枚举、联合和函数中定义的变量视为全局变量，而在函数中定义的变量视为局部变量。它是存储类型标识符，表明变量（自动）具有本地范围。块范围的变量声明（如for循环体内的变量声明）默认为auto存储类型。 mutable关键是为了针对const而提出来的关键字。extern关键字则是针对static（C语言）提出来的关键字。register关键字是针对volatile提出来的关键字。volatile与const一样需要弄清楚修饰的是变量本身还是指针，以及哪一级的指针。 const int &amp;a =100是正确的，但去掉const就是错误的。 浮点数 浮点数是不能用 unsigned来规范的。unsigned 的意思就是把内存中的数据第一位也用来表示数据，而不用于表示符号位。而浮点数规定内存中数据的第一位必须是符号位（Double-precision floating-point format）。因此两者之间是互相矛盾的，这也就是为什么浮点数不会有unsigned类型。在某些编译器下unsigned float 和 unsigned double会被自动转换成unsigned int类型，而不报错。这时sizeof(unsigned float)和sizeof(unsigned double)的值是4。 定点数的优点是很简单，大部分运算实现起来和整数一样或者略有变化，但是缺点则是表示范围，而且在表示很小的数的时候，大部分位都是0，精度很差，不能充分运用存储单元。浮点数就是设计来克服这个缺点的，它相当于一个定点数加上一个阶码，阶码表示将这个定点数的小数点移动若干位。由于可以用阶码移动小数点，因此称为浮点数。（为什么叫浮点数?） 类型float和double通过==,&gt;,&lt;等比较不会引起编译错误，但是非常可能得到错误的结果。这是因为它们的内存分布不同，不可以直接比较。正确的方法是转换为同一类型后比较两者差值，如果结果小于规定的小值，则视为相等。 数组 对于数组char buff[] = “hello”，将buff 和 &amp;buff 用指针形式输出，结果是一样的。（Address of array - difference between having an ampersand and no ampersand） How to initialize all members of an array to the same value? int array[100] = {0};可以将100个元素都设置成0，但int array[100] = {-1};只能将第一个元素设置成-1，声誉99个元素则设置成0。 不能将一个char[100]的实参传给一个char*&amp;的形参，因为对于引用，必须是类型严格相同的。char[100]跟char*虽然可以相互转换，但编译时类型并不相同。可以将形参改成int (&amp;arr)[100]这样。 char * arr[n] = &#123; &quot;aaa&quot;, &quot;bbb&quot; &#125;是对的，是char的数组，每一个char指向一个字符串常量。而char ** arr = &#123; &quot;aaa&quot;, &quot;bbb&quot; &#125;是语法错误的，这是指向char*数组的二维指针，所以必须先arr = new char*[n]。 当数组定义时没有指定大小，当初始化采用列表初始化了，那么数组的大小由初始化时列表元素个数决定。如果明确指定了数组大小，当在初始化时指定的元素个数超过这个大小就会产生错误。如果初始化时指定的的元素个数比数组大小少，剩下的元素都回被初始化为0。字符数组可以方便地采用字符串直接初始化。因此，int a[10] = &#123;0&#125;这种写法，其实本来是将第一个元素置为0，但后续所有元素都会被默认置为0。 new/delete/malloc/free 深入探究C++的new/delete操作符 new/new[]调用的是operator new/new[]，前者是C++关键字，而后者其实就是（全局或class内的）操作符重载。所谓的placement new其实就是operator new/new[]的重载版本，我们也可以自定义提供了更多参数的placement new版本如operator(size_t size, P2, P3, P4)，然后通过new(P2, P3, P4)这样的语法进行调用。 calloc返回的是一个数组，而malloc返回的是一个对象。calloc的效率一般是比较低的。calloc相当于malloc后再加memset。关于realloc，原来的指针会被Free，申请可能不成功，会返回NULL。新增区域内的初始值则不确定。alloca是在栈(stack)上申请空间，用完马上就释放。某些系统在函数已被调用后不能增加栈帧长度，于是也就不能支持alloca函数。 cookie信息 当我们使用 operator new 为一个自定义类型对象分配内存时，实际上我们得到的内存要比实际对象的内存大一些，这些内存除了要存储对象数据外，还需要记录这片内存的大小，此方法称为 cookie。这一点上的实现依据不同的编译器不同。（例如 MFC 选择在所分配内存的头部存储对象实际数据，而后面的部分存储边界标志和内存大小信息。g++ 则采用在所分配内存的头4个字节存储相关信息，而后面的内存存储对象实际数据。）当我们使用 delete operator 进行内存释放操作时，delete operator 就可以根据这些信息正确的释放指针所指向的内存块。 以上论述的是对于单个对象的内存分配/释放，当我们为数组分配/释放内存时，虽然我们仍然使用 new operator 和 delete operator，但是其内部行为却有不同：new operator 调用了operator new 的数组版的兄弟－ operator new[]，而后针对每一个数组成员调用构造函数。而 delete operator 先对每一个数组成员调用析构函数，而后调用 operator delete[] 来释放内存。需要注意的是，当我们创建或释放由自定义数据类型所构成的数组时，编译器为了能够标识出在 operator delete[] 中所需释放的内存块的大小，也使用了编译器相关的 cookie 技术。 根据Inside The C++ Object Model上所言，现在的编译器大多使用两种方法， 一种是cookie, 一个记录分配空间大小的内存小块绑定在分配内存的地址头部。二是使用表来对分配了的指针进行管理，每一个分配了空间的指针都在表中对应着分配空间的大小。 指针 ANSI规定不能对void指针做++/+=等操作，但GNU将void的这些操作当作和char*一样。（Increment void pointer by one byte? by two?） 定义指向public成员函数的指针变量的一般形式为数据类型名 (类名::*指针变量名)(参数表列)。使指针变量指向一个公用成员函数的一般形式为指针变量名=&amp;类名::成员函数名。对于普通函数，函数名本身加不加&amp;都能表示函数指针，但是成员函数则必须加&amp;才能取地址。 在C语言里，一个指针可以指向一个函数。这个指针也有两个属性，但一个是函数的入口地址，另一个是函数的返值类型。但是C里面函数指针的形参列表可以不写出（obsolescent），而C++中则强制要求写出。（Function pointer without arguments types?） 函数 默认值可以是全局变量、全局常量，甚至是一个函数。但不可以是局部变量。因为默认参数的调用是在编译时确定的，而局部变量位置与默认值在编译时无法确定。 当一个stack上的数组如char arr[5]作为参数传递给一个函数void func(char* p)或void func(char p[5])时，就降级为一个指针，sizeof只能取到指针本身的大小。要记得sizeof是一个编译器的行为，对于函数而言，有可能被多处调用到，传递来不同大小的数组，因此不可能在编译器完成sizeof。（Why does a C-Array have a wrong sizeof() value when it’s passed to a function?）如果要保留数组类型，则要声明函数为void func(char (&amp;a)[5])。（When a function has a specific-size array parameter, why is it replaced with a pointer?） 数组的长度与参数声明无关。因此，下列三个声明是等价的： &lt;!--hexoPostRenderEscape:&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;void putValues(int*);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;void putValues(int[]);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;void putValues(int[10]);&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;:hexoPostRenderEscape--&gt; 数组长度不是参数类型的一部分。函数不知道传递给它的数组的实际长度，编译器也不知道，当编译器对实参类型进行参数类型检查时，并不检查数组的长度。 一个返回void的函数，可以在内部return另一个返回void的函数。 参数默认值可以写在函数声明处，也可以写在函数定义处，但是不能两处同时写，即使两处写的默认值是一样的。但是不同的cpp在声明一个外部函数时，应该可以使用不同的函数参数默认值声明，虽然在同一个cpp中不能看见有两次同一个函数的声明，即使是同样的默认值。这说明，无论是函数的声明还是定义，无论默认值是否相同，同一个函数的默认值定义不能出现两次。 string, char*参数都可以用字符串常量作为默认值，说明一个类A的对象，并且支持类型B到A的隐式转换，就可以用B的一个实例b作为参数默认值。（How to set default parameter as class object in c++?） 如何定义一个函数指针，指向一个带有默认值参数的函数? 结论是做不到，只能用类似functor或者std::function等来科里化其中的一个或部分参数值。 普通的函数不需要通过&amp;来取地址，但是成员方法取地址则必须加上&amp;。（If ampersands aren’t needed for function pointers, why does boost::bind require one?） 内联函数 inline关键字更主要的含义是允许一个函数在不同的编译单元（cpp）同时存在实现，这和在h文件的class定义中直接实现一个方法，而不是将方法的实现放到cpp中，本质上是样的。至于是否会用内联代码代替函数调用，则是由编译器自身决定的。（Difference between implementing a class inside a .h file or in a .cpp file） 在cpp中定义inline函数（即使在头文件中再次用inline声明了这个函数），对于其它的cpp而言是没有inline效果的。因为对于编译器而言，每个cpp都是独立的编译单元，因此一个cpp是不能inline另一个cpp中定义实现的inline函数的。（C++ inline member function in .cpp file） 构造函数 explicit关键字用于取消构造函数的隐式转换，对有多个参数的构造函数使用explicit是个语法错误。即用explict修饰的构造函数有且只能有一个参数。 在构造函数里调用另一个构造函数的关键是让第二个构造函数在第一次分配好的内存上执行，而不是分配新的内存，这个可以用标准库的placement new做到。 &lt;!--hexoPostRenderEscape:&lt;figure class=&quot;highlight c++&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;A()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt; &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt;(&lt;span class=&quot;keyword&quot;&gt;this&lt;/span&gt;)A(&lt;span class=&quot;number&quot;&gt;11&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;:hexoPostRenderEscape--&gt; 注： 若构造函数调用自身，则会出现无限递归调用。 成员初始化列表不能对基类成员变量赋值，而应该通过调用基类的构造函数达成目标。 C++初始化类成员时，是按照声明的顺序初始化的，而不是按照出现在初始化列表中的顺序。（Order of execution in constructor initialization list） 删除一个强转成void*的对象指针，会释放内存，但不会调用其析构函数。Is it safe to delete a void pointer? 不要在有虚表的类的构造函数和析构函数中调用虚函数，会调用到基类的函数。（[Calling virtual functions inside constructors(https://stackoverflow.com/questions/962132/calling-virtual-functions-inside-constructors)]，https://stackoverflow.com/questions/962132/calling-virtual-functions-inside-constructors） What is a non-trivial constructor in C++? 也就是说，trivial构造函数即没有定义构造函数。但没有自定义任何构造函数（包括拷贝构造函数）时，应该会由编译器自动生成trivial构造函数（其实就是什么都不做，进行对象的拷贝赋值时，直接进行memory copy）。如果只定义了一个带参数的构造函数的话，则不会再生成默认的构造函数。 If you define a constructor yourself, it is considered non-trivial, even if it doesn’t do anything, so a trivial constructor must be implicitly defined by the compiler. For a default constructor and destructor being “trivial” means literally “do nothing at all”. For copy-constructor and copy-assignment operator, being “trivial” means literally “be equivalent to simple raw memory copying” (like copy with memcpy). 成员函数隐藏 Reason for C++ member function hiding 当编译器于某一层找到能用（不一定最好，也许需要强制转换参数类型）的方法时，就不会继续再向上一层（父类）查找。不仅仅是类与类之间，嵌套的namespace也存在这个现象。 哑元函数 C++的哑元参数是指operator ++(int)这种。某个参数如果在子程序或函数中没有用到，那就被称为哑元。函数的形参又称“哑元”，实参又称“实元”。 。友元关系不能被继承。（Why does C++ not allow inherited friendship?） 至少在gcc里，int a; string b; 1 == 1 ? a : b;这种写法是可以的，但如果将这个表达式进行cout，就会编译报错提示两边类型不一直的。（Return type of ‘?:’ (ternary conditional operator)） 库函数 memmove和memcpy的区别，在于前者当src &lt; dest并且两者区间有重叠时，会改用从后向前复制。memmove函数为什么要先判断重叠，而不是直接从尾部向头部复制？因为当dst的头部和src的尾部覆盖时，从尾部开始复制是正确的。但是当dst的尾部和src的头部覆盖时，从头部开始复制才是正确的。所以要区分处理。 memccpy用来拷贝src所指的内存内容前n个字节到dest所指的地址上。与memcpy不同的是，memccpy如果在src中遇到某个特定值(int c)立即停止复制。 strtok是一个线程不安全的函数，因为它使用了静态分配的空间来存储被分割的字符串位置（C库还有其它使用了静态空间的线程不安全函数）。运用strtok来判断ip或者mac的时候务必要先用其他的方法判断’.‘或’:'的个数，因为用strtok截断的话，比如：&quot;192…168.0…8…&quot;这个字符串，strtok只会截取四次，中间的…无论多少都会被当作一个key。而这个函数的线程安全版本在linux中是strtok_r，在vc中则是strtok_s。 sprintf和vsprintf的区别，以及snprintf和vsnprintf的区别，在于后者接收的是va_list，而前者是不变参数列表，后者几乎不会被直接使用，而是在不定参数的函数内部调用，作为一种“转发”。 在C++中用longjmp，可能导致析构函数不被调用。 std::bind是基于functor仿函数实现的，也就是函数对象实现存储固定的参数值。 C++的4种cast操作 static_cast/dynamic_cast/reinterpret_cast不能将一个const T*转为T*，当然如果是将const T转化T是可以的。只有const_cast能将const T*转为T*。 reinterpret_cast转换后的bits是不变的，因此在将double=1.0转变为int时，显然reinterpret_cast会得到诡异的结果。 rtti包括typeid(type_info)和dynamic_cast两者，都需要虚表的支持。如果是没有虚函数的类，则dynamic_cast就只能从下往上安全转换了。此外，dynamic_cast只能对指针（引用）操作。static_cast是C++里面的类型安全转换，这个转换不允许将毫无关系的两个数据类型的指针互相转化。例如不能把int**转成void**，因为int*和void*没有关系，但是可以将int*转成void*。 综上所述，C风格的强制转换=static_cast + reinterpret_cast。对于有虚表的类的指针，reinterpret_cast由于不会调整this指针，也不会将vptr指针进行上溯或下溯，因此将会造成不可预期的结果。 static_cast可以将void*转换为A*，但是不能量B*转换为A*。 bad_cast这个关键字和bad_typeid类似，是dynamic_cast转换失败时（一般是错误地想把基类转换为子类时，此时转换结果为空指针），会抛出的异常。 const_cast：允许添加或删除表达式类型的const或volatile关键字. dynamic_cast：仅适用于多态类型的向下转换，被转换的类型必须是一个指向含有虚函数的类类型的指针，否则会编译错误。 reinterpret_cast：从位的角度来看待一个对象，从而允许将一个东西看成是完全不同的另一个东西，最强的一种转换。这个操作符能够在非相关的类型之间转换。操作结果只是简单的从一个指针到别的指针的值的二进制拷贝。在类型之间指向的内容不做任何类型的检查和转换。例如将一个double转化为int，reinterpret_cast仅仅复制bits，导致转化的值无意义。而static_cast就能得到正确的退一法值。 只有dynamic_cast是运行期行为，其它三种cast都是编译器行为。 How is dynamic_cast implemented dynamic_cast can know this by keeping this knowledge around. When the compiler generates code it keeps around the data about the class hierarchies in some sort of table that dynamic_cast can look up later. That table can be attached to the vtable pointer for easy lookup by the dynamic_cast implementation. The data neeeded for typeid for those classes can also be stored along with those. How dynamic_cast works internally? Formally, of course, it’s implementation defined, but in practice, there will be an additional pointer in the vtable, which points to a description of the object, probably as a DAG of objects which contain pointers to the various children (derived classes) and information regarding their type (a pointer to a type_info, perhaps). The compiler then generates code which walks the different paths in the graph until it either finds the targeted type, or has visited all of the nodes. If it finds the targeted type, the node will also contain the necessary information as to how to convert the pointer. One additional point occurs to me. Even if the generated code finds a match, it may have to continue navigating in order to ensure that it isn’t ambiguous. dynamic_cast失败时，有时是返回null，有时是抛出异常。原因在于C++没有null reference，所以只能throw exception。 123Base* b1 = new Derived;Derived* pd1 = dynamic_cast&lt;Derived *&gt;(b1); // fails: returns &#x27;NULL&#x27;Derived d1 = dynamic_cast&lt;Derived &amp;*&gt;(b1); // fails: exception thrown Why can’t I static_cast between char * and unsigned char *? 不同的两种类型的指针相互之间不能用static_cast转换，而必须用reinterprete_cast。而普通指针和void*之间则可以用static_cast相互转换。 sizeof sizeof，终极无惑（上） sizeof有三种语法形式，如下： sizeof( object ); // sizeof( 对象 ); sizeof( type_name ); // sizeof( 类型 ); sizeof object; // sizeof 对象; size_t sz = sizeof( foo() ); // foo() 的返回值类型为char，所以sz = sizeof( char )，foo()并不会被调用。但是foo不能返回为void。 c99标准支持对VLA取sizeof。 结构体的sizeof到底多大？ 在VC中规定， 结构体变量的首地址能够被其最宽基本类型成员的大小所整除；而在gcc中规定对齐模数最大只能是4，也就是说，即使结构体中有double类型，对齐模数还是4。 sizeof也是运算符，虽然不能被重载。 不能重载的运算符只有5个（Which operator cannot be overloaded in C++ and why?）： - (成员访问运算符) .* (成员指针访问运算符) :: (域运算符) sizeof (长度运算符) ?: (条件运算符） 为什么C++中空类和空结构体大小为1？ 这是因为，C++标准中规定，“no object shall have the same address in memory as any other variable” ，就是任何不同的对象不能拥有相同的内存地址。 如果空类大小为0，若我们声明一个这个类的对象数组，那么数组中的每个对象都拥有了相同的地址，这显然是违背标准的。 基本上所有的指针运算都依赖于sizeof T。 typedef typdef定义的struct/class如何前置声明？ 12345678typedef struct my_time_t&#123;int hour, minute, second;&#125; MY_TIME;struct my_time_t;typedef struct my_time_t MY_TIME;void func(MY_TIME* mt) &#123;&#125; 其实typedef作为一种类似宏的声明，在没有include头文件的情况下要想使用只能重新typedef。 #define没有作用域的限制，只要是之前预定义过的宏，在以后的程序中都可以使用。而typedef有自己的作用域。（Please explain syntax rules and scope for “typedef”） typedef会影响模板参数T的匹配吗？对于func(int, int32_t)和func(int, int)，会优先匹配哪个？实验发现编译错误：func重定义。一个int实参不能传给unsigned&amp;的形参，但typedef可以。以上都表明typedef有点类似define。但是，ifdef/ifndef不能检查typedef。 typedef register int FAST_COUNTER;，这种写法是错误的，编译通不过。问题出在你不能在声明中有多个存储类关键字（storage class specifier）。因为符号typedef已经占据了存储类关键字的位置， typedef声明中不能用register（或任何其它存储类关键字如static）。此外，由于存储类关键字本身并不是类型type的一部分，因此不允许其出现在typedef语句中也是合理的。（Why typedef can not be used with static?） typedef struct tagNode *pNode; struct tagNode &#123; &#125;;，在这个例子中，你用typedef给一个还未完全声明的类型起新名字。C语言编译器支持这种做法。 typedef struct tagNode &#123; &#125; *pNode;，定义了一种新的类型pNode，等于一个结构体指针类型。 typedef char *pStr1; #define pStr2 char *; pStr2 s3, s4; pStr2 s3, s4;，在上述的变量定义中，s1、s2、s3都被定义为char *，而s4则定义成了char，不是我们所预期的指针变量，根本原因就在于#define只是简单的字符串替换而typedef则是为一个类型起新名字。 typedef也有一个特别的长处：它符合范围规则（scope），使用typedef定义的变量类型其作用范围限制在所定义的函数或者文件内（取决于此变量定义的位置），而宏定义则没有这种特性。但typedef定义的类型不能用#ifdef 、#ifndef去检测。 typedef是一个语句，后面要加分号；。而define是预处理宏，不能加分号。 typedef char Line[81];，定义了一种新的类型Line，等于char[81]，不能错误地写作``typedef char[81] Line;。此外，最好用typedef struct Line &#123; char line[81]; &#125; Line; typedef char * pstr;，定义了一种新的类型pstr，等于char*。按照顺序，const pstr被解释为char * const（一个指向 char 的常量指针），而不是const char *（指向常量 char 的指针）。这个问题很容易解决：typedef const char * cpstr;。 cdecl和stdcall 实际上__cdecl和__stdcall函数参数都是从右到左入栈，它们的区别在于由谁来清栈，__cdecl由外部调用函数清栈，而__stdcall由被调用函数本身清栈， 显然对于可变参数的函数，函数本身没法知道外部函数调用它时传了多少参数（也许有人说例如printf，分析format string不就可以知道传了哪些参数了，但实际上，caller在调用printf时，可以额外多传一些没有用到的参数啊），所以没法支持被调用函数本身清栈（__stdcall）， 所以可变参数只能用__cdecll。 另外还要理解函数参数传递过程中堆栈是如何生长和变化的，从堆栈低地址到高地址，依次存储 被调用函数局部变量，上一函数堆栈桢基址，函数返回地址，参数1， 参数2， 参数3… 多态、继承 C++的派生类在重写virtual函数时，访问修饰符可以和基类不同，但是要注意派生类中对基类方法的重载将会导致罕见的“隐藏”问题，无论这个函数是不是虚函数。 重载方法（包括运算符重载）时是可以改变返回值类型的，因为返回值类型不是函数签名的一部分。 当有虚函数时，应该把析构函数声明为虚析构函数，否则通过基类指针释放派生类对象时，有可能会存在内存泄漏（object的空间本身应该无论如何是可以释放掉的，只是基类的析构函数由于没有被调用，可能会泄露基类对象本身拥有的一些其它内存或资源）。 三种继承方式下基类的私有成员对派生类都不可见，而公共成员和保护成员对派生类的方法而言都可以访问。三者的区别是，公共继承时基类的公共成员和保护成员对派生类而言仍然是公共成员和保护成员，私有继承时基类的公共成员和保护成员都成为派生类的私有成员，保护继承时基类的公共成员和保护成员都成为派生类的保护成员（而对于外界，无论是哪种继承方式，保护成员和私有成员都是不可见的）。 在protected和private继承时，基类指针不能指向派生类对象。简单的说这两种继承方式并不是所谓的is-a关系。详细一点讲,用了这两种继承方式后,子类对象中的继承方法都是在main中不能访问的，如果允许基类指针指向子类对象,就会出错了。当然你也可以用(Base*)进行强制转化。 C++的默认继承方式是private继承。 模板 函数模板的偏特化 严格的来说，函数模板并不支持偏特化，但由于可以对函数进行重载，所以可以达到类似于类模板偏特化的效果。 template &lt;class T&gt; void f(T); (a) 根据重载规则，对（a）进行重载 template &lt; class T&gt; void f(T*); (b) 如果将（a）称为基模板，那么（b）称为对基模板（a）的重载，而非对（a）的偏特化。C++的标准委员会仍在对下一个版本中是否允许函数模板的偏特化进行讨论。 C++ traits C++ traits是利用模板特化编译器来完成一定功能的技巧，本质是“利用类型固有的特性，判断类型是否具有特定的特性”，例如简单的例子（利用偏特化）： 1234567template &lt;typename T&gt;struct is_void&#123; static const bool value = false; &#125;template &lt;&gt;struct is_void&lt;void&gt;&#123; static const bool value = true; &#125; STL C++语言中的std::remove(vec.begin(), vec.end(), 5);并非是删除容器里变所有值等于5的数，而是用类似LeetCode中的27. Remove Element的算法，将后面的元素向前复制移动。因此vector的长度并不会改变，需要和erase方法结合使用：vec.erase(std::remove(vec.begin(), vec.end(), 5), vec.end());。 vector为了防止大量分配连续内存的开销，保持一块默认的尺寸的内存，clear只是清数据了，未清内存，因为vector的capacity容量未变化，系统维护一个的默认值。有什么方法可以释放掉vector中占用的全部内存呢？根据StackOverflow上的方法，可以用vector&lt; T &gt; vtTemp; veTemp.swap(vt);。 multimap/multiset不支持下标运算，可能是因为[]运算符可能有多个元素匹配。 const map不支持下标操作，根据StackOverflow的说法，[]运算符在key不存在时会插入新的元素，不符合const的语境。 stl的deque,queue,stack,heap：deque和vector、list一样是一种基础数据结构。然后stack，queue，priority_queue则是可以使用某个基础数据机构作为底层存储的二级数据结构。而heap本身不能持有数据存储，只能将某个基础数据结构对象作为托管的数据存储。 STL的模板参数T类型也可以带const修饰符。 STL中有bitset这种数据结构。 编译器优化 Object returned from function and copy constructor That is called Named Return Value Optimization and copy elision, and basically means that the compiler has figured out that the copy can be avoided by carefully placing the temporary and the object in the same memory location. By default there would be three objects in that piece of code, temp inside fun, the return value and ob inside main, and as many as two copies, but by carefully placing temp in the same memory location as the returned object inside fun and placing ob in the same memory address the two copies can be optimized away. Ref to Value semantics: NRVO, and Value semantics: Copy elision. 本文地址：http://xnerv.wang/cpp-bean-knowledge/","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://xnerv.wang/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"编程语言","slug":"编程语言","permalink":"https://xnerv.wang/tags/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"},{"name":"原创","slug":"原创","permalink":"https://xnerv.wang/tags/%E5%8E%9F%E5%88%9B/"},{"name":"C++","slug":"C","permalink":"https://xnerv.wang/tags/C/"}]},{"title":"Linux session和进程组概述（转载）","slug":"linux-session-and-processgroup-summary","date":"2017-04-21T07:00:00.000Z","updated":"2023-08-21T02:24:19.694Z","comments":true,"path":"linux-session-and-processgroup-summary/","link":"","permalink":"https://xnerv.wang/linux-session-and-processgroup-summary/","excerpt":"在上一篇中介绍了tty的相关原理，这篇将介绍跟tty密切相关的session和进程组。 本篇主要目的是澄清一些概念，不涉及细节","text":"在上一篇中介绍了tty的相关原理，这篇将介绍跟tty密切相关的session和进程组。 本篇主要目的是澄清一些概念，不涉及细节 session session就是一组进程的集合，session id就是这个session中leader的进程ID。 session的特点 session的主要特点是当session的leader退出后，session中的所有其它进程将会收到SIGHUP信号，其默认行为是终止进程，即session的leader退出后，session中的其它进程也会退出。 如果session和tty关联的话，它们之间只能一一对应，一个tty只能属于一个session，一个session只能打开一个tty。当然session也可以不和任何tty关联。 session的创建 session可以在任何时候创建，调用setsid函数即可，session中的第一个进程即为这个session的leader，leader是不能变的。常见的创建session的场景是： 用户登录后，启动shell时将会创建新的session，shell会作为session的leader，随后shell里面运行的进程都将属于这个session，当shell退出后，所有该用户运行的进程将退出。这类session一般都会和一个特定的tty关联，session的leader会成为tty的控制进程，当session的前端进程组发生变化时，控制进程负责更新tty上关联的前端进程组，当tty要关闭的时候，控制进程所在session的所有进程都会收到SIGHUP信号。 启动deamon进程，这类进程需要和父进程划清界限，所以需要启动一个新的session。这类session一般不会和任何tty关联。 进程组 进程组（process group）也是一组进程的集合，进程组id就是这个进程组中leader的进程ID。 进程组的特点 进程组的主要特点是可以以进程组为单位通过函数killpg发送信号 进程组的创建 进程组主要用在shell里面，shell负责进程组的管理，包括创建、销毁等。（这里shell就是session的leader） 对大部分进程来说，它自己就是进程组的leader，并且进程组里面就只有它自己一个进程 shell里面执行类似ls|more这样的以管道连接起来的命令时，两个进程就属于同一个进程组，ls是进程组的leader。 shell里面启动一个进程后，一般都会将该进程放到一个单独的进程组，然后该进程fork的所有进程都会属于该进程组，比如多进程的程序，它的所有进程都会属于同一个进程组，当在shell里面按下CTRL+C时，该程序的所有进程都会收到SIGINT而退出。 后台进程组 shell中启动一个进程时，默认情况下，该进程就是一个前端进程组的leader，可以收到用户的输入，并且可以将输出打印到终端，只有当该进程组退出后，shell才可以再响应用户的输入。 但我们也可以将该进程组运行在后台，这样shell就可以继续相应用户的输入，常见的方法如下： 启动程序时，在后面加&amp;，如sleep 1000 &amp;，进程将会进入后台继续运行 程序启动后，可以按CTRL+Z让它进入后台，和后面加&amp;不同的是，进程会被暂停执行 对于后台运行的进程组，在shell里面体现为job的概念，即一个后台进程组就是一个job，job有如下限制： 默认情况下，只要后台进程组的任何一个进程读tty，将会使整个进程组的所有进程暂停 默认情况下，只要后台进程组的任何一个进程写tty，将有可能会使整个进程组的所有进程暂停（依赖于tty的配置，请参考TTY/PTS概述） 所有后台运行的进程组可以通过jobs命令查看到，也可以通过fg命令将后台进程组切换到前端，这样就可以继续接收用户的输入了。这两个命令的具体用法请参考它们的帮助文件，这里只给出一个简单的例子： 12345678910111213141516171819202122232425262728293031#通常情况下，sleep命令会一直等待在那里，直到指定的时间过去后才退出。#shell启动sleep程序时，就将sleep放到了一个新的进程组，#并且该进程组为前端进程组，虽然sleep不需要输入，也没有输出，#但当前session的标准输入和输出还是归它，别人用不了，#只有我们按下CTRL+C使sleep进程退出后，shell自己重新变成了前端进程组，#于是shell重新具备了响应输入以及输出能力dev@debian:~$ sleep 1000^C#我们可以在命令行的后面加上&amp;符号，shell还是照样会创建新的进程组，#并且sleep进程就是新进程组的leader，#但是shell会将sleep进程组放到后端，让它成为后台进程组#这里[1]是job id，1627是进程组的ID，即sleep进程的iddev@debian:~$ sleep 1000 &amp;[1] 1627#可以通过jobs命令看到当前有哪些后台进程组（job）dev@debian:~$ jobs[1]+ Running sleep 1000 &amp;#使用fg命令带上job id，即可让后端进程组回到前端，#然后我们使用CTRL+Z命令可以让它再次回到后端，并暂停进程的执行#CTRL+Z和&amp;不一样的地方就是CTRL+Z会让进程暂停执行，而&amp;不会dev@debian:~$ fg 1sleep 1000^Z[1]+ Stopped sleep 1000#Stopped状态表示进程在后台已经暂停执行了dev@debian:~$ jobs[1]+ Stopped sleep 1000 session和进程组的关系 deamon程序虽然也是一个session的leader，但一般它不会创建新的进程组，也没有job的管理功能，所以这种情况下一个session就只有一个进程组，所有的进程都属于同样的进程组和session。 我们这里看一下shell作为session leader的情况，假设我们在shell里面执行了这些命令： 1234567dev@debian:~$ sleep 1000 &amp;[1] 1646dev@debian:~$ cat | wc -l &amp;[2] 1648dev@debian:~$ jobs[1]- Running sleep 1000 &amp;[2]+ Stopped cat | wc -l 下面这张图标明了这种情况下它们之间的关系： 1234567891011+--------------------------------------------------------------+| || pg1 pg2 pg3 pg4 || +------+ +-------+ +-----+ +------+ || | bash | | sleep | | cat | | jobs | || +------+ +-------+ +-----+ +------+ || session leader | wc | || +-----+ || |+--------------------------------------------------------------+ session pg = process group(进程组) bash是session的leader，sleep、cat、wc和jobs这四个进程都由bash fork而来，所以他们也属于这个session bash也是自己所在进程组的leader bash会为自己启动的每个进程都创建一个新的进程组，所以这里sleep和jobs进程属于自己单独的进程组 对于用管道符号“|”连接起来的命令，bash会将它们放到一个进程组中 nohup nohup是咋回事呢？nohup干了这么几件事： 将stdin重定向到/dev/null，于是程序读标准输入将会返回EOF 将stdout和stderr重定向到nohup.out或者用户通过参数指定的文件，程序所有输出到stdout和stderr的内容将会写入该文件（有时在文件中看不到输出，有可能是程序没有调用flush） 屏蔽掉SIGHUP信号 调用exec启动指定的命令（nohup进程将会被新进程取代，但进程ID不变） 从上面nohup干的事可以看出，通过nohup启动的程序有这些特点： nohup程序不负责将进程放到后台，这也是为什么我们经常在nohup命令后面要加上符号“&amp;”的原因 由于stdin、stdout和stderr都被重定向了，nohup启动的程序不会读写tty 由于stdin重定向到了/dev/null，程序读stdin的时候会收到EOF返回值 nohup启动的进程本质上还是属于当前session的一个进程组，所以在当前shell里面可以通过jobs看到nohup启动的程序 当session leader退出后，该进程会收到SIGHUP信号，但由于nohup帮我们忽略了该信号，所以该进程不会退出 由于session leader已经退出，而nohup启动的进程属于该session，于是出现了一种情况，那就是通过nohup启动的这个进程组所在的session没有leader，这是一种特殊的情况，内核会帮我们处理这种特殊情况，这里就不再深入介绍 通过nohup，我们最后达到了就算session leader（一般是shell）退出后，进程还可以照常运行的目的。 deamon 通过nohup，就可以实现让进程在后台一直执行的功能，为什么我们还要写deamon进程呢？ 从上面的nohup的介绍中可以看出来，虽然进程是在后台执行，但进程跟当前session还是有着千丝万缕的关系，至少其父进程还是被session管着的，所以我们还是需要一个跟任何session都没有关系的进程来实现deamon的功能。实现deamon进程的大概步骤如下： 调用fork生成一个新进程，然后原来的进程退出，这样新进程就变成了孤儿进程，于是被init进程接收，这样新进程就和调用进程没有父子关系了。 调用setsid，创建新的session，新进程将成为新session的leader，同时该新session不和任何tty关联。 切换当前工作目录到其它地方，一般是切换到根目录，这样就取消了对原工作目录的引用，如果原工作目录是某个挂载点下面的目录，这样就不会影响该挂载点的卸载。 关闭一些从父进程继承过来而自己不需要的fd，避免不小心读写这些fd。 重定向stdin、stdout和stderr，避免读写它们出现错误。 参考 Processes Job Control 那些永不消逝的进程 本文地址：http://xnerv.wang/linux-session-and-processgroup-summary/ 转载自：(SegmentFault) Linux session和进程组概述","categories":[{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/categories/Linux/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/tags/Linux/"},{"name":"Shell","slug":"Shell","permalink":"https://xnerv.wang/tags/Shell/"},{"name":"Session","slug":"Session","permalink":"https://xnerv.wang/tags/Session/"}]},{"title":"Linux TTY/PTS概述（转载）","slug":"linux-tty-pts-summary","date":"2017-04-16T07:00:00.000Z","updated":"2023-08-21T02:24:19.714Z","comments":true,"path":"linux-tty-pts-summary/","link":"","permalink":"https://xnerv.wang/linux-tty-pts-summary/","excerpt":"当我们在键盘上敲下一个字母的时候，到底是怎么发送到相应的进程的呢？我们通过ps、who等命令看到的类似tty1、pts/0这样的输出，它们的作用和区别是什么呢？","text":"当我们在键盘上敲下一个字母的时候，到底是怎么发送到相应的进程的呢？我们通过ps、who等命令看到的类似tty1、pts/0这样的输出，它们的作用和区别是什么呢？ TTY历史 支持多任务的计算机出现之前 在计算机出来以前，人们就已经在使用一种叫teletype的设备，用来相互之间传递信息，看起来像下面这样： 123+----------+ Physical Line +----------+| teletype |&lt;---------------------&gt;| teletype |+----------+ +----------+ 两个teletype之间用线连接起来，线两端可能也有类似于调制解调器之类的设备（这里将它们忽略），在一端的teletype上敲键盘时，相应的数据会发送到另一端的teletype，具体功能是干什么的，我也不太了解。(我脑袋里面想到画面是在一端敲字，另一端打印出来) 这些都是老古董了，完全没接触过，所以只能简单的推测。 支持多任务的计算机出现之后 等到计算机支持多任务后，人们想到把这些teletype连到计算机上，作为计算机的终端，从而可以操作计算机。 使用teletype的主要原因有两个（个人见解）： 现实中已经存在了大量不同厂商的teletype，可以充分利用现有资源 teletype的相关网络已经比较成熟，连起来方便 于是连接就发展成这样： 12345 +----------++----------+ +-------+ Physical Line +-------+ +------+ | || Terminal |&lt;-&gt;| Modem |&lt;---------------------&gt;| Modem |&lt;-&gt;| UART |&lt;-&gt;| Computer |+----------+ +-------+ +-------+ +------+ | | +----------+ 左边的Terminal就是各种各样的teletype 物理线路两边用上了Modem，就是我们常说的“猫”，那是因为后来网络已经慢慢的变发达了，大家可以共享连接了。（大概推测，可能不对） UART可以理解为将teletype的信号转换成计算机能识别的信号的设备 内核TTY子系统 计算机为了支持这些teletype，于是设计了名字叫做TTY的子系统，内部结构如下： 1234567891011 +-----------------------------------------------+ | Kernel | | +--------+ | | +--------+ +------------+ | | | +----------------+ | | UART | | Line | | TTY |&lt;----------&gt;| User process A |&lt;------&gt;| |&lt;-&gt;| |&lt;-&gt;| | | +----------------+ | | driver | | discipline | | driver |&lt;----------&gt;| User process B | | +--------+ +------------+ | | | +----------------+ | +--------+ | | | +-----------------------------------------------+ UART driver对接外面的UART设备 Line discipline主要是对输入和输出做一些处理，可以理解它是TTY driver的一部分 TTY driver用来处理各种终端设备 用户空间的进程通过TTY driver来和终端打交道 为了简单起见，后面的介绍中不再单独列出UART driver和Line discipline，可以认为它们是TTY driver的一部分 TTY设备 对于每一个终端，TTY driver都会创建一个TTY设备与它对应，如果有多个终端连接过来，那么看起来就是这个样子的： 12345678910111213141516 +----------------+ | TTY Driver | | | | +-------+ | +----------------++------------+ | | |&lt;----------&gt;| User process A || Terminal A |&lt;---------&gt;| ttyS0 | | +----------------++------------+ | | |&lt;----------&gt;| User process B | | +-------+ | +----------------+ | | | +-------+ | +----------------++------------+ | | |&lt;----------&gt;| User process C || Terminal B |&lt;---------&gt;| ttyS1 | | +----------------++------------+ | | |&lt;----------&gt;| User process D | | +-------+ | +----------------+ | | +----------------+ 当驱动收到一个终端的连接时，就会根据终端的型号和参数创建相应的tty设备（上图中设备名称叫ttyS0是因为大部分终端的连接都是串行连接），由于每个终端可能都不一样，有自己的特殊命令和使用习惯，于是每个tty设备的配置可能都不一样。比如按delete键的时候，有些可能是要删前面的字符，而有些可能是删后面的，如果没配置对，就会导致某些按键不是自己想要的行为，这也是我们在使用模拟终端时，如果默认的配置跟我们的习惯不符，需要做一些个性化配置的原因。 后来随着计算机的不断发展，teletype这些设备逐渐消失，我们不再需要专门的终端设备了，每个机器都有自己的键盘和显示器，每台机器都可以是其它机器的终端，远程的操作通过ssh来实现，但是内核TTY驱动这一架构没有发生变化，我们想要和系统中的进程进行I/O交互，还是需要通过TTY设备，于是出现了各种终端模拟软件，并且模拟的也是常见的几种终端，如VT100、VT220、XTerm等。 可以通过命令toe -a列出系统支持的所有终端类型 可以通过命令infocmp来比较两个终端的区别，比如infocmp vt100 vt220将会输出vt100和vt220的区别。 程序如何和TTY打交道 在讨论TTY设备是如何被创建及配置之前，我们先来看看TTY是如何被进程使用的： 123456789101112131415161718#先用tty命令看看当前bash关联到了哪个ttydev@debian:~$ tty/dev/pts/1#看tty都被哪些进程打开了dev@debian:~$ lsof /dev/pts/1COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEbash 907 dev 0u CHR 136,1 0t0 4 /dev/pts/1bash 907 dev 1u CHR 136,1 0t0 4 /dev/pts/1bash 907 dev 2u CHR 136,1 0t0 4 /dev/pts/1bash 907 dev 255u CHR 136,1 0t0 4 /dev/pts/1lsof 1118 dev 0u CHR 136,1 0t0 4 /dev/pts/1lsof 1118 dev 1u CHR 136,1 0t0 4 /dev/pts/1lsof 1118 dev 2u CHR 136,1 0t0 4 /dev/pts/1#往tty里面直接写数据跟写标准输出是一样的效果dev@dev:~$ echo aaa &gt; /dev/pts/2aaa pts也是tty设备，它们的关系后面会介绍到 通过上面的lsof可以看出，当前运行的bash和lsof进程的stdin(0u)、stdout(1u)、stderr(2u)都绑定到了这个TTY上。 下面是tty和进程以及I/O设备交互的结构图： 123456 Input +--------------------------+ R&#x2F;W +------+-----------&gt;| |&lt;----------&gt;| bash | | pts&#x2F;1 | +------+&lt;-----------| |&lt;----------&gt;| lsof | Output | Foreground process group | R&#x2F;W +------+ +--------------------------+ 可以把tty理解成一个管道（pipe），在一端写的内容可以从另一端读取出来，反之亦然。 这里input和output可以简单的理解为键盘和显示器，后面会介绍在各种情况下input/ouput都连接的什么东西。 tty里面有一个很重要的属性，叫Foreground process group，记录了当前前端的进程组是哪一个。process group的概念会在下一篇文章中介绍，这里可以简单的认为process group里面只有一个进程。 当pts/1收到input的输入后，会检查当前前端进程组是哪一个，然后将输入放到进程组的leader的输入缓存中，这样相应的leader进程就可以通过read函数得到用户的输入 当前端进程组里面的进程往tty设备上写数据时，tty就会将数据输出到output设备上 当在shell中执行不同的命令时，前端进程组在不断的变化，而这种变化会由shell负责更新到tty设备中 从上面可以看出，进程和tty打交道很简单，只要保证后台进程不要读写tty就可以了，即写后台程序时，要将stdin/stdout/stderr重定向到其它地方（当然deamon程序还需要做很多其它处理）。 先抛出两个问题(后面有答案)： 当非前端进程组里面的进程（后台进程）往tty设备上写数据时，会发生什么？会输出到outpu上吗？ 当非前端进程组里面的进程（后台进程）从tty设备上读数据时，会发生什么？进程会阻塞吗？ TTY是如何被创建的 下面介绍几种常见的情况下tty设备是如何创建的，以及input和output设备都是啥。 键盘显示器直连（终端） 先看图再说话： 1234567891011 +-----------------------------------------+ | Kernel | | +--------+ | +----------------++----------+ | +-------------------+ | tty1 |&lt;----------&gt;| User processes || Keyboard |---------&gt;| | +--------+ | +----------------++----------+ | | Terminal Emulator |&lt;-&gt;| tty2 |&lt;----------&gt;| User processes || Monitor |&lt;---------| | +--------+ | +----------------++----------+ | +-------------------+ | tty3 |&lt;----------&gt;| User processes | | +--------+ | +----------------+ | | +-----------------------------------------+ 键盘、显示器都和内核中的终端模拟器相连，由模拟器决定创建多少tty，比如你在键盘上输入ctrl+alt+F1时，模拟器首先捕获到该输入，然后激活tty1，这样键盘的输入会转发到tty1，而tty1的输出会转发到显示器，同理用输入ctrl+alt+F2，就会切换到tty2。 当模拟器激活tty时如果发现没有进程与之关联，意味着这是第一次打开该tty，于是会启动配置好的进程并和该tty绑定，一般该进程就是负责login的进程。 当切换到tty2后，tty1里面的输出会输出到哪里呢？tty1的输出还是会输出给模拟器，模拟器里会有每个tty的缓存，不过由于模拟器的缓存空间有限，所以下次切回tty1的时候，只能看到最新的输出，以前的输出已经不在了。 不确定这里的终端模拟器对应内核中具体的哪个模块，但肯定有这么个东西存在 SSH远程访问 1234567891011121314151617181920212223242526272829+----------+ +------------+| Keyboard |------&gt;| |+----------+ | Terminal || Monitor |&lt;------| |+----------+ +------------+ | | ssh protocol | ↓ +------------+ | | | ssh server |--------------------------+ | | fork | +------------+ | | ↑ | | | | write | | read | | | | +-----|---|-------------------+ | | | | | ↓ | ↓ | +-------+ | +-------+ | +--------+ | pts&#x2F;0 |&lt;----------&gt;| shell | | | | +-------+ | +-------+ | | ptmx |&lt;-&gt;| pts&#x2F;1 |&lt;----------&gt;| shell | | | | +-------+ | +-------+ | +--------+ | pts&#x2F;2 |&lt;----------&gt;| shell | | +-------+ | +-------+ | Kernel | +-----------------------------+ 这里的Terminal可能是任何地方的程序，比如windows上的putty，所以不讨论客户端的Terminal程序是怎么和键盘、显示器交互的。由于Terminal要和ssh服务器打交道，所以肯定要实现ssh的客户端功能。 这里将建立连接和收发数据分两条线路解释，为了描述简洁，这里以sshd代替ssh服务器程序： 建立连接 Terminal请求和sshd建立连接 如果验证通过，sshd将创建一个新的session 调用API（posix_openpt()）请求ptmx创建一个pts，创建成功后，sshd将得到和ptmx关联的fd，并将该fd和session关联起来。 12345678910#pty（pseudo terminal device）由两部分构成，ptmx是master端，pts是slave端，#进程可以通过调用API请求ptmx创建一个pts，然后将会得到连接到ptmx的读写fd和一个新创建的pts，#ptmx在内部会维护该fd和pts的对应关系，随后往这个fd的读写会被ptmx转发到对应的pts。#这里可以看到sshd已经打开了&#x2F;dev&#x2F;ptmxdev@debian:~$ sudo lsof &#x2F;dev&#x2F;ptmxCOMMAND PID USER FD TYPE DEVICE SIZE&#x2F;OFF NODE NAMEsshd 1191 dev 8u CHR 5,2 0t0 6531 &#x2F;dev&#x2F;ptmxsshd 1191 dev 10u CHR 5,2 0t0 6531 &#x2F;dev&#x2F;ptmxsshd 1191 dev 11u CHR 5,2 0t0 6531 &#x2F;dev&#x2F;ptmx 同时sshd创建shell进程，将新创建的pts和shell绑定 收发消息 Terminal收到键盘的输入，Terminal通过ssh协议将数据发往sshd sshd收到客户端的数据后，根据它自己管理的session，找到该客户端对应的关联到ptmx上的fd 往找到的fd上写入客户端发过来的数据 ptmx收到数据后，根据fd找到对应的pts（该对应关系由ptmx自动维护），将数据包转发给对应的pts pts收到数据包后，检查绑定到自己上面的当前前端进程组，将数据包发给该进程组的leader 由于pts上只有shell，所以shell的read函数就收到了该数据包 shell对收到的数据包进行处理，然后输出处理结果（也可能没有输出） shell通过write函数将结果写入pts pts将结果转发给ptmx ptmx根据pts找到对应的fd，往该fd写入结果 sshd收到该fd的结果后，找到对应的session，然后将结果发给对应的客户端 键盘显示器直连（图形界面） 1234567891011121314151617181920+----------+ +------------+| Keyboard |------&gt;| |+----------+ | Terminal |--------------------------+| Monitor |&lt;------| | fork |+----------+ +------------+ | | ↑ | | | | write | | read | | | | +-----|---|-------------------+ | | | | | ↓ | ↓ | +-------+ | +-------+ | +--------+ | pts&#x2F;0 |&lt;----------&gt;| shell | | | | +-------+ | +-------+ | | ptmx |&lt;-&gt;| pts&#x2F;1 |&lt;----------&gt;| shell | | | | +-------+ | +-------+ | +--------+ | pts&#x2F;2 |&lt;----------&gt;| shell | | +-------+ | +-------+ | Kernel | +-----------------------------+ 为了简化起见，本篇不讨论Linux下图形界面里Terminal程序是怎么和键盘、显示器交互的。 这里和上面的不同点就是，这里的Terminal不需要实现ssh客户端，但需要把ssh服务器要干的活也干了（当然ssh通信相关的除外）。 SSH + Screen/Tmux 常用Linux的同学应该对screen和tmux不陌生，通过它们启动的进程，就算网络断开了，也不会受到影响继续执行，下次连上去时还能看到进程的所有输出，还能继续接着干活。 这里以tmux为例介绍其原理： 1234567891011121314151617181920212223242526272829303132333435363738+----------+ +------------+| Keyboard |------&gt;| |+----------+ | Terminal || Monitor |&lt;------| |+----------+ +------------+ | | ssh protocol | ↓ +------------+ | | | ssh server |--------------------------+ | | fork | +------------+ | | ↑ | | | | write | | read | | | | +-----|---|-------------------+ | | ↓ | | ↓ | +--------+ +-------+ | +-------+ fork +-------------+ | | ptmx |&lt;-&gt;| pts&#x2F;0 |&lt;----------&gt;| shell |--------&gt;| tmux client | | +--------+ +-------+ | +-------+ +-------------+ | | | | ↑ | +--------+ +-------+ | +-------+ | | | ptmx |&lt;-&gt;| pts&#x2F;2 |&lt;----------&gt;| shell | | | +--------+ +-------+ | +-------+ | | ↑ | Kernel | ↑ | +-----|---|-------------------+ | | | | | | |w&#x2F;r| +---------------------------+ | | | | fork | | ↓ | | +-------------+ | | | | | tmux server |&lt;--------------------------------------------+ | | +-------------+ 系统中的ptmx只有一个，上图中画出来了两个，目的是为了表明tmux服务器和sshd都用ptmx，但它们之间又互不干涉。 这种情况要稍微复杂一点，不过原理都是一样的，前半部分和普通ssh的方式是一样的，只是pts/0关联的前端进程不是shell了，而是变成了tmux客户端，所以ssh客户端发过来的数据包都会被tmux客户端收到，然后由tmux客户端转发给tmux服务器，而tmux服务器干的活和ssh的类似，也是维护一堆的session，为每个session创建一个pts，然后将tmux客户端发过来的数据转发给相应的pts。 由于tmux服务器只和tmux客户端打交道，和sshd没有关系，当终端和sshd的连接断开时，虽然pts/0会被关闭，和它相关的shell和tmux客户端也将被kill掉，但不会影响tmux服务器，当下次再用tmux客户端连上tmux服务器时，看到的还是上次的内容。 TTY和PTS的区别 从上面的流程中应该可以看出来了，对用户空间的程序来说，他们没有区别，都是一样的；从内核里面来看，pts的另一端连接的是ptmx，而tty的另一端连接的是内核的终端模拟器，ptmx和终端模拟器都只是负责维护会话和转发数据包；再看看ptmx和内核终端模拟器的另一端，ptmx的另一端连接的是用户空间的应用程序，如sshd、tmux等，而内核终端模拟器的另一端连接的是具体的硬件，如键盘和显示器。 常见的TTY配置 先先来看看当前tty的所有配置： 1234567dev@dev:~$ stty -aspeed 38400 baud; rows 51; columns 204; line &#x3D; 0;intr &#x3D; ^C; quit &#x3D; ^\\; erase &#x3D; ^?; kill &#x3D; ^U; eof &#x3D; ^D; eol &#x3D; M-^?; eol2 &#x3D; M-^?; swtch &#x3D; &lt;undef&gt;; start &#x3D; ^Q; stop &#x3D; ^S; susp &#x3D; ^Z; rprnt &#x3D; ^R; werase &#x3D; ^W; lnext &#x3D; ^V; discard &#x3D; ^O; min &#x3D; 1; time &#x3D; 0;-parenb -parodd -cmspar cs8 -hupcl -cstopb cread -clocal -crtscts-ignbrk -brkint -ignpar -parmrk -inpck -istrip -inlcr -igncr icrnl ixon -ixoff -iuclc ixany imaxbel -iutf8opost -olcuc -ocrnl onlcr -onocr -onlret -ofill -ofdel nl0 cr0 tab0 bs0 vt0 ff0isig icanon iexten echo echoe echok -echonl -noflsh -xcase -tostop -echoprt echoctl echoke -flusho -extproc stty还可以用来修改tty的参数，用法请参考man stty 只要是有权限的程序，都可以通过Linux提供的API来修改TTY的配置，下面介绍一些常见的的配置项。 rows 51; columns 204; 这个配置一般由终端控制，当终端的窗口大小发生变化时，需要通过一定的手段修改该配置，比如ssh协议里面就有修改窗口大小的参数，sshd收到客户端的请求后，会通过API修改tty的这个参数，然后由tty通过信号SIGWINCH通知前端程序（比如shell或者vim），前端程序收到信号后，再去读tty的这个参数，然后就知道如何调整自己的输出排版了。 intr = ^C tty除了在终端和前端进程之间转发数据之外，还支持很多控制命令，比如终端输入了CTRL+C，那么tty不会将该输入串转发给前端进程，而是将它转换成信号SIGINT发送给前端进程。这个就是用来配置控制命令对应的输入组合的，比如我们可以配置“intr = ^E”表示用CTRL+E代替CTRL+C。 start = ^Q; stop = ^S; 这是两个特殊的控制命令，估计经常有人会碰到，在键盘上不小心输入CTRL+S后，终端没反应了，即没输出，也不响应任何输入。这是因为这个命令会告诉TTY暂停，阻塞所有读写操作，即不转发任何数据，只有按了CTRL+Q后，才会继续。这个功能应该是历史遗留，以前终端和服务器之间没有流量控制功能，所以有可能服务器发送数据过快，导致终端处理不过来，于是需要这样一个命令告诉服务器不要再发了，等终端处理完了后在通知服务器继续。 该命令现在比较常用的一个场景就是用tail -f命令监控日志文件的内容时，可以随时按CTRL+S让屏幕停止刷新，看完后再按CTRL+Q让它继续刷，如果不这样的话，需要先CTRL+C退出，看完后在重新运行tail -f命令。 echo 在终端输入字符的时候，之所以我们能及时看到我们输入的字符，那是因为TTY在收到终端发过去的字符后，会先将字符原路返回一份，然后才交给前端进程处理，这样终端就能及时的显示输入的字符。echo就是用来控制该功能的配置项，如果是-echo的话表示disable echo功能。 -tostop 如果你在shell中运行程序的时候，后面添加了&amp;，比如./myapp &amp;，这样myapp这个进程就会在后台运行，但如果这个进程继续往tty上写数据呢？这个参数就用来控制是否将输出转发给终端，也即结果会不会在终端显示，这里“-tostop”表示会输出到终端，如果配置为“tostop”的话，将不输出到终端，并且tty会发送信号SIGTTOU给myapp，该信号的默认行为是将暂停myapp的执行。 TTY相关信号 除了上面介绍配置时提到的SIGINT，SIGTTOU，SIGWINCHU外，还有这么几个跟TTY相关的信号 SIGTTIN 当后台进程读tty时，tty将发送该信号给相应的进程组，默认行为是暂停进程组中进程的执行。暂停的进程如何继续执行呢？请参考下一篇文章中的SIGCONT。 SIGHUP 当tty的另一端挂掉的时候，比如ssh的session断开了，于是sshd关闭了和ptmx关联的fd，内核将会给和该tty相关的所有进程发送SIGHUP信号，进程收到该信号后的默认行为是退出进程。 SIGTSTP 终端输入CTRL+Z时，tty收到后就会发送SIGTSTP给前端进程组，其默认行为是将前端进程组放到后端，并且暂停进程组里所有进程的执行。 跟tty相关的信号都是可以捕获的，可以修改它的默认行为 结束语 本文介绍了常见的tty功能和特点，下一篇中将详细介绍和tty密切相关的进程session id，进程组，job，后台程序等，敬请期待。 参考 The TTY demystified 本文地址：http://xnerv.wang/linux-tty-pts-summary/ 转载自：(SegmentFault) Linux TTY/PTS概述","categories":[{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/categories/Linux/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/tags/Linux/"},{"name":"Shell","slug":"Shell","permalink":"https://xnerv.wang/tags/Shell/"},{"name":"TTY","slug":"TTY","permalink":"https://xnerv.wang/tags/TTY/"},{"name":"PTS","slug":"PTS","permalink":"https://xnerv.wang/tags/PTS/"}]},{"title":"DDL commands in Transactions in SQL Server versus Oracle（转载）","slug":"ddl-commands-in-transactions-in-sql-server-versus-oracle","date":"2016-12-09T08:00:00.000Z","updated":"2023-08-21T02:24:18.665Z","comments":true,"path":"ddl-commands-in-transactions-in-sql-server-versus-oracle/","link":"","permalink":"https://xnerv.wang/ddl-commands-in-transactions-in-sql-server-versus-oracle/","excerpt":"Problem Transactions are widely used in database development, especially when a database supports an application where a lot of correlated changes are needed. A transaction is a group of SQL statements that are all committed together (all changes done by these statements become a permanent part of the database) or none of these statements are committed (all these changes are rolled back). More often statements included in transaction are DML (Data Manipulation Language) statements, such as INSERT, UPDATE, DELETE and so on. But what about DDL (Data Definition Language) statements? Is it possible to include DDL commands such as CREATE, ALTER, DROP, etc., in a transaction? In this tip we are going to answer these questions for both - MS SQL Server and Oracle databases. Solution The approaches to use DDL commands within transactions are quite different in Microsoft SQL Server vs. Oracle. Let’s discuss this for each RDBMS separately.","text":"Problem Transactions are widely used in database development, especially when a database supports an application where a lot of correlated changes are needed. A transaction is a group of SQL statements that are all committed together (all changes done by these statements become a permanent part of the database) or none of these statements are committed (all these changes are rolled back). More often statements included in transaction are DML (Data Manipulation Language) statements, such as INSERT, UPDATE, DELETE and so on. But what about DDL (Data Definition Language) statements? Is it possible to include DDL commands such as CREATE, ALTER, DROP, etc., in a transaction? In this tip we are going to answer these questions for both - MS SQL Server and Oracle databases. Solution The approaches to use DDL commands within transactions are quite different in Microsoft SQL Server vs. Oracle. Let’s discuss this for each RDBMS separately. DDL and Transactions in Microsoft SQL Server Generally it is possible to include DDL statements in one transaction in Microsoft SQL Server. However, there are exceptions: some DDL statements are not allowed in transactions, for example CREATE/ALTER/DROP DATABASE commands, CREATE/ALTER/DROP FULLTEXT INDEX and so on. When including DDL statements in a transaction, like DML commands they all are either committed or rolled back. This means that it is possible to ROLLBACK a created table or ROLLBACK truncated data. Let’s prepare a database for testing and see an example: 1234567891011121314151617181920212223242526USE masterGOCREATE DATABASE TestDBGOUSE TestDBGOCREATE TABLE TableA( ID INT NOT NULL PRIMARY KEY, Value CHAR(1))INSERT INTO TableA(ID, Value)VALUES (1,&#x27;A&#x27;),(2,&#x27;B&#x27;),(3, &#x27;C&#x27;)CREATE TABLE TableB( ID INT NOT NULL PRIMARY KEY, Value CHAR(1))INSERT INTO TableB(ID, Value)VALUES (1,&#x27;X&#x27;),(2,&#x27;Y&#x27;),(3, &#x27;Z&#x27;) We have two tables with data in our TestDB database. Now let’s start a transaction and do some DDL changes in the TestDB database: 1234567891011121314151617USE TestDBGOBEGIN TRANSACTION TRUNCATE TABLE TableA DROP TABLE TableB CREATE TABLE TableC(ID INT)ROLLBACKSELECT * FROM TableASELECT * FROM TableBSELECT * FROM TableC We can see that after the rollback there is no TableC in the database: TableA and TableB exist and contain data: This means that all these changes made by the DDL commands, that are included in the transaction, have been rolled back. So, we can include DDL commands (with some exceptions) in transactions in MS SQL Server. DDL and Transactions in Oracle Unlike SQL Server, transactions in Oracle are always implicit. This means that a logical transaction starts in the event of a data change in the database. The other big difference from SQL Server is that in Oracle DDL commands automatically commit transactions (xnerv: So it’s not weird that Oracle bought MySQL since InnoDB has the same behavior… Just a joke : ). Every new database connection opens a new transaction and an explicit COMMIT command is needed to make them a permanent part of database (as mentioned above DDL commands automatically COMMIT a transaction and in this case an explicit COMMIT is not needed). Commands issued after a COMMIT open a new transaction and so on. Let’s look at an example: 1234567891011CREATE TABLE TableA (Value INT);INSERT INTO TableA(Value) VALUES(1);INSERT INTO TableA(Value) VALUES(2);INSERT INTO TableA(Value) VALUES(3);SELECT * FROM V$TRANSACTION WHERE STATUS=&#x27;ACTIVE&#x27;;COMMIT;SELECT * FROM V$TRANSACTION WHERE STATUS=&#x27;ACTIVE&#x27;; In this example TableA is created and after that 3 rows are inserted. We can see that there is one active transaction after the INSERT statements and after the COMMIT there are no active transactions: In the next script we’ll issue a DELETE command and after that we create a new table. After creating TableB, we delete one more row from TableA and then issue a ROLLBACK command: 12345678910111213141516DELETE FROM TableA WHERE Value=2;SELECT * FROM V$TRANSACTION WHERE STATUS=&#x27;ACTIVE&#x27;;CREATE TABLE TableB (Value INT);DELETE FROM TableA WHERE Value=3;ROLLBACK;SELECT * FROM V$TRANSACTION WHERE STATUS=&#x27;ACTIVE&#x27;;SELECT * FROM TableA;SELECT * FROM TableB;SELECT * FROM V$TRANSACTION WHERE STATUS=&#x27;ACTIVE&#x27;; As a result, we can see that although a ROLLBACK is issued, TableB is created and the row with Value=2 from TableA is deleted. The reason is that the CREATE TABLE command which is a DDL command commits the transaction. However, the second DELETE statement has been rolled back, because it is issued after the DDL and before the ROLLBACK command. So, now it becomes obvious that DDL commands in Oracle cannot be rolled back and included in one transaction. Create multiple tables in a single transaction in Oracle Sometimes it is necessary to create more than one table in a single transaction. So, how can we solve this problem in Oracle? We can do that by using a CREATE SCHEMA statement. This statement allows us to include multiple CREATE TABLE or CREATE VIEW statements as well as multiple GRANT statements in a single transaction in your own schema. If all statements in the CREATE SCHEMA are executed successfully the transaction is committed. In case of an error, all commands included in the CREATE SCHEMA statement are rolled back. It is important to note that CREATE SCHEMA statement does not create a schema. It’s used to create tables, views or grant privileges in one transaction. In the example below, we are using the CREATE SCHEMA statement to create two tables in one transaction. Before executing this statement it is essential to make sure that you are creating objects in your own schema and have necessary permissions to issue the statements: 123CREATE SCHEMA AUTHORIZATION MyUserCREATE TABLE TableC (Value INT)CREATE TABLE TableD (Value INT); Two tables has been successfully created: In the next example we are trying to create two tables: 123CREATE SCHEMA AUTHORIZATION MyUserCREATE TABLE TableC (Value INT)CREATE TABLE TableE (Value INT); The transaction is rolled back (because TableC already exists), therefore TableE has not been created: Conclusion In conclusion, the SQL Server and Oracle database engines manage transactions and DDL commands differently. As we can see in this tip SQL Server allows us to include multiple DDL commands in a single transaction in contrast to Oracle. The latter commits transactions when a DDL command is issued, so it is not possible to combine DDL statements in one transaction. However, in Oracle it is possible to issue multiple table and view creation statements, as well as multiple grant statements in a single transaction by using the CREATE SCHEMA statement. Next Steps Check out this related information: https://msdn.microsoft.com/en-us/library/ms174377.aspx https://msdn.microsoft.com/en-us/library/ms188929.aspx https://msdn.microsoft.com/en-us/library/ff848799.aspx https://msdn.microsoft.com/en-us/library/ms191544.aspx https://docs.oracle.com/cd/B19306_01/appdev.102/b14261/sqloperations.htm#i7105 https://docs.oracle.com/database/121/CNCPT/transact.htm#CNCPT89320 https://docs.oracle.com/cd/B19306_01/server.102/b14200/statements_6014.htm 本文地址：http://xnerv.wang/ddl-commands-in-transactions-in-sql-server-versus-oracle/ 转载自：DDL commands in Transactions in SQL Server versus Oracle","categories":[{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/categories/DBMS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/tags/DBMS/"},{"name":"SQL Server","slug":"SQL-Server","permalink":"https://xnerv.wang/tags/SQL-Server/"},{"name":"Oracle","slug":"Oracle","permalink":"https://xnerv.wang/tags/Oracle/"},{"name":"Transaction","slug":"Transaction","permalink":"https://xnerv.wang/tags/Transaction/"}]},{"title":"缓存更新的套路（转载）","slug":"cache-update-routines","date":"2016-07-27T07:00:00.000Z","updated":"2023-08-21T02:24:20.689Z","comments":true,"path":"cache-update-routines/","link":"","permalink":"https://xnerv.wang/cache-update-routines/","excerpt":"看到好些人在写更新缓存数据代码时，先删除缓存，然后再更新数据库，而后续的操作会把数据再装载的缓存中。然而，这个是逻辑是错误的。试想，两个并发操作，一个是更新操作，另一个是查询操作，更新操作删除缓存后，查询操作没有命中缓存，先把老数据读出来后放到缓存中，然后更新操作更新了数据库。于是，在缓存中的数据还是老的数据，导致缓存中的数据是脏的，而且还一直这样脏下去了。 我不知道为什么这么多人用的都是这个逻辑，当我在微博上发了这个贴以后，我发现好些人给了好多非常复杂和诡异的方案，所以，我想写这篇文章说一下几个缓存更新的Design Pattern（让我们多一些套路吧）。 这里，我们先不讨论更新缓存和更新数据这两个事是一个事务的事，或是会有失败的可能，我们先假设更新数据库和更新缓存都可以成功的情况（我们先把成功的代码逻辑先写对）。 更新缓存的的Design Pattern有四种：Cache aside, Read through, Write through, Write behind caching，我们下面一一来看一下这四种Pattern。","text":"看到好些人在写更新缓存数据代码时，先删除缓存，然后再更新数据库，而后续的操作会把数据再装载的缓存中。然而，这个是逻辑是错误的。试想，两个并发操作，一个是更新操作，另一个是查询操作，更新操作删除缓存后，查询操作没有命中缓存，先把老数据读出来后放到缓存中，然后更新操作更新了数据库。于是，在缓存中的数据还是老的数据，导致缓存中的数据是脏的，而且还一直这样脏下去了。 我不知道为什么这么多人用的都是这个逻辑，当我在微博上发了这个贴以后，我发现好些人给了好多非常复杂和诡异的方案，所以，我想写这篇文章说一下几个缓存更新的Design Pattern（让我们多一些套路吧）。 这里，我们先不讨论更新缓存和更新数据这两个事是一个事务的事，或是会有失败的可能，我们先假设更新数据库和更新缓存都可以成功的情况（我们先把成功的代码逻辑先写对）。 更新缓存的的Design Pattern有四种：Cache aside, Read through, Write through, Write behind caching，我们下面一一来看一下这四种Pattern。 Cache Aside Pattern 这是最常用最常用的pattern了。其具体逻辑如下： 失效：应用程序先从cache取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。 命中：应用程序从cache中取数据，取到后返回。 更新：先把数据存到数据库中，成功后，再让缓存失效。 注意，我们的更新是先更新数据库，成功后，让缓存失效。那么，这种方式是否可以没有文章前面提到过的那个问题呢？我们可以脑补一下。 一个是查询操作，一个是更新操作的并发，首先，没有了删除cache数据的操作了，而是先更新了数据库中的数据，此时，缓存依然有效，所以，并发的查询操作拿的是没有更新的数据，但是，更新操作马上让缓存的失效了，后续的查询操作再把数据从数据库中拉出来。而不会像文章开头的那个逻辑产生的问题，后续的查询操作一直都在取老的数据。 这是标准的design pattern，包括Facebook的论文《Scaling Memcache at Facebook》也使用了这个策略。为什么不是写完数据库后更新缓存？你可以看一下Quora上的这个问答《Why does Facebook use delete to remove the key-value pair in Memcached instead of updating the Memcached during write request to the backend?》，主要是怕两个并发的写操作导致脏数据。 那么，是不是Cache Aside这个就不会有并发问题了？不是的，比如，一个是读操作，但是没有命中缓存，然后就到数据库中取数据，此时来了一个写操作，写完数据库后，让缓存失效，然后，之前的那个读操作再把老的数据放进去，所以，会造成脏数据。 但，这个case理论上会出现，不过，实际上出现的概率可能非常低，因为这个条件需要发生在读缓存时缓存失效，而且并发着有一个写操作。而实际上数据库的写操作会比读操作慢得多，而且还要锁表，而读操作必需在写操作前进入数据库操作，而又要晚于写操作更新缓存，所有的这些条件都具备的概率基本并不大。 所以，这也就是Quora上的那个答案里说的，要么通过2PC或是Paxos协议保证一致性，要么就是拼命的降低并发时脏数据的概率，而Facebook使用了这个降低概率的玩法，因为2PC太慢，而Paxos太复杂。当然，最好还是为缓存设置上过期时间。 Read/Write Through Pattern 我们可以看到，在上面的Cache Aside套路中，我们的应用代码需要维护两个数据存储，一个是缓存（Cache），一个是数据库（Repository）。所以，应用程序比较啰嗦。而Read/Write Through套路是把更新数据库（Repository）的操作由缓存自己代理了，所以，对于应用层来说，就简单很多了。可以理解为，应用认为后端就是一个单一的存储，而存储自己维护自己的Cache。 Read Through Read Through 套路就是在查询操作中更新缓存，也就是说，当缓存失效的时候（过期或LRU换出），Cache Aside是由调用方负责把数据加载入缓存，而Read Through则用缓存服务自己来加载，从而对应用方是透明的。 Write Through Write Through 套路和Read Through相仿，不过是在更新数据时发生。当有数据更新的时候，如果没有命中缓存，直接更新数据库，然后返回。如果命中了缓存，则更新缓存，然后再由Cache自己更新数据库（这是一个同步操作） 下图自来Wikipedia的Cache词条。其中的Memory你可以理解为就是我们例子里的数据库。 Write Behind Caching Pattern Write Behind 又叫 Write Back。**一些了解Linux操作系统内核的同学对write back应该非常熟悉，这不就是Linux文件系统的Page Cache的算法吗？是的，你看基础这玩意全都是相通的。**所以，基础很重要，我已经不是一次说过基础很重要这事了。 Write Back套路，一句说就是，在更新数据的时候，只更新缓存，不更新数据库，而我们的缓存会异步地批量更新数据库。这个设计的好处就是让数据的I/O操作飞快无比（因为直接操作内存嘛 ），因为异步，write backg还可以合并对同一个数据的多次操作，所以性能的提高是相当可观的。 但是，其带来的问题是，数据不是强一致性的，而且可能会丢失（我们知道Unix/Linux非正常关机会导致数据丢失，就是因为这个事）。在软件设计上，我们基本上不可能做出一个没有缺陷的设计，就像算法设计中的时间换空间，空间换时间一个道理，有时候，强一致性和高性能，高可用和高性性是有冲突的。软件设计从来都是取舍Trade-Off。 另外，Write Back实现逻辑比较复杂，因为他需要track有哪数据是被更新了的，需要刷到持久层上。操作系统的write back会在仅当这个cache需要失效的时候，才会被真正持久起来，比如，内存不够了，或是进程退出了等情况，这又叫lazy write。 在wikipedia上有一张write back的流程图，基本逻辑如下： 再多唠叨一些 1）上面讲的这些Design Pattern，其实并不是软件架构里的mysql数据库和memcache/redis的更新策略，这些东西都是计算机体系结构里的设计，比如CPU的缓存，硬盘文件系统中的缓存，硬盘上的缓存，数据库中的缓存。基本上来说，这些缓存更新的设计模式都是非常老古董的，而且历经长时间考验的策略，所以这也就是，工程学上所谓的Best Practice，遵从就好了。 2）有时候，我们觉得能做宏观的系统架构的人一定是很有经验的，其实，宏观系统架构中的很多设计都来源于这些微观的东西。比如，云计算中的很多虚拟化技术的原理，和传统的虚拟内存不是很像么？Unix下的那些I/O模型，也放大到了架构里的同步异步的模型，还有Unix发明的管道不就是数据流式计算架构吗？TCP的好些设计也用在不同系统间的通讯中，仔细看看这些微观层面，你会发现有很多设计都非常精妙……所以，请允许我在这里放句观点鲜明的话——如果你要做好架构，首先你得把计算机体系结构以及很多老古董的基础技术吃透了。 3）在软件开发或设计中，我非常建议在之前先去参考一下已有的设计和思路，看看相应的guideline，best practice或design pattern，吃透了已有的这些东西，再决定是否要重新发明轮子。千万不要似是而非地，想当然的做软件设计。 4）上面，我们没有考虑缓存（Cache）和持久层（Repository）的整体事务的问题。比如，更新Cache成功，更新数据库失败了怎么吗？或是反过来。关于这个事，如果你需要强一致性，你需要使用“两阶段提交协议”——prepare, commit/rollback，比如Java 7 的XAResource，还有MySQL 5.7的 XA Transaction，有些cache也支持XA，比如EhCache。当然，XA这样的强一致性的玩法会导致性能下降，关于分布式的事务的相关话题，你可以看看《分布式系统的事务处理》一文。 本文地址：http://xnerv.wang/cache-update-routines/ 转载自：缓存更新的套路","categories":[{"name":"分布式及存储","slug":"分布式及存储","permalink":"https://xnerv.wang/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8F%8A%E5%AD%98%E5%82%A8/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"分布式及存储","slug":"分布式及存储","permalink":"https://xnerv.wang/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8F%8A%E5%AD%98%E5%82%A8/"},{"name":"缓存","slug":"缓存","permalink":"https://xnerv.wang/tags/%E7%BC%93%E5%AD%98/"}]},{"title":"在调试器里看百度云管家（转载）","slug":"debugging-baidu-pan","date":"2016-06-09T07:00:00.000Z","updated":"2023-08-21T02:24:20.056Z","comments":true,"path":"debugging-baidu-pan/","link":"","permalink":"https://xnerv.wang/debugging-baidu-pan/","excerpt":"","text":"因为太了解软件，我很慎重在自己的电脑上安装新软件。大约半年前，有朋友通过百度云盘向我传递dump文件。点击链接下载时失败，提示超过了普通方式允许的上限，必须安装百度云盘客户端软件。于是我的电脑新增了一个软件，名曰“百度云管家”。第一次看到这个名字，就觉得很奇怪，云的家在服务器上，为什么一个终端应用程序叫云管家呢？ 过了一段时间，我慢慢意识到，原来这位云管家管的不是云的家，而是我（用户电脑）的家。名字的含义或曰：“我是百度云，来管你的家”。 为什么这么认为呢？最初的原因是我发现这个管家特别忙碌，即使当我根本没有使用百度云。更让我跌破眼镜的是，即使我把网线拔掉、关闭无线，它依然忙碌。这些反常的表现让我不得不留意它了。多少次，我打开任务管理器，看它忙碌的身影。多少次，我想大声对它说：“管家大哥，你歇歇，告诉我你在忙啥？” 怎一个忙字了得 起初是在任务管理器中发现百度云管家（以下简称其“管家程序”）很忙。图1是我某次看见它忙时做的截图。在这个截图中，系统中一共运行了175个进程，任务列表是按缺页异常总数（Page Faults）排名，管家程序排名第一位，而且遥遥领先，把一向排名靠前的McAfee安全软件（第二名和第三名）远远抛在后头（相差一个数量级）。顺便说下，排在第4位的BaiduProtect是管家程序的同门兄弟，以后台服务方式运行，权限更高。 在Page Faults右侧的那一列是PF Delta，代表最近一秒钟新增的缺页异常个数，管家程序新增4千多个，但这并不是我看到的最高值，有时是7000多。再往右的一列是CPU净时间，即CPU执行管家程序的累计时间，14分37秒。这个数值也算较高了，因为系统中CPU频率高达2.6 GHz，速度很快，排在后面的很多程序（图1中未显示出）的累计时间还不到1秒。排在第二名的安全软件CPU累计时间是1小时55分42秒，比管家程序还高很多。如果把缺页异常总数除以CPU净时间，便得到缺页异常与CPU净时间的比率。这个比率反应了CPU执行程序时触发缺页异常的频繁程度，不妨将其称为缺页异常净频率。为排名前两位的两个程序计算这个指标，其结果如下： 12340:000&gt; ?? 215415833&#x2F;(14*60+37)int 0n2456280:000&gt; ?? 34064443&#x2F;(115*60+42)int 0n4907 可以看出管家程序触发缺页异常的净频率高得惊人，达到24万多次。这意味着CPU平均执行这个程序1秒钟就触发24万多个缺页异常。这也意味着，CPU花在这个程序上的时间有很多都用在了处理缺页异常上。 图2是使用Mark Russinovich先生的Process Explorer来观察管家程序的截图，显示的是管家程序的线程信息。 可以看到，管家程序有四个很活跃的线程，它们的CPU占用率都超过了0.1%。图2中第1列是线程ID，第2列是CPU占用率，第3列是Cycles Delta，即最近一秒钟CPU执行这个线程所用的时钟个数。从Windows Vista开始，NT内核会读取现代处理器的性能计数器来统计CPU花在每个线程上的时钟个数。根据图2，最近1秒里，管家程序的前4个线程使用的CPU时钟数分别为1千4百万、1千5百万、3千2百万和1亿零5百万。 图2下方是排名第一的8864号线程的更多数据，其中的Kernel和User分别是内核态净时间（23秒多）和用户态净时间（1分23秒多）。Context Switches是用户态和内核态之间切换的次数，高达3千1百多万次。左下角的Cycles是CPU执行该线程时所用的总时钟个数，7万多亿个。今天的x86处理器使用的超标量架构有4个发射端口，每次可以发出四条指令乱序执行，这意味着每个时钟周期可能执行多达四条指令。对于比较差的情况，平均每条指令所用的时钟周期（即所谓的CPI指标，Cycles Per Instruction）可能为3。按CPI为3来折算一下，CPU在这个线程上执行的指令数多达2万多亿条。2万多亿条指令是什么概念呢？曾经轰动信息产业的著名CIH病毒，总指令数只有几百条。即使按1千条来说，那么2万多亿条指令相当于把CIH病毒执行了20多亿次。 图3是使用Intel的著名调优工具VTune分析管家程序时得到的线程信息。每行代表一个线程。需要说明的是，因为图2与图3是针对管家程序的不同运行实例，所以无法用线程ID把两个线程对应起来。但观察到的结论是一致的，从VTune视图来看，也是有四个线程很繁忙，而且有很频繁的线程上下文切换。VTune视图给我们的另一个信息是，有多个线程的执行过程都很有规律，尤其是第四个，每隔大约1秒（横轴为时间，单位为秒）有个尖峰，这说明该线程很可能是受定时器触发来工作的。 上调试器 上面使用多个工具观察管家程序得到的结论都是它很忙碌。但不是很清楚到底是在忙什么？熟悉我的朋友一定想到了要上调试器。诚然，要想深刻认识软件，没有比调试器更有力的工具了。唤出WinDBG，附加到管家进程，一切顺利，先执行lm浏览模块信息（图4）。 图4中，第一行是EXE主模块，接下来的kernelbasis、kernel和kernelpromote三个模块的名字中都含有kernel字样，第一次看到这些名字让我一惊，以为与系统的kernel32和kernelbase模块有关，后来确认这是云管家自己的模块，我不禁好奇，这么高大上的名字，不知道出自哪位同行的妙想。 顺便说一下，图4中以Yun开头的YunDb和YunLogic模块也是管家程序的重要模块，后文会提到。 观察EXE模块的详细信息（图5），可以看到目前使用的是2016年3月的版本，这比我最初分析过的版本要新很多。 在已经卸载的模块列表（图6）中，可以看到一个名为AutoUpdateUtil.dll的模块多次出现，它应该是用来做自动更新的。 大致了解模块信息后，执行*观察线程信息。哇，一共40多个线程。执行*e .echo *****; ? @$tid;.ttime观察线程的执行时间信息，可以看到有几个线程的CPU累计时间都超过了秒级。这与前面使用Process Explorer看到的结果一致。 反调试与反反调试 做了以上观察后，执行g命令，希望让管家程序走走看。但是意外出现了，WinDBG很快收到了进程退出事件。第一次看到这一幕时，不禁愕然。凭借多年经验，我意识到这次的对手不一般，也是懂调试的，检测到调试器后，主动退出了。“你上调试器，我不跑了，死给你看。” 管家程序的这招反调试让我刷新了对百度同行的认识。但我并没有被这招吓到，反而兴趣更高了。不禁让我想起曾经在国内某公司的一次交流，在我演讲之后，一位同行提问，“看过了你写的《软件调试》，是否有计划写一本如何反调试的？” 调试是软件世界里的逃生通道，我真的不愿意写反调试的书。 但被逼到这里，只好出几招了。首先需要知道管家程序检测调试器的方法。先退出调试器，触发管家程序重新执行，并再附加WinDBG，然后执行x kernelbase!_debug_列出Windows系统的调试API。其中的IsDebuggerPresent是用来检测是否在被调试的最简单方法，对其设置断点，而后执行g恢复管家程序执行。 刹那之间，断点果然命中，k命令观察，真的是上文曾提到的Yun字辈模块之一YunLogic在调用这个检测调试器的API（图7）。 使用u命令观察IsDebuggerPresent函数，很短。其原理我在《软件调试》中有详细介绍，先通过TEB取得PEB，再访问PEB中的BeingDebugged字段。 12345KERNELBASE!IsDebuggerPresent:76153789 64a118000000 mov eax,dword ptr fs:[00000018h] fs:0053:00000018&#x3D;7efdd0007615378f 8b4030 mov eax,dword ptr [eax+30h]76153792 0fb64002 movzx eax,byte ptr [eax+2]76153796 c3 ret 单步跟踪到ret指令，看EAX寄存器果然为1，代表调试器存在。 120:000&gt; r eaxeax&#x3D;00000001 如果把这个结果返回给管家程序，那么它就发现被调试了，继而就会开始退出。于是，执行r eax=0，“狸猫换太子”。 这样篡改IsDebuggerPresent的结果后，再g恢复执行，发现断点再次命中，看来是“骗过”管家一次，它又一次做检查。 每次修改返回值太麻烦了。执行a命令开始交互式汇编（图8）。 WinDBG的汇编环境虽然简陋，但也足够用了，输入以下两行x86汇编后，直接按回车键结束汇编。 12Mov eax, 0Ret 再观察IsDebuggerPresent API，现在变成了下面这样： 123KERNELBASE!IsDebuggerPresent:76153789 b800000000 mov eax,07615378e c3 ret 也就是永远返回假。这样偷梁换柱之后，先bd * 禁止断点，然后再执行g命令恢复管家执行。这下它不退出了，因为它以为调试器不在。 原来管家程序的反调试设施如此单薄。看了它的模块架构，其实有一种更简单有效的反调试方法，不过老雷不想说，因为我一向不赞成反调试。 折腾堆 解除了管家程序的反调试保护之后，可以进一步寻找它忙碌的原因了。经过一番勘察，我发现管家程序忙碌的第一个原因是非常频繁地分配和释放内存。长话短说，在WinDBG中设置如下断点来监视从堆上的内存分配。 1bp ntdll!RtlAllocateHeap+5 &quot;.echo **allocating heap;r $t1&#x3D;@$t1+1; ? @$t1; kv;.if(poi(ebp+10)&gt;10000)&#123;&#125;.else&#123;gc;&#125;&quot; 先解释一下上面的断点命令，地址部分加5是为了越过函数开头的序言部分，以保证后面获取到参数值是准确的。双引号中包含了多条命令，先是显示提示信息，然后使用一个准变量来统计断点命中次数并打印出来，之后的kv是显示栈回溯，而后判断第三个参数所代表的分配大小是否超过1MB，如果超过则中断，不然则gc继续执行。 设好以上断点，恢复目标执行，发现大量信息喷涌而出，如图9所示。 等待5分钟左右，没有自动中断，说明没有发生参数超过1MB的调用，手工中断下来，可以看到$t1的累计值高达6万多次。 120:043&gt; ? @$t1Evaluate expression: 61434 &#x3D; 0000effa 如果再设置如下断点监视释放堆块的行为，那么即使过了十几分钟之后，t1的值仍然不大。 1bp ntdll!RtlFreeHeap+0x5 &quot;.echo **Releasing heap;r $t1&#x3D;@$t1-1; ? @$t1; kv; gc&quot; 这说明很多内存块是分配了后，很快又释放掉了。有经验的程序员知道，从堆上分配内存是开销比较大的操作，好的程序应该尽可能减少从堆上分配内存的次数，分配好了的堆快如果将来还可能使用，那么最好重复使用，不要释放了又分配，分配了又释放。 枚举进程 管家程序的更大问题是频繁调用很重的系统API。执行如下命令对系统的CreateToolhelp32Snapshot API设置断点。 1bp kernel32!CreateToolhelp32Snapshot &quot;.echo creating snapshot;? @$tid;r $t8&#x3D;@$t8+1;? @$t8;kv;gc&quot; 禁止其他断点后，恢复目标执行，会发现这个断点命中的也很频繁。一分钟调用了100多次，大约每秒钟调用两次，如图10所示。 熟悉Windows操作系统开发的朋友知道，CreateToolhelp32Snapshot的用途是对指定进程或者系统中的所有进程抓取快照。其函数原型为： 1bp kernel32!CreateToolhelp32Snapshot &quot;.echo creating snapshot;? @$tid;r $t8&#x3D;@$t8+1;? @$t8;kv;gc&quot; 参考图10中的kv命令结果，可以看到dwFlags参数为2，代表TH32CS_SNAPPROCESS，意为包含系统中的所有进程。 把上述断点中的gc去掉，不要自动恢复执行，断点命中后，一边观察任务管理器窗口，一边执行gu命令，执行完这个API后中断，可以发现每调用CreateToolhelp32Snapshot一次大约触发60多个缺页异常。 CreateToolhelp32Snapshot返回的是一个句柄，通常拿到这个句柄后再反复调用Process32Next API来获取每个进程的信息。设置如下断点： 1234HANDLE WINAPI CreateToolhelp32Snapshot( _In_ DWORD dwFlags, _In_ DWORD th32ProcessID); 恢复管家程序执行，可以看到以上断点果然反复命中，如图11所示。 根据老雷的试验观察，每调用一次Process32NextW API，大约会触发8次缺页异常。管家程序每调用好一次CreateToolhelp32Snapshot后，会调用165次Process32NextW，那么这两项导致的缺页异常总数加起来便是1300多次，即： 1bp kernel32!Process32NextW &quot;.echo enumerating each process;r $t9&#x3D;@$t9+1;? @$t9;gc&quot; 管家程序每秒钟会做两轮以上循环，于是便是2千多次了。值得说明的是，这个很重的循环操作发生在一个线程中，即前文所说图3中很有规律的第4个线程。有读者可能会问，如果每秒循环两次，那么图3中的尖峰应该是间隔半秒啊？其实不然，因为这个线程是连续循环两次。也就是每次唤醒后，连续做两次拍照和枚举，然后休息不到1秒再做两轮循环，如此往复。执行.ttime观察这个线程的执行时间，可以看到它的执行时间很长。 12340:017&gt; .ttimeCreated: Mon May 2 09:56:18.377 2016 (UTC + 8:00)Kernel: 0 days 0:02:45.579User: 0 days 0:00:24.679 执行~17n命令把这个线程临时挂起，恢复管家程序，再观察任务管理器，发现PF Delta（每秒钟新增的缺页异常）指标立刻降下来了，只有不到十次了。看来导致管家程序那么多的缺页异常的主要原因在于这个枚举系统进程的线程。它忙着给系统里的所有进程拍照，然后再一个个看过来。重要的是，这样的工作不是做一次，而是每秒来两轮，风雨无阻、孜孜不倦，时时刻刻关心着系统里运行着的其它进程，好辛劳的管家啊。 软件的历史不长，但软件的孩提时代已经过去了，因为今天的软件已经丧失了曾经拥有的简单和纯真，变得复杂、贪婪和狡黠。 一年多之前，我曾写过一篇《在调试器里看阿里的软件兵团》，批评了支付宝客户端软件中的性能问题，文章发表后，很高兴看到阿里的同行不断改进，今天已经不再有当时的问题了（图1中还可以看到淘宝的TBSecSvc进程，排名已经比较靠后）。不知百度的同行看过此文有何感想？作为一款客户端软件，能帮助用户管家是好想法，但是管家毕竟是仆人，有事时应该尽心给主人办事，没事时应该安安静静休息，不要肆意挥霍主人家的东西。 本文地址：http://xnerv.wang/debugging-baidu-pan/ 转载自：在调试器里看百度云管家","categories":[{"name":"WinDbg","slug":"WinDbg","permalink":"https://xnerv.wang/categories/WinDbg/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/tags/Windows/"},{"name":"WinDbg","slug":"WinDbg","permalink":"https://xnerv.wang/tags/WinDbg/"}]},{"title":"静态库那些事儿/MT /MD（转载）","slug":"something-about-static-library-mt-md","date":"2016-03-08T18:46:54.000Z","updated":"2023-08-21T02:24:20.395Z","comments":true,"path":"something-about-static-library-mt-md/","link":"","permalink":"https://xnerv.wang/something-about-static-library-mt-md/","excerpt":"单例模式是一种很简单常用的设计模式,常见的做法可能是这样: 12345Renderer&amp; getInstance()&#123; static Renderer renderer; return renderer;&#125; 当然,这个代码在不支持static local variable thread-safe init的编译器上,是没有办法保证线程安全的,c++11标准已经规定static local variable只会被初始化一次了,然而vs2013还没有实现,vs2015里才支持了这条标准.不过这条代码在我们的程序里只有一个线程访问,所以也就不存在线程安全的问题.","text":"单例模式是一种很简单常用的设计模式,常见的做法可能是这样: 12345Renderer&amp; getInstance()&#123; static Renderer renderer; return renderer;&#125; 当然,这个代码在不支持static local variable thread-safe init的编译器上,是没有办法保证线程安全的,c++11标准已经规定static local variable只会被初始化一次了,然而vs2013还没有实现,vs2015里才支持了这条标准.不过这条代码在我们的程序里只有一个线程访问,所以也就不存在线程安全的问题. 然后有一天,写了点代码,程序崩溃了: 1Renderer::getInstance().xxxx(); 分析发现是Renderer里的一个成员变量m_pMap == NULL了,而在项目的其他地方,也有用这个xxxx()函数的地方,竟然没有问题. 那就在xxxx()里下个断点,然后看下this指针的值吧,第一次命中时this的值是0x0456adc4, 第二次命中的时候是0x074324ad(这个地址是我随便写的,反正就是这两次命中断点this的值不一样).这是怎么回事呢?为什么出现了两个Renderer的实例呢? 细心的我发现这两个this指向的Renderer实例并没有在同一个模块内,一个在a.dll里,一个在main.exe里!造成这种现象的原因,是因为开篇的那个getInstance()方法所在工程是一个静态库,然后main.exe工程和a.dll工程均链接了这个静态库,导致main.exe里和a.dll里都存在一个renderer实例.而我们这个renderer实例在使用前,要这样: 1Renderer::getInstance().Create(); 然后才可以初始化m_pMap, 崩溃在main.exe里那行代码之前,并没有调用Create(),所以导致m_pMap == NULL崩溃了. 问题到这里已经水落石出了,想办法弄成在两个模块里共用一个实例就可以了!问题解决. 当然,如果文章就这样结束了,未免太没劲儿了吧,顺便说说另一个大家经常忽略的事情. 在开发dll的过程中,总会有意无意的写出一些跨模块分配释放内存的代码,比如在A模块malloc了一块内存,在B模块free,然后导致崩溃.然后将编译器选项由/MT改为/MD就可以解决问题.为什么会出现这种问题呢?我们来看看malloc的实现(vs2013 crt): 12345678910111213__forceinline void * __cdecl _heap_alloc (size_t size)&#123; if (_crtheap == 0) &#123;#if !defined (_CRT_APP) || defined (_DEBUG) _FF_MSGBANNER(); /* write run-time error banner */ _NMSG_WRITE(_RT_CRT_NOTINIT); /* write message */#endif /* !defined (_CRT_APP) || defined (_DEBUG) */ __crtExitProcess(255); /* normally _exit(255) */ &#125; return HeapAlloc(_crtheap, 0, size ? size : 1);&#125; malloc最后会调用HeapAlloc来分配内存,msdn看看HeapAlloc的说明, HeapAlloc Function Allocates a block of memory from a heap. The allocated memory is not movable. Syntax 12345LPVOID WINAPI HeapAlloc( __in HANDLE hHeap, __in DWORD dwFlags, __in SIZE_T dwBytes); Parameters hHeap A handle to the heap from which the memory will be allocated. This handle is returned by the HeapCreate or GetProcessHeap function. 对着msdn的说明可以知道,malloc用的heap handle是 _crtheap,这个_crtheap是个全局变量,那我们看看是什么时候给_crtheap赋值的吧. 跟着红色箭头从上往下分析,首先在BaseThreadInitThunk上下个断点,然后F5执行,断点命中之后执行 1x *!*_crtheap* 找到crtheap的地址,结果显示在0f7ec190这里,这里的值目前也是0x00000000,证明还没有被赋值,那就在这个地址上打个写断点, 1ba w4 0f7ec190 F5执行,断点命中,输入kb显示调用栈,结果发现是在_heap_init里对_crtheap赋值的,看看heap_init的代码吧: 12345678int __cdecl _heap_init (void)&#123; // Initialize the &quot;big-block&quot; heap first. if ( (_crtheap = GetProcessHeap()) == NULL ) return 0; return 1;&#125; 只是调用GetProcessHeap而已,那还得看看GetProcessHeap的实现: 12345KERNELBASE!GetProcessHeap:75415620 64a118000000 mov eax,dword ptr fs:[00000018h]75415626 8b4030 mov eax,dword ptr [eax+30h]75415629 8b4018 mov eax,dword ptr [eax+18h]7541562c c3 ret 分析汇编可知,首先去TEB + 0x18里取值赋值给eax,根据上图可知,0x18为Self指针,其实就是TEB自己的地址,然后去TEB+0x30里取值赋值给eax,由图可知,0x30为ProcessEnvironmentBlock,即PEB, 然后去PEB+0x18里取值作为返回值,也就是ProcessHeap(见下图),那我们看看PEB+0x18是多少, 由图可知GetProcessHeap会返回005d0000,然后赋值给_crtheap.那和/MT /MD有什么关系呢? /MT: Causes your application to use the multithread, static version of the run-time library. /MD: Causes your application to use the multithread- and DLL-specific version of the run-time library. 如果使用/MT,会使用静态版本的运行时库,每个模块里都会有一个_crtheap全局变量,_heap_init就会在每个模块里都被调用一次,而使用/MD则使用动态库版本的运行时库,整个进程里只有运行时库里才有一份_crtheap全局变量,在crt的dll加载的时候调用一次_heap_init.但是分析_heap_init的实现可知,就算调用多次,返回的依然是PEB里的那个堆句柄啊,也不会导致不同模块里的_crtheap有不同的值,那HeapFree和HeapAlloc使用的handle就是同一个,也不应该引起崩溃啊.事实的确如此,我也是在今天写这篇专栏用windbg分析的时候才发现这个问题.是我之前记错了还是crt的实现有变化呢?于是我查看了下vs2003的crt的_heap_init的实现: 123456789101112131415161718192021222324252627282930313233343536int __cdecl _heap_init ( int mtflag )&#123; // Initialize the &quot;big-block&quot; heap first. if ( (_crtheap = HeapCreate( mtflag ? 0 : HEAP_NO_SERIALIZE, BYTES_PER_PAGE, 0 )) == NULL ) return 0;#ifndef _WIN64 // Pick a heap, any heap __active_heap = __heap_select(); if ( __active_heap == __V6_HEAP ) &#123; // Initialize the small-block heap if (__sbh_heap_init(MAX_ALLOC_DATA_SIZE) == 0) &#123; HeapDestroy(_crtheap); return 0; &#125; &#125;#ifdef CRTDLL else if ( __active_heap == __V5_HEAP ) &#123; if ( __old_sbh_new_region() == NULL ) &#123; HeapDestroy( _crtheap ); return 0; &#125; &#125;#endif /* CRTDLL */#endif /* _WIN64 */ return 1;&#125; 这就明了了,如果在vs2003里开发dll,然后编译dll的时候选择/MT,那这个dll内部就会有一份_crtheap,那在加载这个dll的时候,会调用一次_heap_init,然后调用HeapCreate来初始化_crtheap.这也就意味着其他模块使用/MT选项也会如此,导致每个模块里的_crtheap值是不同的.这样跨模块执行free,就会导致在handle A上分配的内存去handle B上释放导致了崩溃.如果所有模块均使用/MD选项,则只有crt的dll里有一份_crtheap,在crt的dll被加载的时候执行一次HeapCreate并赋值给_crtheap,然后HeapAlloc与 HeapFree使用的HANDLE都是同一个_crtheap,就不会崩溃了. 问题到这里就分析完毕了。 本文地址：http://xnerv.wang/something-about-static-library-mt-md/ 转载自：静态库那些事儿/MT /MD","categories":[{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/categories/Windows/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/tags/Windows/"},{"name":"Heap","slug":"Heap","permalink":"https://xnerv.wang/tags/Heap/"},{"name":"DLL","slug":"DLL","permalink":"https://xnerv.wang/tags/DLL/"},{"name":"DLL Hell","slug":"DLL-Hell","permalink":"https://xnerv.wang/tags/DLL-Hell/"}]},{"title":"进程内存布局（转载）","slug":"process-memory-layout","date":"2016-03-07T07:23:00.000Z","updated":"2023-08-21T02:24:21.149Z","comments":true,"path":"process-memory-layout/","link":"","permalink":"https://xnerv.wang/process-memory-layout/","excerpt":"进程的用户空间 Windows系统中，系统空间与用户空间的分界线为0x80000000,该地址以上为系统空间，以下为用户空间，各占2GB（默认情况下）。2GB的用户空间不是全部可访问的，它被划分成了以下几个部分： 用户空间的两端各有64KB的隔离区，是不允许访问的。上端的隔离区将用户空间与系统空间隔离开，下端的隔离区就是我们编程时遇到的NULL，因此NULL实际所表示的区域不是一个地址，而是一个64KB的地址范围。这样，实际可访问的用户空间地址范围为0x10000-0x7ffeffff。","text":"进程的用户空间 Windows系统中，系统空间与用户空间的分界线为0x80000000,该地址以上为系统空间，以下为用户空间，各占2GB（默认情况下）。2GB的用户空间不是全部可访问的，它被划分成了以下几个部分： 用户空间的两端各有64KB的隔离区，是不允许访问的。上端的隔离区将用户空间与系统空间隔离开，下端的隔离区就是我们编程时遇到的NULL，因此NULL实际所表示的区域不是一个地址，而是一个64KB的地址范围。这样，实际可访问的用户空间地址范围为0x10000-0x7ffeffff。 进程的内存布局 程序加载到内存后的布局如下图所示： （注：图片来自潘爱民编著的《Windows内核原理与实现》） 程序中不同的部分在上图中占据不同的空间。主要为以下几部分： 代码段。为程序编译后的机器码指令，然后映射到进程地址空间。 已初始化数据段。在程序中已经显式初始化的全局变量和静态变量。这些变量的值在编译时保存于可执行文件中，程序加载到进程中时读取这些变量的值，映射到相应的内存空间。 未初始化数据段。程序中声明而没有初始化的全局变量和静态变量。为了缩小可执行文件的大小，这些变量的值在编译时是不会保存到可执行文件中，可执行文件中只记录了所需要的大小，加载程序时才分配空间，映射到相应的内存空间。 堆。运行时动态为变量分配的内存，如new和malloc,堆是由低地址向高地址增长的。 栈。运行时函数调用占用的内存，每个当前调用的函数占据一个栈帧，其中保存了局部变量，参数，返回地址等。栈是由高地址向低地址增长的。 《Linux/UNIX编程手册》第6章中用实例进行了讲解，这里就不重复列出来了。该章的末尾有一个练习，说示例中声明了一个10MB的数组，为什么编译后的可执行文件大小远小于10MB？如下： 1static char mbuf[10240000]; 这就是前面所说的未初始化的静态变量，在可执行文件中不会保存实际的数据，因而只占用很少的空间。 程序可自由分配的空间 不知道大家有没有产生这样的疑问，结合前面所述的，用户空间除去隔离区域，其它都是编程时可以自由支配的吗？首先明确一点，可自由支配的空间是一定的，因此，不要认为new一定会成功，如果不断地分配而不释放，它总会失败的。而且，大家编程时也遇到过无限递归的错误，无限递归同样造成栈帧的分配导致空间耗尽，所以同样会失败。所以，我们到底可以支配多少空间呢？ 回答这个问题之前，先看上面的图，想想编程时几乎必不可少的一个东西——动态链接库（DLL），我们知道，一般情况下，进程只能访问自己的地址空间，而程序要访问动态链接库（包括Windows提供的和我们自己定义的）中的函数和变量，就必须把DLL映射到自己的地址空间，因而，这些动态链接库会消耗一部分空间。此处讲一点题外话，不同的进程使用同一个DLL都会映射到自己的地址空间，但是同一个DLL在物理页面上只有一份，这是通过共享映射区（Section）实现的，使用了同一个DLL的进程的虚拟页面对应同一组物理页面。 如果程序使用到大量DLL，则会占用比较多的空间。同时，进程还有一些自己的数据需要保存，如用户空间的进程结构，这些空间会从堆上分配。因此，程序实际可以自由支配的空间还要压缩。 下面我们实际对一个进程的虚拟地址空间中的空闲页面进行统计，看看有多少可以分配的空间。Windows提供了函数VirtualQuery可以查询当前进程空间中虚拟页面的信息，具体的用法就不说了。下面是代码： 1234567891011121314151617181920212223242526272829303132333435#include &lt;Windows.h&gt;#include &lt;stdio.h&gt;int main( int argc, char* argv[] )&#123; SIZE_T retVal; // VirtualQuery返回值 MEMORY_BASIC_INFORMATION mbi; // 返回页面信息 DWORD dwStartAddress = 0x0; // 起始地址 DWORD dwTotalFreeSize = 0; // 空闲页面总大小 while( true ) &#123; ZeroMemory(&amp;mbi, sizeof(MEMORY_BASIC_INFORMATION)); retVal = VirtualQuery( (LPCVOID)dwStartAddress, &amp;mbi, sizeof(MEMORY_BASIC_INFORMATION) ); if( 0 == retVal ) // 返回0表示失败 break; // 判断mbi中的State标识，累加为MEM_FREE的区间 if( MEM_FREE == mbi.State ) &#123; dwTotalFreeSize += mbi.RegionSize; &#125; // 下一个区间 dwStartAddress += mbi.RegionSize; &#125; // 输出总的空闲页面大小 printf(&quot;%lfGB\\n&quot;, (double)dwTotalFreeSize/1024/1024/1024); system(&quot;pause...&quot;); return 0;&#125; 运行结果： 你可以分配全局变量来查看结果的变化。说明一点，上面我写的程序只统计了MEM_FREE的页面，实际上还要判断页面的访问类型，除去特殊如无法访问的页面。 本文地址：http://xnerv.wang/process-memory-layout/ 转载自：进程内存布局","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://xnerv.wang/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"操作系统","slug":"操作系统","permalink":"https://xnerv.wang/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"内存管理","slug":"内存管理","permalink":"https://xnerv.wang/tags/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"},{"name":"进程","slug":"进程","permalink":"https://xnerv.wang/tags/%E8%BF%9B%E7%A8%8B/"}]},{"title":"linux共享库的版本控制和使用（转载）","slug":"version-control-and-usage-of-linux-shared-library","date":"2015-06-28T07:00:00.000Z","updated":"2023-08-21T02:24:19.758Z","comments":true,"path":"version-control-and-usage-of-linux-shared-library/","link":"","permalink":"https://xnerv.wang/version-control-and-usage-of-linux-shared-library/","excerpt":"linux约定 经常看到linux中，共享库的名字后面跟了一串数字，比如：libperl.so.5.18.2。其实就是版本号，作用是为了更加方便的管理动态库，比如升级。往往系统中存在一个库的多个版本，那么Linux 系统如何控制多个版本的问题？Window之前没有处理好，为此专门有个名词来形容这个问题：“Dll hell”，其严重影响软件的升级和维护。“Dll hell”是指windows上动态库的新版本覆盖了旧版本， 但是却不兼容老版本，所以程序升级之后，动态库更新导致程序运行不起来。在Linux操作系统下也有同样的问题，那么它是怎么解决的呢？ Linux引入了一套机制，如果遵守这个机制就可以避免这个问题。 但是这只事一个约定，不是强制的。通常建议用户遵守这个约定，否则也会出现Linux版的“Dll hell”问题。 下面来介绍一个这个机制。 这个机制是通过文件名，来控制动态库（shared library）的版本。","text":"linux约定 经常看到linux中，共享库的名字后面跟了一串数字，比如：libperl.so.5.18.2。其实就是版本号，作用是为了更加方便的管理动态库，比如升级。往往系统中存在一个库的多个版本，那么Linux 系统如何控制多个版本的问题？Window之前没有处理好，为此专门有个名词来形容这个问题：“Dll hell”，其严重影响软件的升级和维护。“Dll hell”是指windows上动态库的新版本覆盖了旧版本， 但是却不兼容老版本，所以程序升级之后，动态库更新导致程序运行不起来。在Linux操作系统下也有同样的问题，那么它是怎么解决的呢？ Linux引入了一套机制，如果遵守这个机制就可以避免这个问题。 但是这只事一个约定，不是强制的。通常建议用户遵守这个约定，否则也会出现Linux版的“Dll hell”问题。 下面来介绍一个这个机制。 这个机制是通过文件名，来控制动态库（shared library）的版本。 Linux上的shared library有三个名字，分别是： 共享库本身的文件名（real name) 其通常包含完整的版本号，比如：libmath.so.1.1.1234 。lib是Linux库的约定前缀，math是共享库名字，so是共享库的后缀名，1.1.1234的是共享库的版本号，由主版本号+小版本号+build号组成。主版本号，代表当前动态库的版本，如果共享库的接口发生变化，那么这个版本号就要加1；后面的两个版本号（小版本号和 build号）是用来指示库的更新迭代号，表示在接口没有改变的情况下，由于需求发生变化等因素，开发的新代码。 共享库的soname（Short for shared object name） 用来告诉应用程序，在加载共享库的时候，应该使用的文件名。其格式为lib + math + .so + (major version number) 其只包含主版本号，换句话说，也就是只要共享库的接口没有变，soname就能与real name保持一致，因为主版本号一样。所以在库的real name的小版本号和 build号发生改变时，应用程序仍然可以通过soname得知，要使用的是哪个real name。不明白？等会给个例子来说明。 共享库的链接名（link name） 是专门为应用程序在编译时的链接阶段而用的名字。这个名字就是lib + math +.so ,比如libmath.so。其是不带任何版本信息的。在共享库的编译过程中，编译器将生成一个共享库及real name，同时将共享库的soname写在共享库文件里的文件头里面。可以用命令readelf -d sharelibrary | grep soname查看。 在应用程序引用共享库时，链接选项里面用的是共享库的link name。通过link名字找到对应的real name动态库，并且把其中的soname提取出来，写在应用程序自己的文件头的共享库字段里面。当应用程序运行时，就会通过soname，结合动态链接程序（ld.so），在给定的路径下加载real name的共享库。 ##如何使用 这里我们写了一个简单的例子，包含了三个文件，分别是： test.h 1234#ifndef _TEST_H_#define _TEST_H_void print_hello();#endif test.c 123456#include &lt;stdio.h&gt;#include &quot;test.h&quot;void print_hello()&#123; printf(&quot;this is a test for shared lib&quot;);&#125; main.c 1234567#include &lt;stdio.h&gt;#include &quot;test.h&quot;int main()&#123; print_hello(); return 0;&#125; 首先编译共享库： 12gcc -fPIC -o test.o -c test.cgcc -shared -Wl,-soname,libtest.so.0 -o libtest.so.0.0.0 test.o 然后就生成了libtest.so.0.0.0,这就是库的real name。另外，链接选项里面的-Wl,-soname,libtest.so.0告诉编译器，库的soname是libtest.so.0,我们可以看到real name的头文件里面，已经包含了这样的信息： 12readelf -d libtest.so.0.0.0 | grep soname 0x0000000e (SONAME) Library soname: [libtest.so.0] 如果没有指定soname，库的头文件里面是没有这个字段的。 有了库以后，下一步是链接到应用程序里面，我们需要这样写： 12gcc -c -o main.o main.cgcc -L. -o main main.o -ltest 但是会报错“cannot find -ltest”。这里因为，链接选项制定的是link name，而根据linux的规则，此目录下面并没有libtest.so文件，所以需要先生成link name文件。 12ln -s libtest.so.0.0.0 libtest.so.0ln -s libtest.so.0 libtest.so 或者一步到位： 1ln -s libtest.so.0.0.0 libtest.so 然后再进行应用程序的编译：gcc -L. -o main main.o -ltest ok，生成了我们需要的main可执行程序。查看一下，其引用的共享库： 12345ldd main linux-gate.so.1 =&gt; (0xb76fb000) libtest.so.0 =&gt; not found libc.so.6 =&gt; /lib/i386-linux-gnu/libc.so.6 (0xb752e000) /lib/ld-linux.so.2 (0xb76fc000) 没错，链接时，应用程序需要的库正是我们指定的soname，而不是link name或者real name。所以应用程序正是通过soname去寻找真正的real name库。这有什么好处吗？答案后面揭晓。另外，上面的输出中，我们发现，libtest.so.0是not found的状态。为什么呢？因为这个库的当前所在路径并不在链接程序（ld.so)的搜索路径之中，所以无法找到。如何解决？这篇文章就不多说了，这里提供几个方案： 改变LD_LIBRARY_PATH export LD_LIBRARY_PATH=/home/bow/all/program/test/lib_version_test:$LD_LIBRARY_PATH 这里/home/bow/all/program/test/lib_version_test是共享库的路径。虽然改变LD_LIBRARY_PATH能达到目的，但是不推荐使用，因为这是一个全局的变量，其他应用程序可能受此影响，导致各种库的覆盖问题。如果要清楚这个全局变量，使用命令unset LD_LIBRARY_PATH 用rpath 在编译应用程序时，利用rpath指定加载路径。 gcc -L. -Wl,-rpath=/home/bow/all/program/test/lib_version_test -o test main.o -ltest 这样，虽然避免了各种路径找不到的问题，但是也失去了灵活性。因为库的路径被定死了。 改变ld.so.conf 将路径添加到此文件，然后使用ldconfig更新加载程序的cache。 可以使用命令ldconfig -p查看当前所有库的soname-&gt;real name的对应关系信息 这里我们选择最后一种方式。 12345bow@bow-Aspire-4752:vim /etc/ld.so.conf##添加路径到文件末：/home/bow/all/program/test/lib_version_testbow@bow-Aspire-4752:ldconfigbow@bow-Aspire-4752:~/all/program/test/lib_version_test$ ./mainthis is a test for shared lib 最后说一下，应用程序在编译链接和运行加载时，库的搜索路径的先后顺序。 编译链接时，查找顺序 /usr/local/lib /usr/lib 用-L指定的路径，按命令行里面的顺序依次查找 运行加载时的顺序 可执行程序指定的的DT_RPATH LD_LIBRARY_PATH. 但是如果使用了setuid/setgid，由于安全因素，此路径将被忽略. 可执行程序指定的的DT_RUNPATH. 但是如果使用了setuid/setgid，由于安全因素，此路径将被忽略 /etc/ld/so/cache. 如果链接时指定了‘-z nodeflib’，此路径将被忽略. /lib. 如果链接时指定了‘-z nodeflib’，此路径将被忽略 /usr/lib. 如果链接时指定了‘-z nodeflib’，此路径将被忽略 版本控制 基于上面的例子，看看linux的这种约定，如何达到版本控制的目的。上面我们留下了一个问题：通过soname去寻找真正的real name库，这有什么好处？ 假设，我们现在对上面的test共享库进行升级，有2中情况： 修改了原来的接口 增加了新的接口 (1) 修改了原来的接口 我们修改test.c的代码，并修改原来的接口： test.c 1234567#include &lt;stdio.h&gt;#include &quot;test.h&quot;void print_hello()&#123; printf(&quot;this is a test for shared lib&quot;); printf(&quot;this is a new building version&quot;);&#125; 然后我们重新编译生成共享库，并且定义为新的building版本 12gcc -fPIC -o test.o -c test.cgcc -shared -Wl,-soname,libtest.so.0 -o libtest.so.0.0.1 test.o 注意，按照约定，由于新的版本只是修改了接口，可以兼容之前的版本，所以soname并不需要改变。生成新的real name库以后，我们只需要执行ldconfig，即可自动 更新soname到新real name库的软链接。 之前的链接 12345678910111213bow@bow-Aspire-4752:~/all/program/test/lib_version_test$ lltotal 44drwxrwxr-x 2 bow bow 4096 6月 28 12:53 ./drwxrwxr-x 10 bow bow 4096 6月 27 21:21 ../lrwxrwxrwx 1 bow bow 12 6月 28 12:50 libtest.so -&gt; libtest.so.0*lrwxrwxrwx 1 bow bow 16 6月 28 12:50 libtest.so.0 -&gt; libtest.so.0.0.0*-rwxrwxr-x 1 bow bow 6894 6月 28 12:42 libtest.so.0.0.0*-rwxrwxr-x 1 bow bow 7287 6月 28 12:53 main*-rwxrwxr-x 1 bow bow 81 6月 27 23:16 main.c*-rw-rw-r-- 1 bow bow 936 6月 28 12:46 main.o-rw-rw-r-- 1 bow bow 104 6月 27 21:20 test.c-rw-rw-r-- 1 bow bow 61 6月 27 21:19 test.h-rw-rw-r-- 1 bow bow 1344 6月 28 12:39 test.o ldconfig更新之后 1234567891011121314bow@bow-Aspire-4752:~/all/program/test/lib_version_test$ lltotal 52drwxrwxr-x 2 bow bow 4096 6月 28 15:07 ./drwxrwxr-x 10 bow bow 4096 6月 27 21:21 ../lrwxrwxrwx 1 bow bow 12 6月 28 12:50 libtest.so -&gt; libtest.so.0*lrwxrwxrwx 1 root root 16 6月 28 15:07 libtest.so.0 -&gt; libtest.so.0.0.1*-rwxrwxr-x 1 bow bow 6894 6月 28 12:42 libtest.so.0.0.0*-rwxrwxr-x 1 bow bow 6894 6月 28 15:06 libtest.so.0.0.1*-rwxrwxr-x 1 bow bow 7287 6月 28 12:53 main*-rwxrwxr-x 1 bow bow 81 6月 27 23:16 main.c*-rw-rw-r-- 1 bow bow 936 6月 28 12:46 main.o-rw-rw-r-- 1 bow bow 104 6月 27 21:20 test.c-rw-rw-r-- 1 bow bow 61 6月 27 21:19 test.h-rw-rw-r-- 1 bow bow 1344 6月 28 12:39 test.o 看到没有，soname自动更新到了新版本的共享库。所以之前的应用程序main会使用新的共享库。 12bow@bow-Aspire-4752:~/all/program/test/lib_version_test$ ./mainthis is a test for shared libthis is a new building version (2) 增加了新的接口 我们修改test.c的代码，并增加接口： test.h 12345#ifndef _TEST_H_#define _TEST_H_void print_hello();void print_hello2();#endif test.c 1234567891011#include &lt;stdio.h&gt;#include &quot;test.h&quot;void print_hello()&#123; printf(&quot;this is a test for shared lib&quot;); printf(&quot;this is a new release version&quot;);&#125;void print_hello2()&#123; printf(&quot;this is a new release version&quot;);&#125; 然后编译，根据约定，real name的主版本号需要更新。 12gcc -fPIC -o test.o -c test.cgcc -shared -Wl,-soname,libtest.so.1 -o libtest.so.1.0.0 test.o 这时，就会生成新的共享库libtest.so.1.0.0，然后ldconfig更新软链接和cache。 123456789101112131415total 60drwxrwxr-x 2 bow bow 4096 6月 28 15:15 ./drwxrwxr-x 10 bow bow 4096 6月 27 21:21 ../lrwxrwxrwx 1 bow bow 12 6月 28 12:50 libtest.so -&gt; libtest.so.0*lrwxrwxrwx 1 root root 16 6月 28 15:07 libtest.so.0 -&gt; libtest.so.0.0.1*-rwxrwxr-x 1 bow bow 6894 6月 28 12:42 libtest.so.0.0.0*-rwxrwxr-x 1 bow bow 6894 6月 28 15:06 libtest.so.0.0.1*lrwxrwxrwx 1 root root 16 6月 28 15:15 libtest.so.1 -&gt; libtest.so.1.0.0*-rwxrwxr-x 1 bow bow 6894 6月 28 15:15 libtest.so.1.0.0*-rwxrwxr-x 1 bow bow 7287 6月 28 12:53 main*-rwxrwxr-x 1 bow bow 81 6月 27 23:16 main.c*-rw-rw-r-- 1 bow bow 936 6月 28 12:46 main.o-rw-rw-r-- 1 bow bow 104 6月 27 21:20 test.c-rw-rw-r-- 1 bow bow 61 6月 27 21:19 test.h-rw-rw-r-- 1 bow bow 1344 6月 28 12:39 test.o 12bow@bow-Aspire-4752:~/all/program/test/lib_version_test$ ./mainthis is a test for shared libthis is a new building version 这个时候，虽然我们更新了共享库，但是main还是会加载旧的共享库。这就保证了，即使共享库更新，以前的程序也能正常工作。 因为main里面的soname没变，而且soname对应的real name没变。 如果你想更新应用程序main，使其使用新的库，那么需要重新编译，让libtest.so的软链接指向libtest.so.1 1234567891011121314151617181920bow@bow-Aspire-4752:~/all/program/test/lib_version_test$ ln -sf libtest.so.1 libtest.sobow@bow-Aspire-4752:~/all/program/test/lib_version_test$ lltotal 60drwxrwxr-x 2 bow bow 4096 6月 28 15:32 ./drwxrwxr-x 10 bow bow 4096 6月 27 21:21 ../lrwxrwxrwx 1 bow bow 12 6月 28 15:32 libtest.so -&gt; libtest.so.1*lrwxrwxrwx 1 root root 16 6月 28 15:07 libtest.so.0 -&gt; libtest.so.0.0.1*-rwxrwxr-x 1 bow bow 6894 6月 28 12:42 libtest.so.0.0.0*-rwxrwxr-x 1 bow bow 6894 6月 28 15:24 libtest.so.0.0.1*lrwxrwxrwx 1 root root 16 6月 28 15:15 libtest.so.1 -&gt; libtest.so.1.0.0*-rwxrwxr-x 1 bow bow 6923 6月 28 15:25 libtest.so.1.0.0*-rwxrwxr-x 1 bow bow 7287 6月 28 12:53 main*-rwxrwxr-x 1 bow bow 81 6月 27 23:16 main.c*-rw-rw-r-- 1 bow bow 936 6月 28 12:46 main.o-rw-rw-r-- 1 bow bow 216 6月 28 15:25 test.c-rw-rw-r-- 1 bow bow 82 6月 28 15:25 test.h-rw-rw-r-- 1 bow bow 1516 6月 28 15:25 test.obow@bow-Aspire-4752:~/all/program/test/lib_version_test$ gcc -L. -o main main.o -ltestbow@bow-Aspire-4752:~/all/program/test/lib_version_test$ ./mainthis is a test for shared libthis is a new release version ok。通过以上操作，linux下面的共享库的版本约定和控制，就很清楚了。当碰到一些譬如无法找到共享库或接口，或者引用的库 发生变化的时候，看看应用程序到底时如何查找共享库的，已经在使用的是哪个版本的共享库，根据以上原则去分析，一定可以解决问题。 一般通过这几个命令去分析： nm 查看共享库暴露的接口 ldconfig 可以自动生成soname的链接接文件。并更新共享库的搜索cache，加速查找。 example: ldconfig -p | grep libtest readelf 可以查看动态库的信息，本身的soname。 example: readelf -d libtest.so.0.0.0 | grep soname ldd 可以查看应用程序或共享库依赖的库 example: ldd main objdump 与readelf 类似 参考资料 http://tldp.org/HOWTO/Program-Library-HOWTO/shared-libraries.html http://www.cprogramming.com/tutorial/shared-libraries-linux-gcc.html http://www.linuxidc.com/Linux/2012-04/59071p2.htm Linux 共享库指南 当创建一个库文件时，是需要创建 real name 的库文件（包含具体的版本信息）。当安装新版本的库文件时，将它们安装在系统默认的几个文件夹中(/lib，/lib64，/usr/lib，/usr/lib64)，执行 ldconfig 命令，ldconfig 会在系统默认的库文件目录和指定目录( /etc/ld.so.conf 和 /etc/ld.so.conf.d/*.conf )中检索所有的文件，并自动对 real name 的真实库文件创建 soname 的软链接，并将文件路径缓存至 /etc/ld.so.cache 文件中。 ldconfig 不会设置库文件的 linker name，这通常是库文件安装时设置的，linker name 通常是指向对应最新版本库文件 soname 的软链接文件。不自动设置 linker name 的原因是开发时，可能会需要使用旧版本的库文件。 举例来说，对于 libreadline 这个库，/usr/lib64/libreadline.so.5 是其完全限定 soname，ldconfig 命令会将其链接至 real name 库文件如 /usr/lib64/libreadline.so.5.1。还应该有一个 linker name 供编译器使用，如 /usr/lib64/libreadline.so，它是 /usr/lib64/libreadline.so.5 的软链接。 共享库文件查询的目录存放在 /etc/ld.so.conf 中（或临时设置LD_LIBRARY_PATH）。注意，对于红帽 Linux 系统，/etc/ld.so.conf 这个文件中并没有包含 /usr/local/lib 这个常用的库文件目录。 如果只是想要覆盖一个库文件的某些函数，但保留其余的内容，可以将覆盖库文件名（.o 后缀文件）保存至 /etc/ld.so.preload 文件中（或临时设置LD_PRELOAD）。这些覆盖库文件会比标准库文件优先读取，这通常用于紧急的版本补丁。 每次都查找 /etc/ld.so.conf 中的目录是很低效的，因此 ldconfig 程序会读取 /etc/ld.so.conf 文件中的所有目录中的文件，将库文件 real name 设置合适的软链接 soname，并将这些文件名缓存至 /etc/ld.so.cache 中。这样会大大加快库文件的寻找速度。 本文地址：http://xnerv.wang/version-control-and-usage-of-linux-shared-library/ 转载自：linux共享库的版本控制和使用","categories":[{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/categories/Linux/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/tags/Linux/"},{"name":"Shared Library","slug":"Shared-Library","permalink":"https://xnerv.wang/tags/Shared-Library/"}]},{"title":"【MySQL】MySQL5.6新特性之Multi-Range Read（转载）","slug":"mysql56-new-feature-multi-range-read","date":"2015-05-27T21:13:09.000Z","updated":"2023-08-21T02:24:19.403Z","comments":true,"path":"mysql56-new-feature-multi-range-read/","link":"","permalink":"https://xnerv.wang/mysql56-new-feature-multi-range-read/","excerpt":"一 介绍 MySQL 5.6版本提供了很多性能优化的特性，其中之一就是 Multi-Range Read 多范围读(MRR) , 它的作用针对基于辅助/第二索引的查询，减少随机IO，并且将随机IO转化为顺序IO，提高查询效率。 二 原理 在没有MRR之前,或者没有开启MRR特性时，MySQL 针对基于辅助索引的查询策略是这样的： 1select non_key_column from tb wherekey_column=x; MySQL 执行查询的伪代码 123456第一步 先根据where条件中的辅助索引获取辅助索引与主键的集合，结果集为rest。 select key_column, pk_column from tb where key_column=x order by key_column第二步 通过第一步获取的主键来获取对应的值。 for each pk_column value in rest do: select non_key_column from tb where pk_column=val","text":"一 介绍 MySQL 5.6版本提供了很多性能优化的特性，其中之一就是 Multi-Range Read 多范围读(MRR) , 它的作用针对基于辅助/第二索引的查询，减少随机IO，并且将随机IO转化为顺序IO，提高查询效率。 二 原理 在没有MRR之前,或者没有开启MRR特性时，MySQL 针对基于辅助索引的查询策略是这样的： 1select non_key_column from tb wherekey_column=x; MySQL 执行查询的伪代码 123456第一步 先根据where条件中的辅助索引获取辅助索引与主键的集合，结果集为rest。 select key_column, pk_column from tb where key_column=x order by key_column第二步 通过第一步获取的主键来获取对应的值。 for each pk_column value in rest do: select non_key_column from tb where pk_column=val 由于MySQL存储数据的方式： 辅助索引的存储顺序并非与主键的顺序一致，从图中可以看出,根据辅助索引获取的主键来访问表中的数据会导致随机的IO . 不同主键不在同一个page 里面时必然导致多次IO 和随机读。 在使用MRR优化特性的情况下，MySQL 针对基于辅助索引的查询策略是这样的： 1234567第一步 先根据where条件中的辅助索引获取辅助索引与主键的集合，结果集为rest select key_column, pk_column from tb where key_column = x order by key_column第二步 将结果集rest放在buffer里面(`read_rnd_buffer_size`大小直到buffer满了)，然后对结果集rest按照pk_column排序，得到结果集是rest_sort第三步 利用已经排序过的结果集，访问表中的数据，此时是顺序IO&lt;span style=&quot;color:#0000CC;&quot;&gt;.&lt;/span&gt; select non_key_column fromtb where pk_column in ( rest_sort ) 从图示MRR原理，MySQL 将根据辅助索引获取的结果集根据主键进行排序，将乱序化为有序，可以用主键顺序访问基表，将随机读转化为顺序读，多页数据记录可一次性读入或根据此次的主键范围分次读入，以减少IO操作，提高查询效率。 三 相关参数 我们可以通过参数 optimizer_switch 的标记来控制是否使用MRR，当设置mrr=on时，表示启用MRR优化。mrr_cost_based 表示是否通过 cost base 的方式来启用MRR。如果选择mrr=on，mrr_cost_based=off，则表示总是开启MRR优化。 参数 read_rnd_buffer_size 用来控制键值缓冲区的大小。 四 案例介绍 当开启MRR时 1234567MySQL &gt; explain select * from tbl where tbl.key1 between 1000 and 2000;+----+-------------+-------+-------+---------------+------+---------+------+------+-------------------------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+-------+---------------+------+---------+------+------+-------------------------------------------+| 1 | SIMPLE | tbl | range | key1 | key1 | 5 | NULL | 960 | Using index condition; Using MRR |+----+-------------+-------+-------+---------------+------+---------+------+------+-------------------------------------------+1 row in set (0.03 sec) 五 MRR的使用限制 MRR 适用于以下两种情况。 1 range access 2 ref and eq_ref access, when they are using Batched Key Access 六 参考文章 《MariaDB Multi-Range Read Optimization》 《MySQL Multi-Range Read Optimization》 《Multi Range Read (MRR) in MySQL 5.6 and MariaDB 5.5》 本文地址：http://xnerv.wang/mysql56-new-feature-multi-range-read/ 转载自：【MySQL】MySQL5.6新特性之Multi-Range Read","categories":[{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/categories/DBMS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/tags/DBMS/"},{"name":"MySQL","slug":"MySQL","permalink":"https://xnerv.wang/tags/MySQL/"},{"name":"Multi-Range Read","slug":"Multi-Range-Read","permalink":"https://xnerv.wang/tags/Multi-Range-Read/"}]},{"title":"10个帮程序员减压放松的网站（转载）","slug":"10-websites-for-helping-programmers-to-relax","date":"2015-02-05T05:15:00.000Z","updated":"2023-08-21T02:24:21.402Z","comments":true,"path":"10-websites-for-helping-programmers-to-relax/","link":"","permalink":"https://xnerv.wang/10-websites-for-helping-programmers-to-relax/","excerpt":"同学们工作之余，不妨放下微博跟朋友圈，来这10个网站感受一下看着就醉了的情境：「念完往上一推音乐键，我往后一靠，潮乎乎的软皮耳机里头，音乐排山倒海。」今天推荐的网站，利用代入感强的图片与音频，迅速帮你抹平焦虑，获得平和心态，特别献给改稿千遍的设计师们。","text":"同学们工作之余，不妨放下微博跟朋友圈，来这10个网站感受一下看着就醉了的情境：「念完往上一推音乐键，我往后一靠，潮乎乎的软皮耳机里头，音乐排山倒海。」今天推荐的网站，利用代入感强的图片与音频，迅速帮你抹平焦虑，获得平和心态，特别献给改稿千遍的设计师们。 1. Calm 这是同类型中最火的网站了，站如其名，「平和」，通过自然的图像（阳光下的暖流、淙淙的小溪等）与缓缓的音乐，帮你在短时间内放松下来。 左侧有时间设定，从 2 分钟到 20 分钟，右底部可以改变音频、图像，调节音量等。还有 IOS 客户端下载呦。 2. Do Nothing For 2 Minutes 「木头人，两分钟」，这是一个简单到极致的网站，当你打开的时候，自动开始计时，这时间你不能触碰键盘和鼠标，否则 2 分钟会重置。 你需要做的，就是放下手头的工作，静静地享受潮声，这也很棒，不是吗？两分钟足够你冷静下来，休息一下了。 3. Get Relaxed 如果两分钟不足以让你彻底放松，试试这个。如下图，打开网站后，头枕着双手往后仰，欣赏自然风光，聆听网站为你精心挑选的音乐。 图像 3 秒一换，有 15 种，每种持续大概 2 – 4 分钟，现在，开始吧！ 提醒：网站有简陋广告，稍微影响体验。 4. LoungeV Studio 前三个都是图像，现在来个新鲜的。这个网站提供高清的自然风光视频 + 音乐。有沙滩、瀑布、水下景色等等，网站背景是一个温馨的客厅，右侧有视频可选，对喜欢看视频的同学来说，还是蛮不错的。 5. A Soft Murmur 这个网站太棒了！小编玩了好久都舍不得停下来。网站让你自由创造美妙的声音。你可以通过混合不同的声音（雨声、火柴燃烧的声音、打雷声、海潮声……不一而足。但是，总有一款令你爱不释手！）当然，声音的大小也可以自己调节。 如果你对混合的声音非常满意，也可以分享到脸书、谷歌等…. 6. Nature Sounds For Me 这个比上面那个界面稍逊，但是玩起来更嗨！它提供的声音除了以上的自然类声音，还有很多你想不到的：绵羊咩咩、骏马跺脚喷气、不同的鸟叫声，甚至是心脏跳动、厨房叮当的声音，不仅令人身临其境，而且搭配起来简直不能更欢乐！ 当然，它还有 IOS 客户端。 7. Noisli 这个网站根据你的情绪变化，选择不同的音乐和背景颜色。颜色大多朴素平和，背景声音也有对应的图标可以选择。还有一点贴心的设计是，网站右侧有便签本，你可以一边享受静谧时光一边随手记点事。 为了造福大众，网站还提供 IOS 版。 8. Soundrown 网站一进去，有 3 个关键词：放松、专注、逃离。的确，它成功做到了这一点。它有 10 种不同的声音帮助你放松心情，也可以混合使用。不同的声音对应不同的背景，网站非常有设计感，相信你会重新回来体验一次。 9. The Thoughts Room 一句话简洁：世界的秘密——树洞类网站。你可以在这里向全世界倾诉你的任何想法，网站支持 37 种语言，不过看了一下，没有中国… 10. Raining.Fm 有时候，我们需要的仅仅是一点点雨声来帮助我们平静。网站专门提供雨声，因为单一，所以也更加专业。网站有 3 种不同的雨声类型，右侧有定时器可以在你放松时提醒你，简单也好用的一个网站，赞一个。 本文地址：http://xnerv.wang/10-websites-for-helping-programmers-to-relax/ 转载自：10个帮程序员减压放松的网站","categories":[{"name":"程序员生活","slug":"程序员生活","permalink":"https://xnerv.wang/categories/%E7%A8%8B%E5%BA%8F%E5%91%98%E7%94%9F%E6%B4%BB/"}],"tags":[{"name":"程序员生活","slug":"程序员生活","permalink":"https://xnerv.wang/tags/%E7%A8%8B%E5%BA%8F%E5%91%98%E7%94%9F%E6%B4%BB/"},{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"}]},{"title":"The Google File System——论文详解（转载）","slug":"the-google-file-system-paper-analysis","date":"2015-02-04T18:42:32.000Z","updated":"2023-08-21T02:24:20.535Z","comments":true,"path":"the-google-file-system-paper-analysis/","link":"","permalink":"https://xnerv.wang/the-google-file-system-paper-analysis/","excerpt":"“Google文件存储系统（GFS）是构建在廉价服务器之上的大型分布式系统。它将服务器故障视为正常现象，通过软件方式自动容错，在保证系统可用性和可靠性同时，大大降低系统成本。 GFS是Google整个分布式系统的基石，其他存储系统如Google BigTable、GoogleMegastore等系统均直接或间接构建在GFS之上。另外，Google的大规模批处理系统MapReduce也是利用GFS系统作为海量数据的输入输出。” 以下内容为在研读Google_File_System论文时，对其中一些关键技术的理解。要想更加深入的理解GFS，还需进一步研究GFS源码以及论文等。 一、单一Master节点： 如何避免单一的Msaster节点成为系统性能瓶颈： 客户端只向Master请求元数据信息，并不通过Master节点进行chunk数据读写，具体的数据读写操作由ChunkServer直接负责。 客户端在向Master请求某chunk元数据时,Master会一次返回包括该chunk紧接之后的几个chunk信息，有效减少客户端的请求次数。 客户端读取数据流程如下：","text":"“Google文件存储系统（GFS）是构建在廉价服务器之上的大型分布式系统。它将服务器故障视为正常现象，通过软件方式自动容错，在保证系统可用性和可靠性同时，大大降低系统成本。 GFS是Google整个分布式系统的基石，其他存储系统如Google BigTable、GoogleMegastore等系统均直接或间接构建在GFS之上。另外，Google的大规模批处理系统MapReduce也是利用GFS系统作为海量数据的输入输出。” 以下内容为在研读Google_File_System论文时，对其中一些关键技术的理解。要想更加深入的理解GFS，还需进一步研究GFS源码以及论文等。 一、单一Master节点： 如何避免单一的Msaster节点成为系统性能瓶颈： 客户端只向Master请求元数据信息，并不通过Master节点进行chunk数据读写，具体的数据读写操作由ChunkServer直接负责。 客户端在向Master请求某chunk元数据时,Master会一次返回包括该chunk紧接之后的几个chunk信息，有效减少客户端的请求次数。 客户端读取数据流程如下： 二、Chunk尺寸： 选择64MB作为chunk的大小，该尺寸远大于一般系统的Block size。这是由具体的业务特性决定的：即通常操作的是大文件。 三、Master元数据： Master以心跳信息来监控chunkserver的状态。 其中，chunk的副本位置信息，采取启动master时轮询（心跳信息）chunkserver，然后定期轮询来获取chunk的副本位置信息，而不是直接在master中持久化该信息。 这样简化了Master与chunkserver之间的的数据同步问题。 操作日志： 强同步：即只有当操作日志被写入到本地磁盘以及远程机器（备master）磁盘后，才会响应客户端的操作请求。同时master会收集多个日志后，批量写入磁盘。 checkpoint: 在灾难恢复时通过回放操作日志来将系统恢复到最近状态。为了减低回放时间，使用checkpoint方式来使得回放的日志尽量短，即在恢复时只需要回放checkpoint之后的日志文件即可。之前的旧日志文件及checkpoint文件可定期删除。“操作日志相当于一盘录像带，而checkpoint可以在该录像带上间隔性的插上一些标记点（将内存缓冲的数据写入磁盘，完成后，相应的在操作日志带上插入最新的一个标记点）。最新一个标记点之前的所有日志记录的操作都已经持久化保存了，而该点之后的操作保存在内存中。当内存中数据丢失时，只需回放该点之后的部分日志即可恢复。而checkpoint文件就是记录了这些点的位置等信息。” 四、系统交互： Master以带超时的租约来授权主chunkserver来执行具体的操作任务，从而减小Master的管理负担。Master与chunkserver之间通过定期的心跳信息来交互信息：master获取chunkserver’的状态信息，chunkserver申请延长租约时间等。 以下为客户端写数据的操作流程： 五、负载均衡： 本文地址：http://xnerv.wang/the-google-file-system-paper-analysis/ 转载自：The Google File System——论文详解","categories":[{"name":"分布式及存储","slug":"分布式及存储","permalink":"https://xnerv.wang/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8F%8A%E5%AD%98%E5%82%A8/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"分布式及存储","slug":"分布式及存储","permalink":"https://xnerv.wang/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8F%8A%E5%AD%98%E5%82%A8/"},{"name":"Google","slug":"Google","permalink":"https://xnerv.wang/tags/Google/"},{"name":"GFS","slug":"GFS","permalink":"https://xnerv.wang/tags/GFS/"}]},{"title":"Special features of Linux memory management mechanism（转载）","slug":"special-features-of-linux-memory-management-mechanism","date":"2014-12-01T00:27:00.000Z","updated":"2023-08-21T02:24:19.822Z","comments":true,"path":"special-features-of-linux-memory-management-mechanism/","link":"","permalink":"https://xnerv.wang/special-features-of-linux-memory-management-mechanism/","excerpt":"In this article, I am going to describe some general features and some specific ones of the memory management in Linux. It will be mainly on dynamic memory allocation and release, as well as the management of the free memory. The article concerns the Linux kernel versions 2.6.X.","text":"In this article, I am going to describe some general features and some specific ones of the memory management in Linux. It will be mainly on dynamic memory allocation and release, as well as the management of the free memory. The article concerns the Linux kernel versions 2.6.X. Structure of the Linux memory management The term “memory management” refers to the mechanisms implemented by an operating system to provide applications with memory-related services. These services include usage of virtual memory (utilizing of a hard disk or other non-RAM storage media to provide additional program memory), protected memory (exclusive access to a region of memory by a process), and shared memory (cooperative access to a region of memory by multiple processes). Memory management services in the Linux are built on a programming foundation that includes a peripheral device called Memory Management Unit (MMU). MMU translates physical memory addresses to linear addresses used by the operating system, and requests a page fault interrupt, when the CPU tries to access memory that it is not entitled to. Not all processors have MMUs. Therefore, the uClinux distribution (Linux for microcontrollers) supports a single address space of operation. This architecture lacks the protection provided by MMU but makes it possible for Linux to run on another class of processors. For further understanding of structure of the MM services, we need to know that a basic unit of memory under Linux is page, a non-overlapping region of contiguous memory. All available physical memory is organized into pages towards the end of the kernel’s boot process. Size of page depends on processor architecture. Processor designs often allow to have two or more, sometimes simultaneously, page sizes. Traditional page size used by Linux is 4096 bytes. But using memory pages “as is” is not very convenient. Often we need to allocate less than one memory page. There are such possibilities in Linux: in the kernel, you can allocate one of the small kernel objects using slab allocator; you can allocate a memory block by kmalloc, but it will allocate only a block of the nearest bigger size that it has; in the user mode, you can allocate any amount of memory using heap management functions implemented in Standard C Library; you can create your own heap manager on top of the Linux kernel system calls. To provide a simple interface for interaction with Memory Management Unit and perform such interaction in a portable way, in Linux, subsystem of allocating and releasing memory is split into three layers. These layers are: The Slab Allocator The Zone Allocator The Buddy Allocator General scheme of all these layers interaction with user mode code and hardware looks as follows: Figure 1. General scheme of the memory management in Linux Note that in Linux, most of programs directly or indirectly use heap manager of the GCC Standard C Library called glibc, but you still can write your own heap manager on top of the kernel system calls. As we can see on the Figure 1, user space allocation always leads to kernel allocation. Kernel allocates memory using the chain of three kernel allocators and maps allocated pages to the address space of the process, which has requested the allocation. Kernel mode memory management services The Buddy Allocator is responsible for the management of the page allocations in the entire system. This code manages lists of physically contiguous pages and maps them into the MMU page tables to provide other kernel subsystems with the valid physical address ranges, when the kernel requests them (Physical to Virtual Address mapping is handled by a higher layer of the VM). The Buddy Allocator splits memory into pairs of 2n pages where n is in range from 0 to MAX_ORDER constant (defined in the header file &lt;linux/mmzone.h&gt;), and stores information about the free blocks of pages in the array of lists as follows: Figure 2: Array of lists of memory pages in the Buddy Allocator Each list consists of free physically contiguous blocks of 2i memory pages, where i is a list number. Each of such blocks, except the block that consists of 1 page, can be split into two halves and used as 2 blocks of a half size. So if no entries exist in the requested list, an entry from the next upper list is broken into two separate clusters and one is returned to the caller while the other one is added to the next lower list. On the other hand, every two blocks of memory of the same size, which have common border (arranged in memory sequentially, from the standpoint of physical addresses), may be united into the single block of the bigger size. Such neighboring blocks are called Buddies. When allocation is returned to the Buddy Allocator, it checks if buddy of the allocation is free, and if it is so, Buddy Allocator unites them into the bigger block. This operation is repeated until no more block buddies are found. Also we should note that the Buddy Allocator can allocate only blocks of the size in pages that is equal to 2 raised to some power. The Buddy Allocator also interacts with the kernel threads kswapd and bdflush, which are responsible for the maintaining with the swap. Different ranges of physical pages may have different properties, for the purposes of the kernel. For example, Direct Memory Access can work only in specific range of physical addresses in the x86 architecture. On the other hand PPC does not have this constraint. For handling such situation in a hardware, independent way the Zone Allocator was created. The Zone Allocator is used to allocate pages in the specified zone. Today Linux kernel is supporting three memory zones: DMA — This zone consists of memory accessible for direct memory operations of the legacy devices NORMAL — This zone includes memory addresses used by the kernel for internal data structures as well as other system and user space allocations. HIGHMEM — This zone includes all memory used exclusively for system allocations (file system buffers, user space allocations, etc). Note that the Zone Allocator also can manipulate only with memory pages. Since we often need to allocate objects that have size less than the size of a page, we need something to deal with the pages and allocate lesser chunks of memory for us. We know the sizes of the most objects that are often allocated in the kernel space, so we can create allocator that will receive pages of memory from the Zone Allocator and allocate small objects in these memory pages. This subsystem is named the Slab Allocator (An Object-Caching Kernel Memory Allocator). The Slab Allocator organizes memory in caches, one cache for each object type, e.g. inode_cache, dentry_cache, buffer_head, vm_area_struct. Each cache consists of many slabs (usually one page long), and each slab contains multiple initialized objects. This means that the constructor of the objects is used only for newly allocated slabs and you should initialize object before release it to the Slab Allocator. Also the Slab Allocator makes it possible to allocate buffers of memory of one of the specially defined sizes. Such buffers can be got using kernel function kmalloc. You specify the size of allocation, and kmalloc will allocate block of the greater size, the nearest to the one you requested. Sizes of memory blocks, which can be allocated by kmalloc, are available in the header file &lt;linux/kmalloc_sizes.h&gt;. Also the kernel can allocate virtually contiguous memory (memory with contiguous virtual addresses, but not with contiguous physical addresses) using vmalloc function. Special aspects of the linux kernel mechanisms of the memory management Why Linux developers rarely check results of memory allocation. The short answer is: “Because of overcommit”. Under the default memory management strategy, malloc (and kmalloc, and vmalloc) always succeeds, even if the system has not enough memory to satisfy the request. The Linux kernel assumes that you’re not going to use all of the memory you asked for. Real allocation occurs only when you get access to the allocated memory, and if the system does not have enough free memory then it will run Out-of-memory (OOM) killer. OOM killer will try to free some memory for you, killing other processes and releasing their memory or it will kill your process if it deems the task you are performing to have a lower priority. This approach leads to some performance gains for applications, which allocate a lot of memory but use only some part of it, since allocation of the swap pages will not be performed before the swap pages is used. But sometimes such approach is not good because of the chance to be killed by OOM killer at any moment. Fortunately, Linux allows to change default approach of the memory allocation and behavior of the system when the OOM event occurs. The overcommit policy is set via the sysctl vm.overcommit_memory. The Linux kernel supports the following overcommit handling modes associated with the values of the kernel parameter vm.overcommit_memory: Heuristic overcommit handling. Obvious overcommits of address space are refused. This type of handling is used for a typical system. It ensures a seriously wild allocation fails while allowing overcommit to reduce swap usage. Root is allowed to allocate a bit more memory in this mode. This type is set by default. Always overcommit. Appropriate for some scientific applications, which use a lot of memory. Don’t overcommit. The total address space commit for the system is not permitted to exceed swap + a configurable percentage (50% by default) of physical RAM. Depending on the percentage you use, in most situations, this means that a process will not be killed while accessing pages but will receive errors on memory allocation as appropriate. The overcommit percentage is set via the sysctrl vm.overcommit_ratio. The current overcommit limit and amount committed can be viewed in /proc/meminfo as CommitLimit and Committed_AS respectively. How Out-of-memory killer mechanism works There are three strategies to handle Out-of-memory situation: Kill allocating process Kill some other process or processes to allow allocating process get the memory Stop the system and show message about kernel panic. The first and the third strategies are fairly simple unlike the second strategy. Let’s consider it in more detail. According to the second strategy, Linux will try to choose appropriate process to kill when it runs out of memory. “Appropriate” in this context means that: User will lose the minimum amount of work done System will recover a large amount of memory OOM killer don’t kill process that has not allocated a lot of memory OOM killer intends to kill the minimum number of processes (to kill just one process would be the best) OOM killer will kill the process the user expects it to kill In order to choose a process to kill, OOM killer calculates the value named Badness. Then it selects the process with the maximum Badness to be killed. If the allocating process was chosen, OOM terminates its work. If some other process was chosen, OOM killer can be called more than once in case of previous run of the OOM killer did not free enough memory. Badness is calculated according to the next rules: Initial badness of the process A is the size of the virtual memory allocated for it plus a half of size of the virtual memory allocated to the child processes of the process A; Initial badness is divided by the square root of the CPU time consumed by the process A measured in tens of seconds; Result is divided by the root with index 4 of the run time of the process A measured in thousands of seconds. If the process A has “nice” greater than zero (priority is lower than normal), badness of the process A is multiplied by 2; If the process A was started with the root privileges, badness of the process A is divided by 4; If the process A performs raw IO, badness of the process A is divided by 4; If memory usage of the process A might not impact the memory available to the process, which has initiated OOM procedure, then badness of process A is divided by 8; We can write badness of the process A as the equation: Where: if_nice can be either 1 or 2 according to the nice of the process (2 if nice &gt; 2 or 1 otherwise) if_root can be 1 or 4 ( 4 if the process A has root privileges or 1 otherwise ) if_raw_io can be 1 or 4 ( 4 if process has capability to perform raw IO or 1 otherwise ) if_memory_does_not_overlap can be 1 or 8 ( 8 if the process A has no memory that can be allocated to the process, which has initiated OOM procedure). You can see the mechanism of the out-of-memory handler in the source file “/mm/oom_kill.c” in the Linux kernel sources tree. You can tune OOM handling mechanism by setting values to such kernel parameters: vm.oom_dump_tasks This parameter enables a system-wide task dump (excluding kernel threads) to be produced when the kernel performs an OOM-killing and includes such information as pid, uid, tgid, vm size, rss, cpu, oom_adj score, and name. It is helpful to determine why the OOM killer was invoked and to identify the process that caused it. If it is set to zero, this information is suppressed. If it is set to non-zero, this information is shown whenever the OOM killer actually kills a memory-hogging task. By default vm.oom_dump_tasks is set to 1; vm.oom_kill_allocating_task This parameter enables or disables killing the OOM-triggered task in out-of-memory situations. If this parameter is set to non-zero, the OOM killer simply kills the task that triggered the out-of-memory condition. It helps to avoid the resource-expensive tasklist scan and makes OOM killer very predictable. If it is set to zero, the OOM killer will scan through the entire tasklist and select a task on the basis of the badness calculation. By default vm.oom_kill_allocating_task is set to 0; vm.panic_on_oom If this kernel parameter is set to 1 the kernel panics occurs at out-of-memory situation. But, if a process is limited by nodes by mempolicy/cpusets, and those nodes get memory exhaustion status, one process may be killed by OOM killer. No kernel panic occurs in this case, because other memory nodes can be free. If this parameter is set to 2, the kernel panic is forced even in the above-mentioned situation. Even if OOM happens under the memory cgroup, the whole system panics. If this parameter is set to 0, the kernel will kill some processes when OOM happens. By default vm.panic_on_oom is set to 0; Note that panic_on_oom has higher priority than oom_kill_allocating_task. Additional links and literature: Anatomy of the Linux slab allocator http://www.ibm.com/developerworks/linux/library/l-linux-slab-allocator/ Understanding Virtual Memory http://www.redhat.com/magazine/001nov04/features/vm/ When Linux Runs Out of Memory http://linuxdevcenter.com/pub/a/linux/2006/11/30/linux-out-of-memory.html Andries Brouwer, The Linux kernel http://www.win.tue.nl/~aeb/linux/lk/lk-9.html Ops monkey blog entry “Linux memory overcommit” http://opsmonkey.blogspot.com/2007/01/linux-memory-overcommit.html Outline of the Linux Memory Management System http://www.thehackademy.net/madchat/ebooks/Mem_virtuelle/linux-mm/vmoutline.html Linux kernel documentation: sysctl/vm.txt and vm/overcommit-accounting Daniel P. Bovet Marco Cesati “Understanding the Linux Kernel”, O’Reilly ISBN: 0-596-00002-2, 702 pages 本文地址：http://xnerv.wang/special-features-of-linux-memory-management-mechanism/ 转载自：Special features of Linux memory management mechanism","categories":[{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/categories/Linux/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"Heap","slug":"Heap","permalink":"https://xnerv.wang/tags/Heap/"},{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/tags/Linux/"},{"name":"Memory Management","slug":"Memory-Management","permalink":"https://xnerv.wang/tags/Memory-Management/"},{"name":"Buddy System","slug":"Buddy-System","permalink":"https://xnerv.wang/tags/Buddy-System/"},{"name":"Slab Allocator","slug":"Slab-Allocator","permalink":"https://xnerv.wang/tags/Slab-Allocator/"}]},{"title":"进程分配内存的两种方式--brk() 和mmap()（不涉及共享内存）（转载）","slug":"two-ways-of-allocating-memory-brk-and-mmap","date":"2014-09-23T18:31:56.000Z","updated":"2023-08-21T02:24:21.189Z","comments":true,"path":"two-ways-of-allocating-memory-brk-and-mmap/","link":"","permalink":"https://xnerv.wang/two-ways-of-allocating-memory-brk-and-mmap/","excerpt":"如何查看进程发生缺页中断的次数？ 用ps -o majflt,minflt -C program命令查看。 majflt代表major fault，中文名叫大错误，minflt代表minor fault，中文名叫小错误。 这两个数值表示一个进程自启动以来所发生的缺页中断的次数。 发成缺页中断后，执行了那些操作？ 当一个进程发生缺页中断的时候，进程会陷入内核态，执行以下操作： 检查要访问的虚拟地址是否合法 查找/分配一个物理页 填充物理页内容（读取磁盘，或者直接置0，或者啥也不干） 建立映射关系（虚拟地址到物理地址） 重新执行发生缺页中断的那条指令 如果第3步，需要读取磁盘，那么这次缺页中断就是majflt，否则就是minflt。","text":"如何查看进程发生缺页中断的次数？ 用ps -o majflt,minflt -C program命令查看。 majflt代表major fault，中文名叫大错误，minflt代表minor fault，中文名叫小错误。 这两个数值表示一个进程自启动以来所发生的缺页中断的次数。 发成缺页中断后，执行了那些操作？ 当一个进程发生缺页中断的时候，进程会陷入内核态，执行以下操作： 检查要访问的虚拟地址是否合法 查找/分配一个物理页 填充物理页内容（读取磁盘，或者直接置0，或者啥也不干） 建立映射关系（虚拟地址到物理地址） 重新执行发生缺页中断的那条指令 如果第3步，需要读取磁盘，那么这次缺页中断就是majflt，否则就是minflt。 内存分配的原理 从操作系统角度来看，进程分配内存有两种方式，分别由两个系统调用完成：brk和mmap（不考虑共享内存）。 brk是将数据段(.data)的最高地址指针_edata往高地址推； mmap是在进程的虚拟地址空间中（堆和栈中间，称为文件映射区域的地方）找一块空闲的虚拟内存。 这两种方式分配的都是虚拟内存，没有分配物理内存。在第一次访问已分配的虚拟地址空间的时候，发生缺页中断，操作系统负责分配物理内存，然后建立虚拟内存和物理内存之间的映射关系。 在标准C库中，提供了malloc/free函数分配释放内存，这两个函数底层是由brk，mmap，munmap这些系统调用实现的。 下面以一个例子来说明内存分配的原理： 情况一、malloc小于128k的内存，使用brk分配内存，将_edata往高地址推(只分配虚拟空间，不对应物理内存(因此没有初始化)，第一次读/写数据时，引起内核缺页中断，内核才分配对应的物理内存，然后虚拟地址空间建立映射关系)，如下图： 进程启动的时候，其（虚拟）内存空间的初始布局如图1所示。 其中，mmap内存映射文件是在堆和栈的中间（例如libc-2.2.93.so，其它数据文件等），为了简单起见，省略了内存映射文件。 _edata指针（glibc里面定义）指向数据段的最高地址。 进程调用A=malloc(30K)以后，内存空间如图2： malloc函数会调用brk系统调用，将_edata指针往高地址推30K，就完成虚拟内存分配。 你可能会问：只要把_edata+30K就完成内存分配了？ 事实是这样的，_edata+30K只是完成虚拟地址的分配，A这块内存现在还是没有物理页与之对应的，等到进程第一次读写A这块内存的时候，发生缺页中断，这个时候，内核才分配A这块内存对应的物理页。也就是说，如果用malloc分配了A这块内容，然后从来不访问它，那么，A对应的物理页是不会被分配的。 进程调用B=malloc(40K)以后，内存空间如图3。 情况二、malloc大于128k的内存，使用mmap分配内存，在堆和栈之间找一块空闲内存分配(对应独立内存，而且初始化为0)，如下图： 进程调用C=malloc(200K)以后，内存空间如图4： 默认情况下，malloc函数分配内存，如果请求内存大于128K（可由M_MMAP_THRESHOLD选项调节），那就不是去推_edata指针了，而是利用mmap系统调用，从堆和栈的中间分配一块虚拟内存。 这样子做主要是因为:: brk分配的内存需要等到高地址内存释放以后才能释放（例如，在B释放之前，A是不可能释放的，这就是内存碎片产生的原因，什么时候紧缩看下面），而mmap分配的内存可以单独释放。 当然，还有其它的好处，也有坏处，再具体下去，有兴趣的同学可以去看glibc里面malloc的代码了。 进程调用D=malloc(100K)以后，内存空间如图5； 进程调用free©以后，C对应的虚拟内存和物理内存一起释放。 进程调用free(B)以后，如图7所示： B对应的虚拟内存和物理内存都没有释放，因为只有一个_edata指针，如果往回推，那么D这块内存怎么办呢？ 当然，B这块内存，是可以重用的，如果这个时候再来一个40K的请求，那么malloc很可能就把B这块内存返回回去了。 8、进程调用free(D)以后，如图8所示： B和D连接起来，变成一块140K的空闲内存。 9、默认情况下： 当最高地址空间的空闲内存超过128K（可由M_TRIM_THRESHOLD选项调节）时，执行内存紧缩操作（trim）。在上一个步骤free的时候，发现最高地址空闲内存超过128K，于是内存紧缩，变成图9所示。 在了解了内存分配原理以后来看一个现象： 现象 压力测试过程中，发现被测对象性能不够理想，具体表现为： 进程的系统态CPU消耗20，用户态CPU消耗10，系统idle大约70 用ps -o majflt,minflt -C program命令查看，发现majflt每秒增量为0，而minflt每秒增量大于10000。 初步分析 majflt代表major fault，中文名叫大错误，minflt代表minor fault，中文名叫小错误。 这两个数值表示一个进程自启动以来所发生的缺页中断的次数。 当一个进程发生缺页中断的时候，进程会陷入内核态，执行以下操作： 检查要访问的虚拟地址是否合法 查找/分配一个物理页 填充物理页内容(读取磁盘，或者直接置0，或者啥也不干) 建立映射关系(虚拟地址到物理地址) 重新执行发生缺页中断的那条指令 如果第3步，需要读取磁盘，那么这次缺页中断就是majflt，否则就是minflt。 此进程minflt如此之高，一秒10000多次，不得不怀疑它跟进程内核态cpu消耗大有很大关系。 分析代码 查看代码，发现是这么写的：一个请求来，用malloc分配2M内存，请求结束后free这块内存。看日志，发现分配内存语句耗时10us，平均一条请求处理耗时1000us 。 原因已找到！ 虽然分配内存语句的耗时在一条处理请求中耗时比重不大，但是这条语句严重影响了性能。要解释清楚原因，需要先了解一下内存分配的原理。 真相大白 说完内存分配的原理，那么被测模块在内核态cpu消耗高的原因就很清楚了：每次请求来都malloc一块2M的内存，默认情况下，malloc调用mmap分配内存，请求结束的时候，调用munmap释放内存。假设每个请求需要6个物理页，那么每个请求就会产生6个缺页中断，在2000的压力下，每秒就产生了10000多次缺页中断，这些缺页中断不需要读取磁盘解决，所以叫做minflt;缺页中断在内核态执行，因此进程的内核态cpu消耗很大。缺页中断分散在整个请求的处理过程中，所以表现为分配语句耗时(10us)相对于整条请求的处理时间(1000us)比重很小。 解决办法 将动态内存改为静态分配，或者启动的时候，用malloc为每个线程分配，然后保存在threaddata里面。但是，由于这个模块的特殊性，静态分配，或者启动时候分配都不可行。另外，Linux下默认栈的大小限制是10M，如果在栈上分配几M的内存，有风险。 禁止malloc调用mmap分配内存，禁止内存紧缩。 在进程启动时候，加入以下两行代码： 12mallopt(M_MMAP_MAX, 0); // 禁止malloc调用mmap分配内存mallopt(M_TRIM_THRESHOLD, -1); // 禁止内存紧缩 效果：加入这两行代码以后，用ps命令观察，压力稳定以后，majlt和minflt都为0。进程的系统态cpu从20降到10。 小结 可以用命令ps -o majflt minflt -C program来查看进程的majflt, minflt的值，这两个值都是累加值，从进程启动开始累加。在对高性能要求的程序做压力测试的时候，我们可以多关注一下这两个值。 如果一个进程使用了mmap将很大的数据文件映射到进程的虚拟地址空间，我们需要重点关注majflt的值，因为相比minflt，majflt对于性能的损害是致命的，随机读一次磁盘的耗时数量级在几个毫秒，而minflt只有在大量的时候才会对性能产生影响。 本文地址：http://xnerv.wang/two-ways-of-allocating-memory-brk-and-mmap/ 转载自：进程分配内存的两种方式–brk() 和mmap()（不涉及共享内存）","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://xnerv.wang/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"操作系统","slug":"操作系统","permalink":"https://xnerv.wang/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"内存管理","slug":"内存管理","permalink":"https://xnerv.wang/tags/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"},{"name":"进程","slug":"进程","permalink":"https://xnerv.wang/tags/%E8%BF%9B%E7%A8%8B/"}]},{"title":"Memory Allocation Mechanism in Linux（转载）","slug":"memory-allocation-mechanism-in-linux","date":"2014-09-20T07:00:00.000Z","updated":"2023-08-21T02:24:19.774Z","comments":true,"path":"memory-allocation-mechanism-in-linux/","link":"","permalink":"https://xnerv.wang/memory-allocation-mechanism-in-linux/","excerpt":"Each Linux process has its own dedicated address space dynamically translated into physical memory address space by the MMU (and the kernel) [1]. To each individual process, the view is as if it alone has full access to the system’s physical memory [2]. More importantly, the address space of even a single process can be much larger than physical memory. However, the memory usage of each process is bounded with Linux resource limitation (via setrlimit()).","text":"Each Linux process has its own dedicated address space dynamically translated into physical memory address space by the MMU (and the kernel) [1]. To each individual process, the view is as if it alone has full access to the system’s physical memory [2]. More importantly, the address space of even a single process can be much larger than physical memory. However, the memory usage of each process is bounded with Linux resource limitation (via setrlimit()). The space is divided into several parts, including text, data, heap and stack, etc. Stack and heap are the two parts we’ll talk about. At the time of creation, the system creates heap and stack segment for processes. While stack is the reserved memory as scratch space for thread execution, heap is memory set aside for dynamic allocation. Each thread gets a stack, while there’s typically only one heap for the application. The OS allocates the stack for each system-level thread when the thread is created, and the stack size is predetermined in creation. Typically, the OS is called by the language runtime to allocate the heap for the application. The heap size is set on application startup, but it can grow by calling allocators to request more memory from OS. [3] In the runtime, dynamic memory management is operated on heap. Standard C functions malloc() and free() are used to allocate and deallocate memory blocks. malloc() is a library call, not a system call, which directly deals with paged virtual memory instead of physical memory (which is handled by the kernel). As the system creates heap and stack segment for processes at the time of creation, malloc() already has some memory to work with without having to call the OS for every memory request from the program. If more memory is needed (either due to malloc() or due to stack growing), then a system call brk()/sbrk()/mmap() is made to obtain a contiguous (w.r.t virtual memory address) chunk of memory, which the malloc() further slices and dices in smaller chunks and hands out to the application. [3,4] Various different implementations of malloc() attempt to satisfy any given request from the memory which has already been allocated to the process. And, these allocators are categorized mainly by how they keep track of the free blocks that they can use to parcel out memory to applications. Main categories are first-fit, best-fit and worst-fit, etc. [5] Memory allocation inside kernel Actual physical memory is managed by Linux kernel. The kernel treats physical pages as the basic unit of memory management. Although the processor’s smallest addressable unit is usually word (or even a byte), the MMU typically deals in pages. The MMU manages page tables with page-sized granularity. [2] The space in between the heap and the stack is unallocated space, therefore the top of the heap is often referred to as the “break point” because this is where the memory space is split. As more dynamic memory is needed, the application must inform the OS to move up the break point to allocate more memory for the process, thus increasing the heap size. The allocation is usually achieved by brk()/sbrk(). On a call to brk(), the Linux kernel performs a few checks and then allocates the new memory for the process. First, the kernel aligns the old and the new break point to be on page boundaries. A page (usually 4KB) is the smallest unit of memory that the OS will give to any process. Then, the system call checks the limits and the kernel verifies that it is safe to allocate the required memory, and finally the kernel calls do_brk() function to increase the memory for the process. Each process has a map/page-table to take a certain range of addresses in the processes’ address space and to map it physical memory. When the process calls brk(), the OS increases the map size for the heap, thus giving the process more memory to use. When the map size is increased, the new pages that are mapped into the address space do not actually exist, i.e., no physical pages are allocated for the new mapping. The OS, by increasing the map size, only provides a mapping between the new addresses in the process space and the memory that those pages will occupy when they are used. Summary of malloc() workflow The malloc() calls an internal helper, chunk_alloc(), to find the block to return to the user. This helper is the function that looks through the bins for a match to return. If its cannot find a suitable match, it calls malloc_extend_top(), another helper function, that actually calls sbrk(). Once the memory is retrieved from the OS, chunk_alloc() splits off the piece that is required for the current location. It then adds the remainder to the appropriate bin for future use and returns the new block to malloc(), which then returns that block to the user. As it turns out, most of the work that malloc() does deals with deciding how to manage memory that has already been allocated by the OS. [1] A Malloc Tutorial, 2009 [2] http://www.makelinux.net/books/lkd2/?u=ch14 [3] http://stackoverflow.com/questions/79923/what-and-where-are-the-stack-and-heap [4] http://cs.boisestate.edu/~amit/teaching/453/slides/memory-management-handout.pdf [5] http://www.linux-mag.com/id/827/ 本文地址：http://xnerv.wang/memory-allocation-mechanism-in-linux/ 转载自：Memory Allocation Mechanism in Linux","categories":[{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/categories/Linux/"}],"tags":[{"name":"Heap","slug":"Heap","permalink":"https://xnerv.wang/tags/Heap/"},{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/tags/Linux/"},{"name":"Memory Management","slug":"Memory-Management","permalink":"https://xnerv.wang/tags/Memory-Management/"},{"name":"reprint","slug":"reprint","permalink":"https://xnerv.wang/tags/reprint/"}]},{"title":"在调试器中看阿里的软件兵团（转载）","slug":"debugging-alibaba-softwares","date":"2014-09-16T21:48:00.000Z","updated":"2023-08-21T02:24:20.021Z","comments":true,"path":"debugging-alibaba-softwares/","link":"","permalink":"https://xnerv.wang/debugging-alibaba-softwares/","excerpt":"摘要：为了抢占用户桌面，很多客户端软件都是主动送上门的，不需要花钱购买。可能只要你一个认可，就在电脑上安家落户。住下来之后，有些软件偶尔动动，但也有些高调强势，让你不得不注意到它，本文要谈的就是这样的软件。","text":"摘要：为了抢占用户桌面，很多客户端软件都是主动送上门的，不需要花钱购买。可能只要你一个认可，就在电脑上安家落户。住下来之后，有些软件偶尔动动，但也有些高调强势，让你不得不注意到它，本文要谈的就是这样的软件。 谁在消耗电池？ 人们对手机、笔记本电脑等移动设备的依赖越来越大。一旦电池用尽而又不能立刻充电，便可能把人急得双脚跳。面对类似情况，为了让笔记本电池多支撑一会儿，我会采取两项省电措施：一是把屏幕调暗，二是杀掉特别费电的软件。第一项容易理解，略去不谈。第二项的关键是如何找到高功耗软件。一种简单的方法是调出任务管理器，通过View菜单（Windows 8之前）或者在列表的标题行（Windows 8/8.1）调出图1所示的“选择列”对话框，然后将Page faults（缺页异常总次数）和PF Delta（自上次更新后缺页异常的新增次数，默认为每秒更新一次）两项选中。接下来分别按Page faults、PF Delta和CPU使用率指标对进程排序，找出这几项指标高的进程。 为何选择Page faults来衡量软件对功耗的影响？虽然今天的计算机都配备了比较多的物理内存，但仍离不开虚拟内存技术，把暂时不用的数据放在外存中，当CPU访问这样的数据时，会报告缺页异常，让操作系统的内存管理器将数据从外存中读到物理内存，这个操作通常被称为Page In。物理内存是以页为单位来管理的，因此每次Page In的数据至少是一个页，通常为4KB。访问外存意味着系统总线和硬盘等存储设备的运作，在时间和功耗方面都是较大的开销，因此，Page faults常成为系统调优的一个重要指标。 正是使用以上方法查找高耗电高软件时，Alipay引起了我的注意。图2是当时的屏幕截图，可以看到，任务管理器中的各个进程（任务）是按Page faults总数排列的，而位列前三的分别是AlipaySecSvc、AlipayBsm和TaobaoProtect，全是Alipay软件成员。它们导致的Page faults总数分别为四亿五千多万次、一亿三千多万次和八千多万次。假定每次Page fault触发的Page In数据都是4KB，那么它们促使系统Page In的数据量分别为大约1.8TB、500GB和300GB。 另外，从PF Delta列来看，排名第一的AlipaySecSvc进程在最近一秒内就触发了2906次缺页异常。 坦率说，这样的结果让我惊叹不已。通常排在前列的都是安全软件。而自从我的机器上有了Alipay软件后，它们总是可以轻松超越杀毒软件。图2中，第6名是系统窗口合成器，第4、5、7名都是安全软件。 因为处理每次Page fault时要执行比较复杂的逻辑，所以高Page faults也常意味着较高的CPU使用率。在图2中，AlipaySecSvc进程使用的CPU总时间高达1小时20分32秒。今天的CPU速度惊人，很多“分量”轻的软件运行一天可能也用不上CPU几秒钟（大多被挂起）。尽管AlipaySecSvc的名字中也包含安全字样（Sec），但其CPU总时间如此高也着实离谱。简而言之，这个进程的分量重得惊人。 AlipaySecSvc 那么，如此重的AlipaySecSvc进程是做什么的呢？好奇心和职业精神都驱使我深入了解这个进程。打开系统服务控制台，找到AlipaySecSvc服务，查看其属性（如图3），可以看到它的全称（Alipay security service）、官方身份和在磁盘上的位置信息。 根据图3中路径信息，可以在磁盘上找到Alipay目录（如图4），其下有三个子目录。根据布局可以推测，我的机器上已安装了Alipay的三个组件，分别是AliEditPlus、AlipayDHC和SafeTransaction，引起我们注意的AlipaySecSvc是AliEditPlus的一部分。看来，AliEditPlus绝不是孤军奋战，一个兵团已在我的电脑上安营扎寨了。 图3中的描述信息声明了AlipaySecSvc服务的重要性，但同时也说明了它的职权范围是“电子支付”。既然如此，当用户未做支付操作时，AlipaySecSvc应该尽可能保持安静。但事实上，它却始终忙碌着，甚至连浏览器进程没有启动时也是如此，这便不正常了。 上调试器 不轻信，不迷信，还是请出WinDBG来看分明。以管理员身份运行WinDBG，赐予其系统级的督察权利，然后将其附加到AlipaySecSvc进程。 接下来的目标是寻找这个进程躁动的原因。如何做呢？有多种方法，例如以前介绍过的使用~*e .ttime命令观察每个线程的执行时间，再例如使用~* k命令显示每个线程的栈回溯，寻找线索。对于眼下的问题，这两种方法也都有效。但为了避免陈词滥调，这次我打算介绍种新方法。 简单说，就是让被调试对象在调试器里跑一跑，让其“自露马脚”。套用赵本山的话就是“有病没病走两步”。轻扣键盘，发出g命令，恢复AlipaySecSvc运行，端起茶杯，等待WinDBG报告“蛛丝马迹”。 手里的茶杯还没放下，WinDBG便有所发现。屏幕上出现如下信息。 1**(27bc.1b30): Unknown exception – code 000006ba (first chance)** 看来是有真的异常（Windows系统的结构化异常SEH）发生。继续观察，WinDBG连续输出这样的信息，间隔不到一秒。根据经验，这个信息很有价值，可以作为突破口。轻按Ctrl+Break再将AlipaySecSvc断下，然后通过菜单Debug→Event Filters调出图5所示的调试事件过滤器对话框。 因为信息输出的6ba异常不在WinDBG的常见异常列表中，所以点击Add按钮增加一个代码为0x6ba的异常。之后选中新增的项目，再点击Command按钮调出图6所示的过滤器关联命令对话框，并输入： 1**.echo ********bang******;? @$tid;.ttime** 执行如上操作，再恢复AlipaySecSvc运行，让其再走些步。这时我们看到屏幕上持续输出信息，如图7所示（此图为后补，与图1不属同一次调试）。 在软件世界，一次异常就是一起爆炸事件。如此连续不断的爆炸必然会让CPU负担很重。 6ba异常 接下来的问题是为什么有如此多的6ba异常呢？Ctrl+Break断下，执行命令sxe 6ba告诉WinDBG再有6ba异常发生时立刻停下来。恢复运行后，果然很快又停下，位置正是6ba异常的发生现场： 看来是有人调用了著名的RaiseException API发起软件异常。是发生了什么矛盾，以至于要引爆炸弹呢？k一下看缘由吧，结果如图8所示。 细看图8，关注本专栏的读者一定可以看出个“破绽”，符号不精确。是的，诚然老雷偷懒了，没有使用PDB号，只用了导出符号。但对于我们的分析，这样的信息足够了。因为其中包含了以下重要内容： AlipaySecSvc服务调用了WTSEnumerate­Sessions API。 WTSEnumerateSessionsW函数（WTS­Enumerate­Sessions API的Unicode版本实现）使用了RPC机制在做远程调用。 RPC机制的运行时模块（rpcrpt4）检测到不正常情况，“大为关火”，抛出了异常。 有了这些信息，已没必要探究RPC检测到了何种意外，因为我们有了如下结论：AlipaySecSvc服务调用了一个依赖RPC机制的沉重API，而且API执行不顺利，导致了异常。 根据前面的监视结果（见图7），6ba异常是反复发生的，说明这个沉重的过程也是在循环进行。对wtsapi32!WTSEnumerateSessionsW设置断点，果然反复命中，还不止一个线程命中断点，居然有多个线程在调用这个沉重的API和触发异常。从其中的线程ID来看，也有两个（6960和1032）。综合前面的分析，可以对AlipaySecSvc服务进程的躁动原因做出初步诊断：多个线程循环调用沉重的WTSEnumerateSessions API，而且执行时触发异常。 对于性能问题，也可用WPT帮忙。它的全称是Windows Performance Toolkit，曾用名xPerf。安装完成后，先启动WPR（Windows Performance Recorder），让其开启系统中早已埋伏好的ETW（Event Tracing for Windows）事件，重现问题后，停止录制，WPR会把收集到的事件整理到一个庞大的etl文件中。最后再使用WPA（Windows Performance Analyzer）打开etl文件进行分析。 详细介绍WPA的用法超出了本文的范围，这里只做简述。 图9是使用WPA分析CPU占用情况的截图。重点看右侧的采样数据。画面以曲线图为核心分三个部分，下面的表格是详细数据，左上角是进程、模块列表，可选择其中的一个或多个，每个对应一条曲线。我们故意屏蔽了其他进程，只显示AlipaySecSvc的曲线。 每个尖峰代表一次较重的负载（占用CPU较多）。尖峰反复出现说明这些负载是周期性的，与我们前面分析的在循环中反复调用沉重API的结论完全一致。尖峰的幅度不很一致，是因为有多个线程在执行重负荷，发生和叠加的时机不同。 再观察图9中的详细数据，可以看到进程内部模块和函数一级的信息。WPA已根据模块的样本点数做过排序（点数越多，意味着占用CPU越多），内核模块排名第一，说明AlipaySecSvc进程做了多次系统调用，进一步还可发现有一个线程反复分配大堆块和调用NtQuerySystemInformation。 展开函数一级的信息，可以看到占用CPU较多的函数。例如在kernel32.dll模块下，Process32­NextW赫然在列。这告诉我们，AlipaySecSvc除了调用沉重的WTSEnumerate­Sessions API外，还调用了另一个沉重的API Process32­Next。前者枚举系统中的所有登录会话，而后者枚举会话中的进程。据此，我们可以推测，AlipaySecSvc的循环中，先是枚举会话，然后再枚举会话中的每个进程。笔者多年前就分析过Process32Next API，得到的结论是，这个API与缺页异常密切相关，几乎每次调用，都会触发数百次的缺页异常。而我们正分析的AlipaySecSvc进程，有两个线程在以循环的方式反复调用这个API，其结果就是本文开头说的累计缺页异常数排名第一。 API的分量 使用同样的方法分析缺页异常总数排名第二和第三的Alipaybsm（Browser Safe Monitor）以及TaobaoProtect，结果与此相似，或许它们三兄弟在共享循环调用沉重API的经典代码吧，也可能它们都出自一位同行之手。 普通世界中的商品都明确标识重量。这个基本属性非常重要，尤其在今天的网购时代，买家可能根据重量判断货物的质量，卖家很可能根据重量计算运输（快递）成本。但在软件世界中，标记代码的重量还没有任何规范，甚至尚无测量代码重量的标准方法。更严重的是，很多程序员同行会认为，代码有什么重量呢？ 如果说为所有普通函数标记重量还为时过早，那么给操作系统的标准API标识重量该排上议事日程了。这不仅必要而且可行。例如用可能引发的系统调用次数、跨进程调用的次数、触发缺页异常的次数等指标来衡量API的重量。标记之后，程序员就有所依据，避免频繁调用太重的API，尤其不要在当前没有明确任务的进程中调用这些沉重的API，以免白白消耗电池，让用户反感。 话说回来，关键问题还在于写代码的程序员，即使标记重量了，程序员可能也置之不理。不标记重量，有经验的程序员也心中有数。退一步讲，本文讨论的问题，只要打开任务管理器就能察觉，挂一下WinDBG更容易发现，运行WPT也可以发现，但为什么问题就这样发生了呢？ 本文地址：http://xnerv.wang/debugging-alibaba-softwares/ 转载自：在调试器中看阿里的软件兵团","categories":[{"name":"WinDbg","slug":"WinDbg","permalink":"https://xnerv.wang/categories/WinDbg/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/tags/Windows/"},{"name":"WinDbg","slug":"WinDbg","permalink":"https://xnerv.wang/tags/WinDbg/"}]},{"title":"【协程原理】 - 协程不过是用户态的线程（转载）","slug":"coroutine-is-a-type-of-user-mode-thread","date":"2014-09-10T07:00:00.000Z","updated":"2023-08-21T02:24:21.089Z","comments":true,"path":"coroutine-is-a-type-of-user-mode-thread/","link":"","permalink":"https://xnerv.wang/coroutine-is-a-type-of-user-mode-thread/","excerpt":"笔者最美好的记忆来自于早年在6502 cpu的cc800上写汇编的年代， 那个时代的计算机甚至没有操作系统，也没有实模式等保护机制。在6502上写汇编应用其实非常简单，系统会把bin文件加载到一个固定的内存地址中，cpu会固定地从一个特定的位置开始执行。然后cpu就按照你提供的机器指令开始一条一条的执行。在高级语言中的“函数调用”的概念，在汇编里主要体现为两个寄存器。寄存器是cpu内部临时保存数据的区域，相当于高级语言里的变量。但是有一个寄存器是特殊的，它存放了cpu当前正在执行的指令的内存地址(Instruction Register)。一个高级语言中的函数一般会被编译成指令存放在一段连续的内存空间中（data segment）。那么所谓函数执行到了第几行这样的信息其实就是保存在这个Instruction Register中的。另外一个很特殊的寄存器是Stack Register，它其中存放的内存地址指向的内存区域用于函数之间传递参数和返回值，以及存放一个函数内的局部变量。如果不考虑现代计算机cpu中各种各样其他存放中间结果的寄存器，理论上保存了Instruction Register（执行到哪儿了）和Stack Register（堆栈上的变量）就保存了一个函数的当前执行状态，分别是函数当前执行到了哪，以及这个函数局部变量所代表的当前state。","text":"笔者最美好的记忆来自于早年在6502 cpu的cc800上写汇编的年代， 那个时代的计算机甚至没有操作系统，也没有实模式等保护机制。在6502上写汇编应用其实非常简单，系统会把bin文件加载到一个固定的内存地址中，cpu会固定地从一个特定的位置开始执行。然后cpu就按照你提供的机器指令开始一条一条的执行。在高级语言中的“函数调用”的概念，在汇编里主要体现为两个寄存器。寄存器是cpu内部临时保存数据的区域，相当于高级语言里的变量。但是有一个寄存器是特殊的，它存放了cpu当前正在执行的指令的内存地址(Instruction Register)。一个高级语言中的函数一般会被编译成指令存放在一段连续的内存空间中（data segment）。那么所谓函数执行到了第几行这样的信息其实就是保存在这个Instruction Register中的。另外一个很特殊的寄存器是Stack Register，它其中存放的内存地址指向的内存区域用于函数之间传递参数和返回值，以及存放一个函数内的局部变量。如果不考虑现代计算机cpu中各种各样其他存放中间结果的寄存器，理论上保存了Instruction Register（执行到哪儿了）和Stack Register（堆栈上的变量）就保存了一个函数的当前执行状态，分别是函数当前执行到了哪，以及这个函数局部变量所代表的当前state。 事实上，操作系统的几个关键切换也是这么来完成的。操作系统提供了两个执行态，一个是用户态，一般我们的代码都是执行在用户态的。另外一个是内核态，像驱动程序之类的代码会用各种方式被加载到操作系统内部执行在内核之中。内核态里的代码可以完全控制CPU的I/O中断，从而可以和外部设备交互。用户态的代码属于受限代码，必须把I/O请求通过syscall交由运行在内核态的操作系统来完成。当一个cpu的核在执行用户态代码时，其寄存器里存放的状态是你的应用的代码的状态，但是应用要进行I/O操作的时候，cpu要被切换到内核的代码里去执行内核态的代码。这里就需要进行一次context switch，所谓context switch其实原理不会比把寄存器的值存到内存的一个地方，等回来的时候再把内存中临时保存的值加载回寄存器复杂多少。 操作系统还有一个需要进行context switch的地方，那就是在协程与协程之间。操作系统在执行一个ELF或者PE的可执行文件的时候，对于这个可执行文件内的汇编代码来说，整个内存寻址空间是独立的。也就是1.exe的执行状态完全无法感知到2.exe的执行状态的内存。也就是现代操作系统的虚拟内存空间。有cpu在两个进程之间切换状态的时候，需要把内存的映射关系调整过来，否则虚拟内存的地址是无法对应到正确的物理地址的。一个进程内的两个线成切换的时候，要稍微简单一些，只需要把当前线成正在执行的位置和栈做切换就可以了。 无论是操作系统做user/kernel的switch，还是process/process，thread/thread的switch，其实现方式都是大同小异的。通过把“当前执行状态”这样的一个抽象概念落实为一个具体的数据结构存储起来，然后指挥cpu在不同的场合加载不同的数据恢复不同的“当前执行状态”。 在高级语言中，一个函数正在执行的位置以及其状态，内部都可以有一个抽象的表达方式。有的高级语言直接被编译成原生的机器码，那么其执行状态的表述就和操作系统的context switch的context非常类似。有的高级语言自身执行在一个虚拟机之上，那么其context的表述可能是虚拟机的instruction register和stack register，而不是80x86这样原生的机器的物理寄存器。但是原理是非常类似的。 取决于语言设计者的觉悟，有的语言会把这种表达执行状态的能力直接提供出来，让一个函数在执行过程中可以把当前状态保存，然后把执行权交给另外一个函数执行，等那个函数放弃执行权回来的时候再把保存的状态恢复。这也就是所谓的协程（co-routine）。协程与线程的区别在于，协程的context switch是在完全在用户态，由语言的runtime或者是库来完成的。而线程的context switch则是操作系统来完成的。 本文地址：http://xnerv.wang/coroutine-is-a-type-of-user-mode-thread/ 转载自：【协程原理】 - 协程不过是用户态的线程","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://xnerv.wang/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"操作系统","slug":"操作系统","permalink":"https://xnerv.wang/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"协程","slug":"协程","permalink":"https://xnerv.wang/tags/%E5%8D%8F%E7%A8%8B/"},{"name":"线程","slug":"线程","permalink":"https://xnerv.wang/tags/%E7%BA%BF%E7%A8%8B/"}]},{"title":"如何实现一个malloc（转载）","slug":"how-to-implement-a-malloc","date":"2014-08-19T07:00:00.000Z","updated":"2023-08-21T02:24:21.763Z","comments":true,"path":"how-to-implement-a-malloc/","link":"","permalink":"https://xnerv.wang/how-to-implement-a-malloc/","excerpt":"任何一个用过或学过C的人对malloc都不会陌生。大家都知道malloc可以分配一段连续的内存空间，并且在不再使用时可以通过free释放掉。但是，许多程序员对malloc背后的事情并不熟悉，许多人甚至把malloc当做操作系统所提供的系统调用或C的关键字。实际上，malloc只是C的标准库中提供的一个普通函数，而且实现malloc的基本思想并不复杂，任何一个对C和操作系统有些许了解的程序员都可以很容易理解。 这篇文章通过实现一个简单的malloc来描述malloc背后的机制。当然与现有C的标准库实现（例如glibc）相比，我们实现的malloc并不是特别高效，但是这个实现比目前真实的malloc实现要简单很多，因此易于理解。重要的是，这个实现和真实实现在基本原理上是一致的。 这篇文章将首先介绍一些所需的基本知识，如操作系统对进程的内存管理以及相关的系统调用，然后逐步实现一个简单的malloc。为了简单起见，这篇文章将只考虑x86_64体系结构，操作系统为Linux。","text":"任何一个用过或学过C的人对malloc都不会陌生。大家都知道malloc可以分配一段连续的内存空间，并且在不再使用时可以通过free释放掉。但是，许多程序员对malloc背后的事情并不熟悉，许多人甚至把malloc当做操作系统所提供的系统调用或C的关键字。实际上，malloc只是C的标准库中提供的一个普通函数，而且实现malloc的基本思想并不复杂，任何一个对C和操作系统有些许了解的程序员都可以很容易理解。 这篇文章通过实现一个简单的malloc来描述malloc背后的机制。当然与现有C的标准库实现（例如glibc）相比，我们实现的malloc并不是特别高效，但是这个实现比目前真实的malloc实现要简单很多，因此易于理解。重要的是，这个实现和真实实现在基本原理上是一致的。 这篇文章将首先介绍一些所需的基本知识，如操作系统对进程的内存管理以及相关的系统调用，然后逐步实现一个简单的malloc。为了简单起见，这篇文章将只考虑x86_64体系结构，操作系统为Linux。 1 什么是malloc 在实现malloc之前，先要相对正式地对malloc做一个定义。 根据标准C库函数的定义，malloc具有如下原型： 1void* malloc(size_t size); 这个函数要实现的功能是在系统中分配一段连续的可用的内存，具体有如下要求： malloc分配的内存大小至少为size参数所指定的字节数 malloc的返回值是一个指针，指向一段可用内存的起始地址 多次调用malloc所分配的地址不能有重叠部分，除非某次malloc所分配的地址被释放掉 malloc应该尽快完成内存分配并返回（不能使用NP-hard的内存分配算法） 实现malloc时应同时实现内存大小调整和内存释放函数（即realloc和free） 对于malloc更多的说明可以在命令行中键入以下命令查看： 1man malloc 2 预备知识 在实现malloc之前，需要先解释一些Linux系统内存相关的知识。 2.1 Linux内存管理 2.1.1 虚拟内存地址与物理内存地址 为了简单，现代操作系统在处理内存地址时，普遍采用虚拟内存地址技术。即在汇编程序（或机器语言）层面，当涉及内存地址时，都是使用虚拟内存地址。采用这种技术时，每个进程仿佛自己独享一片2N字节的内存，其中N是机器位数。例如在64位CPU和64位操作系统下，每个进程的虚拟地址空间为264Byte。 这种虚拟地址空间的作用主要是简化程序的编写及方便操作系统对进程间内存的隔离管理，真实中的进程不太可能（也用不到）如此大的内存空间，实际能用到的内存取决于物理内存大小。 由于在机器语言层面都是采用虚拟地址，当实际的机器码程序涉及到内存操作时，需要根据当前进程运行的实际上下文将虚拟地址转换为物理内存地址，才能实现对真实内存数据的操作。这个转换一般由一个叫MMU（Memory Management Unit）的硬件完成。 2.1.2 页与地址构成 在现代操作系统中，不论是虚拟内存还是物理内存，都不是以字节为单位进行管理的，而是以页（Page）为单位。一个内存页是一段固定大小的连续内存地址的总称，具体到Linux中，典型的内存页大小为4096Byte（4K）。 所以内存地址可以分为页号和页内偏移量。下面以64位机器，4G物理内存，4K页大小为例，虚拟内存地址和物理内存地址的组成如下： 上面是虚拟内存地址，下面是物理内存地址。由于页大小都是4K，所以页内便宜都是用低12位表示，而剩下的高地址表示页号。 MMU映射单位并不是字节，而是页，这个映射通过查一个常驻内存的数据结构页表来实现。现在计算机具体的内存地址映射比较复杂，为了加快速度会引入一系列缓存和优化，例如TLB等机制。下面给出一个经过简化的内存地址翻译示意图，虽然经过了简化，但是基本原理与现代计算机真实的情况的一致的。 2.1.3 内存页与磁盘页 我们知道一般将内存看做磁盘的的缓存，有时MMU在工作时，会发现页表表明某个内存页不在物理内存中，此时会触发一个缺页异常（Page Fault），此时系统会到磁盘中相应的地方将磁盘页载入到内存中，然后重新执行由于缺页而失败的机器指令。关于这部分，因为可以看做对malloc实现是透明的，所以不再详细讲述，有兴趣的可以参考《深入理解计算机系统》相关章节。 最后附上一张在维基百科找到的更加符合真实地址翻译的流程供大家参考，这张图加入了TLB和缺页异常的流程（图片来源页）。 2.2 Linux进程级内存管理 2.2.1 内存排布 明白了虚拟内存和物理内存的关系及相关的映射机制，下面看一下具体在一个进程内是如何排布内存的。 以Linux 64位系统为例。理论上，64bit内存地址可用空间为0x0000000000000000 ~ 0xFFFFFFFFFFFFFFFF，这是个相当庞大的空间，Linux实际上只用了其中一小部分（256T）。 根据Linux内核相关文档描述，Linux64位操作系统仅使用低47位，高17位做扩展（只能是全0或全1）。所以，实际用到的地址为空间为0x0000000000000000 ~ 0x00007FFFFFFFFFFF和0xFFFF800000000000 ~ 0xFFFFFFFFFFFFFFFF，其中前面为用户空间（User Space），后者为内核空间（Kernel Space）。图示如下： 对用户来说，主要关注的空间是User Space。将User Space放大后，可以看到里面主要分为如下几段： Code：这是整个用户空间的最低地址部分，存放的是指令（也就是程序所编译成的可执行机器码） Data：这里存放的是初始化过的全局变量 BSS：这里存放的是未初始化的全局变量 Heap：堆，这是我们本文重点关注的地方，堆自低地址向高地址增长，后面要讲到的brk相关的系统调用就是从这里分配内存 Mapping Area：这里是与mmap系统调用相关的区域。大多数实际的malloc实现会考虑通过mmap分配较大块的内存区域，本文不讨论这种情况。这个区域自高地址向低地址增长 Stack：这是栈区域，自高地址向低地址增长 下面我们主要关注Heap区域的操作。对整个Linux内存排布有兴趣的同学可以参考其它资料。 2.2.2 Heap内存模型 一般来说，malloc所申请的内存主要从Heap区域分配（本文不考虑通过mmap申请大块内存的情况）。 由上文知道，进程所面对的虚拟内存地址空间，只有按页映射到物理内存地址，才能真正使用。受物理存储容量限制，整个堆虚拟内存空间不可能全部映射到实际的物理内存。Linux对堆的管理示意如下： Linux维护一个break指针，这个指针指向堆空间的某个地址。从堆起始地址到break之间的地址空间为映射好的，可以供进程访问；而从break往上，是未映射的地址空间，如果访问这段空间则程序会报错。 2.2.3 brk与sbrk 由上文知道，要增加一个进程实际的可用堆大小，就需要将break指针向高地址移动。Linux通过brk和sbrk系统调用操作break指针。两个系统调用的原型如下： 12int brk(void *addr);void *sbrk(intptr_t increment); brk将break指针直接设置为某个地址，而sbrk将break从当前位置移动increment所指定的增量。brk在执行成功时返回0，否则返回-1并设置errno为ENOMEM；sbrk成功时返回break移动之前所指向的地址，否则返回(void *)-1。 一个小技巧是，如果将increment设置为0，则可以获得当前break的地址。 另外需要注意的是，由于Linux是按页进行内存映射的，所以如果break被设置为没有按页大小对齐，则系统实际上会在最后映射一个完整的页，从而实际已映射的内存空间比break指向的地方要大一些。但是使用break之后的地址是很危险的（尽管也许break之后确实有一小块可用内存地址）。 2.2.4 资源限制与rlimit 系统对每一个进程所分配的资源不是无限的，包括可映射的内存空间，因此每个进程有一个rlimit表示当前进程可用的资源上限。这个限制可以通过getrlimit系统调用得到，下面代码获取当前进程虚拟内存空间的rlimit： 12345int main() &#123; struct rlimit *limit = (struct rlimit *)malloc(sizeof(struct rlimit)); getrlimit(RLIMIT_AS, limit); printf(&quot;soft limit: %ld, hard limit: %ld\\n&quot;, limit-&gt;rlim_cur, limit-&gt;rlim_max);&#125; 其中rlimit是一个结构体： 1234struct rlimit &#123; rlim_t rlim_cur; /* Soft limit */ rlim_t rlim_max; /* Hard limit (ceiling for rlim_cur) */&#125;; 每种资源有软限制和硬限制，并且可以通过setrlimit对rlimit进行有条件设置。其中硬限制作为软限制的上限，非特权进程只能设置软限制，且不能超过硬限制。 3 实现malloc 3.1 玩具实现 在正式开始讨论malloc的实现前，我们可以利用上述知识实现一个简单但几乎没法用于真实的玩具malloc，权当对上面知识的复习： 1234567891011/* 一个玩具malloc */#include &lt;sys/types.h&gt;#include &lt;unistd.h&gt;void *malloc(size_t size)&#123; void *p; p = sbrk(0); if (sbrk(size) == (void *)-1) return NULL; return p;&#125; 这个malloc每次都在当前break的基础上增加size所指定的字节数，并将之前break的地址返回。这个malloc由于对所分配的内存缺乏记录，不便于内存释放，所以无法用于真实场景。 3.2 正式实现 下面严肃点讨论malloc的实现方案。 3.2.1 数据结构 首先我们要确定所采用的数据结构。一个简单可行方案是将堆内存空间以块（Block）的形式组织起来，每个块由meta区和数据区组成，meta区记录数据块的元信息（数据区大小、空闲标志位、指针等等），数据区是真实分配的内存区域，并且数据区的第一个字节地址即为malloc返回的地址。 可以用如下结构体定义一个block： 12345678typedef struct s_block *t_block;struct s_block &#123; size_t size; /* 数据区大小 */ t_block next; /* 指向下个块的指针 */ int free; /* 是否是空闲块 */ int padding; /* 填充4字节，保证meta块长度为8的倍数 */ char data[1] /* 这是一个虚拟字段，表示数据块的第一个字节，长度不应计入meta */&#125;; 由于我们只考虑64位机器，为了方便，我们在结构体最后填充一个int，使得结构体本身的长度为8的倍数，以便内存对齐。示意图如下： 3.2.2 寻找合适的block 现在考虑如何在block链中查找合适的block。一般来说有两种查找算法： First fit：从头开始，使用第一个数据区大小大于要求size的块所谓此次分配的块 Best fit：从头开始，遍历所有块，使用数据区大小大于size且差值最小的块作为此次分配的块 两种方法各有千秋，best fit具有较高的内存使用率（payload较高），而first fit具有更好的运行效率。这里我们采用first fit算法。 123456789/* First fit */t_block find_block(t_block *last, size_t size) &#123; t_block b = first_block; while(b &amp;&amp; !(b-&gt;free &amp;&amp; b-&gt;size &gt;= size)) &#123; *last = b; b = b-&gt;next; &#125; return b;&#125; find_block从frist_block开始，查找第一个符合要求的block并返回block起始地址，如果找不到这返回NULL。这里在遍历时会更新一个叫last的指针，这个指针始终指向当前遍历的block。这是为了如果找不到合适的block而开辟新block使用的，具体会在接下来的一节用到。 3.2.3 开辟新的block 如果现有block都不能满足size的要求，则需要在链表最后开辟一个新的block。这里关键是如何只使用sbrk创建一个struct： 1234567891011121314#define BLOCK_SIZE 24 /* 由于存在虚拟的data字段，sizeof不能正确计算meta长度，这里手工设置 */t_block extend_heap(t_block last, size_t s) &#123; t_block b; b = sbrk(0); if(sbrk(BLOCK_SIZE + s) == (void *)-1) return NULL; b-&gt;size = s; b-&gt;next = NULL; if(last) last-&gt;next = b; b-&gt;free = 0; return b;&#125; 3.2.4 分裂block First fit有一个比较致命的缺点，就是可能会让很小的size占据很大的一块block，此时，为了提高payload，应该在剩余数据区足够大的情况下，将其分裂为一个新的block，示意如下： 实现代码： 123456789void split_block(t_block b, size_t s) &#123; t_block new; new = b-&gt;data + s; new-&gt;size = b-&gt;size - s - BLOCK_SIZE ; new-&gt;next = b-&gt;next; new-&gt;free = 1; b-&gt;size = s; b-&gt;next = new;&#125; 3.2.5 malloc的实现 有了上面的代码，我们可以利用它们整合成一个简单但初步可用的malloc。注意首先我们要定义个block链表的头first_block，初始化为NULL；另外，我们需要剩余空间至少有BLOCK_SIZE + 8才执行分裂操作。 由于我们希望malloc分配的数据区是按8字节对齐，所以在size不为8的倍数时，我们需要将size调整为大于size的最小的8的倍数： 12345size_t align8(size_t s) &#123; if(s &amp; 0x7 == 0) return s; return ((s &gt;&gt; 3) + 1) &lt;&lt; 3;&#125; 123456789101112131415161718192021222324252627282930313233#define BLOCK_SIZE 24void *first_block=NULL;/* other functions... */void *malloc(size_t size) &#123; t_block b, last; size_t s; /* 对齐地址 */ s = align8(size); if(first_block) &#123; /* 查找合适的block */ last = first_block; b = find_block(&amp;last, s); if(b) &#123; /* 如果可以，则分裂 */ if ((b-&gt;size - s) &gt;= ( BLOCK_SIZE + 8)) split_block(b, s); b-&gt;free = 0; &#125; else &#123; /* 没有合适的block，开辟一个新的 */ b = extend_heap(last, s); if(!b) return NULL; &#125; &#125; else &#123; b = extend_heap(NULL, s); if(!b) return NULL; first_block = b; &#125; return b-&gt;data;&#125; 3.2.6 calloc的实现 有了malloc，实现calloc只要两步： malloc一段内存 将数据区内容置为0 由于我们的数据区是按8字节对齐的，所以为了提高效率，我们可以每8字节一组置0，而不是一个一个字节设置。我们可以通过新建一个size_t指针，将内存区域强制看做size_t类型来实现。 1234567891011void *calloc(size_t number, size_t size) &#123; size_t *new; size_t s8, i; new = malloc(number * size); if(new) &#123; s8 = align8(number * size) &gt;&gt; 3; for(i = 0; i &lt; s8; i++) new[i] = 0; &#125; return new;&#125; 3.2.7 free的实现 free的实现并不像看上去那么简单，这里我们要解决两个关键问题： 如何验证所传入的地址是有效地址，即确实是通过malloc方式分配的数据区首地址 如何解决碎片问题 首先我们要保证传入free的地址是有效的，这个有效包括两方面： 地址应该在之前malloc所分配的区域内，即在first_block和当前break指针范围内 这个地址确实是之前通过我们自己的malloc分配的 第一个问题比较好解决，只要进行地址比较就可以了，关键是第二个问题。这里有两种解决方案：一是在结构体内埋一个magic number字段，free之前通过相对偏移检查特定位置的值是否为我们设置的magic number，另一种方法是在结构体内增加一个magic pointer，这个指针指向数据区的第一个字节（也就是在合法时free时传入的地址），我们在free前检查magic pointer是否指向参数所指地址。这里我们采用第二种方案： 首先我们在结构体中增加magic pointer（同时要修改BLOCK_SIZE）： 123456789typedef struct s_block *t_block;struct s_block &#123; size_t size; /* 数据区大小 */ t_block next; /* 指向下个块的指针 */ int free; /* 是否是空闲块 */ int padding; /* 填充4字节，保证meta块长度为8的倍数 */ void *ptr; /* Magic pointer，指向data */ char data[1] /* 这是一个虚拟字段，表示数据块的第一个字节，长度不应计入meta */&#125;; 然后我们定义检查地址合法性的函数： 1234567891011121314t_block get_block(void *p) &#123; char *tmp; tmp = p; return (p = tmp -= BLOCK_SIZE);&#125;int valid_addr(void *p) &#123; if(first_block) &#123; if(p &gt; first_block &amp;&amp; p &lt; sbrk(0)) &#123; return p == (get_block(p))-&gt;ptr; &#125; &#125; return 0;&#125; 当多次malloc和free后，整个内存池可能会产生很多碎片block，这些block很小，经常无法使用，甚至出现许多碎片连在一起，虽然总体能满足某此malloc要求，但是由于分割成了多个小block而无法fit，这就是碎片问题。 一个简单的解决方式时当free某个block时，如果发现它相邻的block也是free的，则将block和相邻block合并。为了满足这个实现，需要将s_block改为双向链表。修改后的block结构如下： 12345678910typedef struct s_block *t_block;struct s_block &#123; size_t size; /* 数据区大小 */ t_block prev; /* 指向上个块的指针 */ t_block next; /* 指向下个块的指针 */ int free; /* 是否是空闲块 */ int padding; /* 填充4字节，保证meta块长度为8的倍数 */ void *ptr; /* Magic pointer，指向data */ char data[1] /* 这是一个虚拟字段，表示数据块的第一个字节，长度不应计入meta */&#125;; 合并方法如下： 123456789t_block fusion(t_block b) &#123; if (b-&gt;next &amp;&amp; b-&gt;next-&gt;free) &#123; b-&gt;size += BLOCK_SIZE + b-&gt;next-&gt;size; b-&gt;next = b-&gt;next-&gt;next; if(b-&gt;next) b-&gt;next-&gt;prev = b; &#125; return b;&#125; 有了上述方法，free的实现思路就比较清晰了：首先检查参数地址的合法性，如果不合法则不做任何事；否则，将此block的free标为1，并且在可以的情况下与后面的block进行合并。如果当前是最后一个block，则回退break指针释放进程内存，如果当前block是最后一个block，则回退break指针并设置first_block为NULL。实现如下： 123456789101112131415161718void free(void *p) &#123; t_block b; if(valid_addr(p)) &#123; b = get_block(p); b-&gt;free = 1; if(b-&gt;prev &amp;&amp; b-&gt;prev-&gt;free) b = fusion(b-&gt;prev); if(b-&gt;next) fusion(b); else &#123; if(b-&gt;prev) b-&gt;prev-&gt;prev = NULL; else first_block = NULL; brk(b); &#125; &#125;&#125; 3.2.8 realloc的实现 为了实现realloc，我们首先要实现一个内存复制方法。如同calloc一样，为了效率，我们以8字节为单位进行复制： 12345678void copy_block(t_block src, t_block dst) &#123; size_t *sdata, *ddata; size_t i; sdata = src-&gt;ptr; ddata = dst-&gt;ptr; for(i = 0; (i * 8) &lt; src-&gt;size &amp;&amp; (i * 8) &lt; dst-&gt;size; i++) ddata[i] = sdata[i];&#125; 然后我们开始实现realloc。一个简单（但是低效）的方法是malloc一段内存，然后将数据复制过去。但是我们可以做的更高效，具体可以考虑以下几个方面： 如果当前block的数据区大于等于realloc所要求的size，则不做任何操作 如果新的size变小了，考虑split 如果当前block的数据区不能满足size，但是其后继block是free的，并且合并后可以满足，则考虑做合并 下面是realloc的实现： 1234567891011121314151617181920212223242526272829303132333435void *realloc(void *p, size_t size) &#123; size_t s; t_block b, new; void *newp; if (!p) /* 根据标准库文档，当p传入NULL时，相当于调用malloc */ return malloc(size); if(valid_addr(p)) &#123; s = align8(size); b = get_block(p); if(b-&gt;size &gt;= s) &#123; if(b-&gt;size - s &gt;= (BLOCK_SIZE + 8)) split_block(b,s); &#125; else &#123; /* 看是否可进行合并 */ if(b-&gt;next &amp;&amp; b-&gt;next-&gt;free &amp;&amp; (b-&gt;size + BLOCK_SIZE + b-&gt;next-&gt;size) &gt;= s) &#123; fusion(b); if(b-&gt;size - s &gt;= (BLOCK_SIZE + 8)) split_block(b, s); &#125; else &#123; /* 新malloc */ newp = malloc (s); if (!newp) return NULL; new = get_block(newp); copy_block(b, new); free(p); return(newp); &#125; &#125; return (p); &#125; return NULL;&#125; 3.3 遗留问题和优化 以上是一个较为简陋，但是初步可用的malloc实现。还有很多遗留的可能优化点，例如： 同时兼容32位和64位系统 在分配较大快内存时，考虑使用mmap而非sbrk，这通常更高效 可以考虑维护多个链表而非单个，每个链表中的block大小均为一个范围内，例如8字节链表、16字节链表、24-32字节链表等等。此时可以根据size到对应链表中做分配，可以有效减少碎片，并提高查询block的速度 可以考虑链表中只存放free的block，而不存放已分配的block，可以减少查找block的次数，提高效率 还有很多可能的优化，这里不一一赘述。下面附上一些参考文献，有兴趣的同学可以更深入研究。 4 其它参考 这篇文章大量参考了A malloc Tutorial，其中一些图片和代码直接引用了文中的内容，这里特别指出 Computer Systems: A Programmer’s Perspective, 2/E一书有许多值得参考的地方 关于Linux的虚拟内存模型，Anatomy of a Program in Memory是很好的参考资料，另外作者还有一篇How the Kernel Manages Your Memory对于Linux内核中虚拟内存管理的部分有很好的讲解 对于真实世界的malloc实现，可以参考glibc的实现 本文写作过程中大量参考了维基百科，再次感谢这个伟大的网站，并且呼吁大家在手头允许的情况下可以适当捐助维基百科，帮助这个造福人类的系统运行下去 本文地址：http://xnerv.wang/how-to-implement-a-malloc/ 转载自：如何实现一个malloc","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://xnerv.wang/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"编程语言","slug":"编程语言","permalink":"https://xnerv.wang/tags/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"},{"name":"C","slug":"C","permalink":"https://xnerv.wang/tags/C/"},{"name":"C内存管理","slug":"C内存管理","permalink":"https://xnerv.wang/tags/C%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"}]},{"title":"16个很有用的在线工具（转载）","slug":"16-useful-online-tools","date":"2014-08-01T01:49:00.000Z","updated":"2023-08-21T02:24:21.420Z","comments":true,"path":"16-useful-online-tools/","link":"","permalink":"https://xnerv.wang/16-useful-online-tools/","excerpt":"1. ExplainShell.com 命令解释 对于 Linux 用户来说每天都会写各种命令和脚本，那么你可以使用这个网站工具来查看命令式如何工作的，这样可以避免不必要的错误出现；也是一个很好的学习命令的方式。 2. BashrcGenerator.com 定制个性命令提示符 简单说就是个性化生成命令提示符，可将生成的代码写入到用户家目录的 .bashrc 或者可以设置全局变量文件/etc/profile 对所有用户生效。 可参考：http://stackoverflow.com/questions/4133904/ps1-line-with-git-current-branch-and-colors","text":"1. ExplainShell.com 命令解释 对于 Linux 用户来说每天都会写各种命令和脚本，那么你可以使用这个网站工具来查看命令式如何工作的，这样可以避免不必要的错误出现；也是一个很好的学习命令的方式。 2. BashrcGenerator.com 定制个性命令提示符 简单说就是个性化生成命令提示符，可将生成的代码写入到用户家目录的 .bashrc 或者可以设置全局变量文件/etc/profile 对所有用户生效。 可参考：http://stackoverflow.com/questions/4133904/ps1-line-with-git-current-branch-and-colors 3. Vim-adventures.com 通过 RPG 游戏练习 VIM 使用 通过 RPG 游戏练习 VIM 编辑器的使用，使用h,j,k,l字符移动人物来获得新的命令能力和搜集钥匙，查看帮助可使用:help;赶脚这个非常 cool! 4. Try Github 在线学习 Git 版本控制 十五分钟学会 Git，很明显这个网站模拟了一个控制台，以很时尚的界面让人对 Git 不再望而生畏。 5. Shortcutfoo.com 是一个练习快捷键的好地方，涵盖了 vim、sublime、emacs、git 等软件的快捷使用方式和友好的说明。 6. GitHub Free Programming Books 免费编程书籍 以 Github 管理的方式搜集了免费的编程和系统管理等书籍，给作者点 1024 个赞~~，另外连接是 fork 原作者，后续增加中文书籍。 7. Collabedit.com 实时文本交互聊天 先说下使用，你可以创建一个文档http://collabedit.com/yb22u填写相关的用户名和选择语言；然后可以将此文档地址发给另一个人，那么互相之间就可以实时看到对方的输入，有高亮语法；使用场合嘛，比如通过 collabedit 可以考量对方编程能力等。 8. Cpp.sh 在线编写运行分享 C++ 代码编辑器 可在线编辑运行 C++ 代码，亦可 Ctrl+Z 生成 url 分享给好友。 9. Copy.sh 浏览器运行虚拟机 又一个非常 crazy 的工具，在线运行虚拟机，可以选择下载虚拟机镜像也可以上传自己的 iso，copy.sh 在线运行虚拟机源码：https://github.com/copy/v86； 10. Commandlinefu.com 命令或记录网站 做运维的应该都知道这个网站，可以分享自己的 CLI 库，也可以学习借鉴别人的命令脚本。 11. Alias.sh 命令别名数据库 有点类似 commandlinefu 了，可以通过这个网站借鉴获取和分享有用的命令别名。 比如 lr 别名定义了显示目录树。 1alias lr=&#x27;ls -R grep &quot;:$&quot; sed -e &#x27;\\&#x27;&#x27;s/:$//&#x27;\\&#x27;&#x27; -e &#x27;\\&#x27;&#x27;s/[^-][^\\/]*\\//--/g&#x27;\\&#x27;&#x27; -e &#x27;\\&#x27;&#x27;s/^/ /&#x27;\\&#x27;&#x27; -e &#x27;\\&#x27;&#x27;s/-//&#x27;\\&#x27;&#x27;&#x27; 12. Distrowatch.com 提供了 Linux 发行版的详细信息 通过 Distrowath 不仅可以精确的查看互联网都有哪些流行的 Linux 发行版，还可以查看每个发行版的相关信息如默认桌面环境、默认应用程序及镜像的下载链接；堪称 Linux 的数据库。 13. Linuxmanpages.com 在线查看命令帮助 相当于系统内部的 man、help、info 等的综合吧。 14. AwesomeCow.com 适用 Linux 环境的软件搜索引擎** 如果有款 win 下好用的软件想在 linux 下使用，或许可以通过 AwesomeCow 找到与其类似或者一样的软件，或者通过 WINE。 15. PenguSpy.com Linux 好玩游戏合集 16. Linux Cross Reference by Free Electrons 在线查看内核代码及不同版本的差异 对于内核开发者或许有很大的帮助。 本文地址：http://xnerv.wang/16-useful-online-tools/ 转载自：16个很有用的在线工具","categories":[{"name":"程序员生活","slug":"程序员生活","permalink":"https://xnerv.wang/categories/%E7%A8%8B%E5%BA%8F%E5%91%98%E7%94%9F%E6%B4%BB/"}],"tags":[{"name":"程序员生活","slug":"程序员生活","permalink":"https://xnerv.wang/tags/%E7%A8%8B%E5%BA%8F%E5%91%98%E7%94%9F%E6%B4%BB/"},{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"}]},{"title":"每天进步一点点——五分钟理解一致性哈希算法(consistent hashing)（转载）","slug":"study-consistent-hashing-in-5min","date":"2014-04-11T07:21:03.000Z","updated":"2023-08-21T02:24:20.641Z","comments":true,"path":"study-consistent-hashing-in-5min/","link":"","permalink":"https://xnerv.wang/study-consistent-hashing-in-5min/","excerpt":"一致性哈希算法在1997年由麻省理工学院提出的一种分布式哈希（DHT）实现算法，设计目标是为了解决因特网中的热点(Hot spot)问题，初衷和CARP十分类似。一致性哈希修正了CARP使用的简 单哈希算法带来的问题，使得分布式哈希（DHT）可以在P2P环境中真正得到应用。","text":"一致性哈希算法在1997年由麻省理工学院提出的一种分布式哈希（DHT）实现算法，设计目标是为了解决因特网中的热点(Hot spot)问题，初衷和CARP十分类似。一致性哈希修正了CARP使用的简 单哈希算法带来的问题，使得分布式哈希（DHT）可以在P2P环境中真正得到应用。 一致性hash算法提出了在动态变化的Cache环境中，判定哈希算法好坏的四个定义： 平衡性(Balance)：平衡性是指哈希的结果能够尽可能分布到所有的缓冲中去，这样可以使得所有的缓冲空间都得到利用。很多哈希算法都能够满足这一条件。 单调性(Monotonicity)：单调性是指如果已经有一些内容通过哈希分派到了相应的缓冲中，又有新的缓冲加入到系统中。哈希的结果应能够保证原有已分配的内容可以被映射到原有的或者新的缓冲中去，而不会被映射到旧的缓冲集合中的其他缓冲区。 分散性(Spread)：在分布式环境中，终端有可能看不到所有的缓冲，而是只能看到其中的一部分。当终端希望通过哈希过程将内容映射到缓冲上时，由于不同终端所见的缓冲范围有可能不同，从而导致哈希的结果不一致，最终的结果是相同的内容被不同的终端映射到不同的缓冲区中。这种情况显然是应该避免的，因为它导致相同内容被存储到不同缓冲中去，降低了系统存储的效率。分散性的定义就是上述情况发生的严重程度。好的哈希算法应能够尽量避免不一致的情况发生，也就是尽量降低分散性。 负载(Load)：负载问题实际上是从另一个角度看待分散性问题。既然不同的终端可能将相同的内容映射到不同的缓冲区中，那么对于一个特定的缓冲区而言，也可能被不同的用户映射为不同 的内容。与分散性一样，这种情况也是应当避免的，因此好的哈希算法应能够尽量降低缓冲的负荷。 在分布式集群中，对机器的添加删除，或者机器故障后自动脱离集群这些操作是分布式集群管理最基本的功能。如果采用常用的hash(object)%N算法，那么在有机器添加或者删除后，很多原有的数据就无法找到了，这样严重的违反了单调性原则。接下来主要讲解一下一致性哈希算法是如何设计的： 环形Hash空间 按照常用的hash算法来将对应的key哈希到一个具有232次方个桶的空间中，即0~(232)-1的数字空间中。现在我们可以将这些数字头尾相连，想象成一个闭合的环形。如下图 把数据通过一定的hash算法处理后映射到环上 现在我们将object1、object2、object3、object4四个对象通过特定的Hash函数计算出对应的key值，然后散列到Hash环上。如下图： 1234Hash(object1) = key1；Hash(object2) = key2；Hash(object3) = key3；Hash(object4) = key4； 将机器通过hash算法映射到环上 在采用一致性哈希算法的分布式集群中将新的机器加入，其原理是通过使用与对象存储一样的Hash算法将机器也映射到环中（一般情况下对机器的hash计算是采用机器的IP或者机器唯一的别名作为输入值），然后以顺时针的方向计算，将所有对象存储到离自己最近的机器中。 假设现在有NODE1，NODE2，NODE3三台机器，通过Hash算法得到对应的KEY值，映射到环中，其示意图如下： 123Hash(NODE1) = KEY1;Hash(NODE2) = KEY2;Hash(NODE3) = KEY3; 通过上图可以看出对象与机器处于同一哈希空间中，这样按顺时针转动object1存储到了NODE1中，object3存储到了NODE2中，object2、object4存储到了NODE3中。在这样的部署环境中，hash环是不会变更的，因此，通过算出对象的hash值就能快速的定位到对应的机器中，这样就能找到对象真正的存储位置了。 机器的删除与添加 普通hash求余算法最为不妥的地方就是在有机器的添加或者删除之后会照成大量的对象存储位置失效，这样就大大的不满足单调性了。下面来分析一下一致性哈希算法是如何处理的。 节点（机器）的删除 以上面的分布为例，如果NODE2出现故障被删除了，那么按照顺时针迁移的方法，object3将会被迁移到NODE3中，这样仅仅是object3的映射位置发生了变化，其它的对象没有任何的改动。如下图： 节点（机器）的添加 如果往集群中添加一个新的节点NODE4，通过对应的哈希算法得到KEY4，并映射到环中，如下图： 通过按顺时针迁移的规则，那么object2被迁移到了NODE4中，其它对象还保持这原有的存储位置。通过对节点的添加和删除的分析，一致性哈希算法在保持了单调性的同时，还是数据的迁移达到了最小，这样的算法对分布式集群来说是非常合适的，避免了大量数据迁移，减小了服务器的的压力。 平衡性 根据上面的图解分析，一致性哈希算法满足了单调性和负载均衡的特性以及一般hash算法的分散性，但这还并不能当做其被广泛应用的原由，因为还缺少了平衡性。下面将分析一致性哈希算法是如何满足平衡性的。hash算法是不保证平衡的，如上面只部署了NODE1和NODE3的情况（NODE2被删除的图），object1存储到了NODE1中，而object2、object3、object4都存储到了NODE3中，这样就照成了非常不平衡的状态。在一致性哈希算法中，为了尽可能的满足平衡性，其引入了虚拟节点。 ——“虚拟节点”（ virtual node ）是实际节点（机器）在 hash 空间的复制品（ replica ），一实际个节点（机器）对应了若干个“虚拟节点”，这个对应个数也成为“复制个数”，“虚拟节点”在 hash 空间中以hash值排列。 以上面只部署了NODE1和NODE3的情况（NODE2被删除的图）为例，之前的对象在机器上的分布很不均衡，现在我们以2个副本（复制个数）为例，这样整个hash环中就存在了4个虚拟节点，最后对象映射的关系图如下： 根据上图可知对象的映射关系：object1-&gt;NODE1-1，object2-&gt;NODE1-2，object3-&gt;NODE3-2，object4-&gt;NODE3-1。通过虚拟节点的引入，对象的分布就比较均衡了。那么在实际操作中，正真的对象查询是如何工作的呢？对象从hash到虚拟节点到实际节点的转换如下图： “虚拟节点”的hash计算可以采用对应节点的IP地址加数字后缀的方式。例如假设NODE1的IP地址为192.168.1.100。引入“虚拟节点”前，计算 cache A 的 hash 值： Hash(“192.168.1.100”); 引入“虚拟节点”后，计算“虚拟节”点NODE1-1和NODE1-2的hash值： 12Hash(“192.168.1.100#1”); // NODE1-1Hash(“192.168.1.100#2”); // NODE1-2 参考： [1] http://blog.huanghao.me/?p=14 本文地址：http://xnerv.wang/study-consistent-hashing-in-5min/ 转载自：每天进步一点点——五分钟理解一致性哈希算法(consistent hashing)","categories":[{"name":"分布式及存储","slug":"分布式及存储","permalink":"https://xnerv.wang/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8F%8A%E5%AD%98%E5%82%A8/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"分布式及存储","slug":"分布式及存储","permalink":"https://xnerv.wang/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8F%8A%E5%AD%98%E5%82%A8/"},{"name":"Consistent Hashing","slug":"Consistent-Hashing","permalink":"https://xnerv.wang/tags/Consistent-Hashing/"},{"name":"一致性哈希","slug":"一致性哈希","permalink":"https://xnerv.wang/tags/%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C/"}]},{"title":"分布式系统的事务处理（转载）","slug":"transaction-processing-of-distributed-system","date":"2014-01-20T08:00:00.000Z","updated":"2023-08-21T02:24:20.574Z","comments":true,"path":"transaction-processing-of-distributed-system/","link":"","permalink":"https://xnerv.wang/transaction-processing-of-distributed-system/","excerpt":"当我们在生产线上用一台服务器来提供数据服务的时候，我会遇到如下的两个问题： 1）一台服务器的性能不足以提供足够的能力服务于所有的网络请求。 2）我们总是害怕我们的这台服务器停机，造成服务不可用或是数据丢失。 于是我们不得不对我们的服务器进行扩展，加入更多的机器来分担性能上的问题，以及来解决单点故障问题。 通常，我们会通过两种手段来扩展我们的数据服务： 1）数据分区：就是把数据分块放在不同的服务器上（如：uid % 16，一致性哈希等）。 2）数据镜像：让所有的服务器都有相同的数据，提供相当的服务。","text":"当我们在生产线上用一台服务器来提供数据服务的时候，我会遇到如下的两个问题： 1）一台服务器的性能不足以提供足够的能力服务于所有的网络请求。 2）我们总是害怕我们的这台服务器停机，造成服务不可用或是数据丢失。 于是我们不得不对我们的服务器进行扩展，加入更多的机器来分担性能上的问题，以及来解决单点故障问题。 通常，我们会通过两种手段来扩展我们的数据服务： 1）数据分区：就是把数据分块放在不同的服务器上（如：uid % 16，一致性哈希等）。 2）数据镜像：让所有的服务器都有相同的数据，提供相当的服务。 对于第一种情况，我们无法解决数据丢失的问题，单台服务器出问题时，会有部分数据丢失。所以，数据服务的高可用性只能通过第二种方法来完成——数据的冗余存储（一般工业界认为比较安全的备份数应该是3份，如：Hadoop和Dynamo）。 但是，加入更多的机器，会让我们的数据服务变得很复杂，尤其是跨服务器的事务处理，也就是跨服务器的数据一致性。这个是一个很难的问题。 让我们用最经典的Use Case：“A帐号向B帐号汇钱”来说明一下，熟悉RDBMS事务的都知道从帐号A到帐号B需要6个操作： 从A帐号中把余额读出来。 对A帐号做减法操作。 把结果写回A帐号中。 从B帐号中把余额读出来。 对B帐号做加法操作。 把结果写回B帐号中。 为了数据的一致性，这6件事，要么都成功做完，要么都不成功，而且这个操作的过程中，对A、B帐号的其它访问必需锁死，所谓锁死就是要排除其它的读写操作，不然会有脏数据的问题，这就是事务。那么，我们在加入了更多的机器后，这个事情会变得复杂起来： 1）在数据分区的方案中：如果A帐号和B帐号的数据不在同一台服务器上怎么办？我们需要一个跨机器的事务处理。也就是说，如果A的扣钱成功了，但B的加钱不成功，我们还要把A的操作给回滚回去。这在跨机器的情况下，就变得比较复杂了。 2）在数据镜像的方案中：A帐号和B帐号间的汇款是可以在一台机器上完成的，但是别忘了我们有多台机器存在A帐号和B帐号的副本。如果对A帐号的汇钱有两个并发操作（要汇给B和C），这两个操作发生在不同的两台服务器上怎么办？也就是说，在数据镜像中，在不同的服务器上对同一个数据的写操作怎么保证其一致性，保证数据不冲突？ 同时，我们还要考虑性能的因素，如果不考虑性能的话，事务得到保证并不困难，系统慢一点就行了。除了考虑性能外，我们还要考虑可用性，也就是说，一台机器没了，数据不丢失，服务可由别的机器继续提供。 于是，我们需要重点考虑下面的这么几个情况： 1）容灾：数据不丢、结点的Failover 2）数据的一致性：事务处理 3）性能：吞吐量 、 响应时间 前面说过，要解决数据不丢，只能通过数据冗余的方法，就算是数据分区，每个区也需要进行数据冗余处理。这就是数据副本：当出现某个节点的数据丢失时可以从副本读到，数据副本是分布式系统解决数据丢失异常的唯一手段。所以，在这篇文章中，简单起见，我们只讨论在数据冗余情况下考虑数据的一致性和性能的问题。简单说来： 1）要想让数据有高可用性，就得写多份数据。 2）写多份的问题会导致数据一致性的问题。 3）数据一致性的问题又会引发性能问题 这就是软件开发，按下了葫芦起了瓢。 一致性模型 说起数据一致性来说，简单说有三种类型（当然，如果细分的话，还有很多一致性模型，如：顺序一致性，FIFO一致性，会话一致性，单读一致性，单写一致性，但为了本文的简单易读，我只说下面三种）： 1）Weak 弱一致性：当你写入一个新值后，读操作在数据副本上可能读出来，也可能读不出来。比如：某些cache系统，网络游戏其它玩家的数据和你没什么关系，VOIP这样的系统，或是百度搜索引擎（呵呵）。 2）Eventually 最终一致性：当你写入一个新值后，有可能读不出来，但在某个时间窗口之后保证最终能读出来。比如：DNS，电子邮件、Amazon S3，Google搜索引擎这样的系统。 3）Strong 强一致性：新的数据一旦写入，在任意副本任意时刻都能读到新值。比如：文件系统，RDBMS，Azure Table都是强一致性的。 从这三种一致型的模型上来说，我们可以看到，Weak和Eventually一般来说是异步冗余的，而Strong一般来说是同步冗余的，异步的通常意味着更好的性能，但也意味着更复杂的状态控制。同步意味着简单，但也意味着性能下降。 好，让我们由浅入深，一步一步地来看有哪些技术： Master-Slave 首先是Master-Slave结构，对于这种加构，Slave一般是Master的备份。在这样的系统中，一般是如下设计的： 1）读写请求都由Master负责。 2）写请求写到Master上后，由Master同步到Slave上。 从Master同步到Slave上，你可以使用异步，也可以使用同步，可以使用Master来push，也可以使用Slave来pull。 通常来说是Slave来周期性的pull，所以，是最终一致性。这个设计的问题是，如果Master在pull周期内垮掉了，那么会导致这个时间片内的数据丢失。如果你不想让数据丢掉，Slave只能成为Read-Only的方式等Master恢复。 当然，如果你可以容忍数据丢掉的话，你可以马上让Slave代替Master工作（对于只负责计算的结点来说，没有数据一致性和数据丢失的问题，Master-Slave的方式就可以解决单点问题了） 当然，Master Slave也可以是强一致性的， 比如：当我们写Master的时候，Master负责先写自己，等成功后，再写Slave，两者都成功后返回成功，整个过程是同步的，如果写Slave失败了，那么两种方法，一种是标记Slave不可用报错并继续服务（等Slave恢复后同步Master的数据，可以有多个Slave，这样少一个，还有备份，就像前面说的写三份那样），另一种是回滚自己并返回写失败。（注：一般不先写Slave，因为如果写Master自己失败后，还要回滚Slave，此时如果回滚Slave失败，就得手工订正数据了）你可以看到，如果Master-Slave需要做成强一致性有多复杂。 Master-Master Master-Master，又叫Multi-master，是指一个系统存在两个或多个Master，每个Master都提供read-write服务。这个模型是Master-Slave的加强版，数据间同步一般是通过Master间的异步完成，所以是最终一致性。 Master-Master的好处是，一台Master挂了，别的Master可以正常做读写服务，他和Master-Slave一样，当数据没有被复制到别的Master上时，数据会丢失。很多数据库都支持Master-Master的Replication的机制。 另外，如果多个Master对同一个数据进行修改的时候，这个模型的恶梦就出现了——对数据间的冲突合并，这并不是一件容易的事情。看看Dynamo的Vector Clock的设计（记录数据的版本号和修改者）就知道这个事并不那么简单，而且Dynamo对数据冲突这个事是交给用户自己搞的。就像我们的SVN源码冲突一样，对于同一行代码的冲突，只能交给开发者自己来处理。（在本文后后面会讨论一下Dynamo的Vector Clock） Two/Three Phase Commit 这个协议的缩写又叫2PC，中文叫两阶段提交。在分布式系统中，每个节点虽然可以知晓自己的操作时成功或者失败，却无法知道其他节点的操作的成功或失败。当一个事务跨越多个节点时，为了保持事务的ACID特性，需要引入一个作为协调者的组件来统一掌控所有节点(称作参与者)的操作结果并最终指示这些节点是否要把操作结果进行真正的提交(比如将更新后的数据写入磁盘等等)。 两阶段提交的算法如下： 第一阶段： 协调者会问所有的参与者结点，是否可以执行提交操作。 各个参与者开始事务执行的准备工作：如：为资源上锁，预留资源，写undo/redo log…… 参与者响应协调者，如果事务的准备工作成功，则回应“可以提交”，否则回应“拒绝提交”。 第二阶段： 如果所有的参与者都回应“可以提交”，那么，协调者向所有的参与者发送“正式提交”的命令。参与者完成正式提交，并释放所有资源，然后回应“完成”，协调者收集各结点的“完成”回应后结束这个Global Transaction。 如果有一个参与者回应“拒绝提交”，那么，协调者向所有的参与者发送“回滚操作”，并释放所有资源，然后回应“回滚完成”，协调者收集各结点的“回滚”回应后，取消这个Global Transaction。 我们可以看到，2PC说白了就是第一阶段做Vote，第二阶段做决定的一个算法，也可以看到2PC这个事是强一致性的算法。在前面我们讨论过Master-Slave的强一致性策略，和2PC有点相似，只不过2PC更为保守一些——先尝试再提交。 2PC用的是比较多的，在一些系统设计中，会串联一系列的调用，比如：A -&gt; B -&gt; C -&gt; D，每一步都会分配一些资源或改写一些数据。比如我们B2C网上购物的下单操作在后台会有一系列的流程需要做。如果我们一步一步地做，就会出现这样的问题，如果某一步做不下去了，那么前面每一次所分配的资源需要做反向操作把他们都回收掉，所以，操作起来比较复杂。现在很多处理流程（Workflow）都会借鉴2PC这个算法，使用 try -&gt; confirm的流程来确保整个流程的能够成功完成。 举个通俗的例子，西方教堂结婚的时候，都有这样的桥段： 1）牧师分别问新郎和新娘：你是否愿意……不管生老病死……（询问阶段） 2）当新郎和新娘都回答愿意后（锁定一生的资源），牧师就会说：我宣布你们……（事务提交） 这是多么经典的一个两阶段提交的事务处理。 另外，我们也可以看到其中的一些问题， A）其中一个是同步阻塞操作，这个事情必然会非常大地影响性能。 B）另一个主要的问题是在TimeOut上，比如， 1）如果第一阶段中，参与者没有收到询问请求，或是参与者的回应没有到达协调者。那么，需要协调者做超时处理，一旦超时，可以当作失败，也可以重试。 2）如果第二阶段中，正式提交发出后，如果有的参与者没有收到，或是参与者提交/回滚后的确认信息没有返回，一旦参与者的回应超时，要么重试，要么把那个参与者标记为问题结点剔除整个集群，这样可以保证服务结点都是数据一致性的。 3）糟糕的情况是，第二阶段中，如果参与者收不到协调者的commit/fallback指令，参与者将处于“状态未知”阶段，参与者完全不知道要怎么办，比如：如果所有的参与者完成第一阶段的回复后（可能全部yes，可能全部no，可能部分yes部分no），如果协调者在这个时候挂掉了。那么所有的结点完全不知道怎么办（问别的参与者都不行）。为了一致性，要么死等协调者，要么重发第一阶段的yes/no命令。 两段提交最大的问题就是第3）项，如果第一阶段完成后，参与者在第二阶没有收到决策，那么数据结点会进入“不知所措”的状态，这个状态会block住整个事务。也就是说，协调者Coordinator对于事务的完成非常重要，Coordinator的可用性是个关键。 因些，我们引入三段提交，三段提交在Wikipedia上的描述如下，他把二段提交的第一个段break成了两段：询问，然后再锁资源。最后真正提交。三段提交的示意图如下： 三段提交的核心理念是：在询问的时候并不锁定资源，除非所有人都同意了，才开始锁资源。 理论上来说，如果第一阶段所有的结点返回成功，那么有理由相信成功提交的概率很大。这样一来，可以降低参与者Cohorts的状态未知的概率。也就是说，一旦参与者收到了PreCommit，意味他知道大家其实都同意修改了。这一点很重要。下面我们来看一下3PC的状态迁移图：（注意图中的虚线，那些F,T是Failuer或Timeout，其中的：状态含义是 q – Query，a – Abort，w – Wait，p – PreCommit，c – Commit） 从上图的状态变化图我们可以从虚线（那些F,T是Failuer或Timeout）看到——如果结点处在P状态（PreCommit）的时候发生了F/T的问题，三段提交比两段提交的好处是，三段提交可以继续直接把状态变成C状态（Commit），而两段提交则不知所措。 其实，三段提交是一个很复杂的事情，实现起来相当难，而且也有一些问题。 看到这里，我相信你有很多很多的问题，你一定在思考2PC/3PC中各种各样的失败场景，你会发现Timeout是个非常难处理的事情，因为网络上的Timeout在很多时候让你无所事从，你也不知道对方是做了还是没有做。于是你好好的一个状态机就因为Timeout成了个摆设。 一个网络服务会有三种状态：1）Success，2）Failure，3）Timeout，第三个绝对是恶梦，尤其在你需要维护状态的时候。 Two Generals Problem（两将军问题） Two Generals Problem 两将军问题是这么一个思维性实验问题： 有两支军队，它们分别有一位将军领导，现在准备攻击一座修筑了防御工事的城市。这两支军队都驻扎在那座城市的附近，分占一座山头。一道山谷把两座山分隔开来，并且两位将军唯一的通信方式就是派各自的信使来往于山谷两边。不幸的是，这个山谷已经被那座城市的保卫者占领，并且存在一种可能，那就是任何被派出的信使通过山谷是会被捕。 请注意，虽然两位将军已经就攻击那座城市达成共识，但在他们各自占领山头阵地之前，并没有就进攻时间达成共识。两位将军必须让自己的军队同时进攻城市才能取得成功。因此，他们必须互相沟通，以确定一个时间来攻击，并同意就在那时攻击。如果只有一个将军进行攻击，那么这将是一个灾难性的失败。 这个思维实验就包括考虑他们如何去做这件事情。下面是我们的思考： 1）第一位将军先发送一段消息“让我们在上午9点开始进攻”。然而，一旦信使被派遣，他是否通过了山谷，第一位将军就不得而知了。任何一点的不确定性都会使得第一位将军攻击犹豫，因为如果第二位将军不能在同一时刻发动攻击，那座城市的驻军就会击退他的军队的进攻，导致他的军对被摧毁。 2）知道了这一点，第二位将军就需要发送一个确认回条：“我收到您的邮件，并会在9点的攻击。”但是，如果带着确认消息的信使被抓怎么办？所以第二位将军会犹豫自己的确认消息是否能到达。 3）于是，似乎我们还要让第一位将军再发送一条确认消息——“我收到了你的确认”。然而，如果这位信使被抓怎么办呢？ 4）这样一来，是不是我们还要第二位将军发送一个“确认收到你的确认”的信息。 靠，于是你会发现，这事情很快就发展成为不管发送多少个确认消息，都没有办法来保证两位将军有足够的自信自己的信使没有被敌军捕获。 这个问题是无解的。两个将军问题和它的无解证明首先由E.A.Akkoyunlu,K.Ekanadham和R.V.Huber于1975年在《一些限制与折衷的网络通信设计》一文中发表，就在这篇文章的第73页中一段描述两个黑帮之间的通信中被阐明。 1978年，在Jim Gray的《数据库操作系统注意事项》一书中（从第465页开始）被命名为两个将军悖论。作为两个将军问题的定义和无解性的证明的来源，这一参考被广泛提及。 这个实验意在阐明：试图通过建立在一个不可靠的连接上的交流来协调一项行动的隐患和设计上的巨大挑战。 从工程上来说，一个解决两个将军问题的实际方法是使用一个能够承受通信信道不可靠性的方案，并不试图去消除这个不可靠性，但要将不可靠性削减到一个可以接受的程度。比如，第一位将军排出了100位信使并预计他们都被捕的可能性很小。在这种情况下，不管第二位将军是否会攻击或者受到任何消息，第一位将军都会进行攻击。另外，第一位将军可以发送一个消息流，而第二位将军可以对其中的每一条消息发送一个确认消息，这样如果每条消息都被接收到，两位将军会感觉更好。然而我们可以从证明中看出，他们俩都不能肯定这个攻击是可以协调的。他们没有算法可用（比如，收到4条以上的消息就攻击）能够确保防止仅有一方攻击。再者，第一位将军还可以为每条消息编号，说这是1号，2号……直到n号。这种方法能让第二位将军知道通信信道到底有多可靠，并且返回合适的数量的消息来确保最后一条消息被接收到。如果信道是可靠的话，只要一条消息就行了，其余的就帮不上什么忙了。最后一条和第一条消息丢失的概率是相等的。 两将军问题可以扩展成更变态的拜占庭将军问题 (Byzantine Generals Problem)，其故事背景是这样的：拜占庭位于现在土耳其的伊斯坦布尔，是东罗马帝国的首都。由于当时拜占庭罗马帝国国土辽阔，为了防御目的，因此每个军队都分隔很远，将军与将军之间只能靠信差传消息。 在战争的时候，拜占庭军队内所有将军必需达成一致的共识，决定是否有赢的机会才去攻打敌人的阵营。但是，军队可能有叛徒和敌军间谍，这些叛徒将军们会扰乱或左右决策的过程。这时候，在已知有成员谋反的情况下，其余忠诚的将军在不受叛徒的影响下如何达成一致的协议，这就是拜占庭将军问题。 Paxos算法 Wikipedia上的各种Paxos算法的描述非常详细，大家可以去围观一下。 Paxos 算法解决的问题是在一个可能发生上述异常的分布式系统中如何就某个值达成一致，保证不论发生以上任何异常，都不会破坏决议的一致性。一个典型的场景是，在一个分布式数据库系统中，如果各节点的初始状态一致，每个节点都执行相同的操作序列，那么他们最后能得到一个一致的状态。为保证每个节点执行相同的命令序列，需要在每一条指令上执行一个「一致性算法」以保证每个节点看到的指令一致。一个通用的一致性算法可以应用在许多场景中，是分布式计算中的重要问题。从20世纪80年代起对于一致性算法的研究就没有停止过。 Notes：Paxos算法是莱斯利·兰伯特（Leslie Lamport，就是 LaTeX 中的”La”，此人现在在微软研究院）于1990年提出的一种基于消息传递的一致性算法。由于算法难以理解起初并没有引起人们的重视，使Lamport在八年后1998年重新发表到ACM Transactions on Computer Systems上（The Part-Time Parliament）。即便如此paxos算法还是没有得到重视，2001年Lamport 觉得同行无法接受他的幽默感，于是用容易接受的方法重新表述了一遍（Paxos Made Simple）。可见Lamport对Paxos算法情有独钟。近几年Paxos算法的普遍使用也证明它在分布式一致性算法中的重要地位。2006年Google的三篇论文初现“云”的端倪，其中的Chubby Lock服务使用Paxos作为Chubby Cell中的一致性算法，Paxos的人气从此一路狂飙。（Lamport 本人在 他的blog 中描写了他用9年时间发表这个算法的前前后后） 注：Amazon的AWS中，所有的云服务都基于一个ALF（Async Lock Framework）的框架实现的，这个ALF用的就是Paxos算法。我在Amazon的时候，看内部的分享视频时，设计者在内部的Principle Talk里说他参考了ZooKeeper的方法，但他用了另一种比ZooKeeper更易读的方式实现了这个算法。 简单说来，Paxos的目的是让整个集群的结点对某个值的变更达成一致。Paxos算法基本上来说是个民主选举的算法——大多数的决定会成个整个集群的统一决定。任何一个点都可以提出要修改某个数据的提案，是否通过这个提案取决于这个集群中是否有超过半数的结点同意（所以Paxos算法需要集群中的结点是单数）。 这个算法有两个阶段（假设这个有三个结点：A，B，C）： 第一阶段：Prepare阶段 A把申请修改的请求Prepare Request发给所有的结点A，B，C。注意，Paxos算法会有一个Sequence Number（你可以认为是一个提案号，这个数不断递增，而且是唯一的，也就是说A和B不可能有相同的提案号），这个提案号会和修改请求一同发出，任何结点在“Prepare阶段”时都会拒绝其值小于当前提案号的请求。所以，结点A在向所有结点申请修改请求的时候，需要带一个提案号，越新的提案，这个提案号就越是是最大的。 如果接收结点收到的提案号n大于其它结点发过来的提案号，这个结点会回应Yes（本结点上最新的被批准提案号），并保证不接收其它&lt;n的提案。这样一来，结点上在Prepare阶段里总是会对最新的提案做承诺。 优化：在上述 prepare 过程中，如果任何一个结点发现存在一个更高编号的提案，则需要通知 提案人，提醒其中断这次提案。 第二阶段：Accept阶段 如果提案者A收到了超过半数的结点返回的Yes，然后他就会向所有的结点发布Accept Request（同样，需要带上提案号n），如果没有超过半数的话，那就返回失败。 当结点们收到了Accept Request后，如果对于接收的结点来说，n是最大的了，那么，它就会修改这个值，如果发现自己有一个更大的提案号，那么，结点就会拒绝修改。 我们可以看以，这似乎就是一个“两段提交”的优化。其实，2PC/3PC都是分布式一致性算法的残次版本，Google Chubby的作者Mike Burrows说过这个世界上只有一种一致性算法，那就是Paxos，其它的算法都是残次品。 我们还可以看到：对于同一个值的在不同结点的修改提案就算是在接收方被乱序收到也是没有问题的。 关于一些实例，你可以看一下Wikipedia中文中的“Paxos样例”一节，我在这里就不再多说了。对于Paxos算法中的一些异常示例，大家可以自己推导一下。你会发现基本上来说只要保证有半数以上的结点存活，就没有什么问题。 多说一下，自从Lamport在1998年发表Paxos算法后，对Paxos的各种改进工作就从未停止，其中动作最大的莫过于2005年发表的Fast Paxos。无论何种改进，其重点依然是在消息延迟与性能、吞吐量之间作出各种权衡。为了容易地从概念上区分二者，称前者Classic Paxos，改进后的后者为Fast Paxos。 总结 下图来自：Google App Engine的co-founder Ryan Barrett在2009年的google i/o上的演讲《Transaction Across DataCenter》（视频： http://www.youtube.com/watch?v=srOgpXECblk） 前面，我们说过，要想让数据有高可用性，就需要冗余数据写多份。写多份的问题会带来一致性的问题，而一致性的问题又会带来性能问题。从上图我们可以看到，我们基本上来说不可以让所有的项都绿起来，这就是著名的CAP理论：一致性，可用性，分区容忍性，你只可能要其中的两个。 NWR模型 最后我还想提一下Amazon Dynamo的NWR模型。这个NWR模型把CAP的选择权交给了用户，让用户自己的选择你的CAP中的哪两个。 所谓NWR模型。N代表N个备份，W代表要写入至少W份才认为成功，R表示至少读取R个备份。配置的时候要求W+R &gt; N。 因为W+R &gt; N， 所以 R &gt; N-W 这个是什么意思呢？就是读取的份数一定要比总备份数减去确保写成功的倍数的差值要大。 也就是说，每次读取，都至少读取到一个最新的版本。从而不会读到一份旧数据。当我们需要高可写的环境的时候，我们可以配置W = 1 如果N=3 那么R = 3。 这个时候只要写任何节点成功就认为成功，但是读的时候必须从所有的节点都读出数据。如果我们要求读的高效率，我们可以配置 W=N R=1。这个时候任何一个节点读成功就认为成功，但是写的时候必须写所有三个节点成功才认为成功。 NWR模型的一些设置会造成脏数据的问题，因为这很明显不是像Paxos一样是一个强一致的东西，所以，可能每次的读写操作都不在同一个结点上，于是会出现一些结点上的数据并不是最新版本，但却进行了最新的操作。 所以，Amazon Dynamo引了数据版本的设计。也就是说，如果你读出来数据的版本是v1，当你计算完成后要回填数据后，却发现数据的版本号已经被人更新成了v2，那么服务器就会拒绝你。版本这个事就像“乐观锁”一样。 但是，对于分布式和NWR模型来说，版本也会有恶梦的时候——就是版本冲的问题，比如：我们设置了N=3 W=1，如果A结点上接受了一个值，版本由v1 -&gt; v2，但还没有来得及同步到结点B上（异步的，应该W=1，写一份就算成功），B结点上还是v1版本，此时，B结点接到写请求，按道理来说，他需要拒绝掉，但是他一方面并不知道别的结点已经被更新到v2，另一方面他也无法拒绝，因为W=1，所以写一分就成功了。于是，出现了严重的版本冲突。 Amazon的Dynamo把版本冲突这个问题巧妙地回避掉了——版本冲这个事交给用户自己来处理。 于是，Dynamo引入了Vector Clock（矢量钟？!）这个设计。这个设计让每个结点各自记录自己的版本信息，也就是说，对于同一个数据，需要记录两个事：1）谁更新的我，2）我的版本号是什么。 下面，我们来看一个操作序列： 1）一个写请求，第一次被节点A处理了。节点A会增加一个版本信息(A，1)。我们把这个时候的数据记做D1(A，1)。 然后另外一个对同样key的请求还是被A处理了于是有D2(A，2)。这个时候，D2是可以覆盖D1的，不会有冲突产生。 2）现在我们假设D2传播到了所有节点(B和C)，B和C收到的数据不是从客户产生的，而是别人复制给他们的，所以他们不产生新的版本信息，所以现在B和C所持有的数据还是D2(A，2)。于是A，B，C上的数据及其版本号都是一样的。 3）如果我们有一个新的写请求到了B结点上，于是B结点生成数据D3(A,2; B,1)，意思是：数据D全局版本号为3，A升了两新，B升了一次。这不就是所谓的代码版本的log么？ 4）如果D3没有传播到C的时候又一个请求被C处理了，于是，以C结点上的数据是D4(A,2; C,1)。 5）好，最精彩的事情来了：如果这个时候来了一个读请求，我们要记得，我们的W=1 那么R=N=3，所以R会从所有三个节点上读，此时，他会读到三个版本： A结点：D2(A,2) B结点：D3(A,2; B,1); C结点：D4(A,2; C,1) 6）这个时候可以判断出，D2已经是旧版本（已经包含在D3/D4中），可以舍弃。 7）但是D3和D4是明显的版本冲突。于是，交给调用方自己去做版本冲突处理。就像源代码版本管理一样。 很明显，上述的Dynamo的配置用的是CAP里的A和P。 我非常推大家都去看看这篇论文：《Dynamo：Amazon’s Highly Available Key-Value Store》，如果英文痛苦，你可以看看译文（译者不详）。 本文地址：http://xnerv.wang/transaction-processing-of-distributed-system/ 转载自：分布式系统的事务处理","categories":[{"name":"分布式及存储","slug":"分布式及存储","permalink":"https://xnerv.wang/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8F%8A%E5%AD%98%E5%82%A8/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"分布式及存储","slug":"分布式及存储","permalink":"https://xnerv.wang/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8F%8A%E5%AD%98%E5%82%A8/"},{"name":"2PC","slug":"2PC","permalink":"https://xnerv.wang/tags/2PC/"},{"name":"分布式事务","slug":"分布式事务","permalink":"https://xnerv.wang/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"name":"Paxos","slug":"Paxos","permalink":"https://xnerv.wang/tags/Paxos/"}]},{"title":"内存分配-----伙伴算法和slab算法（转载）","slug":"memory-allocation-buddy-system-and-slab-allocator","date":"2014-01-02T17:39:03.000Z","updated":"2023-08-21T02:24:19.905Z","comments":true,"path":"memory-allocation-buddy-system-and-slab-allocator/","link":"","permalink":"https://xnerv.wang/memory-allocation-buddy-system-and-slab-allocator/","excerpt":"内存管理问题 内存碎片大小和管理内存碎片的效率问题(即空间和时间效率的问题): 内存碎片是指当回收一块内存时,一般将内存直接放入free链表中,由于内存越分配越小,内存块就会特别多而且特别小,当需要一块大的内存块的时候无法找到.原因就在于回收内存的时候,不能把相邻两块可用内存合并. 解决方法: 小块内存单独分配,大块内存有系统自动分配.(nginx和stl就是使用这种方法) 伙伴算法. slab算法.","text":"内存管理问题 内存碎片大小和管理内存碎片的效率问题(即空间和时间效率的问题): 内存碎片是指当回收一块内存时,一般将内存直接放入free链表中,由于内存越分配越小,内存块就会特别多而且特别小,当需要一块大的内存块的时候无法找到.原因就在于回收内存的时候,不能把相邻两块可用内存合并. 解决方法: 小块内存单独分配,大块内存有系统自动分配.(nginx和stl就是使用这种方法) 伙伴算法. slab算法. 伙伴算法 将空闲页面分为m个组,第1组存储20个单位的内存块,第2组存储21个单位的内存块,第3组存储22个单位的内存块,第4组存储23个单位的内存块,以此类推.直到m组. 每个组是一个链表,用于连接同等大小的内存块. 伙伴块的大小是相等的,并且第1块和第2块是伙伴,第三块和第四块是伙伴.以此类推. 伙伴算法分配内存 若申请的内存大小为n则将n向上取整为2的幂设次数为s,则需要分配s大小的内存块,定位大相应数组, 如果该数组有剩余内存块,则分配出去. 若没有剩余内存块就沿数组向上查找,然后再将该内存块分割出来s并将剩余的内存块放入相应大小的数组中. 例如分配5大小的内存块 -----------&gt;定位到大小为8的链表中 --------&gt;若该链表中之中没有空余元素,则定位到16的链表中,16中有剩余元素,则取出该元素,并分割出大小为8的内存块供用户使用,然后将剩余的8连接到大小为8的数组中. 伙伴算法的内存合并 当用户用完内存后会归还,然后根据该内存块实际大小(向上取整为2的幂)归入链表中,在归入之前, 我们还要检测他的伙伴内存块是否空闲, 如果空闲就合并在一起,合并后转到1,继续执行. 若果不是空闲的就直接归入链表中. 一般来说,伙伴算法实现中会用位图记录内存块是否被使用,用于伙伴内存的合并. 伙伴算法的特点 显而易见,伙伴算法会浪费大量的内存,(如果需要大小为9的内存块必须分配大小为16的内存块).而优点也是明显的,分配和合并算法都很简单易行.但是,当分配和回收较快的时候,例如分配大小为9的内存块,此时分配16,然后又回收,即合并伙伴内存块,这样会造成不必要的cpu浪费,应该设置链表中内存块的低潮个数,即当链表中内存块个数小于某个值的时候,并不合并伙伴内存块,只要当高于低潮个数的时候才合并. slab算法 一般来说,伙伴算法的改进算法用于操作系统分配和回收内存,而且内存块的单位较大,利于Linux使用的伙伴算法以页为单位.对于小块内存的分配和回收,伙伴算法就显得有些得不偿失了. 对于小块内存,一般采用slab算法,或者叫做slab机制. Linux 所使用的 slab 分配器的基础是 Jeff Bonwick 为SunOS 操作系统首次引入的一种算法。Jeff的分配器是围绕对象缓存进行的。在内核中，会为有限的对象集（例如文件描述符和其他常见结构）分配大量内存。Jeff发现对内核中普通对象进行初始化所需的时间超过了对其进行分配和释放所需的时间。因此他的结论是不应该将内存释放回一个全局的内存池，而是将内存保持为针对特定目而初始化的状态。例如，如果内存被分配给了一个互斥锁，那么只需在为互斥锁首次分配内存时执行一次互斥锁初始化函数（mutex_init）即可。后续的内存分配不需要执行这个初始化函数，因为从上次释放和调用析构之后，它已经处于所需的状态中了。 Linux slab分配器使用了这种思想和其他一些思想来构建一个在空间和时间上都具有高效性的内存分配器。 图中给出了 slab结构的高层组织结构。在最高层是 cache_chain，这是一个 slab 缓存的链接列表。这对于 best-fit算法非常有用，可以用来查找最适合所需要的分配大小的缓存（遍历列表）。cache_chain 的每个元素都是一个 kmem_cache 结构的引用（称为一个 cache）。它定义了一个要管理的给定大小的对象池。 每个缓存都包含了一个 slabs 列表，这是一段连续的内存块（通常都是页面）。存在3 种 slab： slabs_full: 完全分配的slab slabs_partial: 部分分配的slab slabs_empty: 空slab，或者没有对象被分配 slab 列表中的每个 slab都是一个连续的内存块（一个或多个连续页），它们被划分成一个个对象。这些对象是从特定缓存中进行分配和释放的基本元素。注意 slab 是 slab分配器进行操作的最小分配单位，因此如果需要对 slab 进行扩展，这也就是所扩展的最小值。通常来说，每个 slab 被分配为多个对象。 由于对象是从 slab 中进行分配和释放的，因此单个 slab 可以在 slab列表之间进行移动。例如，当一个 slab中的所有对象都被使用完时，就从slabs_partial 列表中移动到 slabs_full 列表中。当一个 slab完全被分配并且有对象被释放后，就从 slabs_full 列表中移动到slabs_partial 列表中。当所有对象都被释放之后，就从 slabs_partial 列表移动到 slabs_empty 列表中。 slab背后的动机 与传统的内存管理模式相比， slab缓存分配器提供了很多优点。首先，内核通常依赖于对小对象的分配，它们会在系统生命周期内进行无数次分配。slab缓存分配器通过对类似大小的对象进行缓存而提供这种功能，从而避免了常见的碎片问题。slab分配器还支持通用对象的初始化，从而避免了为同一目而对一个对象重复进行初始化。最后，slab分配器还可以支持硬件缓存对齐和着色，这允许不同缓存中的对象占用相同的缓存行，从而提高缓存的利用率并获得更好的性能。 本文地址：http://xnerv.wang/memory-allocation-buddy-system-and-slab-allocator/ 转载自：内存分配-----伙伴算法和slab算法","categories":[{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/categories/Linux/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"Heap","slug":"Heap","permalink":"https://xnerv.wang/tags/Heap/"},{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/tags/Linux/"},{"name":"Memory Management","slug":"Memory-Management","permalink":"https://xnerv.wang/tags/Memory-Management/"},{"name":"Buddy System","slug":"Buddy-System","permalink":"https://xnerv.wang/tags/Buddy-System/"},{"name":"Slab Allocator","slug":"Slab-Allocator","permalink":"https://xnerv.wang/tags/Slab-Allocator/"}]},{"title":"Everything You Always Wanted to Know About Fsync()（转载）","slug":"everything-you-always-wanted-to-know-about-fsync","date":"2013-11-15T08:00:00.000Z","updated":"2023-08-21T02:24:19.666Z","comments":true,"path":"everything-you-always-wanted-to-know-about-fsync/","link":"","permalink":"https://xnerv.wang/everything-you-always-wanted-to-know-about-fsync/","excerpt":"And then the developer wondered: is my file properly sync’ed on disk ? You probably know more or less how databases (or things that look like one) store their data on disk in a permanent and safe way. Or at least, you know the basic principles. Or not ?","text":"And then the developer wondered: is my file properly sync’ed on disk ? You probably know more or less how databases (or things that look like one) store their data on disk in a permanent and safe way. Or at least, you know the basic principles. Or not ? Being on AC(I)D There are a bunch of concepts that first must be understood: what is atomicity, consistency, and durability ? These concepts apply on databases (see ACID), but also on the underlying filesystem. Atomicity: a write operation is fully executed at once, and is not interleaved with another one (if, for example, someone else is writing to the same location) Atomicity is typically guaranteed in operations involving filename handling ; for example, for rename, “specification requires that the action of the function be atomic” – that is, when renaming a file from the old name to the new one, at no circumstances should you ever see the two files at the same time. Consistency: integrity of data must be maintained when executing an operation, even in a crash event – for example, a power outage in the middle of a rename() operation shall not leave the filesystem in a “weird” state, with the filename being unreachable because its metadata has been corrupted. (ie. either the operation is lost, or the operation is committed.) Consistency is guaranteed on the filesystem level ; but you also need to have the same guarantee if you build a database on disk, for example by serializing/locking certain operations on a working area, and committing the transaction by changing some kind of generation number. Durability: the write operation is durable, that is, unplugging the power cord (or a kernel panic, a crash…) shall not lose any data (hitting the hard disk with a hammer is however not covered!) This is an important one – at a given point, you must ensure that the data is actually written on disk physically, preventing any loss of data in case of a sudden power outage, for example. This is absolutely critical when dealing with a client/server architecture: the client may have its connection or transaction aborted at any time without troubles (ie. the transaction will be retried later), but once the server acknowledges it, no event should ever cause it to be lost (think of responsibility in a commercial transaction, or a digital signature, for example). For this reason, having the data committed in the internal system or hard disk cache is NOT durable for obvious reasons (unless there is a guarantee that no such power outage could happen – if a battery is used on a RAID array, for example). On POSIX systems, durability is achieved through sync operations (fsync(), fdatasync(), aio_fsync()): “The fsync() function is intended to force a physical write of data from the buffer cache, and to assure that after a system crash or other failure that all data up to the time of the fsync() call is recorded on the disk.”. [Note: The difference between fsync() and fdatasync() is that the later does not necessarily update the meta-data associated with a file – such as the “last modified” date – but only the file data.] Now that these concepts are a bit clearer, let’s go back to our filesystem! Hey, What is a File, By The Way ? If we want to simplify the concept, let’s consider the filesystem on POSIX platforms as a very simple flat storage manager, allowing to read/write data blobs and basic properties (such as the modified time) indexed by an integer number (hint: they sometimes call that the inode number). For example, you may want to read the file #4242’s data. And later, write some data on file #1234. To have a more convenient way to handle files (because “I need to send you the presentation number 155324” would not be really convenient in the real world), we use the filename/directory concepts. A file has a name, and it is contained within a directory structure. You may put files and directories in a directory, building a hierarchical structure. But everything rely on our previous basic data blobs to store both filename and the associated index. As an example, reading the file foo/bar.txt (ie. the file bar.txt within the foo directory) will require to access the data blob associated with the directory foo. After parsing this opaque data blob, the system will fetch the entry for bar.txt, and open the associated data blob. (And yes, there is obviously a root entry, storing references to first-level entries, allowing to access any file top-down) If I now want to create a new file named foo/baz.txt, it will require the system to access the data blob associated with the directory foo, add an entry named baz.txt with a new allocated index for the upcoming file, and write the updated directory blob back, and from this point, write to the newly allocated blob. The operation therefore involves two data structures: the directory entry, and the file itself. Keeping My File(name) Safe Let’s go back to our database problem: what is the impact of having two data structures for our files ? Atomicity and consistency of filenames are handled for us by the filesystem, so this is not really a bother. What about durability ? We know that fsync() provides guarantees related to data and meta-data sync’ing. But if you look closer to the specification, the only data involved are the one related to the file itself – not its directory entry. The “metadata” concept involves modified time, access time etc. – not the directory entry itself. It would be cumbersome for a filesystem to provide this guarantee, by the way: on POSIX systems, you can have an arbitrary number of directory links to a filename (or to another directory entry). The most common case is one, of course. But you may delete a file being used (the file entry will be removed by the system when the file is closed) – the very reason why erasing a log file which is flooding a filesystem is a futile and deadly action – in such case, the number of links will be zero. And you may also create as many hard-links as you want for a given file/directory entry. Therefore, in theory, you may create a file, write some data, synchronize it, close the file, and see your precious file lost forever because of a power outage. Oh, the filesystem must guarantee consistency, of course, but not durability unless explicitly asked by the client – which means that a filesystem check may find your directory entry partially written, and decide to achieve consistency by taking the previous directory blob entry, wiping the unreferenced file entry (note: if you are “lucky” enough, the file will be expelled in lost+found) The filesystem can, of course, decide to be gentle, and commit all filename operations when fsync’ing. It may also, such as for ext3, commit everything when fsync’ing a file – causing the infamous and horrendous lags in firefox or thunderbird. But if you need to have guarantees, and not just hope the filesystem “will be gentle”, and do not want to “trust the filesystem” (yes, someone actually told me that: you need to “trust the filesystem” – I swear it), you have to actually make sure that your filename entry is properly sync’ed on disk following the POSIX specification. Oh, and by the way: according to POSIX, The fsync() function is intended to force a physical write of data from the buffer cache, and to assure that after a system crash or other failure that all data up to the time of the fsync() call is recorded on the disk. But things are sometimes a bit obscure on the implementation side : Linux/ext3: If the underlying hard disk has write caching enabled, then the data may not really be on permanent storage when fsync() / fdatasync() return. (do’h!) Linux/ext4: The fsync() implementations in older kernels and lesser used filesystems does not know how to flush disk caches. (do’h!) – issue adressed quite recently OSX: For applications that require tighter guarantees about the integrity of their data, Mac OS X provides the F_FULLFSYNC fcntl. The F_FULLFSYNC fcntl asks the drive to flush all buffered data to permanent storage (hey, fsync was supposed to do that, no ? guys ?) (Edit: no, fsync is actually not required to do that – thanks for the clarification Florent!) But we may assume that on Linux with ext4 (and OSX with proper flags ?) the system is properly propagating write barriers. On Windows, using FlushFileBuffers() is probably the way to go. Syncing Filenames I told you that a filesystem was actually a bunch of flat data blobs with associated metadata, and that a file had actually two parts: its directory entry (let’s assume there is only one directory entry for the sake of simplicity), and its actual data. We already know how to sync the later one ; do we have a way to do the same for the directory container itself ? On POSIX, you may actually open a directory as if you were opening a file (hint: a directory is a file that contains directory entries). It means that open() may successfully open a directory entry. But on the other hand, you generally can not open a directory entry for writing (see POSIX remark regarding EISDIR: The named file is a directory and oflag includes O_WRONLY or O_RDWR), and this is perfectly logical: by directly writing to the internal directory entry, you may be able to mess up with the directory structure, ruining the filesystem consistency. But can we fsync() written data using a file descriptor opened only for reading ? The question is… yes, or at least “yes it should” – even POSIX group had editorial inconsistencies regarding fdatasync and aio_fsync(), leading to incorrect behavior on various implementations. And the reason it should execute the operation is because requesting the completion of a write operation does not have to require actual write access – which have already been checked and enforced. On Windows… err, there is no clear answer. You can not call FlushFileBuffers() on a directory handle as far as I can see. Oh, a last funny note: how do you sync the content of a symbolic link (and its related meta-data), that is, the filename pointed by this link ? The answer is… you can’t. Nope. This is not possible with the current standard (hint: you can not open() a symbolic link). Which means that if you handle some kind of database generation update based on symbolic links (ie. changing a “last-version” symlink to the latest built generation file), you have zero guarantee over durability. Conclusion Does it means that we need to call fsync() twice, one on the file data, and one on its parent directory ? When you need to achieve durability, the answer is obviously yes. (Remember that file file/filename will be sync’ed on disk anyway by the operating system, so you do not actually need to do that for every single file – only for those you want to have a durability guarantee at a given time) However, the question is causing some headache on the POSIX standard, and as a follow-up to the austin-group (ie. POSIX mailing-list) discussion, an editorial clarification request is still pending and is waiting for feedback from various implementors. (you may also have a look at the comp.unix.programmer discussion) TL;DR: syncing a file is not as simple as it seems! 本文地址：http://xnerv.wang/everything-you-always-wanted-to-know-about-fsync/ 转载自：Everything You Always Wanted to Know About Fsync()","categories":[{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/categories/Linux/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/tags/Linux/"},{"name":"Linux API","slug":"Linux-API","permalink":"https://xnerv.wang/tags/Linux-API/"}]},{"title":"Your visual how-to guide for SELinux policy enforcement（转载）","slug":"your-visual-how-to-guide-for-selinux-policy-enforcement","date":"2013-11-13T08:00:00.000Z","updated":"2023-08-21T02:24:19.879Z","comments":true,"path":"your-visual-how-to-guide-for-selinux-policy-enforcement/","link":"","permalink":"https://xnerv.wang/your-visual-how-to-guide-for-selinux-policy-enforcement/","excerpt":"Image by : opensource.com We are celebrating the SELinux 10th year anversary this year. Hard to believe it. SELinux was first introduced in Fedora Core 3 and later in Red Hat Enterprise Linux 4. For those who have never used SELinux, or would like an explanation… SElinux is a labeling system. Every process has a label. Every file/directory object in the operating system has a label. Even network ports, devices, and potentially hostnames have labels assigned to them. We write rules to control the access of a process label to an a object label like a file. We call this policy. The kernel enforces the rules. Sometimes this enforcement is called Mandatory Access Control (MAC). The owner of an object does not have discretion over the security attributes of a object. Standard Linux access control, owner/group + permission flags like rwx, is often called Discretionary Access Control (DAC). SELinux has no concept of UID or ownership of files. Everything is controlled by the labels. Meaning an SELinux system can be setup without an all powerful root process. Note: _SELinux does not let you side step DAC Controls. SELinux is a parallel enforcement model. An application has to be allowed by BOTH SELinux and DAC to do certain activities. This can lead to confusion for administrators because the process gets Permission Denied. Administrators see Permission Denied means something is wrong with DAC, not SELinux labels.","text":"Image by : opensource.com We are celebrating the SELinux 10th year anversary this year. Hard to believe it. SELinux was first introduced in Fedora Core 3 and later in Red Hat Enterprise Linux 4. For those who have never used SELinux, or would like an explanation… SElinux is a labeling system. Every process has a label. Every file/directory object in the operating system has a label. Even network ports, devices, and potentially hostnames have labels assigned to them. We write rules to control the access of a process label to an a object label like a file. We call this policy. The kernel enforces the rules. Sometimes this enforcement is called Mandatory Access Control (MAC). The owner of an object does not have discretion over the security attributes of a object. Standard Linux access control, owner/group + permission flags like rwx, is often called Discretionary Access Control (DAC). SELinux has no concept of UID or ownership of files. Everything is controlled by the labels. Meaning an SELinux system can be setup without an all powerful root process. Note: _SELinux does not let you side step DAC Controls. SELinux is a parallel enforcement model. An application has to be allowed by BOTH SELinux and DAC to do certain activities. This can lead to confusion for administrators because the process gets Permission Denied. Administrators see Permission Denied means something is wrong with DAC, not SELinux labels. Type enforcement Lets look a little further into the labels. The SELinux primary model or enforcement is called type enforcement. Basically this means we define the label on a process based on its type, and the label on a file system object based on its type. Analogy Imagine a system where we define types on objects like cats and dogs. A cat and dog are process types. *all cartoons by Máirín Duffy We have a class of objects that they want to interact with which we call food. And I want to add types to the food, cat_food and dog_food. As a policy writer, I would say that a dog has permission to eat dog_chow food and a cat has permission to eat cat_chow food. In SELinux we would write this rule in policy. allow cat cat_chow:food eat; allow dog dog_chow:food eat; With these rules the kernel would allow the cat process to eat food labeled cat_chow and the dog to eat food labeled dog_chow. But in an SELinux system everything is denied by default. This means that if the dog process tried to eat the cat_chow, the kernel would prevent it. Likewise cats would not be allowed to touch dog food. Real world We label Apache processes as httpd_t and we label Apache content as httpd_sys_content_t and httpd_sys_content_rw_t. Imagine we have credit card data stored in a mySQL database which is labeled msyqld_data_t. If an Apache process is hacked, the hacker could get control of the httpd_t process and would be allowed to read httpd_sys_content_t files and write to httpd_sys_content_rw_t. But the hacker would not be allowed to read the credit card data (mysqld_data_t) even if the process was running as root. In this case SELinux has mitigated the break in. MCS enforcement _Analogy _ Above, we typed the dog process and cat process, but what happens if you have multiple dogs processes: Fido and Spot. You want to stop Fido from eating Spot’s dog_chow. One solution would be to create lots of new types, like Fido_dog and Fido_dog_chow. But, this will quickly become unruly because all dogs have pretty much the same permissions. To handle this we developed a new form of enforcement, which we call Multi Category Security (MCS). In MCS, we add another section of the label which we can apply to the dog process and to the dog_chow food. Now we label the dog process as dog:random1 (Fido) and dog:random2 (Spot). We label the dog chow as dog_chow:random1 (Fido) and dog_chow:random2 (Spot). MCS rules say that if the type enforcement rules are OK and the random MCS labels match exactly, then the access is allowed, if not it is denied. Fido (dog:random1) trying to eat cat_chow:food is denied by type enforcement. Fido (dog:random1) is allowed to eat dog_chow:random1. Fido (dog:random1) denied to eat spot’s (dog_chow:random2) food. Real world In computer systems we often have lots of processes all with the same access, but we want them separated from each other. We sometimes call this a multi-tenant environment. The best example of this is virtual machines. If I have a server running lots of virtual machines, and one of them gets hacked, I want to prevent it from attacking the other virtual machines and virtual machine images. But in a type enforcement system the KVM virtual machine is labeled svirt_t and the image is labeled svirt_image_t. We have rules that say svirt_t can read/write/delete content labeled svirt_image_t. With libvirt we implemented not only type enforcement separation, but also MCS separation. When libvirt is about to launch a virtual machine it picks out a random MCS label like s0:c1,c2, it then assigns the svirt_image_t:s0:c1,c2 label to all of the content that the virtual machine is going to need to manage. Finally, it launches the virtual machine as svirt_t:s0:c1,c2. Then, the SELinux kernel controls that svirt_t:s0:c1,c2 can not write to svirt_image_t:s0:c3,c4, even if the virtual machine is controled by a hacker and takes it over. Even if it is running as root. We use similar separation in OpenShift. Each gear (user/app process)runs with the same SELinux type (openshift_t). Policy defines the rules controlling the access of the gear type and a unique MCS label to make sure one gear can not interact with other gears. Watch this short video on what would happen if an Openshift gear became root. MLS enforcement Another form of SELinux enforcement, used much less frequently, is called Multi Level Security (MLS); it was developed back in the 60s and is used mainly in trusted operating systems like Trusted Solaris. The main idea is to control processes based on the level of the data they will be using. A secret process can not read top secret data. MLS is very similar to MCS, except it adds a concept of dominance to enforcement. Where MCS labels have to match exactly, one MLS label can dominate another MLS label and get access. Analogy Instead of talking about different dogs, we now look at different breeds. We might have a Greyhound and a Chihuahua. We might want to allow the Greyhound to eat any dog food, but a Chihuahua could choke if it tried to eat Greyhound dog food. We want to label the Greyhound as dog:Greyhound and his dog food as _dog_chow:Greyhound, _and label the Chihuahua as dog:Chihuahua and his food as dog_chow:Chihuahua. With the MLS policy, we would have the MLS Greyhound label dominate the Chihuahua label. This means dog:Greyhound is allowed to eat _dog_chow:Greyhound _and dog_chow:Chihuahua. But dog:Chihuahua is not allowed to eat dog_chow:Greyhound. Of course, dog:Greyhound and dog:Chihuahua are still prevented from eating cat_chow:Siamese by type enforcement, even if the MLS type Greyhound dominates Siamese. Real world I could have two Apache servers: one running as httpd_t:TopSecret and another running as httpd_t:Secret. If the Apache process httpd_t:Secret were hacked, the hacker could read httpd_sys_content_t:Secret but would be prevented from reading httpd_sys_content_t:TopSecret. However, if the Apache server running httpd_t:TopSecret was hacked, it could read httpd_sys_content_t:Secret data as well as httpd_sys_content_t:TopSecret. We use the MLS in military environments where a user might only be allowed to see secret data, but another user on the same system could read top secret data. Conclusion SELinux is a powerful labeling system, controlling access granted to individual processes by the kernel. The primary feature of this is type enforcement where rules define the access allowed to a process is allowed based on the labeled type of the process and the labeled type of the object. Two additional controls have been added to separate processes with the same type from each other called MCS, total separtion from each other, and MLS, allowing for process domination. 本文地址：http://xnerv.wang/your-visual-how-to-guide-for-selinux-policy-enforcement/ 转载自：Your visual how-to guide for SELinux policy enforcement","categories":[{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/tags/Linux/"},{"name":"SELinux","slug":"SELinux","permalink":"https://xnerv.wang/tags/SELinux/"}]},{"title":"孤儿进程与僵尸进程[总结]（转载）","slug":"summary-of-orphan-process-and-zombie-process","date":"2013-08-21T07:57:00.000Z","updated":"2023-08-21T02:24:19.938Z","comments":true,"path":"summary-of-orphan-process-and-zombie-process/","link":"","permalink":"https://xnerv.wang/summary-of-orphan-process-and-zombie-process/","excerpt":"1、前言 之前在看《unix环境高级编程》第八章进程时候，提到孤儿进程和僵尸进程，一直对这两个概念比较模糊。今天被人问到什么是孤儿进程和僵尸进程，会带来什么问题，怎么解决，我只停留在概念上面，没有深入，倍感惭愧。晚上回来google了一下，再次参考APUE，认真总结一下，加深理解。 2、基本概念 我们知道在unix/linux中，正常情况下，子进程是通过父进程创建的，子进程在创建新的进程。子进程的结束和父进程的运行是一个异步过程,即父进程永远无法预测子进程 到底什么时候结束。 当一个 进程完成它的工作终止之后，它的父进程需要调用wait()或者waitpid()系统调用取得子进程的终止状态。 孤儿进程：一个父进程退出，而它的一个或多个子进程还在运行，那么那些子进程将成为孤儿进程。孤儿进程将被init进程(进程号为1)所收养，并由init进程对它们完成状态收集工作。 僵尸进程：一个进程使用fork创建子进程，如果子进程退出，而父进程并没有调用wait或waitpid获取子进程的状态信息，那么子进程的进程描述符仍然保存在系统中。这种进程称之为僵死进程。","text":"1、前言 之前在看《unix环境高级编程》第八章进程时候，提到孤儿进程和僵尸进程，一直对这两个概念比较模糊。今天被人问到什么是孤儿进程和僵尸进程，会带来什么问题，怎么解决，我只停留在概念上面，没有深入，倍感惭愧。晚上回来google了一下，再次参考APUE，认真总结一下，加深理解。 2、基本概念 我们知道在unix/linux中，正常情况下，子进程是通过父进程创建的，子进程在创建新的进程。子进程的结束和父进程的运行是一个异步过程,即父进程永远无法预测子进程 到底什么时候结束。 当一个 进程完成它的工作终止之后，它的父进程需要调用wait()或者waitpid()系统调用取得子进程的终止状态。 孤儿进程：一个父进程退出，而它的一个或多个子进程还在运行，那么那些子进程将成为孤儿进程。孤儿进程将被init进程(进程号为1)所收养，并由init进程对它们完成状态收集工作。 僵尸进程：一个进程使用fork创建子进程，如果子进程退出，而父进程并没有调用wait或waitpid获取子进程的状态信息，那么子进程的进程描述符仍然保存在系统中。这种进程称之为僵死进程。 3、问题及危害 unix提供了一种机制可以保证只要父进程想知道子进程结束时的状态信息， 就可以得到。这种机制就是: 在每个进程退出的时候,内核释放该进程所有的资源,包括打开的文件,占用的内存等。 但是仍然为其保留一定的信息(包括进程号the process ID,退出状态the termination status of the process,运行时间the amount of CPU time taken by the process等)。直到父进程通过wait / waitpid来取时才释放。 但这样就导致了问题，如果进程不调用wait / waitpid的话， 那么保留的那段信息就不会释放，其进程号就会一直被占用，但是系统所能使用的进程号是有限的，如果大量的产生僵死进程，将因为没有可用的进程号而导致系统不能产生新的进程. 此即为僵尸进程的危害，应当避免。 孤儿进程是没有父进程的进程，孤儿进程这个重任就落到了init进程身上，init进程就好像是一个民政局，专门负责处理孤儿进程的善后工作。每当出现一个孤儿进程的时候，内核就把孤 儿进程的父进程设置为init，而init进程会循环地wait()它的已经退出的子进程。这样，当一个孤儿进程凄凉地结束了其生命周期的时候，init进程就会代表党和政府出面处理它的一切善后工作。因此孤儿进程并不会有什么危害。 任何一个子进程(init除外)在exit()之后，并非马上就消失掉，而是留下一个称为僵尸进程(Zombie)的数据结构，等待父进程处理。这是每个 子进程在结束时都要经过的阶段。如果子进程在exit()之后，父进程没有来得及处理，这时用ps命令就能看到子进程的状态是“Z”。如果父进程能及时 处理，可能用ps命令就来不及看到子进程的僵尸状态，但这并不等于子进程不经过僵尸状态。 如果父进程在子进程结束之前退出，则子进程将由init接管。init将会以父进程的身份对僵尸状态的子进程进行处理。 僵尸进程危害场景： 例如有个进程，它定期的产 生一个子进程，这个子进程需要做的事情很少，做完它该做的事情之后就退出了，因此这个子进程的生命周期很短，但是，父进程只管生成新的子进程，至于子进程 退出之后的事情，则一概不闻不问，这样，系统运行上一段时间之后，系统中就会存在很多的僵死进程，倘若用ps命令查看的话，就会看到很多状态为Z的进程。 严格地来说，僵死进程并不是问题的根源，罪魁祸首是产生出大量僵死进程的那个父进程。因此，当我们寻求如何消灭系统中大量的僵死进程时，答案就是把产生大 量僵死进程的那个元凶枪毙掉（也就是通过kill发送SIGTERM或者SIGKILL信号啦）。枪毙了元凶进程之后，它产生的僵死进程就变成了孤儿进 程，这些孤儿进程会被init进程接管，init进程会wait()这些孤儿进程，释放它们占用的系统进程表中的资源，这样，这些已经僵死的孤儿进程 就能瞑目而去了。 3、孤儿进程和僵尸进程测试 孤儿进程测试程序如下所示： 1234567891011121314151617181920212223242526272829303132333435363738#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;errno.h&gt;#include &lt;unistd.h&gt;int main()&#123; pid_t pid; //创建一个进程 pid = fork(); //创建失败 if (pid &lt; 0) &#123; perror(&quot;fork error:&quot;); exit(1); &#125; //子进程 if (pid == 0) &#123; printf(&quot;I am the child process.\\n&quot;); //输出进程ID和父进程ID printf(&quot;pid: %d\\tppid:%d\\n&quot;,getpid(),getppid()); printf(&quot;I will sleep five seconds.\\n&quot;); //睡眠5s，保证父进程先退出 sleep(5); printf(&quot;pid: %d\\tppid:%d\\n&quot;,getpid(),getppid()); printf(&quot;child process is exited.\\n&quot;); &#125; //父进程 else &#123; printf(&quot;I am father process.\\n&quot;); //父进程睡眠1s，保证子进程输出进程id sleep(1); printf(&quot;father process is exited.\\n&quot;); &#125; return 0;&#125; 测试结果如下： 僵尸进程测试程序如下所示： 123456789101112131415161718192021222324252627#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;errno.h&gt;#include &lt;stdlib.h&gt;int main()&#123; pid_t pid; pid = fork(); if (pid &lt; 0) &#123; perror(&quot;fork error:&quot;); exit(1); &#125; else if (pid == 0) &#123; printf(&quot;I am child process.I am exiting.\\n&quot;); exit(0); &#125; printf(&quot;I am father process.I will sleep two seconds\\n&quot;); //等待子进程先退出 sleep(2); //输出进程信息 system(&quot;ps -o pid,ppid,state,tty,command&quot;); printf(&quot;father process is exiting.\\n&quot;); return 0;&#125; 测试结果如下所示： 僵尸进程测试2：父进程循环创建子进程，子进程退出，造成多个僵尸进程，程序如下所示： 1234567891011121314151617181920212223242526272829303132#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;errno.h&gt;int main()&#123; pid_t pid; //循环创建子进程 while(1) &#123; pid = fork(); if (pid &lt; 0) &#123; perror(&quot;fork error:&quot;); exit(1); &#125; else if (pid == 0) &#123; printf(&quot;I am a child process.\\nI am exiting.\\n&quot;); //子进程退出，成为僵尸进程 exit(0); &#125; else &#123; //父进程休眠20s继续创建子进程 sleep(20); continue; &#125; &#125; return 0;&#125; 程序测试结果如下所示： 4、僵尸进程解决办法 （1）通过信号机制 子进程退出时向父进程发送SIGCHILD信号，父进程处理SIGCHILD信号。在信号处理函数中调用wait进行处理僵尸进程。测试程序如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;errno.h&gt;#include &lt;stdlib.h&gt;#include &lt;signal.h&gt;static void sig_child(int signo);int main()&#123; pid_t pid; //创建捕捉子进程退出信号 signal(SIGCHLD,sig_child); pid = fork(); if (pid &lt; 0) &#123; perror(&quot;fork error:&quot;); exit(1); &#125; else if (pid == 0) &#123; printf(&quot;I am child process,pid id %d.I am exiting.\\n&quot;,getpid()); exit(0); &#125; printf(&quot;I am father process.I will sleep two seconds\\n&quot;); //等待子进程先退出 sleep(2); //输出进程信息 system(&quot;ps -o pid,ppid,state,tty,command&quot;); printf(&quot;father process is exiting.\\n&quot;); return 0;&#125;static void sig_child(int signo)&#123; pid_t pid; int stat; //处理僵尸进程 while ((pid = waitpid(-1, &amp;stat, WNOHANG)) &gt;0) printf(&quot;child %d terminated.\\n&quot;, pid);&#125; 测试结果如下所示： 程序测试结果如下所示： （2）fork两次 《Unix 环境高级编程》8.6节说的非常详细。原理是将子进程成为孤儿进程，从而其的父进程变为init进程，通过init进程可以处理僵尸进程。测试程序如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;errno.h&gt;int main()&#123; pid_t pid; //创建第一个子进程 pid = fork(); if (pid &lt; 0) &#123; perror(&quot;fork error:&quot;); exit(1); &#125; //第一个子进程 else if (pid == 0) &#123; //子进程再创建子进程 printf(&quot;I am the first child process.pid:%d\\tppid:%d\\n&quot;,getpid(),getppid()); pid = fork(); if (pid &lt; 0) &#123; perror(&quot;fork error:&quot;); exit(1); &#125; //第一个子进程退出 else if (pid &gt;0) &#123; printf(&quot;first procee is exited.\\n&quot;); exit(0); &#125; //第二个子进程 //睡眠3s保证第一个子进程退出，这样第二个子进程的父亲就是init进程里 sleep(3); printf(&quot;I am the second child process.pid: %d\\tppid:%d\\n&quot;,getpid(),getppid()); exit(0); &#125; //父进程处理第一个子进程退出 if (waitpid(pid, NULL, 0) != pid) &#123; perror(&quot;waitepid error:&quot;); exit(1); &#125; exit(0); return 0;&#125; 测试结果如下图所示： 5、参考资料 《unix环境高级编程》第八章 http://www.rosoo.net/a/201109/15071.html http://blog.chinaunix.net/uid-1829236-id-3166986.html http://forkhope.diandian.com/post/2012-10-01/40040574200 http://blog.csdn.net/metasearch/article/details/2498853 http://blog.csdn.net/yuwenliang/article/details/6770750 本文地址：http://xnerv.wang/summary-of-orphan-process-and-zombie-process/ 转载自：孤儿进程与僵尸进程[总结]","categories":[{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/categories/Linux/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/tags/Linux/"},{"name":"进程管理","slug":"进程管理","permalink":"https://xnerv.wang/tags/%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/"},{"name":"孤儿进程","slug":"孤儿进程","permalink":"https://xnerv.wang/tags/%E5%AD%A4%E5%84%BF%E8%BF%9B%E7%A8%8B/"},{"name":"僵尸进程","slug":"僵尸进程","permalink":"https://xnerv.wang/tags/%E5%83%B5%E5%B0%B8%E8%BF%9B%E7%A8%8B/"}]},{"title":"(Stack Overflow) When does Windows decide to pull out pages from Working Set?","slug":"when-does-windows-decide-to-pull-out-pages-from-working-set","date":"2013-08-15T19:18:31.000Z","updated":"2023-08-21T02:24:20.239Z","comments":true,"path":"when-does-windows-decide-to-pull-out-pages-from-working-set/","link":"","permalink":"https://xnerv.wang/when-does-windows-decide-to-pull-out-pages-from-working-set/","excerpt":"Question I am looking at the distribution of physical memory using the RAMMap tool by SysInternals. The distribution (see image below) shows 7 GB in Free (Zeroed) memory list 2 GB in Working Set list 550 MB in Standby list 125 MB in Modified list I understand that when a process requires memory, Windows searches for memory in the lists in the order Free (Zeroed) -&gt; Standby -&gt; Modified. Hence, as long as we have Free page frames, everything should either be in the Working Set list or Free list. Is this assumption right? If so, why do we see 125 MB of pages in the Modified list? In general, when does Windows decide to pull out pages from Working set list to Modified/Standby list even when there is Free memory left? I am using Windows 7, with 4.0 GB of installed RAM, 3.5 GB being usable.","text":"Question I am looking at the distribution of physical memory using the RAMMap tool by SysInternals. The distribution (see image below) shows 7 GB in Free (Zeroed) memory list 2 GB in Working Set list 550 MB in Standby list 125 MB in Modified list I understand that when a process requires memory, Windows searches for memory in the lists in the order Free (Zeroed) -&gt; Standby -&gt; Modified. Hence, as long as we have Free page frames, everything should either be in the Working Set list or Free list. Is this assumption right? If so, why do we see 125 MB of pages in the Modified list? In general, when does Windows decide to pull out pages from Working set list to Modified/Standby list even when there is Free memory left? I am using Windows 7, with 4.0 GB of installed RAM, 3.5 GB being usable. Answer by Rick Brant No. If a page is going to be read from disk, or if it’s an allocation from kernel mode, the free list is checked first, then the zeroed list. If it isn’t, the zeroed list is checked first, then the free list (if allocated from the free list the page will be zeroed in-line before the faulting process gets to see it). In both cases, the standby list comes next. I don’t believe the modified list is ever used to satisfy page faults… since the pages have to be written out to disk before they can be assigned to other uses… so, might as well wait for the modified page writer to do its regular thing, whereupon the pages will show up on the standby list. 1a. “As long as we have free page frames, everything should either be in the working set list or free list.” No. The standby list is considered “available” but until someone needs that RAM its pages are used for two different types of caches. The second question (stupid automatic list formatting keeps starting over at “1” here): That’s rather a lot. Did you delete your pagefile? The third question: First, keep in mind that there is a working set list for every process, not just one. And each has its own “limit”. When the limits are being enforced (they aren’t if there is plenty of free RAM), the process has to give up a page for every new page it faults in. There is also a working set reclamation process that tries to shrink long-idle processes. When a process loses a page, it goes on the modified list if it was touched with a “modify”-style operand while it was in the working set, or on the standby list if not. If the process later faults to the page before it is assigned to some other use or process, it can be brought back into the process working set without going to disk for it. (This is a common example of a “soft” page fault.) In this way these lists form a sort of common system-wide extension to all processes’ working sets. But everything on the Standby list is still counted as part of “Available” memory, because it can be used by any other process immediately - noting has to be written out to disk. Starting with Vista, there is a new concept called memory priority. Pages on the standby list that are of low priority can be reassigned for use by the new proactive file cache, SuperFetch. But this doesn’t cost any “available” RAM because they’re still on the standby list. (The old reactive cache is still there; it used to be part of the system working set; as of Windows 7 it has its own.) This is but the tip of the iceberg. The Mm chapter of Windows Internals is 200 pages long, enough for a smallish book all by itself. But if you really want to understand this stuff, there isn’t really a substitute. Article link: http://xnerv.wang/when-does-windows-decide-to-pull-out-pages-from-working-set/ Reprinted from: (StackOverflow) When does Windows decide to pull out pages from Working Set?","categories":[{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/categories/Windows/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"OS","slug":"OS","permalink":"https://xnerv.wang/tags/OS/"},{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/tags/Windows/"},{"name":"Stack Overflow","slug":"Stack-Overflow","permalink":"https://xnerv.wang/tags/Stack-Overflow/"},{"name":"Memory Management","slug":"Memory-Management","permalink":"https://xnerv.wang/tags/Memory-Management/"},{"name":"Virtual Memory","slug":"Virtual-Memory","permalink":"https://xnerv.wang/tags/Virtual-Memory/"},{"name":"Working Set","slug":"Working-Set","permalink":"https://xnerv.wang/tags/Working-Set/"}]},{"title":"虚函数与虚继承寻踪（转载）","slug":"virtual-function-and-virtual-table","date":"2013-01-14T17:24:00.000Z","updated":"2023-08-21T02:24:21.781Z","comments":true,"path":"virtual-function-and-virtual-table/","link":"","permalink":"https://xnerv.wang/virtual-function-and-virtual-table/","excerpt":"封装、继承、多态是面向对象语言的三大特性，熟悉C++的人对此应该不会有太多异议。C语言提供的struct，顶多算得上对数据的简单封装，而C++的引入把struct“升级”为class，使得面向对象的概念更加强大。继承机制解决了对象复用的问题，然而多重继承又会产生成员冲突的问题，虚继承在我看来更像是一种“不得已”的解决方案。多态让对象具有了运行时特性，并且它是软件设计复用的本质，虚函数的出现为多态性质提供了实现手段。 如果说C语言的struct相当于对数据成员简单的排列（可能有对齐问题），那么C++的class让对象的数据的封装变得更加复杂。所有的这些问题来源于C++的一个关键字——virtual！virtual在C++中最大的功能就是声明虚函数和虚基类，有了这种机制，C++对象的机制究竟发生了怎样的变化，让我们一起探寻之。 为了查看对象的结构模型，我们需要在编译器配置时做一些初始化。在VS2010中，在项目——属性——配置属性——C/C++——命令行——其他选项中添加选项“/d1reportAllClassLayout”。再次编译时候，编译器会输出所有定义类的对象模型。由于输出的信息过多，我们可以使用“Ctrl+F”查找命令，找到对象模型的输出。","text":"封装、继承、多态是面向对象语言的三大特性，熟悉C++的人对此应该不会有太多异议。C语言提供的struct，顶多算得上对数据的简单封装，而C++的引入把struct“升级”为class，使得面向对象的概念更加强大。继承机制解决了对象复用的问题，然而多重继承又会产生成员冲突的问题，虚继承在我看来更像是一种“不得已”的解决方案。多态让对象具有了运行时特性，并且它是软件设计复用的本质，虚函数的出现为多态性质提供了实现手段。 如果说C语言的struct相当于对数据成员简单的排列（可能有对齐问题），那么C++的class让对象的数据的封装变得更加复杂。所有的这些问题来源于C++的一个关键字——virtual！virtual在C++中最大的功能就是声明虚函数和虚基类，有了这种机制，C++对象的机制究竟发生了怎样的变化，让我们一起探寻之。 为了查看对象的结构模型，我们需要在编译器配置时做一些初始化。在VS2010中，在项目——属性——配置属性——C/C++——命令行——其他选项中添加选项“/d1reportAllClassLayout”。再次编译时候，编译器会输出所有定义类的对象模型。由于输出的信息过多，我们可以使用“Ctrl+F”查找命令，找到对象模型的输出。 一、基本对象模型 首先，我们定义一个简单的类，它含有一个数据成员和一个虚函数。 1234567class MyClass&#123; int var;public: virtual void fun() &#123;&#125;&#125;; 编译输出的MyClass对象结构如下： 123456789101112class MyClass size(8): +--- 0 | &#123;vfptr&#125; 4 | var +---MyClass::$vftable@: | &amp;MyClass_meta | 0 0 | &amp;MyClass::funMyClass::fun this adjustor: 0 从这段信息中我们看出，MyClass对象大小是8个字节。前四个字节存储的是虚函数表的指针vfptr，后四个字节存储对象成员var的值。虚函数表的大小为4字节，就一条函数地址，即虚函数fun的地址，它在虚函数表vftable的偏移是0。因此，MyClass对象模型的结果如图1所示。 图1 MyClass对象模型 MyClass的虚函数表虽然只有一条函数记录，但是它的结尾处是由4字节的0作为结束标记的。 adjust表示虚函数机制执行时，this指针的调整量，假如fun被多态调用的话，那么它的形式如下： 1*(this+0)[0]() 总结虚函数调用形式，应该是： 1*(this指针+调整量)[虚函数在vftable内的偏移]() 二、单重继承对象模型 我们定义一个继承于MyClass类的子类MyClassA，它重写了fun函数，并且提供了一个新的虚函数funA。 123456789class MyClassA:public MyClass&#123; int varA;public: virtual void fun() &#123;&#125; virtual void funA() &#123;&#125;&#125;; 它的对象模型为： 1234567891011121314151617class MyClassA size(12): +--- | +--- (base class MyClass) 0 | | &#123;vfptr&#125; 4 | | var | +--- 8 | varA +---MyClassA::$vftable@: | &amp;MyClassA_meta | 0 0 | &amp;MyClassA::fun 1 | &amp;MyClassA::funAMyClassA::fun this adjustor: 0MyClassA::funA this adjustor: 0 可以看出，MyClassA将基类MyClass完全包含在自己内部，包括vfptr和var。并且虚函数表内的记录多了一条——MyClassA自己定义的虚函数funA。它的对象模型如图2所示。 图2 MyClassA对象模型 我们可以得出结论：在单继承形式下，子类的完全获得父类的虚函数表和数据。子类如果重写了父类的虚函数（如fun），就会把虚函数表原本fun对应的记录（内容MyClass::fun）覆盖为新的函数地址（内容MyClassA::fun），否则继续保持原本的函数地址记录。如果子类定义了新的虚函数，虚函数表内会追加一条记录，记录该函数的地址（如MyClassA::funA）。 使用这种方式，就可以实现多态的特性。假设我们使用如下语句： 12MyClass*pc=new MyClassA;pc-&gt;fun(); 编译器在处理第二条语句时，发现这是一个多态的调用，那么就会按照上边我们对虚函数的多态访问机制调用函数fun。 1*(pc+0)[0]() 因为虚函数表内的函数地址已经被子类重写的fun函数地址覆盖了，因此该处调用的函数正是MyClassA::fun，而不是基类的MyClass::fun。 如果使用MyClassA对象直接访问fun，则不会出发多态机制，因为这个函数调用在编译时期是可以确定的，编译器只需要直接调用MyClassA::fun即可。 三、多重继承对象模型 和前边MyClassA类似，我们也定义一个类MyClassB。 123456789class MyClassB:public MyClass&#123; int varB;public: virtual void fun() &#123;&#125; virtual void funB() &#123;&#125;&#125;; 它的对象模型和MyClassA完全类似，这里就不再赘述了。 为了实现多重继承，我们再定义一个类MyClassC。 123456789class MyClassC:public MyClassA,public MyClassB&#123; int varC;public: virtual void funB() &#123;&#125;virtual void funC() &#123;&#125;&#125;; 为了简化，我们让MyClassC只重写父类MyClassB的虚函数funB，它的对象模型如下： 123456789101112131415161718192021222324252627282930313233class MyClassC size(28): +--- | +--- (base class MyClassA) | | +--- (base class MyClass) 0 | | | &#123;vfptr&#125; 4 | | | var | | +--- 8 | | varA | +--- | +--- (base class MyClassB) | | +--- (base class MyClass)12 | | | &#123;vfptr&#125;16 | | | var | | +---20 | | varB | +---24 | varC +---MyClassC::$vftable@MyClassA@: | &amp;MyClassC_meta | 0 0 | &amp;MyClassA::fun 1 | &amp;MyClassA::funA 2 | &amp;MyClassC::funCMyClassC::$vftable@MyClassB@: | -12 0 | &amp;MyClassB::fun 1 | &amp;MyClassC::funBMyClassC::funB this adjustor: 12MyClassC::funC this adjustor: 0 和单重继承类似，多重继承时MyClassC会把所有的父类全部按序包含在自身内部。而且每一个父类都对应一个单独的虚函数表。MyClassC的对象模型如图3所示。 图3 MyClassC对象模型 多重继承下，子类不再具有自身的虚函数表，它的虚函数表与第一个父类的虚函数表合并了。同样的，如果子类重写了任意父类的虚函数，都会覆盖对应的函数地址记录。如果MyClassC重写了fun函数（两个父类都有该函数），那么两个虚函数表的记录都需要被覆盖！在这里我们发现MyClassC::funB的函数对应的adjust值是12，按照我们前边的规则，可以发现该函数的多态调用形式为： 1*(this+12)[1]() 此处的调整量12正好是MyClassB的vfptr在MyClassC对象内的偏移量。 四、虚拟继承对象模型 虚拟继承是为了解决多重继承下公共基类的多份拷贝问题。比如上边的例子中MyClassC的对象内包含MyClassA和MyClassB子对象，但是MyClassA和MyClassB内含有共同的基类MyClass。为了消除MyClass子对象的多份存在，我们需要让MyClassA和MyClassB都虚拟继承于MyClass，然后再让MyClassC多重继承于这两个父类。相对于上边的例子，类内的设计不做任何改动，先修改MyClassA和MyClassB的继承方式： 123class MyClassA:virtual public MyClassclass MyClassB:virtual public MyClassclass MyClassC:public MyClassA,public MyClassB 由于虚继承的本身语义，MyClassC内必须重写fun函数，因此我们需要再重写fun函数。这种情况下，MyClassC的对象模型如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class MyClassC size(36): +--- | +--- (base class MyClassA) 0 | | &#123;vfptr&#125; 4 | | &#123;vbptr&#125; 8 | | varA | +--- | +--- (base class MyClassB)12 | | &#123;vfptr&#125;16 | | &#123;vbptr&#125;20 | | varB | +---24 | varC +--- +--- (virtual base MyClass)28 | &#123;vfptr&#125;32 | var +---MyClassC::$vftable@MyClassA@: | &amp;MyClassC_meta | 0 0 | &amp;MyClassA::funA 1 | &amp;MyClassC::funCMyClassC::$vftable@MyClassB@: | -12 0 | &amp;MyClassC::funBMyClassC::$vbtable@MyClassA@: 0 | -4 1 | 24 (MyClassCd(MyClassA+4)MyClass)MyClassC::$vbtable@MyClassB@: 0 | -4 1 | 12 (MyClassCd(MyClassB+4)MyClass)MyClassC::$vftable@MyClass@: | -28 0 | &amp;MyClassC::fun MyClassC::fun this adjustor: 28 MyClassC::funB this adjustor: 12 MyClassC::funC this adjustor: 0vbi: class offset o.vbptr o.vbte fVtorDisp MyClass 28 4 4 0 虚继承的引入把对象的模型变得十分复杂，除了每个基类（MyClassA和MyClassB）和公共基类（MyClass）的虚函数表指针需要记录外，每个虚拟继承了MyClass的父类还需要记录一个虚基类表vbtable的指针vbptr。MyClassC的对象模型如图4所示。 图4 MyClassC对象模型 虚基类表每项记录了被继承的虚基类子对象相对于虚基类表指针的偏移量。比如MyClassA的虚基类表第二项记录值为24，正是MyClass::vfptr相对于MyClassA::vbptr的偏移量，同理MyClassB的虚基类表第二项记录值12也正是MyClass::vfptr相对于MyClassA::vbptr的偏移量。 和虚函数表不同的是，虚基类表的第一项记录着当前子对象相对于虚基类表指针的偏移。MyClassA和MyClassB子对象内的虚表指针都是存储在相对于自身的4字节偏移处，因此该值是-4。假定MyClassA和MyClassC或者MyClassB内没有定义新的虚函数，即不会产生虚函数表，那么虚基类表第一项字段的值应该是0。 通过以上的对象组织形式，编译器解决了公共虚基类的多份拷贝的问题。通过每个父类的虚基类表指针，都能找到被公共使用的虚基类的子对象的位置，并依次访问虚基类子对象的数据。至于虚基类定义的虚函数，它和其他的虚函数的访问形式相同，本例中，如果使用虚基类指针MyClass*pc访问MyClassC对象的fun，将会被转化为如下形式： 1*(pc+28)[0]() 通过以上的描述，我们基本认清了C++的对象模型。尤其是在多重、虚拟继承下的复杂结构。通过这些真实的例子，使得我们认清C++内class的本质，以此指导我们更好的书写我们的程序。本文从对象结构的角度结合图例为大家阐述对象的基本模型，和一般描述C++虚拟机制的文章有所不同。作者只希望借助于图表能把C++对象以更好理解的形式为大家展现出来，希望本文对你有所帮助。 编者注： 对于VC++和GCC，内存布局还是有不少细节上的实现区别的，本文仅具参考意义，详情可参考C++ 对象的内存布局和（维基百科）虚继承。 此外，如果Base类没有虚函数，而Derived类有虚函数，目前绝大多数编译器的实现，都是将vfptr放在Base对象之前，参考is pointer to base always &lt;= pointer to derived class?。 本文地址：http://xnerv.wang/virtual-function-and-virtual-table/ 转载自：虚函数与虚继承寻踪","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://xnerv.wang/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"编程语言","slug":"编程语言","permalink":"https://xnerv.wang/tags/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"},{"name":"C++","slug":"C","permalink":"https://xnerv.wang/tags/C/"},{"name":"虚表","slug":"虚表","permalink":"https://xnerv.wang/tags/%E8%99%9A%E8%A1%A8/"},{"name":"多态","slug":"多态","permalink":"https://xnerv.wang/tags/%E5%A4%9A%E6%80%81/"}]},{"title":"Acquire and Release Semantics（转载）","slug":"acquire-and-release-semantics","date":"2012-09-13T07:00:00.000Z","updated":"2023-08-21T02:24:21.012Z","comments":true,"path":"acquire-and-release-semantics/","link":"","permalink":"https://xnerv.wang/acquire-and-release-semantics/","excerpt":"Generally speaking, in lock-free programming, there are two ways in which threads can manipulate shared memory: They can compete with each other for a resource, or they can pass information co-operatively from one thread to another. Acquire and release semantics are crucial for the latter: reliable passing of information between threads. In fact, I would venture to guess that incorrect or missing acquire and release semantics is the #1 type of lock-free programming error.","text":"Generally speaking, in lock-free programming, there are two ways in which threads can manipulate shared memory: They can compete with each other for a resource, or they can pass information co-operatively from one thread to another. Acquire and release semantics are crucial for the latter: reliable passing of information between threads. In fact, I would venture to guess that incorrect or missing acquire and release semantics is the #1 type of lock-free programming error. In this post, I’ll demonstrate various ways to achieve acquire and release semantics in C++. I’ll touch upon the C++11 atomic library standard in an introductory way, so you don’t already need to know it. And to be clear from the start, the information here pertains to lock-free programming without sequential consistency. We’re dealing directly with memory ordering in a multicore or multiprocessor environment. Unfortunately, the terms acquire and release semantics appear to be in even worse shape than the term lock-free, in that the more you scour the web, the more seemingly contradictory definitions you’ll find. Bruce Dawson offers a couple of good definitions (credited to Herb Sutter) about halfway through this white paper. I’d like to offer a couple of definitions of my own, staying close to the principles behind C++11 atomics: Acquire semantics is a property that can only apply to operations that read from shared memory, whether they are read-modify-write operations or plain loads. The operation is then considered a read-acquire. Acquire semantics prevent memory reordering of the read-acquire with any read or write operation that follows it in program order. Release semantics is a property that can only apply to operations that write to shared memory, whether they are read-modify-write operations or plain stores. The operation is then considered a write-release. Release semantics prevent memory reordering of the write-release with any read or write operation that precedes it in program order. Once you digest the above definitions, it’s not hard to see that acquire and release semantics can be achieved using simple combinations of the memory barrier types I described at length in my previous post. The barriers must (somehow) be placed after the read-acquire operation, but before the write-release. [Update: Please note that these barriers are technically more strict than what’s required for acquire and release semantics on a single memory operation, but they do achieve the desired effect.] What’s cool is that neither acquire nor release semantics requires the use of a #StoreLoad barrier, which is often a more expensive memory barrier type. For example, on PowerPC, the lwsync (short for “lightweight sync”) instruction acts as all three #LoadLoad, #LoadStore and #StoreStore barriers at the same time, yet is less expensive than the sync instruction, which includes a #StoreLoad barrier. With Explicit Platform-Specific Fence Instructions One way to obtain the desired memory barriers is by issuing explicit fence instructions. Let’s start with a simple example. Suppose we’re coding for PowerPC, and __lwsync() is a compiler intrinsic function that emits the lwsync instruction. Since lwsync provides so many barrier types, we can use it in the following code to establish either acquire or release semantics as needed. In Thread 1, the store to Ready turns into a write-release, and in Thread 2, the load from Ready becomes a read-acquire. If we let both threads run and find that r1 == 1, that serves as confirmation that the value of A assigned in Thread 1 was passed successfully to Thread 2. As such, we are guaranteed that r2 == 42. In my previous post, I already gave a lengthy analogy for #LoadLoad and #StoreStore to illustrate how this works, so I won’t rehash that explanation here. In formal terms, we say that the store to Ready synchronized-with the load. I’ve written a separate post about synchronizes-with here. For now, suffice to say that for this technique to work in general, the acquire and release semantics must apply to the same variable – in this case, Ready – and both the load and store must be atomic operations. Here, Ready is a simple aligned int, so the operations are already atomic on PowerPC. With Fences in Portable C++11 The above example is compiler- and processor-specific. One approach for supporting multiple platforms is to convert the code to C++11. All C++11 identifiers exist in the std namespace, so to keep the following examples brief, let’s assume the statement using namespace std; was placed somewhere earlier in the code. C++11’s atomic library standard defines a portable function atomic_thread_fence() that takes a single argument to specify the type of fence. There are several possible values for this argument, but the values we’re most interested in here are memory_order_acquire and memory_order_release. We’ll use this function in place of __lwsync(). There’s one more change to make before this example is complete. On PowerPC, we knew that both operations on Ready were atomic, but we can’t make that assumption about every platform. To ensure atomicity on all platforms, we’ll change the type of Ready from int to atomic&lt;int&gt;. I know, it’s kind of a silly change, considering that aligned loads and stores of int are already atomic on every modern CPU that exists today. I’ll write more about this in the post on synchronizes-with, but for now, let’s do it for the warm fuzzy feeling of 100% correctness in theory. No changes to A are necessary. The memory_order_relaxed arguments above mean “ensure these operations are atomic, but don’t impose any ordering constraints/memory barriers that aren’t already there.” Once again, both of the above atomic_thread_fence() calls can be (and hopefully are) implemented as lwsync on PowerPC. Similarly, they could both emit a dmb instruction on ARM, which I believe is at least as effective as PowerPC’s lwsync. On x86/64, both atomic_thread_fence() calls can simply be implemented as compiler barriers, since usually, every load on x86/64 already implies acquire semantics and every store implies release semantics. This is why x86/64 is often said to be strongly ordered. Without Fences in Portable C++11 In C++11, it’s possible to achieve acquire and release semantics on Ready without issuing explicit fence instructions. You just need to specify memory ordering constraints directly on the operations on Ready: Think of it as rolling each fence instruction into the operations on Ready themselves. [Update: Please note that this form is not exactly the same as the version using standalone fences; technically, it’s less strict.] The compiler will emit any instructions necessary to obtain the required barrier effects. In particular, on Itanium, each operation can be easily implemented as a single instruction: ld.acq and st.rel. Just as before, r1 == 1 indicates a synchronizes-with relationship, serving as confirmation that r2 == 42. This is actually the preferred way to express acquire and release semantics in C++11. In fact, the atomic_thread_fence() function used in the previous example was added relatively late in the creation of the standard. Acquire and Release While Locking As you can see, none of the examples in this post took advantage of the #LoadStore barriers provided by acquire and release semantics. Really, only the #LoadLoad and #StoreStore parts were necessary. That’s just because in this post, I chose a simple example to let us focus on API and syntax. One case in which the #LoadStore part becomes essential is when using acquire and release semantics to implement a (mutex) lock. In fact, this is where the names come from: acquiring a lock implies acquire semantics, while releasing a lock implies release semantics! All the memory operations in between are contained inside a nice little barrier sandwich, preventing any undesireable memory reordering across the boundaries. Here, acquire and release semantics ensure that all modifications made while holding the lock will propagate fully to the next thread that obtains the lock. Every implementation of a lock, even one you roll on your own, should provide these guarantees. Again, it’s all about passing information reliably between threads, especially in a multicore or multiprocessor environment. In a followup post, I’ll show a working demonstration of C++11 code, running on real hardware, which can be plainly observed to break if acquire and release semantics are not used. 本文地址：http://xnerv.wang/acquire-and-release-semantics/ 转载自：Acquire and Release Semantics","categories":[{"name":"OS","slug":"OS","permalink":"https://xnerv.wang/categories/OS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"OS","slug":"OS","permalink":"https://xnerv.wang/tags/OS/"},{"name":"Memory Barrier","slug":"Memory-Barrier","permalink":"https://xnerv.wang/tags/Memory-Barrier/"}]},{"title":"无锁队列的实现（转载）","slug":"lock-free-queue-Implementation","date":"2012-09-07T07:00:00.000Z","updated":"2023-08-21T02:24:21.245Z","comments":true,"path":"lock-free-queue-Implementation/","link":"","permalink":"https://xnerv.wang/lock-free-queue-Implementation/","excerpt":"关于无锁队列的实现，网上有很多文章，虽然本文可能和那些文章有所重复，但是我还是想以我自己的方式把这些文章中的重要的知识点串起来和大家讲一讲这个技术。下面开始正文。","text":"关于无锁队列的实现，网上有很多文章，虽然本文可能和那些文章有所重复，但是我还是想以我自己的方式把这些文章中的重要的知识点串起来和大家讲一讲这个技术。下面开始正文。 关于CAS等原子操作 在开始说无锁队列之前，我们需要知道一个很重要的技术就是CAS操作——Compare &amp; Set，或是 Compare &amp; Swap，现在几乎所有的CPU指令都支持CAS的原子操作，X86下对应的是 CMPXCHG 汇编指令。有了这个原子操作，我们就可以用其来实现各种无锁（lock free）的数据结构。 这个操作用C语言来描述就是下面这个样子：（代码来自Wikipedia的Compare And Swap词条）意思就是说，看一看内存*reg里的值是不是oldval，如果是的话，则对其赋值newval。 1234567int compare_and_swap (int* reg, int oldval, int newval)&#123; int old_reg_val = *reg; if (old_reg_val == oldval) *reg = newval; return old_reg_val;&#125; 这个操作可以变种为返回bool值的形式（返回 bool值的好处在于，可以调用者知道有没有更新成功）： 12345678bool compare_and_swap (int *accum, int *dest, int newval)&#123; if ( *accum == *dest ) &#123; *dest = newval; return true; &#125; return false;&#125; 与CAS相似的还有下面的原子操作：（这些东西大家自己看Wikipedia吧） Fetch And Add，一般用来对变量做 +1 的原子操作 Test-and-set，写值到某个内存位置并传回其旧值。汇编指令BST Test and Test-and-set，用来低低Test-and-Set的资源争夺情况 **注：**在实际的C/C++程序中，CAS的各种实现版本如下： GCC的CAS GCC4.1+版本中支持CAS的原子操作（完整的原子操作可参看 GCC Atomic Builtins） 12bool __sync_bool_compare_and_swap (type *ptr, type oldval type newval, ...)type __sync_val_compare_and_swap (type *ptr, type oldval type newval, ...) Windows的CAS 在Windows下，你可以使用下面的Windows API来完成CAS：（完整的Windows原子操作可参看MSDN的 InterLocked Functions） 123InterlockedCompareExchange ( __inout LONG volatile *Target, __in LONG Exchange, __in LONG Comperand); C++11中的CAS C++11中的STL中的atomic类的函数可以让你跨平台。（完整的C++11的原子操作可参看 Atomic Operation Library） 123456template&lt; class T &gt;bool atomic_compare_exchange_weak( std::atomic* obj, T* expected, T desired );template&lt; class T &gt;bool atomic_compare_exchange_weak( volatile std::atomic* obj, T* expected, T desired ); 无锁队列的链表实现 下面的东西主要来自John D. Valois 1994年10月在拉斯维加斯的并行和分布系统系统国际大会上的一篇论文——《Implementing Lock-Free Queues》。 我们先来看一下进队列用CAS实现的方式： 12345678910111213EnQueue(x) //进队列&#123; //准备新加入的结点数据 q = new record(); q-&gt;value = x; q-&gt;next = NULL; do &#123; p = tail; //取链表尾指针的快照 &#125; while( CAS(p-&gt;next, NULL, q) != TRUE); //如果没有把结点链在尾指针上，再试 CAS(tail, p, q); //置尾结点&#125; 我们可以看到，程序中的那个 do- while 的 Re-Try-Loop。就是说，很有可能我在准备在队列尾加入结点时，别的线程已经加成功了，于是tail指针就变了，于是我的CAS返回了false，于是程序再试，直到试成功为止。这个很像我们的抢电话热线的不停重播的情况。 你会看到，为什么我们的“置尾结点”的操作（第12行）不判断是否成功，因为： 如果有一个线程T1，它的while中的CAS如果成功的话，那么其它所有的 随后线程的CAS都会失败，然后就会再循环， 此时，如果T1 线程还没有更新tail指针，其它的线程继续失败，因为tail-&gt;next不是NULL了。 直到T1线程更新完tail指针，于是其它的线程中的某个线程就可以得到新的tail指针，继续往下走了。 这里有一个潜在的问题——如果T1线程在用CAS更新tail指针的之前，线程停掉或是挂掉了，那么其它线程就进入死循环了。下面是改良版的EnQueue() 123456789101112131415EnQueue(x) //进队列改良版&#123; q = new record(); q-&gt;value = x; q-&gt;next = NULL; p = tail; oldp = p do &#123; while (p-&gt;next != NULL) p = p-&gt;next; &#125; while( CAS(p.next, NULL, q) != TRUE); //如果没有把结点链在尾上，再试 CAS(tail, oldp, q); //置尾结点&#125; 我们让每个线程，自己fetch 指针 p 到链表尾。但是这样的fetch会很影响性能。而通实际情况看下来，99.9%的情况不会有线程停转的情况，所以，更好的做法是，你可以接合上述的这两个版本，如果retry的次数超了一个值的话（比如说3次），那么，就自己fetch指针。 好了，我们解决了EnQueue，我们再来看看DeQueue的代码：（很简单，我就不解释了） 12345678910DeQueue() //出队列&#123; do&#123; p = head; if (p-&gt;next == NULL)&#123; return ERR_EMPTY_QUEUE; &#125; while( CAS(head, p, p-&gt;next) != TRUE ); return p-&gt;next-&gt;value;&#125; 我们可以看到，DeQueue的代码操作的是 head-&gt;next，而不是head本身。这样考虑是因为一个边界条件，我们需要一个dummy的头指针来解决链表中如果只有一个元素，head和tail都指向同一个结点的问题，这样EnQueue和DeQueue要互相排斥了。 注：上图的tail正处于更新之前的装态。 CAS的ABA问题 所谓ABA（见维基百科的ABA词条），问题基本是这个样子： 进程P1在共享变量中读到值为A P1被抢占了，进程P2执行 P2把共享变量里的值从A改成了B，再改回到A，此时被P1抢占。 P1回来看到共享变量里的值没有被改变，于是继续执行。 虽然P1以为变量值没有改变，继续执行了，但是这个会引发一些潜在的问题。ABA问题最容易发生在lock free 的算法中的，CAS首当其冲，因为CAS判断的是指针的地址。如果这个地址被重用了呢，问题就很大了。（地址被重用是很经常发生的，一个内存分配后释放了，再分配，很有可能还是原来的地址） 比如上述的DeQueue()函数，因为我们要让head和tail分开，所以我们引入了一个dummy指针给head，当我们做CAS的之前，如果head的那块内存被回收并被重用了，而重用的内存又被EnQueue()进来了，这会有很大的问题。（内存管理中重用内存基本上是一种很常见的行为） 这个例子你可能没有看懂，维基百科上给了一个活生生的例子—— 你拿着一个装满钱的手提箱在飞机场，此时过来了一个火辣性感的美女，然后她很暖昧地挑逗着你，并趁你不注意的时候，把用一个一模一样的手提箱和你那装满钱的箱子调了个包，然后就离开了，你看到你的手提箱还在那，于是就提着手提箱去赶飞机去了。 这就是ABA的问题。 解决ABA的问题 维基百科上给了一个解——使用double-CAS（双保险的CAS），例如，在32位系统上，我们要检查64位的内容 一次用CAS检查双倍长度的值，前半部是指针，后半部分是一个计数器。 只有这两个都一样，才算通过检查，要吧赋新的值。并把计数器累加1。 这样一来，ABA发生时，虽然值一样，但是计数器就不一样（但是在32位的系统上，这个计数器会溢出回来又从1开始的，这还是会有ABA的问题） 当然，我们这个队列的问题就是不想让那个内存重用，这样明确的业务问题比较好解决，论文《Implementing Lock-Free Queues》给出一这么一个方法——使用结点内存引用计数refcnt！ 1234567891011121314151617SafeRead(q)&#123; loop: p = q-&gt;next; if (p == NULL)&#123; return p; &#125; Fetch&amp;Add(p-&gt;refcnt, 1); if (p == q-&gt;next)&#123; return p; &#125;else&#123; Release(p); &#125; goto loop;&#125; 其中的Fetch&amp;Add和Release分是是加引用计数和减引用计数，都是原子操作，这样就可以阻止内存被回收了。 用数组实现无锁队列 本实现来自论文《Implementing Lock-Free Queues》 使用数组来实现队列是很常见的方法，因为没有内存的分部和释放，一切都会变得简单，实现的思路如下： 数组队列应该是一个ring buffer形式的数组（环形数组） 数组的元素应该有三个可能的值：HEAD，TAIL，EMPTY（当然，还有实际的数据） 数组一开始全部初始化成EMPTY，有两个相邻的元素要初始化成HEAD和TAIL，这代表空队列。 EnQueue操作。假设数据x要入队列，定位TAIL的位置，使用double-CAS方法把(TAIL, EMPTY) 更新成 (x, TAIL)。需要注意，如果找不到(TAIL, EMPTY)，则说明队列满了。 DeQueue操作。定位HEAD的位置，把(HEAD, x)更新成(EMPTY, HEAD)，并把x返回。同样需要注意，如果x是TAIL，则说明队列为空。 算法的一个关键是——如何定位HEAD或TAIL？ 我们可以声明两个计数器，一个用来计数EnQueue的次数，一个用来计数DeQueue的次数。 这两个计算器使用使用Fetch&amp;ADD来进行原子累加，在EnQueue或DeQueue完成的时候累加就好了。 累加后求个模什么的就可以知道TAIL和HEAD的位置了。 如下图所示： 小结 以上基本上就是所有的无锁队列的技术细节，这些技术都可以用在其它的无锁数据结构上。 无锁队列主要是通过CAS、FAA这些原子操作，和Retry-Loop实现。 对于Retry-Loop，我个人感觉其实和锁什么什么两样。只是这种“锁”的粒度变小了，主要是“锁”HEAD和TAIL这两个关键资源。而不是整个数据结构。 还有一些和Lock Free的文章你可以去看看： Code Project 上的雄文 《Yet another implementation of a lock-free circular array queue》 Herb Sutter的《Writing Lock-Free Code: A Corrected Queue》– 用C++11的std::atomic模板。 IBM developerWorks的《设计不使用互斥锁的并发数据结构》 【注：我配了一张look-free的自行车，寓意为——如果不用专门的车锁，那么自行得自己锁自己！】 本文地址：http://xnerv.wang/lock-free-queue-Implementation/ 转载自：（ 酷 壳 – CoolShell）无锁队列的实现","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://xnerv.wang/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://xnerv.wang/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"},{"name":"无锁队列","slug":"无锁队列","permalink":"https://xnerv.wang/tags/%E6%97%A0%E9%94%81%E9%98%9F%E5%88%97/"}]},{"title":"(Stack Overflow) Signal handling with multiple threads in Linux","slug":"signal-handling-with-multiple-threads-in-linux","date":"2012-07-27T06:36:00.000Z","updated":"2023-08-21T02:24:19.528Z","comments":true,"path":"signal-handling-with-multiple-threads-in-linux/","link":"","permalink":"https://xnerv.wang/signal-handling-with-multiple-threads-in-linux/","excerpt":"Question In Linux, what happens when a program (that possibly has multiple threads) receives a signal, like SIGTERM or SIGHUP? Which thread intercepts the signal? Can multiple threads get the same signal? Is there a special thread dedicated entirely to handling signals? If not, what happens inside the thread that is to handle the signal? How does the execution resume after the signal handler routine finishes?","text":"Question In Linux, what happens when a program (that possibly has multiple threads) receives a signal, like SIGTERM or SIGHUP? Which thread intercepts the signal? Can multiple threads get the same signal? Is there a special thread dedicated entirely to handling signals? If not, what happens inside the thread that is to handle the signal? How does the execution resume after the signal handler routine finishes? Answer by Alan This is slightly nuanced, based on which version of the Linux kernel you are using. Assuming 2.6 posix threads, and if you are talking about the OS sending SIGTERM or SIGHUP, the signal is sent to process, which is received by and handled by root thread. Using POSIX threads, you can also sent SIGTERM to individual threads as well, but I suspect you are asking about what happens when the OS sends the signal to the process. In 2.6, SIGTERM will cause child threads to exit “cleanly”, where as 2.4, child threads were left in an indeterminate state. Answer by sarnold pthreads(7) describes that POSIX.1 requires all threads in a process share attributes, including: signal dispositions POSIX.1 also requires some attributes to be distinct for each thread, including: signal mask (pthread_sigmask(3)) alternate signal stack (sigaltstack(2)) The Linux kernel’s complete_signal() routine has the following code block – the comments are quite useful: 12345678910111213141516171819202122232425262728293031323334353637383940414243/* * Now find a thread we can wake up to take the signal off the queue. * * If the main thread wants the signal, it gets first crack. * Probably the least surprising to the average bear. */if (wants_signal(sig, p)) t = p;else if (!group || thread_group_empty(p)) /* * There is just one thread and it does not need to be woken. * It will dequeue unblocked signals before it runs again. */ return;else &#123; /* * Otherwise try to find a suitable thread. */ t = signal-&gt;curr_target; while (!wants_signal(sig, t)) &#123; t = next_thread(t); if (t == signal-&gt;curr_target) /* * No thread needs to be woken. * Any eligible threads will see * the signal in the queue soon. */ return; &#125; signal-&gt;curr_target = t;&#125;/* * Found a killable thread. If the signal will be fatal, * then start taking the whole group down immediately. */if (sig_fatal(p, sig) &amp;&amp; !(signal-&gt;flags &amp; (SIGNAL_UNKILLABLE | SIGNAL_GROUP_EXIT)) &amp;&amp; !sigismember(&amp;t-&gt;real_blocked, sig) &amp;&amp; (sig == SIGKILL || !t-&gt;ptrace)) &#123; /* * This signal will be fatal to the whole group. */ So, you see that you are in charge of where signals are delivered: If your process has set a signal’s disposition to SIG_IGN or SIG_DFL, then the signal is ignored (or default – kill, core, or ignore) for all threads. If your process has set a signal’s disposition to a specific handler routine, then you can control which thread will receive the signals by manipulating specific thread signal masks using pthread_sigmask(3). You can nominate one thread to manage them all, or create one thread per signal, or any mixture of these options for specific signals, or you rely on the Linux kernel’s current default behavior of delivering the signal to the main thread. Some signals, however, are special: 1234567891011A signal may be generated (and thus pending) for a process asa whole (e.g., when sent using kill(2)) or for a specificthread (e.g., certain signals, such as SIGSEGV and SIGFPE,generated as a consequence of executing a specific machine-language instruction are thread directed, as are signalstargeted at a specific thread using pthread_kill(3)). Aprocess-directed signal may be delivered to any one of thethreads that does not currently have the signal blocked. Ifmore than one of the threads has the signal unblocked, thenthe kernel chooses an arbitrary thread to which to deliverthe signal. Article link: http://xnerv.wang/signal-handling-with-multiple-threads-in-linux/ Reprinted from: (StackOverflow) Signal handling with multiple threads in Linux","categories":[{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/categories/Linux/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/tags/Linux/"},{"name":"Stack Overflow","slug":"Stack-Overflow","permalink":"https://xnerv.wang/tags/Stack-Overflow/"},{"name":"yield","slug":"yield","permalink":"https://xnerv.wang/tags/yield/"}]},{"title":"Memory Barriers Are Like Source Control Operations（转载）","slug":"memory-barriers-are-like-source-control-perations","date":"2012-07-10T07:00:00.000Z","updated":"2023-08-21T02:24:21.066Z","comments":true,"path":"memory-barriers-are-like-source-control-perations/","link":"","permalink":"https://xnerv.wang/memory-barriers-are-like-source-control-perations/","excerpt":"If you use source control, you’re on your way towards understanding memory ordering, an important consideration when writing lock-free code in C, C++ and other languages. In my last post, I wrote about memory ordering at compile time, which forms one half of the memory ordering puzzle. This post is about the other half: memory ordering at runtime, on the processor itself. Like compiler reordering, processor reordering is invisible to a single-threaded program. It only becomes apparent when lock-free techniques are used – that is, when shared memory is manipulated without any mutual exclusion between threads. However, unlike compiler reordering, the effects of processor reordering are only visible in multicore and multiprocessor systems.","text":"If you use source control, you’re on your way towards understanding memory ordering, an important consideration when writing lock-free code in C, C++ and other languages. In my last post, I wrote about memory ordering at compile time, which forms one half of the memory ordering puzzle. This post is about the other half: memory ordering at runtime, on the processor itself. Like compiler reordering, processor reordering is invisible to a single-threaded program. It only becomes apparent when lock-free techniques are used – that is, when shared memory is manipulated without any mutual exclusion between threads. However, unlike compiler reordering, the effects of processor reordering are only visible in multicore and multiprocessor systems. You can enforce correct memory ordering on the processor by issuing any instruction which acts as a memory barrier. In some ways, this is the only technique you need to know, because when you use such instructions, compiler ordering is taken care of automatically. Examples of instructions which act as memory barriers include (but are not limited to) the following: Certain inline assembly directives in GCC, such as the PowerPC-specific asm volatile(&quot;lwsync&quot; ::: &quot;memory&quot;) Any Win32 Interlocked operation, except on Xbox 360 Many operations on C++11 atomic types, such as load(std::memory_order_acquire) Operations on POSIX mutexes, such as pthread_mutex_lock Just as there are many instructions which act as memory barriers, there are many different types of memory barriers to know about. Indeed, not all of the above instructions produce the same kind of memory barrier – leading to another possible area of confusion when writing lock-free code. In an attempt to clear things up to some extent, I’d like to offer an analogy which I’ve found helpful in understanding the vast majority (but not all) of possible memory barrier types. To begin with, consider the architecture of a typical multicore system. Here’s a device with two cores, each having 32 KiB of private L1 data cache. There’s 1 MiB of L2 cache shared between both cores, and 512 MiB of main memory. A multicore system is a bit like a group of programmers collaborating on a project using a bizarre kind of source control strategy. For example, the above dual-core system corresponds to a scenario with just two programmers. Let’s name them Larry and Sergey. On the right, we have a shared, central repository – this represents a combination of main memory and the shared L2 cache. Larry has a complete working copy of the repository on his local machine, and so does Sergey – these (effectively) represent the L1 caches attached to each CPU core. There’s also a scratch area on each machine, to privately keep track of registers and/or local variables. Our two programmers sit there, feverishly editing their working copy and scratch area, all while making decisions about what to do next based on the data they see – much like a thread of execution running on that core. Which brings us to the source control strategy. In this analogy, the source control strategy is very strange indeed. As Larry and Sergey modify their working copies of the repository, their modifications are constantly leaking in the background, to and from the central repository, at totally random times. Once Larry edits the file X, his change will leak to the central repository, but there’s no guarantee about when it will happen. It might happen immediately, or it might happen much, much later. He might go on to edit other files, say Y and Z, and those modifications might leak into the respository before X gets leaked. In this manner, stores are effectively reordered on their way to the repository. Similarly, on Sergey’s machine, there’s no guarantee about the timing or the order in which those changes leak back from the repository into his working copy. In this manner, loads are effectively reordered on their way out of the repository. Now, if each programmer works on completely separate parts of the repository, neither programmer will be aware of these background leaks going on, or even of the other programmer’s existence. That would be analogous to running two independent, single-threaded processes. In this case, the cardinal rule of memory ordering is upheld. The analogy becomes more useful once our programmers start working on the same parts of the repository. Let’s revisit the example I gave in an earlier post. X and Y are global variables, both initially 0: Think of X and Y as files which exist on Larry’s working copy of the repository, Sergey’s working copy, and the central repository itself. Larry writes 1 to his working copy of X and Sergey writes 1 to his working copy of Y at roughly the same time. If neither modification has time to leak to the repository and back before each programmer looks up his working copy of the other file, they’ll end up with both r1 = 0 and r2 = 0. This result, which may have seemed counterintuitive at first, actually becomes pretty obvious in the source control analogy. Types of Memory Barrier Fortunately, Larry and Sergey are not entirely at the mercy of these random, unpredictable leaks happening in the background. They also have the ability to issue special instructions, called fence instructions, which act as memory barriers. For this analogy, it’s sufficient to define four types of memory barrier, and thus four different fence instructions. Each type of memory barrier is named after the type of memory reordering it’s designed to prevent: for example, #StoreLoad is designed to prevent the reordering of a store followed by a load. As Doug Lea points out, these four categories map pretty well to specific instructions on real CPUs – though not exactly. Most of the time, a real CPU instruction acts as some combination of the above barrier types, possibly in addition to other effects. In any case, once you understand these four types of memory barriers in the source control analogy, you’re in a good position to understand a large number of instructions on real CPUs, as well as several higher-level programming language constructs. #LoadLoad A LoadLoad barrier effectively prevents reordering of loads performed before the barrier with loads performed after the barrier. In our analogy, the #LoadLoad fence instruction is basically equivalent to a pull from the central repository. Think git pull, hg pull, p4 sync, svn update or cvs update, all acting on the entire repository. If there are any merge conflicts with his local changes, let’s just say they’re resolved randomly. Mind you, there’s no guarantee that #LoadLoad will pull the latest, or head, revision of the entire repository! It could very well pull an older revision than the head, as long as that revision is at least as new as the newest value which leaked from the central repository into his local machine. This may sound like a weak guarantee, but it’s still a perfectly good way to prevent seeing stale data. Consider the classic example, where Sergey checks a shared flag to see if some data has been published by Larry. If the flag is true, he issues a #LoadLoad barrier before reading the published value: 12345if (IsPublished) // Load and check shared flag&#123; LOADLOAD_FENCE(); // Prevent reordering of loads return Value; // Load published value&#125; Obviously, this example depends on having the IsPublished flag leak into Sergey’s working copy by itself. It doesn’t matter exactly when that happens; once the leaked flag has been observed, he issues a #LoadLoad fence to prevent reading some value of Value which is older than the flag itself. #StoreStore A StoreStore barrier effectively prevents reordering of stores performed before the barrier with stores performed after the barrier. In our analogy, the #StoreStore fence instruction corresponds to a push to the central repository. Think git push, hg push, p4 submit, svn commit or cvs commit, all acting on the entire repository. As an added twist, let’s suppose that #StoreStore instructions are not instant. They’re performed in a delayed, asynchronous manner. So, even though Larry executes a #StoreStore, we can’t make any assumptions about when all his previous stores finally become visible in the central repository. This, too, may sound like a weak guarantee, but again, it’s perfectly sufficient to prevent Sergey from seeing any stale data published by Larry. Returning to the same example as above, Larry needs only to publish some data to shared memory, issue a #StoreStore barrier, then set the shared flag to true: 123Value = x; // Publish some dataSTORESTORE_FENCE();IsPublished = 1; // Set shared flag to indicate availability of data Again, we’re counting on the value of IsPublished to leak from Larry’s working copy over to Sergey’s, all by itself. Once Sergey detects that, he can be confident he’ll see the correct value of Value. What’s interesting is that, for this pattern to work, Value does not even need to be an atomic type; it could just as well be a huge structure with lots of elements. #LoadStore Unlike #LoadLoad and #StoreStore, there’s no clever metaphor for #LoadStore in terms of source control operations. The best way to understand a #LoadStore barrier is, quite simply, in terms of instruction reordering. Imagine Larry has a set of instructions to follow. Some instructions make him load data from his private working copy into a register, and some make him store data from a register back into the working copy. Larry has the ability to juggle instructions, but only in specific cases. Whenever he encounters a load, he looks ahead at any stores that are coming up after that; if the stores are completely unrelated to the current load, then he’s allowed to skip ahead, do the stores first, then come back afterwards to finish up the load. In such cases, the cardinal rule of memory ordering – never modify the behavior of a single-threaded program – is still followed. On a real CPU, such instruction reordering might happen on certain processors if, say, there is a cache miss on the load followed by a cache hit on the store. But in terms of understanding the analogy, such hardware details don’t really matter. Let’s just say Larry has a boring job, and this is one of the few times when he’s allowed to get creative. Whether or not he chooses to do it is completely unpredictable. Fortunately, this is a relatively inexpensive type of reordering to prevent; when Larry encounters a #LoadStore barrier, he simply refrains from such reordering around that barrier. In our analogy, it’s valid for Larry to perform this kind of LoadStore reordering even when there is a #LoadLoad or #StoreStore barrier between the load and the store. However, on a real CPU, instructions which act as a #LoadStore barrier typically act as at least one of those other two barrier types. #StoreLoad A StoreLoad barrier ensures that all stores performed before the barrier are visible to other processors, and that all loads performed after the barrier receive the latest value that is visible at the time of the barrier. In other words, it effectively prevents reordering of all stores before the barrier against all loads after the barrier, respecting the way a sequentially consistent multiprocessor would perform those operations. #StoreLoad is unique. It’s the only type of memory barrier that will prevent the result r1 = r2 = 0 in the example given in Memory Reordering Caught in the Act; the same example I’ve repeated earlier in this post. If you’ve been following closely, you might wonder: How is #StoreLoad different from a #StoreStore followed by a #LoadLoad? After all, a #StoreStore pushes changes to the central repository, while #LoadLoad pulls remote changes back. However, those two barrier types are insufficient. Remember, the push operation may be delayed for an arbitrary number of instructions, and the pull operation might not pull from the head revision. This hints at why the PowerPC’s lwsync instruction – which acts as all three #LoadLoad, #LoadStore and #StoreStore memory barriers, but not #StoreLoad – is insufficient to prevent r1 = r2 = 0 in that example. In terms of the analogy, a #StoreLoad barrier could be achieved by pushing all local changes to the central repostitory, waiting for that operation to complete, then pulling the absolute latest head revision of the repository. On most processors, instructions that act as a #StoreLoad barrier tend to be more expensive than instructions acting as the other barrier types. If we throw a #LoadStore barrier into that operation, which shouldn’t be a big deal, then what we get is a full memory fence – acting as all four barrier types at once. As Doug Lea also points out, it just so happens that on all current processors, every instruction which acts as a #StoreLoad barrier also acts as a full memory fence. How Far Does This Analogy Get You? As I’ve mentioned previously, every processor has different habits when it comes to memory ordering. The x86/64 family, in particular, has a strong memory model; it’s known to keep memory reordering to a minimum. PowerPC and ARM have weaker memory models, and the Alpha is famous for being in a league of its own. Fortunately, the analogy presented in this post corresponds to a weak memory model. If you can wrap your head around it, and enforce correct memory ordering using the fence instructions given here, you should be able to handle most CPUs. The analogy also corresponds pretty well to the abstract machine targeted by both C++11 (formerly known as C++0x) and C11. Therefore, if you write lock-free code using the standard library of those languages while keeping the above analogy in mind, it’s more likely to function correctly on any platform. In this analogy, I’ve said that each programmer represents a single thread of execution running on a separate core. On a real operating system, threads tend to move between different cores over the course of their lifetime, but the analogy still works. I’ve also alternated between examples in machine language and examples written in C/C++. Obviously, we’d prefer to stick with C/C++, or another high-level language; this is possible because again, any operation which acts as a memory barrier also prevents compiler reordering. I haven’t written about every type of memory barrier yet. For instance, there are also data dependency barriers. I’ll describe those further in a future post. Still, the four types given here are the big ones. If you’re interested in how CPUs work under the hood – things like stores buffers, cache coherency protocols and other hardware implementation details – and why they perform memory reordering in the first place, I’d recommend the fine work of Paul McKenney &amp; David Howells. Indeed, I suspect most programmers who have successfully written lock-free code have at least a passing familiarity with such hardware details. 本文地址：http://xnerv.wang/memory-barriers-are-like-source-control-perations/ 转载自：Memory Barriers Are Like Source Control Operations","categories":[{"name":"OS","slug":"OS","permalink":"https://xnerv.wang/categories/OS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"OS","slug":"OS","permalink":"https://xnerv.wang/tags/OS/"},{"name":"Memory Barrier","slug":"Memory-Barrier","permalink":"https://xnerv.wang/tags/Memory-Barrier/"}]},{"title":"RSYNC的核心算法（转载）","slug":"the-core-algorithm-of-rsync","date":"2012-05-17T07:00:00.000Z","updated":"2023-08-21T02:24:19.805Z","comments":true,"path":"the-core-algorithm-of-rsync/","link":"","permalink":"https://xnerv.wang/the-core-algorithm-of-rsync/","excerpt":"rsync是unix/linux下同步文件的一个高效算法，它能同步更新两处计算机的文件与目录，并适当利用查找文件中的不同块以减少数据传输。rsync中一项与其他大部分类似程序或协定中所未见的重要特性是镜像是只对有变更的部分进行传送。rsync可拷贝／显示目录属性，以及拷贝文件，并可选择性的压缩以及递归拷贝。rsync利用由Andrew Tridgell发明的算法。这里不介绍其使用方法，只介绍其核心算法。我们可以看到，Unix下的东西，一个命令，一个工具都有很多很精妙的东西，怎么学也学不完，这就是Unix的文化啊。 本来不想写这篇文章的，因为原先发现有很多中文blog都说了这个算法，但是看了一下，发现这些中文blog要么翻译国外文章翻译地非常烂，要么就是介绍这个算法介绍得很乱让人看不懂，还有错误，误人不浅，所以让我觉得有必要写篇rsync算法介绍的文章。（当然，我成文比较仓促，可能会有一些错误，请指正）","text":"rsync是unix/linux下同步文件的一个高效算法，它能同步更新两处计算机的文件与目录，并适当利用查找文件中的不同块以减少数据传输。rsync中一项与其他大部分类似程序或协定中所未见的重要特性是镜像是只对有变更的部分进行传送。rsync可拷贝／显示目录属性，以及拷贝文件，并可选择性的压缩以及递归拷贝。rsync利用由Andrew Tridgell发明的算法。这里不介绍其使用方法，只介绍其核心算法。我们可以看到，Unix下的东西，一个命令，一个工具都有很多很精妙的东西，怎么学也学不完，这就是Unix的文化啊。 本来不想写这篇文章的，因为原先发现有很多中文blog都说了这个算法，但是看了一下，发现这些中文blog要么翻译国外文章翻译地非常烂，要么就是介绍这个算法介绍得很乱让人看不懂，还有错误，误人不浅，所以让我觉得有必要写篇rsync算法介绍的文章。（当然，我成文比较仓促，可能会有一些错误，请指正） 问题 首先， 我们先来想一下rsync要解决的问题，如果我们要同步的文件只想传不同的部分，我们就需要对两边的文件做diff，但是这两个问题在两台不同的机器上，无法做diff。如果我们做diff，就要把一个文件传到另一台机器上做diff，但这样一来，我们就传了整个文件，这与我们只想传输不同部的初衷相背。 于是我们就要想一个办法，让这两边的文件见不到面，但还能知道它们间有什么不同。这就出现了rsync的算法。 算法 rsync的算法如下：（假设我们同步源文件名为fileSrc，同步目的文件叫fileDst） 1）分块Checksum算法。首先，我们会把fileDst的文件平均切分成若干个小块，比如每块512个字节（最后一块会小于这个数），然后对每块计算两个checksum， 一个叫rolling checksum，是弱checksum，32位的checksum，其使用的是Mark Adler发明的adler-32算法， 另一个是强checksum，128位的，以前用md4，现在用md5 hash算法。 为什么要这样？因为若干年前的硬件上跑md4的算法太慢了，所以，我们需要一个快算法来鉴别文件块的不同，但是弱的adler32算法碰撞概率太高了，所以我们还要引入强的checksum算法以保证两文件块是相同的。也就是说，弱的checksum是用来区别不同，而强的是用来确认相同。（checksum的具体公式可以参看这篇文章） 2）**传输算法。**同步目标端会把fileDst的一个checksum列表传给同步源，这个列表里包括了三个东西，rolling checksum(32bits)，md5 checksume(128bits)，文件块编号。 我估计你猜到了同步源机器拿到了这个列表后，会对fileSrc做同样的checksum，然后和fileDst的checksum做对比，这样就知道哪些文件块改变了。 但是，聪明的你一定会有以下两个疑问： 如果我fileSrc这边在文件中间加了一个字符，这样后面的文件块都会位移一个字符，这样就完全和fileDst这边的不一样了，但理论上来说，我应该只需要传一个字符就好了。这个怎么解决？ 如果这个checksum列表特别长，而我的两边的相同的文件块可能并不是一样的顺序，那就需要查找，线性的查找起来应该特别慢吧。这个怎么解决？ 很好，让我们来看一下同步源端的算法。 3）checksum查找算法。同步源端拿到fileDst的checksum数组后，会把这个数据存到一个hash table中，用rolling checksum做hash，以便获得O(1)时间复杂度的查找性能。这个hash table是16bits的，所以，hash table的尺寸是2的16次方，对rolling checksum的hash会被散列到0 到 2^16 – 1中的某个整数值。（对于hash table，如果你不清楚，建议回去看大学时的数据结构教科书） 顺便说一下，我在网上看到很多文章说，“要对rolling checksum做排序”（比如这篇和这篇），这两篇文章都引用并翻译了原作者的这篇文章，但是他们都理解错了，不是排序，就只是把fileDst的checksum数据，按rolling checksum做存到2^16的hash table中，当然会发生碰撞，把碰撞的做成一个链表就好了。这就是原文中所说的第二步——搜索有碰撞的情况。 4）比对算法。这是最关键的算法，细节如下： 4.1）取fileSrc的第一个文件块（我们假设的是512个长度），也就是从fileSrc的第1个字节到第512个字节，取出来后做rolling checksum计算。计算好的值到hash表中查。 4.2）如果查到了，说明发现在fileDst中有潜在相同的文件块，于是就再比较md5的checksum，因为rolling checksume太弱了，可能发生碰撞。于是还要算md5的128bits的checksum，这样一来，我们就有 2^-(32+128) = 2^-160的概率发生碰撞，这太小了可以忽略。如果rolling checksum和md5 checksum都相同，这说明在fileDst中有相同的块，我们需要记下这一块在fileDst下的文件编号。 4.3）如果fileSrc的rolling checksum 没有在hash table中找到，那就不用算md5 checksum了。表示这一块中有不同的信息。总之，只要rolling checksum 或 md5 checksum 其中有一个在fileDst的checksum hash表中找不到匹配项，那么就会触发算法对fileSrc的rolling动作。于是，算法会住后step 1个字节，取fileSrc中字节2-513的文件块要做checksum，go to (4.1) – 现在你明白什么叫rolling checksum了吧。 4.4）这样，我们就可以找出fileSrc相邻两次匹配中的那些文本字符，这些就是我们要往同步目标端传的文件内容了。 图示 怎么，你没看懂？ 好吧，我送佛送上西，画个示意图给你看看（对图中的东西我就不再解释了）。 这样，最终，在同步源这端，我们的rsync算法可能会得到下面这个样子的一个数据数组，图中，红色块表示在目标端已匹配上，不用传输（注：我专门在其中显示了两块chunk #5，相信你会懂的），而白色的地方就是需要传输的内容（注意：这些白色的块是不定长的），这样，同步源这端把这个数组（白色的就是实际内容，红色的就放一个标号）压缩传到目的端，在目的端的rsync会根据这个表重新生成文件，这样，同步完成。 最后想说一下，对于某些压缩文件使用rsync传输可能会传得更多，因为被压缩后的文件可能会非常的不同。对此，对于gzip和bzip2这样的命令，记得开启 “rsyncalbe” 模式。 本文地址：http://xnerv.wang/the-core-algorithm-of-rsync/ 转载自：RSYNC 的核心算法","categories":[{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/categories/Linux/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/tags/Linux/"},{"name":"rsync","slug":"rsync","permalink":"https://xnerv.wang/tags/rsync/"}]},{"title":"谷歌技术\"三宝\"之BigTable（转载）","slug":"google-3-gifts-bigtable","date":"2012-05-05T05:09:29.000Z","updated":"2023-08-21T02:24:20.709Z","comments":true,"path":"google-3-gifts-bigtable/","link":"","permalink":"https://xnerv.wang/google-3-gifts-bigtable/","excerpt":"2006年的OSDI有两篇google的论文，分别是BigTable和Chubby。Chubby是一个分布式锁服务，基于Paxos算法；BigTable是一个用于管理结构化数据的分布式存储系统，构建在GFS、Chubby、SSTable等google技术之上。相当多的google应用使用了BigTable，比如Google Earth和Google Analytics，因此它和GFS、MapReduce并称为谷歌技术&quot;三宝&quot;。 与GFS和MapReduce的论文相比，我觉得BigTable的论文难懂一些。一方面是因为自己对数据库不太了解，另一方面又是因为对数据库的理解局限于关系型数据库。尝试用关系型数据模型去理解BigTable就容易&quot;走火入魔&quot;。在这里推荐一篇文章（需要翻墙）：Understanding HBase and BigTable，相信这篇文章对理解BigTable/HBase的数据模型有很大帮助。","text":"2006年的OSDI有两篇google的论文，分别是BigTable和Chubby。Chubby是一个分布式锁服务，基于Paxos算法；BigTable是一个用于管理结构化数据的分布式存储系统，构建在GFS、Chubby、SSTable等google技术之上。相当多的google应用使用了BigTable，比如Google Earth和Google Analytics，因此它和GFS、MapReduce并称为谷歌技术&quot;三宝&quot;。 与GFS和MapReduce的论文相比，我觉得BigTable的论文难懂一些。一方面是因为自己对数据库不太了解，另一方面又是因为对数据库的理解局限于关系型数据库。尝试用关系型数据模型去理解BigTable就容易&quot;走火入魔&quot;。在这里推荐一篇文章（需要翻墙）：Understanding HBase and BigTable，相信这篇文章对理解BigTable/HBase的数据模型有很大帮助。 1 什么是BigTable Bigtable是一个为管理大规模结构化数据而设计的分布式存储系统，可以扩展到PB级数据和上千台服务器。很多google的项目使用Bigtable存储数据，这些应用对Bigtable提出了不同的挑战，比如数据规模的要求、延迟的要求。Bigtable能满足这些多变的要求，为这些产品成功地提供了灵活、高性能的存储解决方案。 Bigtable看起来像一个数据库，采用了很多数据库的实现策略。但是Bigtable并不支持完整的关系型数据模型；而是为客户端提供了一种简单的数据模型，客户端可以动态地控制数据的布局和格式，并且利用底层数据存储的局部性特征。Bigtable将数据统统看成无意义的字节串，客户端需要将结构化和非结构化数据串行化再存入Bigtable。 下文对BigTable的数据模型和基本工作原理进行介绍，而各种优化技术（如压缩、Bloom Filter等）不在讨论范围。 2 BigTable的数据模型 Bigtable不是关系型数据库，但是却沿用了很多关系型数据库的术语，像table（表）、row（行）、column（列）等。这容易让读者误入歧途，将其与关系型数据库的概念对应起来，从而难以理解论文。Understanding HBase and BigTable是篇很优秀的文章，可以帮助读者从关系型数据模型的思维定势中走出来。 本质上说，Bigtable是一个键值（key-value）映射。按作者的说法，Bigtable是一个稀疏的，分布式的，持久化的，多维的排序映射。 先来看看多维、排序、映射。Bigtable的键有三维，分别是行键（row key）、列键（column key）和时间戳（timestamp），行键和列键都是字节串，时间戳是64位整型；而值是一个字节串。可以用 **(row:string, column:string, time:int64)→string **来表示一条键值对记录。 行键可以是任意字节串，通常有10-100字节。行的读写都是原子性的。Bigtable按照行键的字典序存储数据。Bigtable的表会根据行键自动划分为片（tablet），片是负载均衡的单元。最初表都只有一个片，但随着表不断增大，片会自动分裂，片的大小控制在100-200MB。行是表的第一级索引，我们可以把该行的列、时间和值看成一个整体，简化为一维键值映射，类似于： 1234567table&#123; &quot;1&quot; : &#123;sth.&#125;,//一行 &quot;aaaaa&quot; : &#123;sth.&#125;, &quot;aaaab&quot; : &#123;sth.&#125;, &quot;xyz&quot; : &#123;sth.&#125;, &quot;zzzzz&quot; : &#123;sth.&#125;&#125; 列是第二级索引，每行拥有的列是不受限制的，可以随时增加减少。为了方便管理，列被分为多个列族（column family，是访问控制的单元），一个列族里的列一般存储相同类型的数据。一行的列族很少变化，但是列族里的列可以随意添加删除。列键按照family:qualifier格式命名的。这次我们将列拿出来，将时间和值看成一个整体，简化为二维键值映射，类似于： 12345678910111213141516171819202122232425262728293031323334353637table&#123; // ... &quot;aaaaa&quot; : &#123; //一行 &quot;A:foo&quot; : &#123;sth.&#125;,//一列 &quot;A:bar&quot; : &#123;sth.&#125;,//一列 &quot;B:&quot; : &#123;sth.&#125; //一列，列族名为B，但是列名是空字串 &#125;, &quot;aaaab&quot; : &#123; //一行 &quot;A:foo&quot; : &#123;sth.&#125;, &quot;B:&quot; : &#123;sth.&#125; &#125;, // ...&#125;或者可以将列族当作一层新的索引，类似于：```javascripttable&#123; // ... &quot;aaaaa&quot; : &#123; //一行 &quot;A&quot; : &#123; //列族A &quot;foo&quot; : &#123;sth.&#125;, //一列 &quot;bar&quot; : &#123;sth.&#125; &#125;, &quot;B&quot; : &#123; //列族B &quot;&quot; : &#123;sth.&#125; &#125; &#125;, &quot;aaaab&quot; : &#123; //一行 &quot;A&quot; : &#123; &quot;foo&quot; : &#123;sth.&#125;, &#125;, &quot;B&quot; : &#123; &quot;&quot; : &quot;ocean&quot; &#125; &#125;, // ...&#125; 时间戳是第三级索引。Bigtable允许保存数据的多个版本，版本区分的依据就是时间戳。时间戳可以由Bigtable赋值，代表数据进入Bigtable的准确时间，也可以由客户端赋值。数据的不同版本按照时间戳降序存储，因此先读到的是最新版本的数据。我们加入时间戳后，就得到了Bigtable的完整数据模型，类似于： 123456789101112131415161718table&#123; // ... &quot;aaaaa&quot; : &#123; //一行 &quot;A:foo&quot; : &#123; //一列 15 : &quot;y&quot;, //一个版本 4 : &quot;m&quot; &#125;, &quot;A:bar&quot; : &#123; //一列 15 : &quot;d&quot;, &#125;, &quot;B:&quot; : &#123; //一列 6 : &quot;w&quot; 3 : &quot;o&quot; 1 : &quot;w&quot; &#125; &#125;, // ...&#125; 查询时，如果只给出行列，那么返回的是最新版本的数据；如果给出了行列时间戳，那么返回的是时间小于或等于时间戳的数据。比如，我们查询&quot;aaaaa&quot;/“A:foo”，返回的值是&quot;y&quot;；查询&quot;aaaaa&quot;/“A:foo”/10，返回的结果就是&quot;m&quot;；查询&quot;aaaaa&quot;/“A:foo”/2，返回的结果是空。 图1是Bigtable论文里给出的例子，Webtable表存储了大量的网页和相关信息。在Webtable，每一行存储一个网页，其反转的url作为行键，比如maps.google.com/index.html的数据存储在键为com.google.maps/index.html的行里，反转的原因是为了让同一个域名下的子域名网页能聚集在一起。图1中的列族&quot;anchor&quot;保存了该网页的引用站点（比如引用了CNN主页的站点），qualifier是引用站点的名称，而数据是链接文本；列族&quot;contents&quot;保存的是网页的内容，这个列族只有一个空列&quot;contents:&quot;。图1中&quot;contents:&quot;列下保存了网页的三个版本，我们可以用(“com.cnn.www”, “contents:”, t5)来找到CNN主页在t5时刻的内容。 再来看看作者说的其它特征：稀疏，分布式，持久化。持久化的意思很简单，Bigtable的数据最终会以文件的形式放到GFS去。Bigtable建立在GFS之上本身就意味着分布式，当然分布式的意义还不仅限于此。稀疏的意思是，一个表里不同的行，列可能完完全全不一样。 3 支撑技术 Bigtable依赖于google的几项技术。用GFS来存储日志和数据文件；按SSTable文件格式存储数据；用Chubby管理元数据。 GFS参见谷歌技术&quot;三宝&quot;之谷歌文件系统。BigTable的数据和日志都是写入GFS的。 SSTable的全称是Sorted Strings Table，是一种不可修改的有序的键值映射，提供了查询、遍历等功能。每个SSTable由一系列的块（block）组成，Bigtable将块默认设为64KB。在SSTable的尾部存储着块索引，在访问SSTable时，整个索引会被读入内存。BigTable论文没有提到SSTable的具体结构，LevelDb日知录之四： SSTable文件这篇文章对LevelDb的SSTable格式进行了介绍，因为LevelDB的作者JeffreyDean正是BigTable的设计师，所以极具参考价值。每一个片（tablet）在GFS里都是按照SSTable的格式存储的，每个片可能对应多个SSTable。 Chubby是一种高可用的分布式锁服务，Chubby有五个活跃副本，同时只有一个主副本提供服务，副本之间用Paxos算法维持一致性，Chubby提供了一个命名空间（包括一些目录和文件），每个目录和文件就是一个锁，Chubby的客户端必须和Chubby保持会话，客户端的会话若过期则会丢失所有的锁。关于Chubby的详细信息可以看google的另一篇论文：The Chubby lock service for loosely-coupled distributed systems。Chubby用于片定位，片服务器的状态监控，访问控制列表存储等任务。 4 Bigtable集群 Bigtable集群包括三个主要部分：一个供客户端使用的库，一个主服务器（master server），许多片服务器（tablet server）。 正如数据模型小节所说，Bigtable会将表（table）进行分片，片（tablet）的大小维持在100-200MB范围，一旦超出范围就将分裂成更小的片，或者合并成更大的片。每个片服务器负责一定量的片，处理对其片的读写请求，以及片的分裂或合并。片服务器可以根据负载随时添加和删除。这里片服务器并不真实存储数据，而相当于一个连接Bigtable和GFS的代理，客户端的一些数据操作都通过片服务器代理间接访问GFS。 主服务器负责将片分配给片服务器，监控片服务器的添加和删除，平衡片服务器的负载，处理表和列族的创建等。注意，主服务器不存储任何片，不提供任何数据服务，也不提供片的定位信息。 客户端需要读写数据时，直接与片服务器联系。因为客户端并不需要从主服务器获取片的位置信息，所以大多数客户端从来不需要访问主服务器，主服务器的负载一般很轻。 5 片的定位 前面提到主服务器不提供片的位置信息，那么客户端是如何访问片的呢？来看看论文给的示意图，Bigtable使用一个类似B+树的数据结构存储片的位置信息。 首先是第一层，Chubby file。这一层是一个Chubby文件，它保存着root tablet的位置。这个Chubby文件属于Chubby服务的一部分，一旦Chubby不可用，就意味着丢失了root tablet的位置，整个Bigtable也就不可用了。 第二层是root tablet。root tablet其实是元数据表（METADATA table）的第一个分片，它保存着元数据表其它片的位置。root tablet很特别，为了保证树的深度不变，root tablet从不分裂。 第三层是其它的元数据片，它们和root tablet一起组成完整的元数据表。每个元数据片都包含了许多用户片的位置信息。 可以看出整个定位系统其实只是两部分，一个Chubby文件，一个元数据表。注意元数据表虽然特殊，但也仍然服从前文的数据模型，每个分片也都是由专门的片服务器负责，这就是不需要主服务器提供位置信息的原因。客户端会缓存片的位置信息，如果在缓存里找不到一个片的位置信息，就需要查找这个三层结构了，包括访问一次Chubby服务，访问两次片服务器。 6 片的存储和访问 片的数据最终还是写到GFS里的，片在GFS里的物理形态就是若干个SSTable文件。图5展示了读写操作基本情况。 当片服务器收到一个写请求，片服务器首先检查请求是否合法。如果合法，先将写请求提交到日志去，然后将数据写入内存中的memtable。memtable相当于SSTable的缓存，当memtable成长到一定规模会被冻结，Bigtable随之创建一个新的memtable，并且将冻结的memtable转换为SSTable格式写入GFS，这个操作称为minor compaction。 当片服务器收到一个读请求，同样要检查请求是否合法。如果合法，这个读操作会查看所有SSTable文件和memtable的合并视图，因为SSTable和memtable本身都是已排序的，所以合并相当快。 每一次minor compaction都会产生一个新的SSTable文件，SSTable文件太多读操作的效率就降低了，所以Bigtable定期执行merging compaction操作，将几个SSTable和memtable合并为一个新的SSTable。BigTable还有个更厉害的叫major compaction，它将所有SSTable合并为一个新的SSTable。 遗憾的是，BigTable作者没有介绍memtable和SSTable的详细数据结构。 7 BigTable和GFS的关系 集群包括主服务器和片服务器，主服务器负责将片分配给片服务器，而具体的数据服务则全权由片服务器负责。但是不要误以为片服务器真的存储了数据（除了内存中memtable的数据），数据的真实位置只有GFS才知道，主服务器将片分配给片服务器的意思应该是，片服务器获取了片的所有SSTable文件名，片服务器通过一些索引机制可以知道所需要的数据在哪个SSTable文件，然后从GFS中读取SSTable文件的数据，这个SSTable文件可能分布在好几台chunkserver上。 8 元数据表的结构 元数据表（METADATA table）是一张特殊的表，它被用于数据的定位以及一些元数据服务，不可谓不重要。但是Bigtable论文里只给出了少量线索，而对表的具体结构没有说明。这里我试图根据论文的一些线索，猜测一下表的结构。首先列出论文中的线索： The METADATA table stores the location of a tablet under a row key that is an encoding of the tablet's table identifier and its end row. Each METADATA row stores approximately 1KB of data in memory（因为访问量比较大，元数据表是放在内存里的，这个优化在论文的locality groups中提到）.This feature（将locality group放到内存中的特性） is useful for small pieces of data that are accessed frequently: we use it internally for the location column family in the METADATA table. We also store secondary information in the METADATA table, including a log of all events pertaining to each tablet(such as when a server begins serving it). 第一条线索，元数据表的行键是由片所属表名的id和片最后一行编码而成，所以每个片在元数据表中占据一条记录（一行），而且行键既包含了其所属表的信息也包含了其所拥有的行的范围。譬如采取最简单的编码方式，元数据表的行键等于strcat(表名，片最后一行的行键)。 第二点线索，除了知道元数据表的地址部分是常驻内存以外，还可以发现元数据表有一个列族称为location，我们已经知道元数据表每一行代表一个片，那么为什么需要一个列族来存储地址呢？因为每个片都可能由多个SSTable文件组成，列族可以用来存储任意多个SSTable文件的位置。一个合理的假设就是每个SSTable文件的位置信息占据一列，列名为location:filename。当然不一定非得用列键存储完整文件名，更大的可能性是把SSTable文件名存在值里。获取了文件名就可以向GFS索要数据了。 第三个线索告诉我们元数据表不止存储位置信息，也就是说列族不止location，这些数据暂时不是咱们关心的。 通过以上信息，我画了一个简化的Bigtable结构图： 结构图以Webtable表为例，表中存储了网易、百度和豆瓣的几个网页。当我们想查找百度贴吧昨天的网页内容，可以向Bigtable发出查询Webtable表的(com.baidu.tieba, contents:, yesterday)。 假设客户端没有该缓存，那么Bigtable访问root tablet的片服务器，希望得到该网页所属的片的位置信息在哪个元数据片中。使用METADATA.Webtable.com.baidu.tieba为行键在root tablet中查找，定位到最后一个比它大的是METADATA.Webtable.com.baidu.www，于是确定需要的就是元数据表的片A。访问片A的片服务器，继续查找Webtable.com.baidu.tieba，定位到Webtable.com.baidu.www是比它大的，确定需要的是Webtable表的片B。访问片B的片服务器，获得数据。 这里需要注意的是，每个片实际都由若干SSTable文件和memtable组成，而且这些SSTable和memtable都是已排序的。这就导致查找片B时，可能需要将所有SSTable和memtable都查找一遍；另外客户端应该不会直接从元数据表获得SSTable的文件名，而只是获得片属于片服务器的信息，通过片服务器为代理访问SSTable。 参考文献 [1] Bigtable: A Distributed Storage System for Structured Data. In proceedings of OSDI'06. [2] Understanding HBase and BigTable. 本文地址：http://xnerv.wang/google-3-gifts-bigtable/ 转载自：谷歌技术&quot;三宝&quot;之BigTable","categories":[{"name":"分布式及存储","slug":"分布式及存储","permalink":"https://xnerv.wang/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8F%8A%E5%AD%98%E5%82%A8/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"分布式及存储","slug":"分布式及存储","permalink":"https://xnerv.wang/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8F%8A%E5%AD%98%E5%82%A8/"},{"name":"Google","slug":"Google","permalink":"https://xnerv.wang/tags/Google/"},{"name":"BigTable","slug":"BigTable","permalink":"https://xnerv.wang/tags/BigTable/"}]},{"title":"谷歌技术\"三宝\"之MapReduce（转载）","slug":"google-3-gifts-mapreduce","date":"2012-04-27T05:01:57.000Z","updated":"2023-08-21T02:24:20.731Z","comments":true,"path":"google-3-gifts-mapreduce/","link":"","permalink":"https://xnerv.wang/google-3-gifts-mapreduce/","excerpt":"江湖传说永流传：谷歌技术有&quot;三宝&quot;，GFS、MapReduce和大表（BigTable）！ 谷歌在03到06年间连续发表了三篇很有影响力的文章，分别是03年SOSP的GFS，04年OSDI的MapReduce，和06年OSDI的BigTable。SOSP和OSDI都是操作系统领域的顶级会议，在计算机学会推荐会议里属于A类。SOSP在单数年举办，而OSDI在双数年举办。 那么这篇博客就来介绍一下MapReduce。","text":"江湖传说永流传：谷歌技术有&quot;三宝&quot;，GFS、MapReduce和大表（BigTable）！ 谷歌在03到06年间连续发表了三篇很有影响力的文章，分别是03年SOSP的GFS，04年OSDI的MapReduce，和06年OSDI的BigTable。SOSP和OSDI都是操作系统领域的顶级会议，在计算机学会推荐会议里属于A类。SOSP在单数年举办，而OSDI在双数年举办。 那么这篇博客就来介绍一下MapReduce。 1. MapReduce是干啥的 因为没找到谷歌的示意图，所以我想借用一张Hadoop项目的结构图来说明下MapReduce所处的位置，如下图。 Hadoop实际上就是谷歌三宝的开源实现，Hadoop MapReduce对应Google MapReduce，HBase对应BigTable，HDFS对应GFS。HDFS（或GFS）为上层提供高效的非结构化存储服务，HBase（或BigTable）是提供结构化数据服务的分布式数据库，Hadoop MapReduce（或Google MapReduce）是一种并行计算的编程模型，用于作业调度。 GFS和BigTable已经为我们提供了高性能、高并发的服务，但是并行编程可不是所有程序员都玩得转的活儿，如果我们的应用本身不能并发，那GFS、BigTable也都是没有意义的。MapReduce的伟大之处就在于让不熟悉并行编程的程序员也能充分发挥分布式系统的威力。 简单概括的说，MapReduce是将一个大作业拆分为多个小作业的框架（大作业和小作业应该本质是一样的，只是规模不同），用户需要做的就是决定拆成多少份，以及定义作业本身。 下面用一个贯穿全文的例子来解释MapReduce是如何工作的。 2. 例子：统计词频 如果我想统计下过去10年计算机论文出现最多的几个单词，看看大家都在研究些什么，那我收集好论文后，该怎么办呢？ 方法一：我可以写一个小程序，把所有论文按顺序遍历一遍，统计每一个遇到的单词的出现次数，最后就可以知道哪几个单词最热门了。 这种方法在数据集比较小时，是非常有效的，而且实现最简单，用来解决这个问题很合适。 方法二：写一个多线程程序，并发遍历论文。 这个问题理论上是可以高度并发的，因为统计一个文件时不会影响统计另一个文件。当我们的机器是多核或者多处理器，方法二肯定比方法一高效。但是写一个多线程程序要比方法一困难多了，我们必须自己同步共享数据，比如要防止两个线程重复统计文件。 方法三：把作业交给多个计算机去完成。 我们可以使用方法一的程序，部署到N台机器上去，然后把论文集分成N份，一台机器跑一个作业。这个方法跑得足够快，但是部署起来很麻烦，我们要人工把程序copy到别的机器，要人工把论文集分开，最痛苦的是还要把N个运行结果进行整合（当然我们也可以再写一个程序）。 方法四：让MapReduce来帮帮我们吧！ MapReduce本质上就是方法三，但是如何拆分文件集，如何copy程序，如何整合结果这些都是框架定义好的。我们只要定义好这个任务（用户程序），其它都交给MapReduce。 在介绍MapReduce如何工作之前，先讲讲两个核心函数map和reduce以及MapReduce的伪代码。 3. map函数和reduce函数 map函数和reduce函数是交给用户实现的，这两个函数定义了任务本身。 map函数：接受一个键值对（key-value pair），产生一组中间键值对。MapReduce框架会将map函数产生的中间键值对里键相同的值传递给一个reduce函数。 reduce函数：接受一个键，以及相关的一组值，将这组值进行合并产生一组规模更小的值（通常只有一个或零个值）。 统计词频的MapReduce函数的核心代码非常简短，主要就是实现这两个函数。 12345678910111213map(String key, String value): // key: document name // value: document contents for each word w in value: EmitIntermediate(w, &quot;1&quot;);reduce(String key, Iterator values): // key: a word // values: a list of counts int result = 0; for each v in values: result += ParseInt(v); Emit(AsString(result)); 在统计词频的例子里，map函数接受的键是文件名，值是文件的内容，map逐个遍历单词，每遇到一个单词w，就产生一个中间键值对&lt;w, &quot;1&quot;&gt;，这表示单词w咱又找到了一个；MapReduce将键相同（都是单词w）的键值对传给reduce函数，这样reduce函数接受的键就是单词w，值是一串&quot;1&quot;（最基本的实现是这样，但可以优化），个数等于键为w的键值对的个数，然后将这些“1”累加就得到单词w的出现次数。最后这些单词的出现次数会被写到用户定义的位置，存储在底层的分布式存储系统（GFS或HDFS）。 4. MapReduce是如何工作的 上图是论文里给出的流程图。一切都是从最上方的user program开始的，user program链接了MapReduce库，实现了最基本的Map函数和Reduce函数。图中执行的顺序都用数字标记了。 MapReduce库先把user program的输入文件划分为M份（M为用户定义），每一份通常有16MB到64MB，如图左方所示分成了split0~4；然后使用fork将用户进程拷贝到集群内其它机器上。 user program的副本中有一个称为master，其余称为worker，master是负责调度的，为空闲worker分配作业（Map作业或者Reduce作业），worker的数量也是可以由用户指定的。 被分配了Map作业的worker，开始读取对应分片的输入数据，Map作业数量是由M决定的，和split一一对应；Map作业从输入数据中抽取出键值对，每一个键值对都作为参数传递给map函数，map函数产生的中间键值对被缓存在内存中。 缓存的中间键值对会被定期写入本地磁盘，而且被分为R个区，R的大小是由用户定义的，将来每个区会对应一个Reduce作业；这些中间键值对的位置会被通报给master，master负责将信息转发给Reduce worker。 master通知分配了Reduce作业的worker它负责的分区在什么位置（肯定不止一个地方，每个Map作业产生的中间键值对都可能映射到所有R个不同分区），当Reduce worker把所有它负责的中间键值对都读过来后，先对它们进行排序，使得相同键的键值对聚集在一起。因为不同的键可能会映射到同一个分区也就是同一个Reduce作业（谁让分区少呢），所以排序是必须的。 reduce worker遍历排序后的中间键值对，对于每个唯一的键，都将键与关联的值传递给reduce函数，reduce函数产生的输出会添加到这个分区的输出文件中。 当所有的Map和Reduce作业都完成了，master唤醒正版的user program，MapReduce函数调用返回user program的代码。 所有执行完毕后，MapReduce输出放在了R个分区的输出文件中（分别对应一个Reduce作业）。用户通常并不需要合并这R个文件，而是将其作为输入交给另一个MapReduce程序处理。整个过程中，输入数据是来自底层分布式文件系统（GFS）的，中间数据是放在本地文件系统的，最终输出数据是写入底层分布式文件系统（GFS）的。而且我们要注意Map/Reduce作业和map/reduce函数的区别：Map作业处理一个输入数据的分片，可能需要调用多次map函数来处理每个输入键值对；Reduce作业处理一个分区的中间键值对，期间要对每个不同的键调用一次reduce函数，Reduce作业最终也对应一个输出文件。 我更喜欢把流程分为三个阶段。第一阶段是准备阶段，包括1、2，主角是MapReduce库，完成拆分作业和拷贝用户程序等任务；第二阶段是运行阶段，包括3、4、5、6，主角是用户定义的map和reduce函数，每个小作业都独立运行着；第三阶段是扫尾阶段，这时作业已经完成，作业结果被放在输出文件里，就看用户想怎么处理这些输出了。 5. 词频是怎么统计出来的 结合第四节，我们就可以知道第三节的代码是如何工作的了。假设咱们定义M=5，R=3，并且有6台机器，一台master。 这幅图描述了MapReduce如何处理词频统计。由于map worker数量不够，首先处理了分片1、3、4，并产生中间键值对；当所有中间值都准备好了，Reduce作业就开始读取对应分区，并输出统计结果。 6. 用户的权利 用户最主要的任务是实现map和reduce接口，但还有一些有用的接口是向用户开放的。 an input reader。这个函数会将输入分为M个部分，并且定义了如何从数据中抽取最初的键值对，比如词频的例子中定义文件名和文件内容是键值对。 a partition function。这个函数用于将map函数产生的中间键值对映射到一个分区里去，最简单的实现就是将键求哈希再对R取模。 a compare function。这个函数用于Reduce作业排序，这个函数定义了键的大小关系。 an output writer。负责将结果写入底层分布式文件系统。 a combiner function。实际就是reduce函数，这是用于前面提到的优化的，比如统计词频时，如果每个&lt;w, “1”&gt;要读一次，因为reduce和map通常不在一台机器，非常浪费时间，所以可以在map执行的地方先运行一次combiner，这样reduce只需要读一次&lt;w, “n”&gt;了。 map和reduce函数就不多说了。 7. MapReduce的实现 目前MapReduce已经有多种实现，除了谷歌自己的实现外，还有著名的hadoop，区别是谷歌是c++，而hadoop是用java。另外斯坦福大学实现了一个在多核/多处理器、共享内存环境内运行的MapReduce，称为Phoenix（介绍），相关的论文发表在07年的HPCA，是当年的最佳论文哦！ 参考文献 [1] MapReduce : Simplified Data Processing on Large Clusters. In proceedings of OSDI’04. [2] Wikipedia. http://en.wikipedia.org/wiki/Mapreduce [3] Phoenix. http://mapreduce.stanford.edu/ [4] Evaluating MapReduce for Multi-core and Multiprocessor Systems. In proceedings of HPCA’07. 本文地址：http://xnerv.wang/google-3-gifts-mapreduce/ 转载自：谷歌技术&quot;三宝&quot;之MapReduce","categories":[{"name":"分布式及存储","slug":"分布式及存储","permalink":"https://xnerv.wang/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8F%8A%E5%AD%98%E5%82%A8/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"分布式及存储","slug":"分布式及存储","permalink":"https://xnerv.wang/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8F%8A%E5%AD%98%E5%82%A8/"},{"name":"Google","slug":"Google","permalink":"https://xnerv.wang/tags/Google/"},{"name":"MapReduce","slug":"MapReduce","permalink":"https://xnerv.wang/tags/MapReduce/"}]},{"title":"谷歌技术\"三宝\"之谷歌文件系统（转载）","slug":"google-3-gifts-gfs","date":"2012-04-22T00:28:31.000Z","updated":"2023-08-21T02:24:20.767Z","comments":true,"path":"google-3-gifts-gfs/","link":"","permalink":"https://xnerv.wang/google-3-gifts-gfs/","excerpt":"题记：初学分布式文件系统，写篇博客加深点印象。GFS的特点是使用一堆廉价的商用计算机支撑大规模数据处理。 虽然&quot;The Google File System &quot; 是03年发表的老文章了，但现在仍被广泛讨论，其对后来的分布式文件系统设计具有指导意义。然而，作者在设计GFS时，是基于过去很多实验观察的，并提出了很多假设作为前提，这等于给出了一个GFS的应用场景。所以我们自己在设计分布式系统时，一定要注意自己的应用场景是否和GFS相似，不能盲从GFS。 GFS的主要假设如下： GFS的服务器都是普通的商用计算机，并不那么可靠，集群出现结点故障是常态。因此必须时刻监控系统的结点状态，当结点失效时，必须能检测到，并恢复之。 系统存储适当数量的大文件。理想的负载是几百万个文件，文件一般都超过100MB，GB级别以上的文件是很常见的，必须进行有效管理。支持小文件，但不对其进行优化。 负载通常包含两种读：大型的流式读（顺序读），和小型的随机读。前者通常一次读数百KB以上，后者通常在随机位置读几个KB。 负载还包括很多连续的写操作，往文件追加数据（append）。文件很少会被修改，支持随机写操作，但不必进行优化。 系统必须实现良好定义的语义，用于多客户端并发写同一个文件。同步的开销必须保证最小。 高带宽比低延迟更重要，GFS的应用大多需要快速处理大量的数据，很少会严格要求单一操作的响应时间。 从这些假设基本可以看出GFS期望的应用场景应该是大文件，连续读，不修改，高并发。国内的淘宝文件系统（TFS）就不一样，专门为处理小文件进行了优化。","text":"题记：初学分布式文件系统，写篇博客加深点印象。GFS的特点是使用一堆廉价的商用计算机支撑大规模数据处理。 虽然&quot;The Google File System &quot; 是03年发表的老文章了，但现在仍被广泛讨论，其对后来的分布式文件系统设计具有指导意义。然而，作者在设计GFS时，是基于过去很多实验观察的，并提出了很多假设作为前提，这等于给出了一个GFS的应用场景。所以我们自己在设计分布式系统时，一定要注意自己的应用场景是否和GFS相似，不能盲从GFS。 GFS的主要假设如下： GFS的服务器都是普通的商用计算机，并不那么可靠，集群出现结点故障是常态。因此必须时刻监控系统的结点状态，当结点失效时，必须能检测到，并恢复之。 系统存储适当数量的大文件。理想的负载是几百万个文件，文件一般都超过100MB，GB级别以上的文件是很常见的，必须进行有效管理。支持小文件，但不对其进行优化。 负载通常包含两种读：大型的流式读（顺序读），和小型的随机读。前者通常一次读数百KB以上，后者通常在随机位置读几个KB。 负载还包括很多连续的写操作，往文件追加数据（append）。文件很少会被修改，支持随机写操作，但不必进行优化。 系统必须实现良好定义的语义，用于多客户端并发写同一个文件。同步的开销必须保证最小。 高带宽比低延迟更重要，GFS的应用大多需要快速处理大量的数据，很少会严格要求单一操作的响应时间。 从这些假设基本可以看出GFS期望的应用场景应该是大文件，连续读，不修改，高并发。国内的淘宝文件系统（TFS）就不一样，专门为处理小文件进行了优化。 1 体系结构 GFS包括一个master结点（元数据服务器），多个chunkserver（数据服务器）和多个client（运行各种应用的客户端）。在可靠性要求不高的场景，client和chunkserver可以位于一个结点。图1是GFS的体系结构示意图，每一结点都是普通的Linux服务器，GFS的工作就是协调成百上千的服务器为各种应用提供服务。 chunkserver提供存储。GFS会将文件划分为定长数据块，每个数据块都有一个全局唯一不可变的id（chunk_handle），数据块以普通Linux文件的形式存储在chunkserver上，出于可靠性考虑，每个数据块会存储多个副本，分布在不同chunkserver。 GFS master就是GFS的元数据服务器，负责维护文件系统的元数据，包括命名空间、访问控制、文件-块映射、块地址等，以及控制系统级活动，如垃圾回收、负载均衡等。 应用需要链接client的代码，然后client作为代理与master和chunkserver交互。master会定期与chunkserver交流（心跳），以获取chunkserver的状态并发送指令。 图1还描述了应用读取数据的流程。1.应用指定读取某个文件的某段数据，因为数据块是定长的，client可以计算出这段数据跨越了几个数据块，client将文件名和需要的数据块索引发送给master；2.master根据文件名查找命名空间和文件-块映射表，得到需要的数据块副本所在的地址，将数据块的id和其所有副本的地址反馈给client；3.client选择一个副本，联系chunkserver索取需要的数据；4.chunkserver返回数据给client。 2 数据的布局 GFS将文件条带化，按照类似RAID0的形式进行存储，可以提高聚合带宽。事实上，大多数分布式存储系统都会采取这种策略。GFS将文件按固定长度切分为数据块，master在创建一个新数据块时，会给每个数据块分配一个全局唯一且不可变的64位id。每个数据块以Linux文件的形式存储在chunkserver的本地文件系统里。 GFS为数据块设置了一个很大的长度，64MB，这比传统文件系统的块长要大多了。大块长会带来很多好处：1.减少client和master的交互次数，因为读写同一个块只需要一次交互，在GFS假设的顺序读写负载的场景下特别有用；2.同样也减少了client和chunkserver的交互次数，降低TCP/IP连接等网络开销；3.减少了元数据的规模，因此master可以将元数据完全放在内存，这对于集中式元数据模型的GFS尤为重要。 大数据块也有缺点。最大的缺点可能就是内部碎片了，不过考虑到文件一般都相当大，所以碎片也只存在于文件的最后一个数据块。还有一个缺点不是那么容易看出来，由于小文件可能只有少量数据块，极端情况只有一个，那么当这个小文件是热点文件时，存储该文件数据块的chunkserver可能会负载过重。不过正如前面所说，小文件不在GFS的优化范围。 为了提高数据的可靠性和并发性，每一个数据块都有多个副本。当客户端请求一个数据块时，master会将所有副本的地址都通知客户端，客户端再择优（距离最短等）选择一个副本。一个典型的GFS集群可能有数百台服务器，跨越多个子网，因此在考虑副本的放置时，不仅要考虑机器级别的错误，还要考虑整个子网瘫痪了该怎么办。将副本分布到多个子网去，还可以提高系统的聚合带宽。因此创建一个数据块时，主要考虑几个因素：1.优先考虑存储利用率低于平均水平的结点；2.限制单个结点同时创建副本的数量；3.副本尽量跨子网。 3 元数据服务 GFS是典型的集中式元数据服务，所有的元数据都存放在一个master结点内。元数据主要包括三种：文件和数据块的命名空间，文件-数据块映射表，数据块的副本位置。所有的元数据都是放在内存里的。 前两种元数据会被持久化到本地磁盘中，以操作日志的形式。操作日志会记录下这两种元数据的每一次关键变化，因此当master宕机，就可以根据日志恢复到某个时间点。日志的意义还在于，它提供了一个时间线，用于定义操作的先后顺序，文件、数据块的版本都依赖于这个时间顺序。 数据块的副本位置则没有持久化，因为动辄数以百计的chunkserver是很容易出错的，因此只有chunkserver对自己存储的数据块有绝对的话语权，而master上的位置信息很容易因为结点失效等原因而过时。取而代之的方法是，master启动时询问每个chunkserver的数据块情况，而且chunkserver在定期的心跳检查中也会汇报自己存储的部分数据块情况。 GFS物理上没有目录结构，也不支持链接操作，使用一张表来映射文件路径名和元数据。 4 缓存和预取 GFS的客户端和chunkserver都不会缓存任何数据，这是因为GFS的典型应用是顺序访问大文件，不存在时间局部性。空间局部性虽然存在，但是数据集一般很大，以致没有足够的空间缓存。 我们知道集中式元数据模型的元数据服务器容易成为瓶颈，应该尽量减少客户端与元数据服务器的交互。因此GFS设计了元数据缓存。client需要访问数据时，先询问master数据在哪儿，然后将这个数据地址信息缓存起来，之后client对该数据块的操作都只需直接与chunkserver联系了，当然缓存的时间是有限的，过期作废。 master还会元数据预取。因为空间局部性是存在，master可以将逻辑上连续的几个数据块的地址信息一并发给客户端，客户端缓存这些元数据，以后需要时就可以不用找master的麻烦了。 5 出错了肿么办 引用：“We treat component failures as the norm rather than the exception.&quot; 分布式系统整体的可靠性是至关重要的。GFS集群使用的都是普通的商用计算机，而且机器的数量众多，设备故障经常出现，如何处理结点失效的问题是GFS最大的挑战。 5.1 完整性 GFS使用数以千计的磁盘，磁盘出错导致数据被破坏时有发生，我们可以用其它副本来恢复数据，但首先必须能检测出错误。chunksever会使用校验和来检测错误数据。每一个块（chunk）都被划分为64KB的单元（block），每个block对应一个32位的校验和。校验和与数据分开存储，内存有一份，然后以日志的形式在磁盘备一份。 chunkserver在发送数据之前会核对数据的校验和，防止错误的数据传播出去。如果校验和与数据不匹配，就返回错误，并且向master反映情况。master会开始克隆副本的操作，完成后就命令该chunkserver删除非法副本。 5.2 一致性 一致性指的是master的元数据和chunkserver的数据是否一致，多个数据块副本之间是否一致，多个客户端看到的数据是否一致。 先来看看元数据一致性。GFS的命名空间操作是原子性的，并且用日志记录下操作顺序。虽然GFS没有目录结构，但是仍然有一颗逻辑的目录树，树的每个结点都有自己的读写锁，每个元数据操作都需要获得一系列的锁，应该是写锁会阻塞其它的锁，而读锁只阻塞写锁而不阻塞读锁。比如/home/user “目录” 正在创建快照，需要获得/home的读锁和/home/user的写锁，这时如果想创建文件/home/user/foo会被阻塞，因为需要获得/home、/home/user的读锁以及/home/user/foo的写锁，快照会阻塞创建操作获取/home/user的读锁。如果是在一个有传统目录树结构的文件系统里，创建一个文件需要修改父目录的数据，因此需要获得父目录的写锁。这种锁机制允许在一个目录里并发修改数据（如并发创建文件等），这在传统文件系统里是不允许的。 再来看看GFS是如何并发写（write）的，GFS必须将对数据块的修改同步到每一个副本。考虑一下多个应用同时修改同一数据块的情况，我们必须为修改操作定义统一的时序，不然多个副本会出现不一致的情况，那么定义时序由谁做呢？还记得前面提到的元数据缓存么，为了减少master的负担，client在获得副本位置后就不再和master交互，所以必然需要选出一个master代理来完成这个任务。事实上GFS采用了租约（lease）的机制，master会将租约授权给某个副本，称为primary，由这个primary来确定数据修改的顺序，其它副本照做就是。 图2是写操作的控制流和数据流： client需要更新一个数据块，询问master谁拥有该数据块的租约（谁是primary）； master将持有租约的primary和其它副本的位置告知client，client缓存之； client向所有副本传输数据，这里副本没有先后顺序，根据网络拓扑情况找出最短路径，数据从client出发沿着路径流向各个chunkserver，这个过程采用流水线（网络和存储并行）。chunkserver将数据放到LRU缓存； 一旦所有的副本都确定接受数据，client向primary发送写请求，primary为这个前面接受到的数据分配序列号（primary为所有的写操作分配连续的序列号表示先后顺序），并且按照顺序执行数据更新； primary将写请求发送给其它副本，每个副本都按照primary确定的顺序执行更新； 其它副本向primary汇报操作情况； primary回复client操作情况，任何副本错误都导致此次请求失败，并且此时副本处于不一致状态（写操作完成情况不一样）。client会尝试几次3到7的步骤，实在不行就只能重头来过了。 如果一个写请求太大了或者跨越了chunk，GFS的client会将其拆分为多个写请求，每个写请求都遵循上述过程，但是可能和其它应用的写操作交叉在一起。所以这些写操作共享的数据区域就可能包含几个写请求的碎片（就是下文提到的undefined状态）。 GFS还提供另一种写操作append record，append只在文件的尾部以record为单位（为了避免内部碎片，record一般不会很大）写入数据。append是原子性的，GFS保证将数据顺序地写到文件尾部至少一次。append record的流程和图2类似，只是在primary有点区别，GFS必须保证一个record存储在一个chunk内，所以当primary判断当前chunk无足够空间时，就通知所有副本将chunk填充，然后汇报失败，client会申请创建新chunk并重新执行一次append record操作。如果该chunk大小合适，primary会将数据写到数据块的尾部，然后通知其它副本将数据写到一样的偏移。任何副本append失败，各个副本会处于不一致状态（完成或未完成），这时primary必然是成功写入的（不然就没有4以后的操作了）。客户端重试append record操作时，因为primary的chunk长度已经变化了，primary就必须在新的偏移写入数据，而其它副本也是照做。这就导致上一个失败的偏移位置，各个副本处于不一致状态，应用必须自己区分record正确与否，我称这为无效数据区。 表1说明了GFS的一致性保证，明白write和append操作后就容易理解了。consistent指的是多个副本之间是完全一致的；defined指的是副本数据修改后不仅一致而且client能看到其修改的数据全貌。 成功的连续write是已定义的，各个副本数据一致； 成功的并发write能保证一致性性，即各个副本是一样的，但数据并不一定如用户所期望，如前所述，多个用户的修改可能交错在一起； 失败的write操作，使得副本之间不一致，而且数据undefined，不同client可能看到不同的数据（注意区别defined、undefined数据的方法）； 成功的append操作，不管是顺序还是并发都是defined，因为GFS保证了append是原子性的（atomically at least once）。有效数据区确实是defined的，但是失败append操作留下的无效数据区可能会有不一致的情况，所以中间可能零散分布着不一致的数据。 如上所述，在primary的协调下，能保证并发write的一致性。但还有一些可能会导致数据不一致，比如chunkserver宕机错过了数据更新，这时就会出现新旧版本的数据，GFS为每个数据块分配版本来解决这个问题。master每次授权数据块租约给primary之前，都会增加数据块的版本号，并且通知所有副本更新版本号。客户端需要读数据时当然会根据这个版本号来判断副本的数据是否最新。 5.3 可用性 为了保证数据的可用性，GFS为每个数据块存储了多个副本，在数据的布局里有介绍，这里主要关注下元数据的可用性。 GFS是典型的集中式元数据模型，一个元数据服务器承担了巨大的压力，不仅有性能的问题，还有单点故障问题。master为了能快速从故障中恢复过来，采用了log和checkpoint技术，log记录了元数据的每一次变化。用咱们备份的话来说，checkpoint就相当于一次全量备份，log相当于连续数据保护，master宕机后，就先恢复到checkpoint，然后根据log恢复到最新状态。每次创建一个新的checkpoint，log就可以清空，这有效控制了log的大小。 这还不够，如果master完全坏了肿么办？GFS设置了“影子”服务器，master将日志备份到影子上，影子按照日志把元数据与master同步。如果master悲剧了，我们还有影子。平时正常工作时，影子可以分担一部分元数据访问工作，当然不提供直接的写操作。 6 测试 6.1 模拟 实验的网络拓扑图大概就是这样。集群包括一个master和它的两个影子，16个chunkserver，16个client，和两个HP交换机。两个交换机之间是1Gbps的链路，结点与交换机的链路是100Mbps。聚合带宽的理论上限是125MB/s，而单个client的理论带宽上限是12.5MB/s。 实验一：N个client同时随机各自读取1GB数据。 图3(a)，x轴是N，y轴是聚合带宽，上面一条线是理论值，下面一条线是实际值。当N=1时，吞吐率是10MB/s，达到了理论值的80%。当N=16时，吞吐率是94MB/s，达到理论值的75%。此时瓶颈可能是在chunkserver，因为client数量很多，同时读一个chunkserver的概率很大。 实验二：N个client同时写N个不同文件，各自连续写1GB。 图3(b)。理论值上限是67MB/s（The limit plateaus at 67 MB/s because we need to write each byte to 3 of the 16 chunkservers, each with a 12.5 MB/s input connection），这个理论值我没有看明白是怎么算的。当N=1，吞吐率是6.3MB/s，达到极限的一半，主要的瓶颈是数据在副本之间传递。N=16时，聚合带宽为35MB/s（每个client有2.2MB/s），达到极限的一半，这里chunkserver同时接受多个请求的情况比读更严重，因为得写3个副本。写比期望的要慢。 实验三：N个client同时向一个文件append record。 图3©。理论上限值是一台chunkserver的带宽，即12.5MB/s。当N=1时，吞吐率有6.0MB/s。当N=16时，下降到4.8MB/s。这可能是因为拥塞。 6.2 现实 文章里还介绍了两个真实集群的使用情况，cluster A and B。 表2是两个集群的情况。A和B都有数百个chunkserver，存储利用率很高，冗余度是3，所以实际存储的数据是18TB和52TB。文件数相当，dead file是需要删除的文件，但还没被垃圾回收（GFS会为删除的文件保留三天才回收空间）。B的块数更多，意味着文件更大，不过A和B的文件平均都只有1到2个块。chunkserver的元数据主要是block（64KB）的校验和（4 Bytes），以及数据块的版本信息。master的元数据（文件和数据块的名字，文件-数据块映射，块位置等）相当小，平均每个文件只有100B元数据，大部分是用于存储文件名。平均下来，每个服务器有50~100MB的元数据。 7 胡说八道 画了个全分布式元数据模型。 这估计是最简单的，命名空间被划分为几个区域，master各管各的。为了可靠性，每个master做几个副本。映射方法可以是文件名计算哈希取模，好的哈希函数可以使文件随机分布，负载比较均衡。当然扩展性不是很好，加入新的结点，所有文件得重新映射。而且冗余度只能靠机器堆了，不能软件控制。 又构思了个复杂点的。将文件映射到r(&lt;=N)个master，利用参数r控制冗余度，当r=N时就变成全对等元数据集群。这个模型需要r个不同的哈希函数，为了减少开销，可以用两个函数模拟多个函数。比如有随机性很好的f(x)和g(x)函数，我们用式子f(x)+i*g(x)来模拟，其中i为非负整数。当我们处理文件x时，就用前面式子求出r个不同的位置（有冲突的概率，实际可能不止计算r次）。 胡说八道，切勿当真；如有雷同，纯属巧合。 参考文献： [1] The Google File System. 本文地址：http://xnerv.wang/google-3-gifts-gfs/ 转载自：谷歌技术&quot;三宝&quot;之谷歌文件系统","categories":[{"name":"分布式及存储","slug":"分布式及存储","permalink":"https://xnerv.wang/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8F%8A%E5%AD%98%E5%82%A8/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"分布式及存储","slug":"分布式及存储","permalink":"https://xnerv.wang/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8F%8A%E5%AD%98%E5%82%A8/"},{"name":"Google","slug":"Google","permalink":"https://xnerv.wang/tags/Google/"},{"name":"GFS","slug":"GFS","permalink":"https://xnerv.wang/tags/GFS/"}]},{"title":"(Stack Overflow) SAVE TRANSACTION vs BEGIN TRANSACTION (SQL Server) how to nest transactions nicely","slug":"save-transaction-vs-begin-transaction-sql-server-how-to-nest-transactions-nicely","date":"2012-03-15T09:51:00.000Z","updated":"2023-08-21T02:24:18.594Z","comments":true,"path":"save-transaction-vs-begin-transaction-sql-server-how-to-nest-transactions-nicely/","link":"","permalink":"https://xnerv.wang/save-transaction-vs-begin-transaction-sql-server-how-to-nest-transactions-nicely/","excerpt":"Question I have a stored procedure that needs to set a save point so that it can, under certain circumstances, undo everything it did and return an error code to the caller, or accept/commit it and return success to the caller. But I need it to work whether the caller has already started a transaction or not. The doc is extremely confusing on this subject. Here is what I think will work, but I’m not certain of all the ramifications. The thing is - this Stored Procedure (SP) is called by others. So I don’t know if they’ve started a transaction or not… Even if I require users to start a transaction to use my SP, I still have questions about the proper use of Save Points … My SP will test if a transaction is in progress, and if not, start one with BEGIN TRANSACTION. If a transaction is already in progress, it will instead create a save point with SAVE TRANSACTION MySavePointName, and save the fact this is what I did.","text":"Question I have a stored procedure that needs to set a save point so that it can, under certain circumstances, undo everything it did and return an error code to the caller, or accept/commit it and return success to the caller. But I need it to work whether the caller has already started a transaction or not. The doc is extremely confusing on this subject. Here is what I think will work, but I’m not certain of all the ramifications. The thing is - this Stored Procedure (SP) is called by others. So I don’t know if they’ve started a transaction or not… Even if I require users to start a transaction to use my SP, I still have questions about the proper use of Save Points … My SP will test if a transaction is in progress, and if not, start one with BEGIN TRANSACTION. If a transaction is already in progress, it will instead create a save point with SAVE TRANSACTION MySavePointName, and save the fact this is what I did. Then if I have to roll back my changes, if I did a BEGIN TRANSACTION earlier, then I will ROLLBACK TRANSACTION. If I did the save point, then I will ROLLBACK TRANSACTION MySavePointName. This scenario seems to work great. Here is where I get a little confused - if I want to keep the work I’ve done, if I started a transaction I will execute COMMIT TRANSACTION. But if I created a save point? I tried COMMIT TRANSACTION MySavePointName, but then the caller tries to commit its transaction and gets an error: The COMMIT TRANSACTION request has no corresponding BEGIN TRANSACTION. So I’m wondering then - a save point can be rolled back (that works: ROLLBACK TRANSACTION MySavePointName will NOT roll back the caller’s transaction). But perhaps one never needs to “commit” it? It just stays there, in case you need to roll back to it, but goes away once the original transaction is committed (or rolled back)? If there is a “better” way to “nest” a transaction, please shed some light as well. I haven’t figured out how to nest with BEGIN TRANSACTION but only rollback or commit my internal transaction. Seems ROLLBACK will always roll back to the top transaction, while COMMIT simply decrements @@trancount. Answer by Brian B I believe I’ve figured this all out now, so I will answer my own question… I’ve even blogged my findings if you want more details at http://geekswithblogs.net/bbiales/archive/2012/03/15/how-to-nest-transactions-nicely—quotbegin-transactionquot-vs-quotsave.aspx So my SP starts with something like this, to start a new transaction if there is none, but use a Save Point if one is already in progress: 12345678DECLARE @startingTranCount intSET @startingTranCount = @@TRANCOUNTIF @startingTranCount &gt; 0 SAVE TRANSACTION mySavePointNameELSE BEGIN TRANSACTION-- … Then, when ready to commit the changes, you only need to commit if we started the transaction ourselves: 12IF @startingTranCount = 0 COMMIT TRANSACTION And finally, to roll back just your changes so far: 12345-- Roll back changes...IF @startingTranCount &gt; 0 ROLLBACK TRANSACTION MySavePointNameELSE ROLLBACK TRANSACTION Article link: http://xnerv.wang/save-transaction-vs-begin-transaction-sql-server-how-to-nest-transactions-nicely/ Reprinted from: (StackOverflow) SAVE TRANSACTION vs BEGIN TRANSACTION (SQL Server) how to nest transactions nicely","categories":[{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/categories/DBMS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/tags/DBMS/"},{"name":"SQL Server","slug":"SQL-Server","permalink":"https://xnerv.wang/tags/SQL-Server/"},{"name":"Transaction","slug":"Transaction","permalink":"https://xnerv.wang/tags/Transaction/"},{"name":"Savepoint","slug":"Savepoint","permalink":"https://xnerv.wang/tags/Savepoint/"}]},{"title":"多版本并发控制(MVCC)在分布式系统中的应用（转载）","slug":"application-of-mvcc-in-distributed-system","date":"2012-03-13T07:00:00.000Z","updated":"2023-08-21T02:24:20.615Z","comments":true,"path":"application-of-mvcc-in-distributed-system/","link":"","permalink":"https://xnerv.wang/application-of-mvcc-in-distributed-system/","excerpt":"问题 最近项目中遇到了一个分布式系统的并发控制问题。该问题可以抽象为：某分布式系统由一个数据中心D和若干业务处理中心L1，L2 … Ln组成；D本质上是一个key-value存储，它对外提供基于HTTP协议的CRUD操作接口。L的业务逻辑可以抽象为下面3个步骤： read: 根据keySet {k1, … kn}从D获取keyValueSet {k1:v1, … kn:vn} do: 根据keyValueSet进行业务处理，得到需要更新的数据集keyValueSet’ {k1′:v1′, … km’:vm’} (注：读取的keySet和更新的keySet’可能不同) update: 把keyValueSet’更新到D （注：D保证在一次调用更新多个key的原子性） 在没有事务支持的情况下，多个L进行并发处理可能会导致数据一致性问题。比如，考虑L1和L2的如下执行顺序： L1从D读取key:123对应的值100 L2从D读取key:123对应的100 L1将key:123更新为100 + 1 L2将key:123更新为100 + 2 如果L1和L2串行执行，key:123对应的值将为103，但上面并发执行中L1的执行效果完全被L2所覆盖，实际key:123所对应的值变成了102。","text":"问题 最近项目中遇到了一个分布式系统的并发控制问题。该问题可以抽象为：某分布式系统由一个数据中心D和若干业务处理中心L1，L2 … Ln组成；D本质上是一个key-value存储，它对外提供基于HTTP协议的CRUD操作接口。L的业务逻辑可以抽象为下面3个步骤： read: 根据keySet {k1, … kn}从D获取keyValueSet {k1:v1, … kn:vn} do: 根据keyValueSet进行业务处理，得到需要更新的数据集keyValueSet’ {k1′:v1′, … km’:vm’} (注：读取的keySet和更新的keySet’可能不同) update: 把keyValueSet’更新到D （注：D保证在一次调用更新多个key的原子性） 在没有事务支持的情况下，多个L进行并发处理可能会导致数据一致性问题。比如，考虑L1和L2的如下执行顺序： L1从D读取key:123对应的值100 L2从D读取key:123对应的100 L1将key:123更新为100 + 1 L2将key:123更新为100 + 2 如果L1和L2串行执行，key:123对应的值将为103，但上面并发执行中L1的执行效果完全被L2所覆盖，实际key:123所对应的值变成了102。 解决方案1：基于锁的事务 为了让L的处理具有可串行化特性(Serializability)，一种最直接的解决方案就是考虑为D加上基于锁的简单事务。让L在进行业务处理前先锁定D，完成以后释放锁。另外，为了防止持有锁的L由于某种原因长时间未提交事务，D还需要具有超时机制，当L尝试提交一个已超时的事务时会得到一个错误响应。 本方案的优点是实现简单，缺点是锁定了整个数据集，粒度太大；时间上包含了L的整个处理时间，跨度太长。虽然我们可以考虑把锁定粒度降低到数据项级别，按key进行锁定，但这又会带来其他的问题。由于更新的keySet’可能是事先不确定的，所以可能无法在开始事务时锁定所有的key；如果分阶段来锁定需要的key，又可能出现死锁(Deadlock)问题。另外，按key锁定在有锁争用的情况下并不能解决锁定时间太长的问题。所以，按key锁定仍然存在重要的不足之处。 解决方案2：多版本并发控制 为了实现可串行化，同时避免锁机制存在的各种问题，我们可以采用基于多版本并发控制（Multiversion concurrency control，MVCC）思想的无锁事务机制。人们一般把基于锁的并发控制机制称成为悲观机制，而把MVCC机制称为乐观机制。这是因为锁机制是一种预防性的，读会阻塞写，写也会阻塞读，当锁定粒度较大，时间较长时并发性能就不会太好；而MVCC是一种后验性的，读不阻塞写，写也不阻塞读，等到提交的时候才检验是否有冲突，由于没有锁，所以读写不会相互阻塞，从而大大提升了并发性能。我们可以借用源代码版本控制来理解MVCC，每个人都可以自由地阅读和修改本地的代码，相互之间不会阻塞，只在提交的时候版本控制器会检查冲突，并提示merge。目前，Oracle、PostgreSQL和MySQL都已支持基于MVCC的并发机制，但具体实现各有不同。 MVCC的一种简单实现是基于CAS（Compare-and-swap）思想的有条件更新（Conditional Update）。普通的update参数只包含了一个keyValueSet’，Conditional Update在此基础上加上了一组更新条件conditionSet { … data[keyx]=valuex, … }，即只有在D满足更新条件的情况下才将数据更新为keyValueSet’；否则，返回错误信息。这样，L就形成了如下图所示的Try/Conditional Update/(Try again)的处理模式： 虽然对单个L来讲不能保证每次都成功更新，但从整个系统来看，总是有任务能够顺利进行。这种方案利用Conditional Update避免了大粒度和长时间的锁定，当各个业务之间资源争用不大的情况下，并发性能很好。不过，由于Conditional Update需要更多的参数，如果condition中value的长度很长，那么每次网络传送的数据量就会比较大，从而导致性能下降。特别是当需要更新的keyValueSet’很小，而condition很大时，就显得非常不经济。 为了避免condition太大所带来的性能问题，可以为每条数据项增加一个int型的版本号字段，由D维护该版本号，每次数据有更新就增加版本号；L在进行Conditional Update时，通过版本号取代具体的值。 另一个问题是上面的解决方案假设了D是可以支持Conditional Update的；那么，如果D是一个不支持Conditional Update的第三方的key-value存储怎么办呢？这时，我们可以在L和D之间增加一个P作为代理，所有的CRUD操作都必须经过P，让P来进行条件检查，而实际的数据操作放在D。这种方式实现了条件检查和数据操作的分离，但同时降低了性能，需要在P中增加cache，提升性能。由于P是D的唯一客户端；所以，P的cache管理是非常简单的，不必像多客户端情形担心缓存的失效。不过，实际上，据我所知redis和Amazon SimpleDB都已经有了Conditional Update的支持。 悲观锁和MVCC对比 上面介绍了悲观锁和MVCC的基本原理，但是对于它们分别适用于什么场合，不同的场合下两种机制优劣具体表现在什么地方还不是很清楚。这里我就对一些典型的应用场景进行简单的分析。需要注意的是下面的分析不针对分布式，悲观锁和MVCC两种机制在分布式系统、单数据库系统、甚至到内存变量各个层次都存在。 场景1：对读的响应速度要求高 有一类系统更新特别频繁，并且对读的响应速度要求很高，如股票交易系统。在悲观锁机制下，写会阻塞读，那么当有写操作时，读操作的响应速度就会受到影响；而MVCC不存在读写锁，读操作是不受任何阻塞的，所以读的响应速度会更快更稳定。 场景2：读远多于写 对于许多系统来讲，读操作的比例往往远大于写操作，特别是某些海量并发读的系统。在悲观锁机制下，当有写操作占用锁，就会有大量的读操作被阻塞，影响并发性能；而MVCC可以保持比较高且稳定的读并发能力。 场景3：写操作冲突频繁 如果系统中写操作的比例很高，且冲突频繁，这时就需要仔细评估。假设两个有冲突的业务L1和L2，它们在单独执行是分别耗时t1，t2。在悲观锁机制下，它们的总时间大约等于串行执行的时间： T = t1 + t2 而在MVCC下，假设L1在L2之前更新，L2需要retry一次，它们的总时间大约等于L2执行两次的时间（这里假设L2的两次执行耗时相等，更好的情况是，如果第1次能缓存下部分有效结果，第二次执行L2耗时是可能减小的）： T’ = 2 * t2 这时关键是要评估retry的代价，如果retry的代价很低，比如，对某个计数器递增，又或者第二次执行可以比第一次快很多，这时采用MVCC机制就比较适合。反之，如果retry的代价很大，比如，报表统计运算需要算几小时甚至一天那就应该采用锁机制避免retry。 从上面的分析，我们可以简单的得出这样的结论：对读的响应速度和并发性要求比较高的场景适合MVCC；而retry代价越大的场景越适合悲观锁机制。 总结 本文介绍了一种基于多版本并发控制（MVCC）思想的Conditional Update解决分布式系统并发控制问题的方法。和基于悲观锁的方法相比，该方法避免了大粒度和长时间的锁定，能更好地适应对读的响应速度和并发性要求高的场景。 参考 Wikipedia – Serializability Wikipedia – Compare-and-swap Wikipedia – Multiversion concurrency control Lock-free algorithms: The try/commit/(try again) pattern Amazon SimpleDB FAQs – Does Amazon SimpleDB support transactions? redis – Transactions A Quick Survey of MultiVersion Concurrency Algorithms 非阻塞算法思想在关系数据库应用程序开发中的使用 友情推荐 本文的图是用我自己开发的TextDiagram工具画的，欢迎试用！如果您喜欢，请推荐给朋友，谢谢！ 本文地址：http://xnerv.wang/application-of-mvcc-in-distributed-system/ 转载自：多版本并发控制(MVCC)在分布式系统中的应用","categories":[{"name":"分布式及存储","slug":"分布式及存储","permalink":"https://xnerv.wang/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8F%8A%E5%AD%98%E5%82%A8/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"分布式及存储","slug":"分布式及存储","permalink":"https://xnerv.wang/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8F%8A%E5%AD%98%E5%82%A8/"},{"name":"MVCC","slug":"MVCC","permalink":"https://xnerv.wang/tags/MVCC/"}]},{"title":"How does a mutex work? What does it cost?（转载）","slug":"how-does-a-mutex-work-what-does-it-cost","date":"2011-12-16T08:00:00.000Z","updated":"2023-08-21T02:24:21.038Z","comments":true,"path":"how-does-a-mutex-work-what-does-it-cost/","link":"","permalink":"https://xnerv.wang/how-does-a-mutex-work-what-does-it-cost/","excerpt":"Concurrent programming requires synchronization. We can’t have more than one thread accessing data at the same time; otherwise, we end up with a data race. The most common solution is to wrap the critical data access in a mutex. Mutexes are, of course, not free. A mutex can have a significant impact on the cost of the code we are writing. When used correctly we’ll barely notice the overhead. When misused it can cause a program to run worse in threaded mode than it would have single threaded!","text":"Concurrent programming requires synchronization. We can’t have more than one thread accessing data at the same time; otherwise, we end up with a data race. The most common solution is to wrap the critical data access in a mutex. Mutexes are, of course, not free. A mutex can have a significant impact on the cost of the code we are writing. When used correctly we’ll barely notice the overhead. When misused it can cause a program to run worse in threaded mode than it would have single threaded! Also read CPU Memory – Why do I need a mutex?. What is a mutex? A mutex, in its most fundamental form, is just an integer in memory. This memory can have a few different values depending on the state of the mutex. Though usually when we speak of mutexes, we also talk of the locks which use the mutex. The integer in memory is not intriguing, but the operations around it are. There are two fundamental operations which a mutex must provide to be useful: lock unlock unlock is a simple case since it’s usually just one function. Unlocking a mutex makes it available for another process to lock. lock on the other hand usually has several variants. In most cases, we’d like to wait until we can lock the mutex, so the most common lock operation does exactly this. Other users may wish to only wait for a given period, and yet some other users may not want to wait at all. There can be only one lock on a mutex at any given time. If another thread wishes to gain control, it must wait for the first to unlock it. This mutual exclusion is the primary goal of the mutex, and indeed the origin of the name. Attempting to lock an already locked mutex is called contention. In a well-planned program, contention should be quite low; you should be designing your code so that most attempts to lock the mutex will not block. There are two reasons why you want to avoid contention. The first is that any thread waiting on a mutex is obviously not doing anything else — possibly resulting in unused CPU cycles. The second reason is more interesting, in particular for high-performance code. Locking a currently unlocked mutex is cheap compared to the contention case. We have to look at how the mutex works to understand why. How does it work? As mentioned before, the data of a mutex is simply an integer in memory. Its value starts as 0, meaning that it is unlocked. If you wish to lock the mutex, you check if it is zero and then assign one. The mutex is now locked, and you are the owner of it. The trick is that the test and set operation has to be atomic. If two threads happen to read 0 at the same time, then both would write 1 and think they own the mutex. Without CPU support there is no way to implement a mutex in user space: this operation must be atomic with respect to the other threads. Fortunately, CPUs has a function called “compare-and-set” or “test-and-set” which does exactly this. This function takes the address of the integer, and two integer values: a compare and set value. If the comparison value matches the current value of the integer then it is replaced with the new value. In C style code this might like look this: 12345int compare_set( int * to_compare, int compare, int set );int mutex_value;int result = compare_set( &amp;mutex_value, 0, 1 );if( !result ) &#123; /* we got the lock */ &#125; The caller determines what happens by inspecting the return value. It is the dereferenced to_compare pointer value before the swap. If this value is equal to the compare value the caller knows the set was successful. If the value is different, then the call was unsuccessful. When the section of code no longer requires the lock it can set the value back to 0. This makes up the basic part of our mutex. Atomic increment/decrement functions could also be used and are the recommended way if using the Linux futex. What about waiting? Now comes the tricky part. Well, only in a way is it tricky, in another way it is simple. The above test-and-set mechanism provides no support for a thread to wait on the value (aside from a CPU intensive spin-lock). The CPU doesn’t really understand high-level threads and processes, so it isn’t in a position to implement waiting. The OS must provide the waiting functionality. For the CPU to wait correctly, a caller is going to need to go through a system call. It is the only thing that can synchronize the various threads and provide the waiting functionality. So if we have to wait on a mutex, or release a waiting mutex, we have no choice but to call the OS. Most OSs have built in mutex primitives. In some cases, they provide full fledged mutexes. So if a system call does provide a full mutex why would we bother with any sort of test-and-set in user space? The answer is that system calls have quite a bit of overhead and should be avoided when possible. Various operating systems diverge at this point, and will likely change as time goes on. Under Linux, there is a system call futex which provides mutex like semantics. Non-contention cases are resolved in user space. Contention cases are delegated to the operating system to handle in a safe, albeit far costlier manner. The waiting is handled as part of the OS process scheduler. futex is quite flexible in allowing the creation of various locking mechanisms in addition to a mutex, such as a semaphore, a barrier, a read-write mutex, and event signaling. The Costs There are a few points of interest when it comes to the cost of a mutex. The first most vital point is waiting time. Your threads should spend only a fraction of their time waiting on mutexes. If they are waiting too often, then you are losing concurrency. In a worst case scenario many threads always trying to lock the same mutex may result in performance worse than a single thread serving all requests. This isn’t a cost of the mutex itself, but a serious concern with concurrent programming. The overhead costs of a mutex relate to the test-and-set operation and the system call that implements a mutex. The test-and-set is likely a minuscule cost; being essential to concurrent processing the CPUs have a strong incentive to make it efficient. We’ve ignored another important instruction, however: the fence. This is used in all high-level mutexes and may have a higher cost than the test-and-set operation. Most costly however is the system call. Not only do you suffer the context switch overhead of the system call, the kernel now spends some time in its scheduling code. 本文地址：http://xnerv.wang/how-does-a-mutex-work-what-does-it-cost/ 转载自：How does a mutex work? What does it cost?","categories":[{"name":"OS","slug":"OS","permalink":"https://xnerv.wang/categories/OS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"OS","slug":"OS","permalink":"https://xnerv.wang/tags/OS/"},{"name":"Mutex","slug":"Mutex","permalink":"https://xnerv.wang/tags/Mutex/"},{"name":"Multi Threads","slug":"Multi-Threads","permalink":"https://xnerv.wang/tags/Multi-Threads/"}]},{"title":"(Stack Overflow) Why are hard links to directories not allowed in UNIX/Linux?","slug":"why-are-hard-links-to-directories-not-allowed-in-unix-linux","date":"2011-10-11T11:21:00.000Z","updated":"2023-08-21T02:24:19.553Z","comments":true,"path":"why-are-hard-links-to-directories-not-allowed-in-unix-linux/","link":"","permalink":"https://xnerv.wang/why-are-hard-links-to-directories-not-allowed-in-unix-linux/","excerpt":"Question I read in text books that Unix/Linux doesn’t allow hard links to directories but does allow soft links. Is it because, when we have cycles and if we create hard links, and after some time we delete the original file, it will point to some garbage value? If cycles were the sole reason behind not allowing hard links, then why are soft links to directories allowed?","text":"Question I read in text books that Unix/Linux doesn’t allow hard links to directories but does allow soft links. Is it because, when we have cycles and if we create hard links, and after some time we delete the original file, it will point to some garbage value? If cycles were the sole reason behind not allowing hard links, then why are soft links to directories allowed? Answer by Chris Down This is just a bad idea, as there is no way to tell the difference between a hard link and an original name. Allowing hard links to directories would break the directed acyclic graph structure of the filesystem, possibly creating directory loops and dangling directory subtrees, which would make fsck and any other file tree walkers error prone. First, to understand this, let’s talk about inodes. The data in the filesystem is held in blocks on the disk, and those blocks are collected together by an inode. You can think of the inode as THE file. Inodes lack filenames, though. That’s where links come in. A link is just a pointer to an inode. A directory is an inode that holds links. Each filename in a directory is just a link to an inode. Opening a file in Unix also creates a link, but it’s a different type of link (it’s not a named link). A hard link is just an extra directory entry pointing to that inode. When you ls -l, the number after the permissions is the named link count. Most regular files will have one link. Creating a new hard link to a file will make both filenames point to the same inode. Note: 12345678910111213141516% ls -l testls: test: No such file or directory% touch test% ls -l test-rw-r--r-- 1 danny staff 0 Oct 13 17:58 test% ln test test2% ls -l test*-rw-r--r-- 2 danny staff 0 Oct 13 17:58 test-rw-r--r-- 2 danny staff 0 Oct 13 17:58 test2% touch test3% ls -l test*-rw-r--r-- 2 danny staff 0 Oct 13 17:58 test-rw-r--r-- 2 danny staff 0 Oct 13 17:58 test2-rw-r--r-- 1 danny staff 0 Oct 13 17:59 test3 ^ ^ this is the link count Now, you can clearly see that there is no such thing as a hard link. A hard link is the same as a regular name. In the above example, test or test2, which is the original file and which is the hard link? By the end, you can’t really tell (even by timestamps) because both names point to the same contents, the same inode: 1234% ls -li test*14445750 -rw-r--r-- 2 danny staff 0 Oct 13 17:58 test14445750 -rw-r--r-- 2 danny staff 0 Oct 13 17:58 test214445892 -rw-r--r-- 1 danny staff 0 Oct 13 17:59 test3 The -i flag to ls shows you inode numbers in the beginning of the line. Note how test and test2 have the same inode number, but test3 has a different one. Now, if you were allowed to do this for directories, two different directories in different points in the filesystem could point to the same thing. In fact, a subdir could point back to its grandparent, creating a loop. Why is this loop a concern? Because when you are traversing, there is no way to detect you are looping (without keeping track of inode numbers as you traverse). Imagine you are writing the du command, which needs to recurse through subdirs to find out about disk usage. How would du know when it hit a loop? It is error prone and a lot of bookkeeping that du would have to do, just to pull off this simple task. Symlinks are a whole different beast, in that they are a special type of “file” that many file filesystem APIs tend to automatically follow. Note, a symlink can point to a nonexistent destination, because they point by name, and not directly to an inode. That concept doesn’t make sense with hard links, because the mere existence of a “hard link” means the file exists. So why can du deal with symlinks easily and not hard links? We were able to see above that hard links are indistinguishable from normal directory entries. Symlinks, however, are special, detectable, and skippable! du notices that the symlink is a symlink, and skips it completely! 123456789% ls -ltotal 4drwxr-xr-x 3 danny staff 102 Oct 13 18:14 test1/lrwxr-xr-x 1 danny staff 5 Oct 13 18:13 test2@ -&gt; test1% du -ah242M ./test1/bigfile242M ./test14.0K ./test2242M . Article link: http://xnerv.wang/why-are-hard-links-to-directories-not-allowed-in-unix-linux/ Reprinted from: (StackOverflow) Why are hard links to directories not allowed in UNIX/Linux?","categories":[{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/categories/Linux/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/tags/Linux/"},{"name":"Bash","slug":"Bash","permalink":"https://xnerv.wang/tags/Bash/"},{"name":"File System","slug":"File-System","permalink":"https://xnerv.wang/tags/File-System/"},{"name":"Stack Overflow","slug":"Stack-Overflow","permalink":"https://xnerv.wang/tags/Stack-Overflow/"}]},{"title":"(Stack Overflow) Execute vs Read bit. How do directory permissions in Linux work?","slug":"execute-vs-read-bit-how-do-directory-permissions-in-linux-work","date":"2011-09-22T19:13:00.000Z","updated":"2023-08-21T02:24:19.453Z","comments":true,"path":"execute-vs-read-bit-how-do-directory-permissions-in-linux-work/","link":"","permalink":"https://xnerv.wang/execute-vs-read-bit-how-do-directory-permissions-in-linux-work/","excerpt":"","text":"Question In my CMS, I noticed that directories need the executable bit (+x) set for the user to open them. Why is the execute permission required to read a directory, and how do directory permissions in Linux work? Answer by Chris Down When applying permissions to directories on Linux, the permission bits have different meanings than on regular files. The write bit allows the affected user to create, rename, or delete files within the directory, and modify the directory’s attributes The read bit allows the affected user to list the files within the directory The execute bit allows the affected user to enter the directory, and access files and directories inside The sticky bit states that files and directories within that directory may only be deleted or renamed by their owner (or root) Article link: http://xnerv.wang/execute-vs-read-bit-how-do-directory-permissions-in-linux-work/ Reprinted from: (StackOverflow) Execute vs Read bit. How do directory permissions in Linux work?","categories":[{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/categories/Linux/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/tags/Linux/"},{"name":"Bash","slug":"Bash","permalink":"https://xnerv.wang/tags/Bash/"},{"name":"File System","slug":"File-System","permalink":"https://xnerv.wang/tags/File-System/"},{"name":"Stack Overflow","slug":"Stack-Overflow","permalink":"https://xnerv.wang/tags/Stack-Overflow/"}]},{"title":"(Stack Overflow) What are the main uses of yield(), and how does it differ from join() and interrupt()?","slug":"what-are-the-main-uses-of-yield-and-how-does-it-differ-from-join-and-interrupt","date":"2011-08-08T16:01:00.000Z","updated":"2023-08-21T02:24:20.931Z","comments":true,"path":"what-are-the-main-uses-of-yield-and-how-does-it-differ-from-join-and-interrupt/","link":"","permalink":"https://xnerv.wang/what-are-the-main-uses-of-yield-and-how-does-it-differ-from-join-and-interrupt/","excerpt":"Question I am a little bit confused about the use of yield() method in Java, specifically in the example code below. I’ve also read that yield() is ‘used to prevent execution of a thread’. My questions are: I believe the code below result in the same output both when using yield() and when not using it. Is this correct? What are, in fact, the main uses of yield()? In what ways is yield() different from the join() and interrupt() methods?","text":"Question I am a little bit confused about the use of yield() method in Java, specifically in the example code below. I’ve also read that yield() is ‘used to prevent execution of a thread’. My questions are: I believe the code below result in the same output both when using yield() and when not using it. Is this correct? What are, in fact, the main uses of yield()? In what ways is yield() different from the join() and interrupt() methods? The code example: 123456789101112131415161718public class MyRunnable implements Runnable &#123; public static void main(String[] args) &#123; Thread t = new Thread(new MyRunnable()); t.start(); for(int i=0; i&lt;5; i++) &#123; System.out.println(&quot;Inside main&quot;); &#125; &#125; public void run() &#123; for(int i=0; i&lt;5; i++) &#123; System.out.println(&quot;Inside run&quot;); Thread.yield(); &#125; &#125;&#125; I obtain the same output using the code above both with and without using yield(): 12345678910Inside mainInside mainInside mainInside mainInside mainInside runInside runInside runInside runInside run Answer by Sathwick Source: http://www.javamex.com/tutorials/threads/yield.shtml Windows In the Hotspot implementation, the way that Thread.yield() works has changed between Java 5 and Java 6. In Java 5, Thread.yield() calls the Windows API call Sleep(0). This has the special effect of clearing the current thread’s quantum and putting it to the end of the queue for its priority level. In other words, all runnable threads of the same priority (and those of greater priority) will get a chance to run before the yielded thread is next given CPU time. When it is eventually re-scheduled, it will come back with a full full quantum, but doesn’t “carry over” any of the remaining quantum from the time of yielding. This behaviour is a little different from a non-zero sleep where the sleeping thread generally loses 1 quantum value (in effect, 1/3 of a 10 or 15ms tick). In Java 6, this behaviour was changed. The Hotspot VM now implements Thread.yield() using the Windows SwitchToThread() API call. This call makes the current thread give up its current timeslice, but not its entire quantum. This means that depending on the priorities of other threads, the yielding thread can be scheduled back in one interrupt period later. (See the section on thread scheduling for more information on timeslices.) Linux Under Linux, Hotspot simply calls sched_yield(). The consequences of this call are a little different, and possibly more severe than under Windows: a yielded thread will not get another slice of CPU until all other threads have had a slice of CPU; (at least in kernel 2.6.8 onwards), the fact that the thread has yielded is implicitly taken into account by the scheduler’s heuristics on its recent CPU allocation— thus, implicitly, a thread that has yielded could be given more CPU when scheduled in the future. (See the section on thread scheduling for more details on priorities and scheduling algorithms.) When to use yield()? I would say practically never. Its behaviour isn’t standardly defined and there are generally better ways to perform the tasks that you might want to perform with yield(): if you’re trying to use only a portion of the CPU, you can do this in a more controllable way by estimating how much CPU the thread has used in its last chunk of processing, then sleeping for some amount of time to compensate: see the sleep() method; if you’re waiting for a process or resource to complete or become available, there are more efficient ways to accomplish this, such as by using join() to wait for another thread to complete, using the wait/notify mechanism to allow one thread to signal to another that a task is complete, or ideally by using one of the Java 5 concurrency constructs such as a Semaphore or blocking queue. Article link: http://xnerv.wang/what-are-the-main-uses-of-yield-and-how-does-it-differ-from-join-and-interrupt/ Reprinted from: (StackOverflow) What are the main uses of yield(), and how does it differ from join() and interrupt()?","categories":[{"name":"OS","slug":"OS","permalink":"https://xnerv.wang/categories/OS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"OS","slug":"OS","permalink":"https://xnerv.wang/tags/OS/"},{"name":"Stack Overflow","slug":"Stack-Overflow","permalink":"https://xnerv.wang/tags/Stack-Overflow/"},{"name":"yield","slug":"yield","permalink":"https://xnerv.wang/tags/yield/"}]},{"title":"Everything You Never Wanted To Know About DLLs（转载）","slug":"everything-you-never-wanted-to-know-about-dlls","date":"2011-07-04T07:00:00.000Z","updated":"2023-08-21T02:24:20.346Z","comments":true,"path":"everything-you-never-wanted-to-know-about-dlls/","link":"","permalink":"https://xnerv.wang/everything-you-never-wanted-to-know-about-dlls/","excerpt":"For the chinese translated version, please click 关于DLL的一些你不会想要知道的知识. I’ve recently had cause to investigate how dynamic linking is implemented on Windows. This post is basically a brain dump of everything I’ve learnt on the issue. This is mostly for my future reference, but I hope it will be useful to others too as I’m going to bring together lots of information you would otherwise have to hunt around for. Without further ado, here we go:","text":"For the chinese translated version, please click 关于DLL的一些你不会想要知道的知识. I’ve recently had cause to investigate how dynamic linking is implemented on Windows. This post is basically a brain dump of everything I’ve learnt on the issue. This is mostly for my future reference, but I hope it will be useful to others too as I’m going to bring together lots of information you would otherwise have to hunt around for. Without further ado, here we go: Export and import directories The Windows executable loader is responsible for doing all dynamic loading and symbol resolution before running the code. The linker works out what functions are exported or imported by each image (an image is a DLL or EXE file) by inspecting the .edata and .idata sections of those images, respectively. The contents of these sections is covered in detail by the PE/COFF specification. The .edata section This section records the exports of the image (yes, EXEs can export things). This takes the form of: The export address table: an array of length N holding the addresses of the exported functions/data (the addresses are stored relative to the image base). Indexes into this table are called ordinals. The export name pointer table: an array of length M holding pointers to strings that represent the name of an export. This array is lexically ordered by name, to allow binary searches for a given export. The export ordinal table: a parallel array of length M holding the ordinal of the corresponding name in the export name pointer table. (As an alternative to importing an image’s export by its name, it is possible to import by specifying an ordinal. Importing by ordinal is slightly faster at runtime because the dynamic linker doesn’t have to do a lookup. Furthermore, if the import is not given a name by the exporting DLL, importing by ordinal is the only way to do the import.) How does the .edata section get created in the first place? There are two main methods: Most commonly, they start life in the object files created by compiling some source code that defines a function/some data that was declared with the __declspec(dllimport) modifier. The compiler just emits an appropriate .edata section naming these exports. Less commonly, the programmer might write a .def file specifying which functions they would like to export. By supplying this to dlltool --output-exp, an export file can be generated. An export file is just an object file which only contains a .edata section, exporting (via some unresolved references that will be filled in by the linker in the usual way) the symbols named in the .def file. This export library must be named by the programmer when he comes to link together his object files into a DLL. In both these cases, the linker collects the .edata sections from all objects named on the link line to build the .edata for the overall image file. One last possible way that the .edata can be created is by the linker itself, without having to put .edata into any object files: The linker could choose to export all symbols defined by object files named on the link line. For example, this is the default behaviour of GNU ld (the behaviour can also be explicitly asked for using –-export-all-symbols). In this case, the linker generates the .edata section itself. (GNU ld also supports specifying a .def file on the command line, in which case the generated section will export just those things named by the .def). The .idata section The .idata section records those things that the image imports. It consists of: For every image from which symbols are imported: The filename of the image. Used by the dynamic linker to locate it on disk. The import lookup table: an array of length N, which each entry is either an ordinal or a pointer to a string representing the name to import. The import address table: an array of N pointers. The dynamic linker is responsible for filling out this array with the address of the function/data named by the corresponding symbol in the import lookup table. The ways in which .idata entries are created are as follows: Most commonly, they originate in a library of object files called an import library. This import library can be created by usingdlltool on the DLL you wish to export or a .def file of the type we discussed earlier. Just like the export library, the import library must be named by the user on the link line. Alternatively, some linkers (like GNU ld) let you specify a DLL directly on the link line. The linker will automatically generate .idata entries for any symbols that you must import from the DLL. Notice that unlike the case when we were exporting symbols, __declspec(dllimport) does not cause .idata sections to be generated. Import libraries are a bit more complicated than they first appear. The Windows dynamic loader fills the import address table with the addresses of the imported symbols (say, the address of a function Func). However, when the assembly code in other object files says call Func they expect that Func to name the address of that code. But we don’t know that address until runtime: the only thing we know statically is the address where that address will be placed by the dynamic linker. We will call this address __imp__Func. To deal with this extra level of indirection, the import library exports a function Func that just dereferences __imp__Func (to get the actual function pointer) and then jmps to it. All of the other object files in the project can now say call Func just as they would if Func had been defined in some other object file, rather than a DLL. For this reason, saying __declspec(dllimport) in the declaration of a dynamically linked function is optional (though in fact you will get slightly more efficient code if you add them, as we will see later). Unfortunately, there is no equivalent trick if you want to import data from another DLL. If we have some imported data myData, there is no way the import library can be defined so that a mov $eax, myData in an object file linked against it writes to the storage for myData in that DLL. Instead, the import library defines a symbol __imp__myData that resolves to the address at which the linked-in address of the storage can be found. The compiler then ensures that when you read or write from a variable defined with __declspec(dllimport) those reads and writes go through the __imp_myData indirection. Because different code needs to be generated at the use site, __declspec declarations on data imports are not optional. Practical example Theory is all very well but it can be helpful to see all the pieces in play. Building a DLL First, lets build a simple DLL exporting both functions and data. For maximum clarity, we’ll use an explicit export library rather instead of decorating our functions with declspec(dllexport) or supply a .def file to the linker. First lets write the .def file, library.def: 1234LIBRARY libraryEXPORTS function_export data_export DATA (The DATA keyword and LIBRARY line only affects how the import library is generated, as explained later on. Ignore them for now.) Build an export file from that: 1$ dlltool --output-exp library_exports.o -d library.def The resulting object basically just contains an .edata section that exports the symbols _data_export and _function_export under the names data_export and function_export respectively: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778$ objdump -xs library_exports.o...There is an export table in .edata at 0x0The Export Tables (interpreted .edata section contents)Export Flags 0Time&#x2F;Date stamp 4e10e5c1Major&#x2F;Minor 0&#x2F;0Name 00000028 library_exports.o.dllOrdinal Base 1Number in: Export Address Table 00000002 [Name Pointer&#x2F;Ordinal] Table 00000002Table Addresses Export Address Table 00000040 Name Pointer Table 00000048 Ordinal Table 00000050Export Address Table -- Ordinal Base 1[Ordinal&#x2F;Name Pointer] Table [ 0] data_export [ 1] function_exportSections:Idx Name Size VMA LMA File off Algn 0 .text 00000000 00000000 00000000 00000000 2**2 ALLOC, LOAD, READONLY, CODE 1 .data 00000000 00000000 00000000 00000000 2**2 ALLOC, LOAD, DATA 2 .bss 00000000 00000000 00000000 00000000 2**2 ALLOC 3 .edata 00000070 00000000 00000000 000000b4 2**2 CONTENTS, ALLOC, LOAD, RELOC, READONLY, DATASYMBOL TABLE:[ 0](sec -2)(fl 0x00)(ty 0)(scl 103) (nx 1) 0x00000000 fakeFile[ 2](sec 4)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000028 name[ 3](sec 4)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000040 afuncs[ 4](sec 4)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000048 anames[ 5](sec 4)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000050 anords[ 6](sec 4)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000054 n1[ 7](sec 4)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000060 n2[ 8](sec 1)(fl 0x00)(ty 0)(scl 3) (nx 1) 0x00000000 .textAUX scnlen 0x0 nreloc 0 nlnno 0[ 10](sec 2)(fl 0x00)(ty 0)(scl 3) (nx 1) 0x00000000 .dataAUX scnlen 0x0 nreloc 0 nlnno 0[ 12](sec 3)(fl 0x00)(ty 0)(scl 3) (nx 1) 0x00000000 .bssAUX scnlen 0x0 nreloc 0 nlnno 0[ 14](sec 4)(fl 0x00)(ty 0)(scl 3) (nx 1) 0x00000000 .edataAUX scnlen 0x70 nreloc 8 nlnno 0[ 16](sec 0)(fl 0x00)(ty 0)(scl 2) (nx 0) 0x00000000 _data_export[ 17](sec 0)(fl 0x00)(ty 0)(scl 2) (nx 0) 0x00000000 _function_exportRELOCATION RECORDS FOR [.edata]:OFFSET TYPE VALUE0000000c rva32 .edata0000001c rva32 .edata00000020 rva32 .edata00000024 rva32 .edata00000040 rva32 _data_export00000044 rva32 _function_export00000048 rva32 .edata0000004c rva32 .edataContents of section .edata: 0000 00000000 c1e5104e 00000000 28000000 .......N....(... 0010 01000000 02000000 02000000 40000000 ............@... 0020 48000000 50000000 6c696272 6172795f H...P...library_ 0030 6578706f 7274732e 6f2e646c 6c000000 exports.o.dll... 0040 00000000 00000000 54000000 60000000 ........T...&#96;... 0050 00000100 64617461 5f657870 6f727400 ....data_export. 0060 66756e63 74696f6e 5f657870 6f727400 function_export. We’ll fulfil these symbol with a trivial implementation of the DLL, library.c: 12345int data_export = 42;int function_export() &#123; return 1337 + data_export;&#125; We can put it together into a DLL: 1$ gcc -shared -o library.dll library.c library_exports.o The export table for the DLL is as follows, showing that we have exported what we wanted: 12345678910111213141516171819202122The Export Tables (interpreted .edata section contents)Export Flags 0Time&#x2F;Date stamp 4e10e5c1Major&#x2F;Minor 0&#x2F;0Name 00005028 library_exports.o.dllOrdinal Base 1Number in: Export Address Table 00000002 [Name Pointer&#x2F;Ordinal] Table 00000002Table Addresses Export Address Table 00005040 Name Pointer Table 00005048 Ordinal Table 00005050Export Address Table -- Ordinal Base 1 [ 0] +base[ 1] 200c Export RVA [ 1] +base[ 2] 10f0 Export RVA[Ordinal&#x2F;Name Pointer] Table [ 0] data_export [ 1] function_export Using the DLL When we come to look at using the DLL, things become a lot more interesting. First, we need an import library: 1$ dlltool --output-lib library.dll.a -d library.def (The reason that we have an import library but an export object is because using a library for the imports allows the linker to discard .idata for any imports that are not used. Contrariwise ,he linker can never discard any .edata entry because any export may potentially be used by a user of the DLL). This import library is rather complex. It contains one object for each export (disds00000.o and disds00001.o) but also two other object files (distdt.o and disdh.o) that set up the header and footer of the import list. (The header of the import list contains, among other things, the name of the DLL to link in at runtime, as derived from the LIBRARY line of the .def file.) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214$ objdump -xs library.dll.aIn archive library.dll.a:disdt.o: file format pe-i386...Sections:Idx Name Size VMA LMA File off Algn 0 .text 00000000 00000000 00000000 00000000 2**2 ALLOC, LOAD, READONLY, CODE 1 .data 00000000 00000000 00000000 00000000 2**2 ALLOC, LOAD, DATA 2 .bss 00000000 00000000 00000000 00000000 2**2 ALLOC 3 .idata$4 00000004 00000000 00000000 00000104 2**2 CONTENTS, ALLOC, LOAD, DATA 4 .idata$5 00000004 00000000 00000000 00000108 2**2 CONTENTS, ALLOC, LOAD, DATA 5 .idata$7 0000000c 00000000 00000000 0000010c 2**2 CONTENTS, ALLOC, LOAD, DATASYMBOL TABLE:[ 0](sec -2)(fl 0x00)(ty 0)(scl 103) (nx 1) 0x00000000 fakeFile[ 2](sec 1)(fl 0x00)(ty 0)(scl 3) (nx 1) 0x00000000 .textAUX scnlen 0x0 nreloc 0 nlnno 0[ 4](sec 2)(fl 0x00)(ty 0)(scl 3) (nx 1) 0x00000000 .dataAUX scnlen 0x0 nreloc 0 nlnno 0[ 6](sec 3)(fl 0x00)(ty 0)(scl 3) (nx 1) 0x00000000 .bssAUX scnlen 0x0 nreloc 0 nlnno 0[ 8](sec 4)(fl 0x00)(ty 0)(scl 3) (nx 1) 0x00000000 .idata$4AUX scnlen 0x4 nreloc 0 nlnno 0[ 10](sec 5)(fl 0x00)(ty 0)(scl 3) (nx 1) 0x00000000 .idata$5AUX scnlen 0x4 nreloc 0 nlnno 0[ 12](sec 6)(fl 0x00)(ty 0)(scl 3) (nx 1) 0x00000000 .idata$7AUX scnlen 0x7 nreloc 0 nlnno 0[ 14](sec 6)(fl 0x00)(ty 0)(scl 2) (nx 0) 0x00000000 __library_dll_a_inameContents of section .idata$4: 0000 00000000 ....Contents of section .idata$5: 0000 00000000 ....Contents of section .idata$7: 0000 6c696272 6172792e 646c6c00 library.dll.disdh.o: file format pe-i386...Sections:Idx Name Size VMA LMA File off Algn 0 .text 00000000 00000000 00000000 00000000 2**2 ALLOC, LOAD, READONLY, CODE 1 .data 00000000 00000000 00000000 00000000 2**2 ALLOC, LOAD, DATA 2 .bss 00000000 00000000 00000000 00000000 2**2 ALLOC 3 .idata$2 00000014 00000000 00000000 00000104 2**2 CONTENTS, ALLOC, LOAD, RELOC, DATA 4 .idata$5 00000000 00000000 00000000 00000000 2**2 ALLOC, LOAD, DATA 5 .idata$4 00000000 00000000 00000000 00000000 2**2 ALLOC, LOAD, DATASYMBOL TABLE:[ 0](sec -2)(fl 0x00)(ty 0)(scl 103) (nx 1) 0x00000000 fakeFile[ 2](sec 6)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 hname[ 3](sec 5)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 fthunk[ 4](sec 1)(fl 0x00)(ty 0)(scl 3) (nx 1) 0x00000000 .textAUX scnlen 0x0 nreloc 0 nlnno 0[ 6](sec 2)(fl 0x00)(ty 0)(scl 3) (nx 1) 0x00000000 .dataAUX scnlen 0x0 nreloc 0 nlnno 0[ 8](sec 3)(fl 0x00)(ty 0)(scl 3) (nx 1) 0x00000000 .bssAUX scnlen 0x0 nreloc 0 nlnno 0[ 10](sec 4)(fl 0x00)(ty 0)(scl 3) (nx 1) 0x00000000 .idata$2AUX scnlen 0x14 nreloc 3 nlnno 0[ 12](sec 6)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 .idata$4[ 13](sec 5)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 .idata$5[ 14](sec 4)(fl 0x00)(ty 0)(scl 2) (nx 0) 0x00000000 __head_library_dll_a[ 15](sec 0)(fl 0x00)(ty 0)(scl 2) (nx 0) 0x00000000 __library_dll_a_inameRELOCATION RECORDS FOR [.idata$2]:OFFSET TYPE VALUE00000000 rva32 .idata$40000000c rva32 __library_dll_a_iname00000010 rva32 .idata$5Contents of section .idata$2: 0000 00000000 00000000 00000000 00000000 ................ 0010 00000000 ....disds00001.o: file format pe-i386...Sections:Idx Name Size VMA LMA File off Algn 0 .text 00000008 00000000 00000000 0000012c 2**2 CONTENTS, ALLOC, LOAD, RELOC, READONLY, CODE 1 .data 00000000 00000000 00000000 00000000 2**2 ALLOC, LOAD, DATA 2 .bss 00000000 00000000 00000000 00000000 2**2 ALLOC 3 .idata$7 00000004 00000000 00000000 00000134 2**2 CONTENTS, RELOC 4 .idata$5 00000004 00000000 00000000 00000138 2**2 CONTENTS, RELOC 5 .idata$4 00000004 00000000 00000000 0000013c 2**2 CONTENTS, RELOC 6 .idata$6 00000012 00000000 00000000 00000140 2**1 CONTENTSSYMBOL TABLE:[ 0](sec 1)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 .text[ 1](sec 2)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 .data[ 2](sec 3)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 .bss[ 3](sec 4)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 .idata$7[ 4](sec 5)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 .idata$5[ 5](sec 6)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 .idata$4[ 6](sec 7)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 .idata$6[ 7](sec 1)(fl 0x00)(ty 0)(scl 2) (nx 0) 0x00000000 _function_export[ 8](sec 5)(fl 0x00)(ty 0)(scl 2) (nx 0) 0x00000000 __imp__function_export[ 9](sec 0)(fl 0x00)(ty 0)(scl 2) (nx 0) 0x00000000 __head_library_dll_aRELOCATION RECORDS FOR [.text]:OFFSET TYPE VALUE00000002 dir32 .idata$5RELOCATION RECORDS FOR [.idata$7]:OFFSET TYPE VALUE00000000 rva32 __head_library_dll_aRELOCATION RECORDS FOR [.idata$5]:OFFSET TYPE VALUE00000000 rva32 .idata$6RELOCATION RECORDS FOR [.idata$4]:OFFSET TYPE VALUE00000000 rva32 .idata$6Contents of section .text: 0000 ff250000 00009090 .%......Contents of section .idata$7: 0000 00000000 ....Contents of section .idata$5: 0000 00000000 ....Contents of section .idata$4: 0000 00000000 ....Contents of section .idata$6: 0000 01006675 6e637469 6f6e5f65 78706f72 ..function_expor 0010 7400 t.disds00000.o: file format pe-i386...Sections:Idx Name Size VMA LMA File off Algn 0 .text 00000000 00000000 00000000 00000000 2**2 ALLOC, LOAD, READONLY, CODE 1 .data 00000000 00000000 00000000 00000000 2**2 ALLOC, LOAD, DATA 2 .bss 00000000 00000000 00000000 00000000 2**2 ALLOC 3 .idata$7 00000004 00000000 00000000 0000012c 2**2 CONTENTS, RELOC 4 .idata$5 00000004 00000000 00000000 00000130 2**2 CONTENTS, RELOC 5 .idata$4 00000004 00000000 00000000 00000134 2**2 CONTENTS, RELOC 6 .idata$6 0000000e 00000000 00000000 00000138 2**1 CONTENTSSYMBOL TABLE:[ 0](sec 1)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 .text[ 1](sec 2)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 .data[ 2](sec 3)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 .bss[ 3](sec 4)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 .idata$7[ 4](sec 5)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 .idata$5[ 5](sec 6)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 .idata$4[ 6](sec 7)(fl 0x00)(ty 0)(scl 3) (nx 0) 0x00000000 .idata$6[ 7](sec 5)(fl 0x00)(ty 0)(scl 2) (nx 0) 0x00000000 __imp__data_export[ 8](sec 0)(fl 0x00)(ty 0)(scl 2) (nx 0) 0x00000000 __head_library_dll_aRELOCATION RECORDS FOR [.idata$7]:OFFSET TYPE VALUE00000000 rva32 __head_library_dll_aRELOCATION RECORDS FOR [.idata$5]:OFFSET TYPE VALUE00000000 rva32 .idata$6RELOCATION RECORDS FOR [.idata$4]:OFFSET TYPE VALUE00000000 rva32 .idata$6Contents of section .idata$7: 0000 00000000 ....Contents of section .idata$5: 0000 00000000 ....Contents of section .idata$4: 0000 00000000 ....Contents of section .idata$6: 0000 00006461 74615f65 78706f72 7400 ..data_export. Note that the object corresponding to data_export has an empty .text section, whereas function_export does define some code. If we disassemble it we get this: 1234500000000 &lt;_function_export&gt;: 0: ff 25 00 00 00 00 jmp *0x0 2: dir32 .idata$5 6: 90 nop 7: 90 nop The relocation of type dir32 tells the linker how to fill in the address being dereferenced by the jmp. We can see that _function_export, when entered, will jump directly to the function at the address loaded from the memory named .idata$5. Inspection of the complete .idata section satisfies us that .idata$5 corresponds to the address of the fragment of the import address table corresponding to the function_export import name, and hence the address where the absolute address of the loaded function_export import can be found. Although only function_export gets a corresponding _function_export function, both of the exports have lead to a symbol with the __imp__ prefix (__imp__data_export and __imp__function_export) being defined in the import library. As discussed before, this symbol stands for the address at which the pointer to the data/function will be inserted by the dynamic linker. As such, the __imp__ symbols always point directly into the import address table. With an import library in hand, we are capable of writing some client code that uses our exports, main1.c: 12345678910111213141516#include &lt;stdio.h&gt;__declspec(dllimport) extern int function_export(void);__declspec(dllimport) extern int data_export;int main(int argc, char **argv) &#123; printf(&quot;%d\\n&quot;, function_export()); printf(&quot;%d\\n&quot;, data_export); data_export++; printf(&quot;%d\\n&quot;, function_export()); printf(&quot;%d\\n&quot;, data_export); return 0;&#125; Build and link it against the import library and we will get the results we expect: 12345$ gcc main1.c library.dll.a -o main1 &amp;&amp; .&#x2F;main1137942138043 The reason that this works even though there is no data_export symbol defined by library.dll.a is because the __declspec(dllimport) qualifier on our data_export declaration in main.c has caused the compiled to generate code that uses the __imp_data_export symbol directly, as we can see if we disassemble the generated code: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465$ gcc -c main1.c -o main1.o &amp;&amp; objdump --disassemble -r main1.omain1.o: file format pe-i386Disassembly of section .text:00000000 &lt;_main&gt;: 0: 8d 4c 24 04 lea 0x4(%esp),%ecx 4: 83 e4 f0 and $0xfffffff0,%esp 7: ff 71 fc pushl -0x4(%ecx) a: 55 push %ebp b: 89 e5 mov %esp,%ebp d: 51 push %ecx e: 83 ec 14 sub $0x14,%esp 11: e8 00 00 00 00 call 16 &lt;_main+0x16&gt; 12: DISP32 ___main 16: a1 00 00 00 00 mov 0x0,%eax 17: dir32 __imp__function_export 1b: ff d0 call *%eax 1d: 89 44 24 04 mov %eax,0x4(%esp) 21: c7 04 24 00 00 00 00 movl $0x0,(%esp) 24: dir32 .rdata 28: e8 00 00 00 00 call 2d &lt;_main+0x2d&gt; 29: DISP32 _printf 2d: a1 00 00 00 00 mov 0x0,%eax 2e: dir32 __imp__data_export 32: 8b 00 mov (%eax),%eax 34: 89 44 24 04 mov %eax,0x4(%esp) 38: c7 04 24 00 00 00 00 movl $0x0,(%esp) 3b: dir32 .rdata 3f: e8 00 00 00 00 call 44 &lt;_main+0x44&gt; 40: DISP32 _printf 44: a1 00 00 00 00 mov 0x0,%eax 45: dir32 __imp__data_export 49: 8b 00 mov (%eax),%eax 4b: 8d 50 01 lea 0x1(%eax),%edx 4e: a1 00 00 00 00 mov 0x0,%eax 4f: dir32 __imp__data_export 53: 89 10 mov %edx,(%eax) 55: a1 00 00 00 00 mov 0x0,%eax 56: dir32 __imp__function_export 5a: ff d0 call *%eax 5c: 89 44 24 04 mov %eax,0x4(%esp) 60: c7 04 24 00 00 00 00 movl $0x0,(%esp) 63: dir32 .rdata 67: e8 00 00 00 00 call 6c &lt;_main+0x6c&gt; 68: DISP32 _printf 6c: a1 00 00 00 00 mov 0x0,%eax 6d: dir32 __imp__data_export 71: 8b 00 mov (%eax),%eax 73: 89 44 24 04 mov %eax,0x4(%esp) 77: c7 04 24 00 00 00 00 movl $0x0,(%esp) 7a: dir32 .rdata 7e: e8 00 00 00 00 call 83 &lt;_main+0x83&gt; 7f: DISP32 _printf 83: b8 00 00 00 00 mov $0x0,%eax 88: 83 c4 14 add $0x14,%esp 8b: 59 pop %ecx 8c: 5d pop %ebp 8d: 8d 61 fc lea -0x4(%ecx),%esp 90: c3 ret 91: 90 nop 92: 90 nop 93: 90 nop In fact, we can see that the generated code doesn’t even use the _function_export symbol, preferring __imp__function_export. Essentially, the code of the _function_export symbol in the import library has been inlined at every use site. This is why using __declspec(dllimport) can improve performance of cross-DLL calls, even though it is entirely optional on function declarations. We might wonder what happens if we drop the __declspec(dllimport) qualifier on our declarations. Because of our discussion about the difference between data and function imports earlier, you might expect linking to fail. Our test file, main2.c is: 12345678910111213141516#include &lt;stdio.h&gt;extern int function_export(void);extern int data_export;int main(int argc, char **argv) &#123; printf(&quot;%d\\n&quot;, function_export()); printf(&quot;%d\\n&quot;, data_export); data_export++; printf(&quot;%d\\n&quot;, function_export()); printf(&quot;%d\\n&quot;, data_export); return 0;&#125; Let’s try it out: 12345$ gcc main2.c library.dll.a -o main2 &amp;&amp; .&#x2F;main2137942138043 What the hell – it worked? This is a bit uprising. The reason that it works despite the fact that the import library library.dll.a not defining the _data_export symbol is because of a nifty feature of GNU ld called auto-import. Without auto-import the link fails as we would expect: 123456$ gcc main2.c library.dll.a -o main2 -Wl,--disable-auto-import &amp;&amp; .&#x2F;main2&#x2F;tmp&#x2F;ccGd8Urx.o:main2.c:(.text+0x2c): undefined reference to &#96;_data_export&#39;&#x2F;tmp&#x2F;ccGd8Urx.o:main2.c:(.text+0x41): undefined reference to &#96;_data_export&#39;&#x2F;tmp&#x2F;ccGd8Urx.o:main2.c:(.text+0x49): undefined reference to &#96;_data_export&#39;&#x2F;tmp&#x2F;ccGd8Urx.o:main2.c:(.text+0x63): undefined reference to &#96;_data_export&#39;collect2: ld returned 1 exit status The Microsoft linker does not implement auto-import, so this is the error you would get if you were using the Microsoft toolchain. However, there is a way to write client code that does not depend on auto-import or use the __declspec(dllimport) keyword. Our new client, main3.c is as follows: 12345678910111213141516171819#include &lt;stdio.h&gt;extern int (*_imp__function_export)(void);extern int *_imp__data_export;#define function_export (*_imp__function_export)#define data_export (*_imp__data_export)int main(int argc, char **argv) &#123; printf(&quot;%d\\n&quot;, function_export()); printf(&quot;%d\\n&quot;, data_export); data_export++; printf(&quot;%d\\n&quot;, function_export()); printf(&quot;%d\\n&quot;, data_export); return 0;&#125; In this code, we directly use the __imp__-prefixed symbols from the import library. These name an address at which the real address of the import can be found, which is reflected by our C-preprocessor definitions of data_export and function_export. This code compiles perfectly even without auto-import: 12345$ gcc main3.c library.dll.a -o main3 -Wl,--disable-auto-import &amp;&amp; .&#x2F;main3137942138043 If you have followed along until this point you should have a solid understanding of how DLL import and export are implemented on Windows. How auto-import works As a bonus, I’m going to explain how auto-import is implemented by the GNU linker. It is a rather cute hack you may get a kick out of. As a reminder, auto-import is a feature of the linker that allows the programmer to declare an item of DLL-imported data with a simple extern keyword, without having to explicitly use __declspec(dllimport). This is extremely convenient because this is exactly how most _nix source code declares symbols it expects to import from a shared library, so by supporting this use case that_nix code becomes more portable to Windows. Auto-import kicks in whenever the linker finds an object file making use of a symbol foo which is not defined by any other object in the link, but where a symbol __imp_foo is defined by some object. In this case, it assumes that the use of foo is an attempt to access some DLL-imported data item called foo. Now, the problem is that the linker needs to replace the use of foo with the address of foo itself. However, all we seem to know statically is an address where that address will be placed at runtime (__imp_foo). To square the circle, the linker plays a clever trick. The trick is to extend the .idata of the image being created with an entry for a “new” DLL. The new entry is set up as follows: The filename of the image being imported is set to the same filename as the .idata entry covering __imp_foo. So if __imp_foo was being filled out by an address in Bar.dll, our new .idata entry will use Bar.dll here. The import lookup table is of length 1, whose sole entry is a pointer to the name of the imported symbol corresponding to __imp_foo. So if __imp_foo is filled out by the address of the foo export from Bar.dll, the name of the symbol we put in here will be foo. The import address table is of length 1 – and here is the clever bit – is located precisely at the location in the object file that was referring to the (undefined) symbol foo. This solution neatly defers the task of filling out the address that the object file wants to the dynamic linker. The reason that the linker can play this trick is that it can see all of the object code that goes into the final image, and can thus fix all of the sites that need to refer to the imported data. Note that in general the final image’s .idata will contain several entries for the same DLL: one from the import library, and one for every place in any object file in the link which referred to some data exported by the DLL. Although this is somewhat unusual behaviour, the Windows linker has no problem with there being several imports of the same DLL. A wrinkle Unfortunately, the scheme described above only works if the object code has an undefined reference to foo itself. What if instead it has a reference to foo+N, an address N bytes after the address of foo itself? There is no way to set up the .idata so that the dynamic linker adds a constant to the address it fills in, so we seem to be stuck. Alas, such relocations are reasonably common, and originate from code that accesses a field of a DLL-imported structure type. Cygwin actually contains another hack to make auto-import work in such cases, known as “pseudo-relocations”. If you want to know the details of how these works, there is more information in the original thread on the topic. Conclusion Dynamic linking on Windows is hairier than it at first appears. I hope this article has gone some way to clearing up the meaning of the mysterious dllimport and dllexport keywords, and at clarifying the role of the import and export libraries. Linux and friends implement dynamic linking in a totally different manner to Windows. The scheme they use is more flexible and allows more in-memory sharing of code, but incurs a significant runtime penalty (especially on i386). For more details see here and the Dynamic Linking section of the the ELF spec. 本文地址：http://xnerv.wang/everything-you-never-wanted-to-know-about-dlls/ 转载自：Everything You Never Wanted To Know About DLLs","categories":[{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/categories/Windows/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/tags/Windows/"},{"name":"DLL","slug":"DLL","permalink":"https://xnerv.wang/tags/DLL/"}]},{"title":"深入探讨PageRank（四）：PageRank的危机及搜索引擎的未来（转载）","slug":"further-disscusion-of-pagerank-4","date":"2011-06-21T18:22:00.000Z","updated":"2023-08-21T02:24:20.902Z","comments":true,"path":"further-disscusion-of-pagerank-4/","link":"","permalink":"https://xnerv.wang/further-disscusion-of-pagerank-4/","excerpt":"别到处找了，本系列文章没有（三）的。。。 作为10多年前搜索引擎代表性的技术成果之一，PageRank创造了Google辉煌的10年，同时也缔造了Google搜索的时代。然而，互联网越是往前发展，搜索服务越趋向于多元化、个性化、社区化和垂直化等，传统的通用搜索引擎越来越不能满足不同人群、不同习惯、不同场景的搜索需求，精而深的垂直搜索引擎的兴起对传统搜索市场引发了巨大的挑战。 正如同生物的生长一样，任何事物都是有其生命周期，PageRank也不例外。PageRank是否已经越过了它繁盛生命的顶锋？它是否能够担负起当今Web2.0，甚至是云计算时代的搜索排名之重任？我们慢慢来看~","text":"别到处找了，本系列文章没有（三）的。。。 作为10多年前搜索引擎代表性的技术成果之一，PageRank创造了Google辉煌的10年，同时也缔造了Google搜索的时代。然而，互联网越是往前发展，搜索服务越趋向于多元化、个性化、社区化和垂直化等，传统的通用搜索引擎越来越不能满足不同人群、不同习惯、不同场景的搜索需求，精而深的垂直搜索引擎的兴起对传统搜索市场引发了巨大的挑战。 正如同生物的生长一样，任何事物都是有其生命周期，PageRank也不例外。PageRank是否已经越过了它繁盛生命的顶锋？它是否能够担负起当今Web2.0，甚至是云计算时代的搜索排名之重任？我们慢慢来看~ 一、第二代搜索引擎的局限 第二代搜索引擎是基于用户输入关键字做文本的相关性分析，通过排序算法（包含PageRank）将排序后的结果反馈给用户，其代表是Google和百度。大体而言，第二代搜索引擎的局限可概括为以下4个方面： （1）搜索结果不具备个性化因素，任何两个人得到的结果是一致的，没有考虑用户本身的搜索习惯。缺乏智能的感知体系，不能依据用户搜索偏好进行个性化的搜索推荐。 在第二代搜索引擎中毋庸质疑的是任何两个人所享受到的搜索服务是完全一致的，不存在个性化的搜索。但是用户本身的搜索习惯是不同的，打个比方，一个从来不看江苏卫视《非诚勿扰》相亲节目的人，有一天突然在搜索栏中键入“非诚勿扰”，他更多地是想找《非诚勿扰》这部电影；然而对于一个经常在看这个相亲节目的人而言，搜索结果最好能够把江苏卫视的那个娱乐节目反馈给她。也就是说，不同的人在输入同样的关键词之后，所期待的输出结果是不同的，搜索引擎应该能够依据不同的人，反馈不同的搜索结果。 当然有人会说：我怎么知道是哪个用户在使用我的搜索引擎，嘿嘿问的好。Google可以发个公告说：Google搜索目前提供个性化搜索服务，使用的前提需要进行用户注册这并不是说面向大众的搜索服务不需要了，你还可以选择切换或自定义，想用个性就用，不想用还按常理出牌就可以了。我个人感觉这招还是挺吸引人的，当然，上面只是个原型的说辞，具体怎样推广就看Google的产品部那帮牛人怎么设计了。 既然，搜索引擎的发展必然是趋向于精细化、个性化的模式，不同用户应该享受和获得不同的搜索体验和搜索反馈。那么，如何才能make it to be true？本文第三部分会给出解释~ （2）搜索内容是基本文本匹配的，搜索引擎本身并不理解你所输入的查询语句是什么意思，缺少语义分析。 当前的搜索引擎的搜索过程全部是基于文本字符串匹配的，即先通过爬虫从互联网上下载数据，对数据进行清洗、格式化处理，对所有的文本内容分词，并创建倒排索引。用户输入检索词Query，搜索引擎会对Query分词，通过查找倒排索引表，返回与Query最相关的文本集合，利用PageRank算法排序后反馈给用户，但搜索引擎本身并不理解你所输入的Query是什么含义。 打个比方，比如你输入“愤怒的小狐狸是谁？”，搜索引擎就先分词为“愤怒”、“小”、“狐狸”、“是谁”，然后根据倒排索引表查找到底是哪些文档同时包含了这几个词条，排序后作为结果输出。然而，用户真正关心的是这只“愤怒的小狐狸”真人是谁，他并不需要知道哪些文档同时包含了这几个词，而只需要知道一个真实的人名即可。 这样的例子举不胜举，再比如说检索“华中科技大学计算机科学院董勐同学的个人信息”，则用户期待搜索引擎反馈给他一个整合的数据信息，包括了董勐同学的姓名、年龄、联系方式、生日、住址、兴趣爱好等等，而不是说只是简单地把找到包含这句话的网页给返回过来。 其实到这个时候，搜索引擎已经脱离了我们传统意义上理解的文本搜索引擎，而是能够智能地理解你要搜索的含义，并智能地生成你想要的结果。就像你跟一个真正的人在交流一样，你问他答，他在回答你话的同时，会根据他所储备的知识和信息推理演算你想要的结果，它是在思考问题，这属于问答式智能化的搜索引擎。 （3）面向文本内容的搜索，对于多媒体类型内容，如图片、视频、音乐、影像的检索不给力或力不从心。 在Web2.0时代，人们在互联网上所能接触、交流和分享的数据类型可谓琳琅满目，除了传统的文本信息外，多媒体的数据交互形式深受大众追捧。有例可考证像人人网、新浪微博这样的SNS网络，各种文本、视频、图片、音乐信息的共享铺天盖地。然而，搜索引擎在应对多媒体数据的时候总是显得非常的不给力或非常的力不从心。 这里面的原因，一方面是由于这些信息无法用自然语言量化或很难量化，另一方面是如果对多媒体文件进行细粒度的分析和建模，那对于存储、计算资源的消耗可以非常恐怖的，你想想一部高清电影有多少帧需要分析就明白了。 当然，不是没有办法做，现在是有一些基础性质的研究，其做法有点曲线救国的味道。以早期的Google图片搜索为例，你输入“海贼王”的话，搜索引擎实质上还是对文字的检索，返回的是打了“海贼王”标签的图片而已，本质上跟传统的文本搜索无异。如果是一张明明是“海贼王”里面乔巴的图片，你的标签描述写成了“茄菲猫”，那么即便它确实是“海贼王”也不可能被检索到。 如果你输入“即有蓝天、又有白云、即有溪水、又有远山、即有美女、又有竹筏、即有细雨、又有花伞的图片”，那么很抱歉，你将得到一大堆跟你想要的主题毫无关联的图片结果。因为现阶段基于标签的图片搜索与文本搜索是一样的，只能做到如此，它无法像人一样能够智能地识别某个图片是否是用户想要的。 目前，是有一些音频搜索的系统可以试用，可以对用户哼唱的曲目进行检索，但查准率还有待提升。Google刚刚推出了一项新的图片搜索服务，用户可以上传照片，Google能够检索与之类似的图片。我试了一下，在当前技术背景下，能够推出这样一款商用的图片搜索引擎，已经可以说是相当不错了，虽然说查准率还不是太高。 对于静态图片已经比较困难了，更不用说是动态的视频影像了。当然科技是在发展，相信未来10年之内的多媒体搜索技术肯定能够呈现一个爆炸性的发展趋势，这些个难题的求解并不是没有可能的，让我们拭目以待吧。 （4）搜索结果排序依赖于人工智能，并没有考虑依靠真实用户的行为来引导和影响搜索结果的排序。 PageRank虽然说是以网页之间民主投票的方式产生了网页的重要性/级别，但这毕竟是一种死板、单一的排序方式，并没有考虑到实际用户参与的情况，能不能有一种人直接参与、以人类集体智慧做引导的排序方法呢？ 打个比方，当你检索“如何用U盘做引导盘”类似这种问问题的Query时，很多依据PageRank算出排名靠前的网页通常会让你无比失望，你点开一个一个又一个就是无法解决你的问题。然而，有可能你在刷了5、6页之后，偶然点开一个链接，却很轻易地解决了你的问题。这也就是说，按照PageRank排出来的结果未必真正是你所需要的。机器并不知道这个网页是否是真的好，它只是按照算法去一步步的执行而已，那么如何找到对网页质量更为智能的判定呢？ 我这里提供一种思路：话说如果为每个页面质量设置一个打分器，问题是否会清楚化呢？对于一个页面而言，所有注册用户都可以打分。当用户输入Query时，搜索结果页面左侧还是会显示按照传统PageRank算法的排列项目，右侧则会显示出与此Query相似输入所产生的网页中用户打分较高的项目。 比如，我们可以这样说：对于“如何用U盘做引导盘”的检索，有100万的用户觉得网页A很赞，98万的用户觉得网页B很赞。如果你是一个用户的话，这样的搜索推荐对你有没有吸引力？那可是100万的人跟你输入同样问题的人都觉得很赞的网页啊！你说你会不会去点着试一下，你说你是去点PageRank排出来的页面，还是去点别人推荐的？这样我们就做到了依靠集体用户的智慧产生更为精确的结果推荐。当然，页面质量的评判还需要考虑到当前用户所输入的关键词，同一个页面对于不同的关键词而言，其质量也应该是有所不同的。 这个原型想法跟Facebook的那个“赞”按钮很相似，具体怎样去推广运营还没有过多的思考，毕竟咱也只是一个小小的程序员而已。 二、浮现出的第三代搜索引擎 在看到pagerank的局限性以后，一些新兴的搜索公司开始尝试通过提供更精准、更个性化的搜索结果，目前关于第三代搜索引擎的商业化雏形或产品还是有一些的，我大概收集整理了一下： 最近在美国颇受用户赞誉的另一个搜索引擎swicki，也在个性化和精准搜索方面可圈可点。虽然swicki的大部分内容来自Google，但同样针是对关键词，swicki可以根据用户注册时的使用偏好、搜索习惯，提供出不同的搜索结果。通过对搜索结果的二次评判，swicki还可以逐步校正搜索结果列表。 在国内，除了类似bbmao这样的社会化搜索引擎开始提供自动分类、聚类、用户收藏等功能而崭露头角外，老牌搜索厂商雅虎中国，也在搜索算法和呈现方式上进行了诸多改进，不仅强化了对社区内容、blog等微内容的数据抓取，而且在个性化呈现、模糊搜索等方面也有较大举措。 一个例子是，此前一个月，雅虎中国、雅虎全球、阿里巴巴三方联合推出了一个具有智能模糊匹配功能的搜索引擎——雅虎Imatch。据称，该系统可以根据用户的搜索习惯和意图，智能匹配相关的搜索结果。 Clusty、bbmao等元搜索引擎的自动分类、聚类功能一出，即大受用户追捧，专家也认为其提供了比之Google更精准、细分的呈现方式，殊不知Clusty、bbmao等所提供的自动分类、聚类功能本身一点都不新鲜。早在10年前，英国的企业搜索巨擘Autonomy已经提供了同类乃至更智能的呈现方式。例如，Autonomy基于某种专有的模式匹配和概念搜索的算法，可以自动根据文本中的概念进行分类，自动标引，并基于用户兴趣自动匹配出个性化、多侧面的直接或隐含的相关档案。当用户在搜索框中键入某个关键词，出现的结果可能被系统自动分为10类（或更多类），若其中9类与用户的查询期望距离较大，用户就可以将接近的那个结果作为查询条件，进行第二次查询，直到找到最需要的搜索结果。而Google、百度等第二代搜索引擎则主要使用SVM和KNN算法进行分类，因为算法的先天缺陷，分类准确率仅能达到80%到85%。并且，如果分类树有变更，如增加、修改或删除某个分类节点，整颗分类树就必须重新学习。 三、搜索引擎的未来：情景搜索？ 关于第三代搜索，众多的创新者已经为我们勾勒出一个大致的轮廓，作为对第二代搜索的一种超越，未来的搜索引擎发展套路将趋向于个性化、社会化、垂直化、知识问答化的方向。而搜索引擎的核心技术将从传统的索引结构转向包含数据挖掘、机器学习、人工智能、模式识别和语义分析等领域。 虽然迄今为止，计算机还无法做到完全理解语言，但通过采用基于统计学、概率论和信息论的概念识别技术，可以将信息和信息之间建立相应的关联规则。用户可以用自然语言描述自己的问题，搜索引擎会自动判断用户查询条件所描述的概念，借助于自身的知识库寻找与用户搜索概念相关的文档。显然，这种语义搜索比传统关键词搜索更能精准定位用户的搜索意图。 试想，为什么Google要做Chrome？抛开云计算、云操作系统不谈，其一个非常重要的原因在于，利用浏览器可以获取、分析用户对网站的访问行为，获取用户的操作历史记录，从而能够对PageRank算法规则形成补充。因为PageRank它只考虑了网页之间的链接关系来确定网页的级别/重要性，并未考虑用户具体检索的内容，用户检索的意图以及用户当前所处的环境。这说明PageRank并不是完美的，它确实存在些很多不完善的地方。 早在2009年，腾讯就提出了“情境搜索”的概念，目前基于这一概念诞生出了一系列的产品，比如QQ “表情搜索”、QQ的划词搜索、QQ聊天过程中会主动地帮你提取关键词并标明，点击后会触发信息检索。基于情境搜索更能贴近用户需求，搜索对用户来说将变得无处不在，如影随形。 打个比方，你在腾讯在线平台上关注或与好友讨论电影《让子弹飞》，情境搜索将自动挖掘你最关心的内容：效果最好的影院在哪？如何预订电影票？网友的评价如何？哪些好友支持这部电影？预告片和音乐在哪里下载？此时你甚至无需打开浏览器输入检索词，一次又一次的甄选结果。情境搜索通过深入挖掘用户的“情境”需求，深入地了解到你需要什么，他不仅会把需要的信息直接推送到你面前，还会整合在线预定、购买等后续服务，甚至可以帮你找到志同道合的“搜友”。 跟传统搜索引擎相比，情境搜索是基于用户历史、用户偏好、用户环境，计算用户情境搜索需求，进而提供信息融合及主动推送的搜索模式，传统的搜索则需要用户依靠用户键入关键词，并主动的触发检索过程。然而，很多时候，用户他并不清楚自己到底想要什么，他的信息量与知识面是有限的，与海量搜索引擎所掌握的信息是不对称的，甚至用户根本无法用语言或关键词来表述他的需求。 情境搜索则打破了这一弊端，综合考虑用户背景、兴趣爱好以及环境的智能化搜索，通过对用户意图的深入理解，在用户使用互联网服务的各种场景下提供给用户的最贴切的搜索服务。情境搜索包含7个要素（6W&amp;1H），它强调“以人（Who）为本”，也就是以用户为中心，根据其搜索行为的时间（When）、地点（Where）、输入（What）、需求（Want）、习惯（How）、背景（Why）等因素，由情境计算得到最适合的搜索结果，再将这一结果通过用户的搜索情境直接呈现。 Facebook的兴起，被视为Google的颠覆者。颠覆Google，不是在于Facebook流量已经超越了Facebook，也不在于Facebook的技术有很多强，而是Facebook对人的了解远远地超过Google，这对Google未来的搜索战略是极大的障碍。所以Google觊觎社交网站，更多是出于一种自卫的思想。 意识到危机的Google目前也提到了“情境”这个词，具体技术是“情境发现”（Contextual Discovery），据说2011年会有应用上线。这显然和PageRank体系的核心有很大区别。决定搜索结果及排序的规则，多出了很多维度，比如上下文关联、浏览习惯、搜索者所处的“情境”等。微软公司一直在研究一种叫“BrowserRank”的算法，其思路也是引入更多维的衡量模型，比如将用户在网站停留的时间作为考量标准之一。实际上，类似这种BrowserRank算法在腾讯等公司也早已经在应用了。 由此可见，随着情境搜索时代的来临，“人”的因素将在搜索技术中占据越来越重要的比重。换句话说，搜索服务商对“人”了解越深刻，对其所处环境了解越透彻，则其越能在情境搜索中占据主动。显然，拥有最海量用户群、最长停留时间、最深的互动关系、最强用户粘度的社交网络平台将在这一领域占得先机，代表的SNS如Facebook、人人网、新浪微博、QQ在线平台等。 以腾讯为例，其定位在提供“在线生活”平台，打造互联网一条龙的在线服务的战略发展方向，从即时通信的QQ、腾讯微博、QQ空间到Web QQ再到正在推行的腾讯开放平台，这几乎是100%的网民覆盖，这些都将为情境计算提供基础的信息源，从而衍生出智能化的搜索服务。现阶段，腾讯野心是占领移动平台，对于拥有庞大客户端和多年移动而已的腾讯来说，也是相当的如鱼得水。 Google曾经希望用户“找到信息，然后快速离开”，这句话在现在来看是非常荒谬的，而Google明显也已经意识到这一点了。所以Google也在通过iGoogle等手段将用户留下来，为未来的搜索演变做储备。 值得注意的是，“情境搜索”毕竟还处于初级阶段，更像是一种“搜索概念”，从传统搜索到它的演变过程将是缓慢、递进的过程。但是情境搜索发展的大趋势是无法阻挡的了的，传统的搜索服务将在这场历史变革中被逐步取代，而像PageRank这样的算法能否经受的住下一个时代搜索科技创新大风大浪的挑战？能否在搜索引擎发展的历史长河中沉淀下来？我们还需要拭目以待~ 本文地址：http://xnerv.wang/further-disscusion-of-pagerank-4/ 转载自：深入探讨PageRank（四）：PageRank的危机及搜索引擎的未来","categories":[{"name":"搜索引擎","slug":"搜索引擎","permalink":"https://xnerv.wang/categories/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"搜索引擎","slug":"搜索引擎","permalink":"https://xnerv.wang/tags/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/"},{"name":"PageRank","slug":"PageRank","permalink":"https://xnerv.wang/tags/PageRank/"}]},{"title":"深入探讨PageRank（二）：PageRank原理剖析（转载）","slug":"further-disscusion-of-pagerank-2","date":"2011-06-20T21:55:00.000Z","updated":"2023-08-21T02:24:20.873Z","comments":true,"path":"further-disscusion-of-pagerank-2/","link":"","permalink":"https://xnerv.wang/further-disscusion-of-pagerank-2/","excerpt":"关于PageRank的基础知识简介请参见博文：《深入探讨PageRank（一）：PageRank算法原理入门》。 一、PageRank算法的简单举例 Google PageRank算法的思想精华在于：将一个网页级别/重要性的排序问题转化成了一个公共参与、以群体民主投票的方式求解的问题，网页之间的链接即被认为是投票行为。同时，各个站点投票的权重不同，重要的网站投票具有较大的分量，而该网站是否重要的标准还需要依照其PageRank值。这看似是一个矛盾的过程：即我们需要用PageRank值来计算PageRank值。 听起来有点不可思议，既像是递归，又像是迭代，似乎陷入了一个漩涡，Google的创始人佩奇和布林证明了这个过程最终收敛值与初始值无关。遗憾的是我一直都没有找到这个证明，甚至我把佩奇他们当年那篇论文找出来看也没有发现。 对于PageRank的收敛性，我们是可以找到反例的，这说明PageRank至少在某些情况下是不可能收敛的，或者说是收敛不完备的。在本文的第三部分，我们将PageRank的问题转化为了马尔可夫链的概率转移问题，其收敛性的证明也即转化为了马氏链的平稳分布是否存在的证明。我们先来看一个简单的例子：","text":"关于PageRank的基础知识简介请参见博文：《深入探讨PageRank（一）：PageRank算法原理入门》。 一、PageRank算法的简单举例 Google PageRank算法的思想精华在于：将一个网页级别/重要性的排序问题转化成了一个公共参与、以群体民主投票的方式求解的问题，网页之间的链接即被认为是投票行为。同时，各个站点投票的权重不同，重要的网站投票具有较大的分量，而该网站是否重要的标准还需要依照其PageRank值。这看似是一个矛盾的过程：即我们需要用PageRank值来计算PageRank值。 听起来有点不可思议，既像是递归，又像是迭代，似乎陷入了一个漩涡，Google的创始人佩奇和布林证明了这个过程最终收敛值与初始值无关。遗憾的是我一直都没有找到这个证明，甚至我把佩奇他们当年那篇论文找出来看也没有发现。 对于PageRank的收敛性，我们是可以找到反例的，这说明PageRank至少在某些情况下是不可能收敛的，或者说是收敛不完备的。在本文的第三部分，我们将PageRank的问题转化为了马尔可夫链的概率转移问题，其收敛性的证明也即转化为了马氏链的平稳分布是否存在的证明。我们先来看一个简单的例子： Google PageRank取值范围是010，为了叙述方便，我们使用01的区间作为度量，这并不会影响我们对PageRank原理的剖析，并且在初始化的时候，我们假设所有网站的PageRank的值是均匀分布的。这意味着，如果有N个网站，那么每个网站的PageRank初始值都是1/N。现在假设有4个网站A、B、C、D，则它们的初始PageRank都是0.25，它们的链接关系如下： 则初始值PR(A) = PR(B) = PR© = PR(D) = 0.25，又因为B、C、D都有指向A的链接，因此，它们每人都为A贡献了0.25的PageRank值，重新计算A的PageRank值为：PR(A) = PR(B) + PR© + PR(D) = 0.75，由于B、C和D并没有外部链接指向它们，因此PR(B)、PR©、PR(D)在这次计算中将被赋值为0。反复套用PageRank的计算公式，来看一下，这种情况下PageRank的收敛性，在第二次迭代之后，所有的PageRank值就都是0了： PageRank PR(A) PR(B) PR© PR(D) 初始值 0.25 0.25 0.25 0.25 第一次迭代后 0.75 0 0 0 第二次迭代后 0 0 0 0 我们来分析一下这个例子PageRank收敛的情况，由于没有网站链接到D，那么第一次迭代之后PR(D)=0，这将导致PR(B)=0，继而导致PR©=0和PR(A)=0。 现在来看第个例子，假设网站B还有C链接，网站D上有其他三个网站的链接。对于B而言的话，它把自己的总价值分散投给了A和C，各占一半的PageRank，即0.125，C和D的情况同理。即一个网站投票给其它网站PageRank的值，需要除以它所链接到的网站总数。此时PageRank的计算公式为： 1234PR(A) &#x3D; PR(B) &#x2F; 2 + PR(C) &#x2F; 1 + PR(D) &#x2F; 3PR(B) &#x3D; PR(D) &#x2F; 3PR(C) &#x3D; PR(B) &#x2F; 2 + PR(D) &#x2F; 3PR(D) &#x3D; 0 PageRank PR(A) PR(B) PR© PR(D) 初始值 0.25 0.25 0.25 0.25 第一次迭代后 0.4583 0.0833 0.2083 0 第二次迭代后 0.25 0 0.0417 0 第三次迭代后 0.417 0 0 0 第四次迭代后 0 0 0 0 PageRank值计算过程的一般步骤可以概括如下： 为每个网站设置一个初始的PageRank值。 第一次迭代：每个网站得到一个新的PageRank。 第二次迭代：用这组新的PageRank再按上述公式形成另一组新的PageRank。 …… 当然，我们最关心的问题是，如此迭代下去，这些PageRank的值最终会收敛吗？我们上述的两个例子都是收敛的，但是不是所有情况都是如此呢？而且，上述例子中，我们发现，一旦某个页面的外部链接数目为0的话，那必然将导致全部网页最终收敛值为0。 二、PageRank算法的“黑洞效应” 为了讨论收敛性的问题，我们暂时抛开具体的网站，把问题做一个抽象化的描述，我们可以把网页之间的关联关系理解为是若干张有向图，图与图之间是互不连通的，那我们只考虑每一部分的收敛性，并不会影响其他部分的收敛性。我们考虑把边权值当作网站所传递的PageRank值，则对于任意一个顶点而言，其出边的权值之和必为1。 一个很显然的结论是，如果连通图中有一个顶点的入度为0，则经过有限次迭代之后，该连通图内的所有顶点的PageRank均为0，形象的说，这个顶点就像一个黑洞一样，把整体的PageRank值慢慢地“吸收”了。由于它不对外贡献任何PR值，所以整体的PR总和是在不断地减少，直到最终收敛到0。我把它称之为：PageRank的“黑洞效应”。至于说Google是如何防止这种情况的发生，毕竟一个网站没有外链是完全有可能的，我也尚未找到确切的答案。不过网上道是有人给出了一种解决办法：即如果一个网站没有外链，那么就假定该连通图内其余所有的网点都是它的外链，这样我们就避免了整体PageRank值被吸收的现象。 当一个连通图内部每一个顶点入度均大于0时，不难看出，PR值在内部流通过程中，整体的PR值是守恒的。如果是存在一个顶点的入度为0呢？通过一次迭代，它的PR值就会变成0，而把它的那部分PR值贡献给了图中剩余的部分。所以，最终入度为0的顶点的PR值都将是0，而整体的PR仍然守恒。那么整体的PR值守恒就一定能够保证每个顶点的PR值最终会收敛吗？下面看一个简单的例子： 按照之前的迭代步骤，会得到一个迭代的结果表。这将是一个无限循环，且不会收敛的过程。 PageRank PR(A) PR(B) PR© PR(D) 初始值 0.25 0.25 0.25 0.25 第一次迭代后 0 0.375 0.25 0.375 第二次迭代后 0 0.375 0.375 0.25 第三次迭代后 0 0.25 0.375 0.375 第四次迭代后 0 0.375 0.25 0.375 第五次迭代后 0 … … … 其实，同样的问题我们还可以换一个角度来考虑，因为本质上有向图和矩阵是可以相互转化的，令A[i][j]表示从顶点i到达顶点j的概率，那么目力的矩阵表示就是： 12340 0.5 0 0.50 0 1 00 0 0 10 1 0 0 而我们所给定的初始向量是：(0.25 0.25 0.25 0.25)，做第一次迭代，就相当于用初始向量乘以上面的矩阵。第二次迭代就相当于第一次迭代的结果再乘以上面的矩阵……实际上，在随机过程理论中，上述矩阵被称为“转移概率矩阵”。这种离散状态按照离散时间的随机转移过程称为马氏链（马尔可夫链，Markov Chain）。设转移概率矩阵为P，若存在正整数N，使得P^N&gt;0（每个元素大于0），这种链被称作正则链，它存在唯一的极限状态概率，并且与初始状态无关。 在这里，我们仅仅是非常简单地讨论了一下PageRank的原理，这与Google PageRank的实际算法实现相当甚远。域名数据、内容质量、用户数据、建站时间等都有可能被考虑进去，从而形成一个完善的算法。 当然，最让人惊叹的是，Google的PageRank能够应对互联网所产生的如此海量的网页信息和实时的变化，并能够在有限的时间内计算出所有网站的PageRank！这里面到底蕴涵着什么样的奥秘，我也会继续地追寻下去！ 三、PageRank算法的马尔科夫过程分析 从第二节的陈述中我们知道，事实上，PageRank值在转移过程中变化规律是完全可以用马尔科夫的状态转移来进行表征的，两者本质属于同一个问题。则当PageRank值收敛时，即为马尔可科夫链达到平衡分布。推荐大家去读《随机过程》的教材，这里不在详细地讨论马氏链的内容，只给出相应的结论。 为了形象说明马氏链，这里举一个例子。假设一{A, B, C}为马氏链，其转移概率矩阵如下所示： 1230.7 0.1 0.20.1 0.8 0.10.05 0.05 0.9 因为该马氏链是不可约的非周期的有限状态，平稳分布存在，则我们要求其平衡分布为： 1234X &#x3D; 0.7X + 0.1Y + 0.05ZY &#x3D; 0.1X + 0.8Y + 0.05ZZ &#x3D; 0.2X + 0.1Y + 0.9ZX + Y + Z &#x3D; 1 解得上述方程组的平稳分布为：X = 0.1765，Y = 0.2353，Z = 0.5882。 既然，说我们把PageRank收敛性问题转化为了求马尔可夫链的平稳分布的问题，那么我们就可以从马氏链的角度来分析问题。因此，对于PageRank的收敛性问题的证明也就迎刃而解了，只需要证明马氏链在什么情况下才会出现平稳分布即可。我们可以知道马氏链有三个推论： 推论1. 有限状态的不可约非周期马尔可夫链必存在平稳分布。 推论2. 若不可约马尔可夫链的所有状态是非常返或零常返的，则不存在平稳分布。 推论3. 若{Xi}是不可约的非周期马氏链的平稳分布，则lim(n→∞)Pj(n) = Xi。 上面的三个推论看不懂不要紧，找本《随机过程》的书就明白了，这里不再详细讨论了。既然问题得以转化，那么我们还计算一个实例，看看PageRank是如何工作的。假设这里有相互链接关系的7个HTML网页，并且HTML网页之间的链接关系闭合于这1~7个网页中，也即是说，除了这些网页之外，没有任何链接的出入。 那么我们可以很容易地将这个链接关系使用数学的方式表示出来。首先，分析链接的关系，列举出各个链接源的ID及其所链接的目标ID。 链接源I D 链接目标 ID 12345671 2,3 ,4,5, 72 13 1,24 2,3,55 1,3,4,66 1,57 5 使用邻接矩阵的形式表述网页之间的链接关系，A[i][j]=1表示从i到j有链接，否则表示无链接，A为7*7的矩阵。 123456789A &#x3D; [ 0, 1, 1, 1, 1, 0, 1; 1, 0, 0, 0, 0, 0, 0; 1, 1, 0, 0, 0, 0, 0; 0, 1, 1, 0, 1, 0, 0; 1, 0, 1, 1, 0, 1, 0; 1, 0, 0, 0, 1, 0, 0; 0, 0, 0, 0, 1, 0, 0; ] 我们现假设，每个网页初始的PageRank均为1，则会形成一个初始的PageRank转移矩阵。 123456789A &#x3D; [0, 1&#x2F;5, 1&#x2F;5, 1&#x2F;5, 1&#x2F;5, 0, 1&#x2F;5;1, 0, 0, 0, 0, 0, 0;1&#x2F;2, 1&#x2F;2, 0, 0, 0, 0, 0;0, 1&#x2F;3, 1&#x2F;3, 0, 1&#x2F;3, 0, 0;1&#x2F;4, 0, 1&#x2F;4, 1&#x2F;4, 0, 1&#x2F;4, 0;1&#x2F;2, 0, 0, 0, 1&#x2F;2, 0, 0;0, 0, 0, 0, 1, 0, 0;] 这样的话，我们就可以按照求马氏链平稳分布的方式，求得PageRank收敛结果，方程组为： 12345678X1 &#x3D; X2 + X3 &#x2F; 2 + X5 &#x2F; 4 + X6 &#x2F; 2X2 &#x3D; X1 &#x2F; 5 + X3 &#x2F; 2 + X4 &#x2F; 3X3 &#x3D; X1 &#x2F; 5 + X4 &#x2F; 3 + X5 &#x2F; 4X4 &#x3D; X1 &#x2F; 5 + X5 &#x2F; 4X5 &#x3D; X1 &#x2F; 5 + X4 &#x2F; 3 + X6 &#x2F; 2 + X7X6 &#x3D; X5 &#x2F; 4X7 &#x3D; X1 &#x2F; 5X1 + X2 + X3 + X4 + X5 + X6 + x7 &#x3D; 1 解这个方程，最终我们得到每个网页的PageRank收敛值分别为： X1 = 0.303514，X2 = 0.38286，X3 = 0.32396，X4 = 0.24297，X5 = 0.41231，X6 = 0.10308，X7 = 0.13989。 将PageRank的评价按顺序排列，小数点3位四舍五入，可以得到下表： 12345678名次 PageRank 文件ID 发出链接ID 被链接ID 1 0.304 1 2,3,4,5,7 2,3,5,6 2 0.179 5 1,3,4,6 1,4,6,7 3 0.166 2 1 1,3,4 4 0.141 3 1,2 1,4,5 5 0.105 4 2,3,5 1,5 6 0.061 7 5 1 7 0.045 6 1,5 5 让我们详细地看一下。ID=1 的文件的 PageRank 是0.304，占据全体的三分之一，成为了第1位。特别需要说明的是，起到相当大效果的是从排在第3位的 ID=2 页面中得到了所有的 PageRank（0.166）数。ID=2页面有从3个地方过来的反向链接，而只有面向 ID=1页面的一个链接，因此（面向ID=1页面的）链接就得到了所有的 PageRank 数。不过，就因为 ID=1页面是正向链接和反向链接最多的页面，也可以理解它是最受欢迎的页面吧。 依据上图的PageRank值，我们实际地试着计算一下PageRank的收支，只要将自各页的流入量单纯相加即可。譬如 ID=1 的流入量为： 1ID&#x3D;1的流入量＝(ID&#x3D;2发出的Rank)+(ID&#x3D;3发出的Rank) + (ID&#x3D;5发出的Rank) + (ID&#x3D;6发出的Rank) &#x3D; 0.166 + 0.141 &#x2F; 2 + 0.179 &#x2F; 4 + 0.045 &#x2F; 2 &#x3D; 0.30375 在误差范围内PageRank的收支相符合。其他页面ID的情况也一样。以上的 PageRank 推移图正表示了这个收支。沿着各自的链接发出的PageRank等于此页面原有的PageRank除以发出链接数的值，而且和各自的页面的PageRank收支相平衡。 不过，这样绝妙均衡的本身，对理解线形代数的人来说当然不会是让人惊讶的事情。因为这正是“特性值和固有矢量的性质”，总之这样被选的数值的组就是固有矢量。以上就是 PageRank 的基本原理。 Google 做的就是大规模地处理这样的非常特性值问题。 PS：LZ系保研，由于没有参加考研，像《线性代数》、《随机过程》好多年没摸过了，很多知识都有所遗忘，所以写的不深入。本文的一些内容是参考了别人的博客，自己又加入了些新元素，算是做一次探讨。当然，接下来LZ会开始复习一下相关的数学知识，后续会重写本文，以便于让本文显得更为Strong~ 参考的相关博客地址： http://www.charlesgao.com/?p=157 http://www.kreny.com/pagerank_cn.htm 本文地址：http://xnerv.wang/further-disscusion-of-pagerank-2/ 转载自：深入探讨PageRank（二）：PageRank原理剖析","categories":[{"name":"搜索引擎","slug":"搜索引擎","permalink":"https://xnerv.wang/categories/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"搜索引擎","slug":"搜索引擎","permalink":"https://xnerv.wang/tags/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/"},{"name":"PageRank","slug":"PageRank","permalink":"https://xnerv.wang/tags/PageRank/"}]},{"title":"深入探讨PageRank（一）：PageRank算法原理入门（转载）","slug":"further-disscusion-of-pagerank-1","date":"2011-06-19T19:22:00.000Z","updated":"2023-08-21T02:24:20.851Z","comments":true,"path":"further-disscusion-of-pagerank-1/","link":"","permalink":"https://xnerv.wang/further-disscusion-of-pagerank-1/","excerpt":"一、PageRank简介 大名鼎鼎的PageRank算法是Google排名运算法则（排名公式）的一个非常重要的组成部分，其用于衡量一个网站好坏的标准。在揉合了诸如Title、Keywords标识等所有其它因素之后，Google利用PageRank来调整网页的排名，使得“等级/重要性”的网页会相对排在前面。简单来说，Google通过下述几个步骤来实现网页在其搜索结果页面中排名： 找到所有与搜索关键词匹配的网页 根据页面因素如标题、关键词密度等排列等级 计算导入链接的锚文本中关键词 通过PageRank得分调整网站排名结果 事实上，真正的网站的排名过程并非这么简单，我们会在后面进行详细深入阐述。","text":"一、PageRank简介 大名鼎鼎的PageRank算法是Google排名运算法则（排名公式）的一个非常重要的组成部分，其用于衡量一个网站好坏的标准。在揉合了诸如Title、Keywords标识等所有其它因素之后，Google利用PageRank来调整网页的排名，使得“等级/重要性”的网页会相对排在前面。简单来说，Google通过下述几个步骤来实现网页在其搜索结果页面中排名： 找到所有与搜索关键词匹配的网页 根据页面因素如标题、关键词密度等排列等级 计算导入链接的锚文本中关键词 通过PageRank得分调整网站排名结果 事实上，真正的网站的排名过程并非这么简单，我们会在后面进行详细深入阐述。 PageRank于2001年9月被授予美国专利，专利人是Google创始人之一的拉里.佩奇（Larry Page）。所以，PageRank里面的Page并不是指网页，而是指佩奇PageRank对于网页重要性的级别分为110级，10级为满级。PR值越高说明该网页越受欢迎，也即越重要。一个PR值为1的网站表明该网站不具备流行度，而PR值为7~10的网站则表明该网站是非常受欢迎的，或者说极其重要。一般PR值达到4，就算是一相当不错的网站了。Google把自己网站的PR值设置为10~类似里氏震级，PageRank级别并不是线性增长的，而是按照一种指数刻度，打个比方PageRank4比PageRank3虽然只是高了一级，但却在影响力上高上6~7倍，因此，一个PageRank5的网页和一个PageRank8的网页之间差距会比你可能认为的要大的多。 在讨论之前，先介绍两个概念：导入链接，又称逆向链接，是指链至你网站的站点，也就是我们一般所说的外部链接。而当你链至另外一个站点，那么这个站点就是你的导出链接，即你向其他网站提供本站的链接。 PageRank的思路很简单，打个比方：如何判断一篇论文的价值，即被其他论文引述的次数越多就越重要，如果被权威的论文引用，那么该论文也很重要。PageRank就是借鉴于这一思路，根据网站的外部链接和内部链接的数量和质量来衡量这个网站的价值，相当于每个到该页面的链接都是对该页面的一次投票，被链接的越多，就意味着被其他网站投票越多。这个就是所谓的链接流行度----衡量多少人愿意将他们的网站和你的网站挂钩。 搜索引擎网站排名算法中各排名因子的重要性取决于它们所提供信息的质量。但如果排名因子具有易操纵性，则往往会被一些网站管理员利用来实现不良竞争。例如初引入的排名因子之一----关键词元标识（Meta Keywords），是由于理论上它可以很好地概括反映一个页面的内容，但后来却由于一些网站管理员的恶意操纵而不得不黯然退出。 PageRank最初推出时针对的只是链接的数量，PageRank值较高的页面排名往往要比PageRank值较低的页面高，这导致了人们对于链接引用的着魔。在过去几年间，整个SEO社区人们忙于争夺、交换甚至销售链接，它是人们关注的焦点，所以被一些网站管理员钻了空子，利用链接工厂和访问簿大量低劣外部链接轻而易举地达到了自己的目的。Google意识到这个问题之后，便在系统中融合了对链接质量分析，开始放弃某些类型的链接，并对发现作弊的站点进行封杀，从而不但有效地打击了这种作法，而且保证了结果的和精准度。比如，被人们广泛接受的一条规定，来自缺乏内容的“link farm”（链接工厂）网站的链接将不会提供页面的PageRank，从PageRank较高的页面得到的链接但是内容不相差，比如说某个流行音乐网站链接到一个汽车网站就不会提供页面的PageRank。Google选择降低了对PageRank的更新频率，以便不鼓励人们不断地对其进行监测。 PageRank一般一年更新4次，所以刚上线不久的新网站是不可能获得PR值的。你的网站很有可能在相当长的时间内是看不到PR值的变化的，特别是一些新的网站。PR值暂时没有，这不是什么不好的事情，耐心等待就好~ 那么，我们如何知道一个网页的PageRank值呢？可以从http://toolbar.google.com上下载安装Google工具栏，这样就能显示所浏览网页的PageRank值了。若不能显示，可检查所安装版本号，需将老版本完全卸载，重启机器后安装最新版本即可 为你的网站获得外部的链接是一件好事，但是无视其他SEO领域的工作而进行急迫的链接建设就是在浪费时间，要时刻保持一个整体思路并记住以下几点： Google的排名算法并不是完全基于外部链接的。 高PageRank并不能保证Google的高排名。 PageRank值更新的比较慢，今天看到的PageRank的值可能是三个月前的值。 一般来说，网站排名因素包括网页的标题（META TITLE），网页正文中的关键词密度，锚文本（也叫链接文本，指链接或超链的文本内容）和PageRank所决定的。请记住：单靠PageRank是无法使你获得比较理想的网站排名的。PageRank只是网站排名算法中的一个乘积因子，若你网站的其它排名因子的得分是0，就算你的PageRank值是1个亿都木有用，最后得分还是0。但这并不是说PageRank就毫无价值，而是在什么情况下PageRank能够完全发挥其功力。 如果在Google上进行广泛搜索，看起来好象有几千个结果，但实际显示最多前1000项结果。例如对“car rental”，显示搜索结果为5,110,000，但实际显示结果只有826个。而且用时只有0.81秒。试想一下，0.84秒的时间就可以计算这五百万搜索结果的每个排名因子得分，然后给出最终我们所看到的网站排名结果吗？ 答案就在于：搜索引擎选取与查询条件最相关的那些网页形成一个子集来加快搜索的速度。例如：假设子集中包含2000个元素，搜索引擎所做的就是使用排名因子中的两到三个因素对整个数据库进行查询，找到针对这两三个排名因子得分较高的前2000个网页。(请记住，虽然可能有五百多万搜索结果，但最终实际显示的1000项搜索结果却是从这个2000页的子集中提炼出来的。) 然后搜索引擎再把所有排名因子整合进这2000项搜索结果组成的子集中并进行相应的网站排名。由于按相性进行排序，子集中越靠后的搜索结果(不是指网页)相关性(质量)也就越低，所以搜索引擎只向用户显示与查询条件最相关的前1000项搜索结果。 请注意，在搜索引擎生成这2000项网页的子集中我们强调了“相关性”这个词。即搜索引擎找寻的是与查询条件有共同主题的网页。如果这时候我们把PageRank考虑进去，就很可能得到一些PageRank很高但主题只是略微相关的一些搜索结果。显然这有违搜索引擎为用户提供最为相关和精准的搜索结果的原则。 一旦理解了为什么会如此，就说明了为什么你应当首先努力在“页面”因子和锚文本上下足工夫，最后才是PageRank。所以关键在于：你必须首先在页面因素和/或锚文本上下足工夫，使这些排名因子能够获得足够的得分，从而使你的网站能够按目标关键词跻身于这2,000项搜索结果的子集中，否则PageRank再高也与事无补。 因此，我们不鼓励刻意地去追求PageRank，因为决定排名的因素可以有上百种。尽管如此，PageRank还是一个用来了解Google对你的网站页面如何评价的相当好的指标，建议网站设计者要充分认识PageRank在Google判断网站质量的重要作用，从设计前的考虑到后期网站更新都要给予PageRank足够的分析，很好的利用。我们要将PageRank看作一种业余爱好而不是一种信仰。 二、PageRank原理 通过对由超过50000万个变量和20亿个词汇组成的方程进行计算，PageRank能够对网页的重要性做出客观评价。PageRank并不计算直接链接的数量，而是将从网页A指向网页B的链接解释为由网页A对网页B所投的一票。这样，PageRank会根据网页B所收到的投票数量来评估该网页的重要性。此外，PageRank还会评估每个投票网页的重要性，因为某些重要网页的投票被认为具有较高的价值，这样，它所链接的网页就能获得较高的价值。这就是PageRank的核心思想，当然PageRank算法的实际实现上要复杂很多。 但是问题又来了，计算其他网页PageRank的值需要用到网页本身的PageRank值，而其他网页的PageRank值反过来又影响本网页的PageRank的值，这不就成了一个先有鸡还是先有蛋的问题了吗？Google的两个创始人拉里.佩奇（Larry Page）和谢耳盖.布林（Sergey Brin）把这个问题变成一个二维矩阵相乘的问题，并且用迭代的方法解决了这个问题。他们先假定所有网页的排名是相同的，并且根据这个初始值，算出各个网页的第一次迭代的排名，然后再根据第一次迭代排名算出第二次的排名。他们两人从理论上证明了不论初始值如何选取，这种算法都将能够保证了网页排名的估计值能够收敛到它们就有的真实值。值得一提的是，这种算法的执行是完全没有任何人工干预的。 理论问题解决了，但在实际的应用中，互联网上网页的数量是巨大的，上面提到的二维矩阵从理论上讲有网页数目平方之多个元素。如果我们假定有10亿个网页，那么这个矩阵就要有100亿亿个元素。这样大的矩阵相乘，计算量是非常之大。怎么办？怎么办？Larry和Sergey两利用稀疏矩阵计算的技巧，大大简化了计算量，并实现了这个网页排名算法。今天Google的工程师把这个算法移植移植到并行的计算机中，进一步缩短了计算的时间，使得网页的周期比以前短了许多。 网页排名的高明之处在于它把整个互联网当作了一个整体对等。它无意识中符合了系统论的观点。相比之下，之前的信息检索大多把每一个网页当作独立的个体对等，很多人当初只注意了网页的内容和查询语句的相差性，忽略了网页之间的关联。 今天，Google搜索引擎比最初复杂、完善了许多。但是网页的排名在Google所有算法中依然是到头重要的。在学术界，这个算法被公认为是文献检索中最大的贡献之一，并且被很多大学引入了信息检索课程的教程。 在计算网站排名时，PageRank会将网站的外部链接数考虑进去。并不能说一个网站的外部链接数越多其PR值就越高，如果这样的话，一个网站尽可能地获得最多的外部链接就OK了，这种想法是错误的。Google对一个网站上的外部链接数的重视程度并不意味着你因此可以不求策略与任何网站建立连接。这是因为Google并不是简单地由计算网站的外部链接数来决定其等级的。Google的PageRank系统不单考虑一个网站的外部链接数量，也会考虑其质量，这个问题看来很复杂。 首先来解释一下阻尼系数：当你投票或链接到另外一个站点时所获得的实际PR分值。阻尼系数一般是0.85。当然比起你网站的实际PR值，它就显得微不足道了。具体的PR值计算公式为： 1PR(A) &#x3D; (1 - d) + d (PR(t1) &#x2F; C(t1) + … + PR(tn) &#x2F; C(tn)) 其中，PR(A)表示从一个外部链接站点t1上，依据PageRank系统给你的网站所增加的PR值。PR(t1)表示该外部链接网站本身的PR值，C(t1)表示该外部链接站点所拥有的外部链接数量。大家要谨记：一个网站的投票权值只有该网站PR值的0.85倍。 必须要注意的一点是：PageRank不单考虑一个网站的外部链接质量，还需要考虑其数量。打个比方：对于网站X而言，网站Y是它唯一的一个外部链接，那么Google就相信网站X将网站Y视为它最好的一个外部链接，从而给网站Y更多的分值。可是，如果网站X上已经存在了49个外部链接，那么Google就相信网站X只是将网站Y视为它第50个好网站。因而一个网站上外部链接的数量越多，它所能够提供的PR值则会越低。如果一个PR值大于等于6的外部链接站点，可显著提升你的PR值。但如果这个外部链接站点已经有100个其它的外部链接时，那么你能够得到的PR值就几乎为0了。同样，如果一个外部链接站点PR值为2，但你却是它唯一一个外部链接，那么你所能够获得的PR值要远远大于那个PR值为6，外部链接数为100的网站。 影响Google PageRank的几个重要因素： 与PR高的网站做链接 内容质量高的网站链接 加入搜索引擎分类目录 加入免费开源目录 你的链接出现在流量大、知名度高、频繁更新的重要网站上 Google对PDF格式的文件比较看重 域名和Title标题出现关键词与Meta标签等 反向链接数量和反向链接等级 Google抓取你网站的页面数量 导出链接数量 PageRank和其他排名因子之间存在不同：网页Title标识仅能被列出一次；正文中出现的关键词连续的重复只会降低关键词的重要性，重要的是接近度；锚文本加权值极高，但存在上限，超过上限的锚文本信息将被忽略或降低权值；PageRank潜质无穷，没有上限的限制，但需要大量工作。除了PageRank外，其它排名因子都存在一个阙值，也叫临界值或差值。即当增长到一定值时，因子的重要性反而开始慢慢降低，则该值就是非PageRank因子的阙值。 设阙值为1000，如果网页A和B是我们对某一查询条件的其中两个查询结果，且A的总分数(包括页面因子得分和PageRank得分)是900，B是500，则显然A会排在B的前面。但由于A和B的分数均低于我们上面假设的非PageRank因子阙值，因而在不改变PageRank的情况下，我们可以通过对B页进行精心的页面优化使页面因子分数得到提高来使其排名超过A。但如果A的总得分升至1100分，则B若还只是一味优化页面因子是远远不够的。在这种情况下，提升PageRank就成为首要任务了。 一般说来，Google的查询结果页中既可能包含一些分数超过阙值的网页，也可能包含一些分数低于阙值的网页。所以为了提高竞争能力，必须在阙值范围内尽可能提高页面的搜索引擎排名得分，否则会降低页面的竞争力。“页面因子”是接近和达到阙值最迅捷的方式，它与PageRank的结合使用才是提升网站排名得分的最佳优化策略。阙值解释了搜索引擎商所遵循的原则和不同的实施途径，同时亦阐述了为什么会产生关于PageRank的一些误解。我们可以把这两种策略当成两个人A和B。 A认为“PageRank”并不重要。他们已有数年网页优化经验并知道如何完美地利用“页面因素”来达到优化的目的。他们亦理解基本的锚文本，但对PageRank得分毫不在意。结果如何呢？由于最大化地使用了“页面因子”，从而使A迅速达到“非PageRank因子的阙值”。所以通过精心选择关键词可使他们获得较好的网站排名。而且只要网站内容比较好，随着时间推移总会有排名高的站点链接，涓涓细流汇成河。A最后亦得到了PageRank得分，并籍此巩固了排名。 B认为“PageRank”十分重要。他掌握了很多关于提升PageRank得分的信息，并为提高该得分下足了工夫。结果又如何呢？B的做法和A相反，但A在非PageRank因子上下工夫，结果却得到了PageRank得分。而B在PageRank因子上下工夫，结果却得到非PageRank因子得分。究其原因，就是由于提高PageRank得分需要外部链接，链接又具有锚文本，从而通过精心挑选外部链接的锚文本，B自发提高了其非PageRank因子的得分，从而赢得了较高的PageRank得分。虽然这只是两个极端，但我们可以利用它们来推知这两种途径各自的优缺点： A：忽略PageRank 网站排名在短期内就可得到提升，自我生成链接节省了工作量，需投入大量工作维持网站排名，对新竞争者的应变速度较慢。 B：忽略页面排名因子，可获得可靠网站排名，并可在需要时轻松修改页面因素使排名迅速提升，极可能从非搜索类引擎来源上获得更高访问量，网站排名提升较慢，操作难度较大，容易为SPAM过滤程序所制。 事实上，我们前面说过，最终排名得分=所有非PageRank因子实际得分x实际PageRank得分。亦即二者相辅相成，再加上随着网上营销方式的发展壮大，关键词的竞争也变的愈来愈激烈，这种情况下只靠非PageRank因子得到好排名显然是不可能的。而且非PageRank因子存在着阙值的局限性。同时，对于竞争性极高的关键词，还存在着PageRank下限的问题。也就是说，除非网站的PageRank得分超过这个下限标准，否则网站排名很难上去。PageRank的下限由关键词的竞争度所决定。竞争性一般的关键词PageRank下限也不高，而对竞争较为激烈的关键词来说，它所要求的PageRank下限相应就要高。而PageRank得分的提升又非常有难，这时候非PageRank因子就变的非常重要了。 综上所述：我们需要充分发挥各排名因子的优势来赢取理想的综合排名得分。同时关键词（竞争度适宜）的精心选择亦变的非常重要，它可以节省大量的支出。 三、总结 关于PageRank，最权威的发言人自然还是Google。虽然Google不会也不可能提供相关的技术信息，但我们亦可从中窥得一斑： Chris：PageRank的命名是基于“Page”，还是和某个创始人有关？ Google：PageRank是以Google的联合创始人兼总裁Larry Page的名字命名的。 Chris：Google是否把PageRank视做显著区别于其它搜索引擎的一个特性？ Google：PageRank是一种能够使Google在搜索速度和搜索结果的相关性上区别于其它搜索引擎的技术。不唯如此，在排名公式中Google还使用了100种其它的算法。 Chris：Google是否认为引入PageRank可以显著提高搜索结果的质量？以后是否仍将继续使用PageRank？ Google：由于PageRank使用了量化方法来分析链接，所以它仍将是决定Google搜索结果页排名的一个重要因素。 Chris：您认为Google工具栏上的PageRank的信息对普通用户/网站管理员/搜索引擎优化专家来说各有什么意义？ Google：Google工具栏上所提供的PageRank信息仅作为一种网站评估信息使用。用户们会觉得它很有趣，网站管理员一般用它来衡量网站性能。不过，由于PageRank只是一个大体评估，所以对搜索引擎专家的价值并不大。 Chris：常有网站试图通过“链接工厂”和访客簿的手段达到提升PageRank的目的。对这样的网站Google有什么举措？ Google：Google的工程师会经常更新Google的排名算法以防止对Google排名的恶意操纵。 选择导入链接时应首先考虑对方网站的内容如何，然后再考察其导出链接的数量进行决策。而在建立本站的导出链接时则应尽量使自己网站的PageRank维持在最大回馈和最小流失上。应确保合理的网站设计结构和内部联接方式。网站的结构和内部联接方式也会对PageRank产生影响，可利用其特性有效进行PagaRank在网站内部页面的再分布及尽可能保持网站整体的PageRank。网站的PageRank的提升应与该网站的访问者体验息息相关。即使获得再高的PageRank，如果没有客户访问，一样毫无价值。所以网站的内容始终是提升PageRank最关键的因素之一。 本文地址：http://xnerv.wang/further-disscusion-of-pagerank-1/ 转载自：深入探讨PageRank（一）：PageRank算法原理入门","categories":[{"name":"搜索引擎","slug":"搜索引擎","permalink":"https://xnerv.wang/categories/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"搜索引擎","slug":"搜索引擎","permalink":"https://xnerv.wang/tags/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/"},{"name":"PageRank","slug":"PageRank","permalink":"https://xnerv.wang/tags/PageRank/"}]},{"title":"Spin Lock in C++（转载）","slug":"spin-lock-in-cpp","date":"2011-05-09T07:00:00.000Z","updated":"2023-08-21T02:24:21.721Z","comments":true,"path":"spin-lock-in-cpp/","link":"","permalink":"https://xnerv.wang/spin-lock-in-cpp/","excerpt":"","text":"Download source code - 8.28 KB Introduction If we use common synchronization primitives like mutexes and critical sections, then the following sequence of events occur between two threads that are looking to acquire a lock: Thread 1 acquires lock L and executes. T2 tries to acquire lock L, but it’s already held and therefore blocks incurring a context switch. T1 releases the lock L. This signals T2 and at lower level, this involves some sort of kernel transition. T2 wakes up and acquires the lock L incurring another context switch. So there are always at least two context switches when primitive synchronization objects are used. A spin lock can get away with expensive context switches and kernel transition. Most modern hardware supports atomic instructions and one of them is called ‘compare and swap’ (CAS). On Win32 systems, they are called interlocked operations. Using these interlocked functions, an application can compare and store a value in an atomic uninterruptible operation. With interlocked functions, it is possible to achieve lock freedom to save expensive context switches and kernel transitions which can be a bottleneck in a low latency application. On a multiprocessor machine, a spin lock (a kind of busy waiting) can avoid both of the above issues to save thousands of CPU cycles in context switches. However, the downside of using spin locks is that they become wasteful if held for a longer period of time, in which case they can prevent other threads from acquiring the lock and progressing. The implementation shown in this article is an effort to develop a general purpose spin lock. Algorithm A typical (or basic) spin lock acquire and release function would look something like below: 12345678910111213141516171819202122232425// acquire the lockclass Lock&#123; volatile int dest = 0; int exchange = 100; int compare = 0; void acquire() &#123; While(true) &#123; if(interlockedCompareExchange(&amp;dest, exchange, compare) == 0) &#123; // lock acquired break; &#125; &#125; &#125; // release the lock Void release() &#123; // lock released dest = 0; &#125; &#125;;....... Here, thread T1 acquires the lock by calling the function acquire(). In this case, the value of dest would become 100. When thread T2 tries to acquire the lock, it will loop continuously (a.k.a. busy waiting) as the values of dest and compare are different and therefore the function InterlockedCompareExchange will fail. When T1 calls release(), it sets the value of dest to 0 and therefore allows T2 to acquire the lock. Because only those threads that acquire() will call release(), mutual exclusion is guaranteed. Above is a simple implementation of a spin lock. However, this implementation alone is not production fit because spinning consumes CPU cycles without doing any useful work, meaning that the thread spinning will still be scheduled on the processor until it is pre-empted. Another downside of spinning is that it will continuously access memory to re-evaluate the value of dest in the function Interlockedxxx and this also puts the pressure on bus communication. On a single processor machine, spin wait would be a total waste of CPU as another thread T2 wouldn’t even get scheduled until the spinning thread is switched by the kernel. So far this implementation isn’t good enough. A general purpose spin lock requires a bit more work in terms of falling back to true waiting in a worst case scenario when it spins for a longer period. Here are some of the points which must be considered: Yield Processor The Win32 function YieldProcessor() emits a ‘no operation’ instruction on processors. This makes the processor aware that the code is currently performing spin waits and will make the processor available to other logical processors in a hyper threading enabled processor so that the other logical processors can make progress. Switch to Another Thread Sometimes it is useful to force a context switch when a spinning thread has already consumed enough time spinning equivalent to its thread time slice allocated by the kernel. Here, it makes good sense to allow another thread to do useful work instead. The function SwitchToThread() relinquishes the calling thread’s time slice and runs another thread in the ready state. It returns true when a switch occurs, otherwise false. Sleeping SwitchToThread() may not consider all threads on the system for execution, therefore it may be wise to sometimes call Sleep() or Sleepex(). Calling Sleep() with an argument of 0 is a good approach as it does not result in a context switch if there are no threads of equal priority in the ready state. Sleep(0) will result in a context switch if a higher priority thread is in ready state. Other Considerations A pure spin lock is only good enough when the lock is held for a very short period of time. Here the critical region may have not more than 10 instructions and practically even simple memory allocation or virtual calls or file I/O can take more than 10 instructions. Secondly, as mentioned above, it would wasteful to use spin locks when an application runs on a single processor. Sample Project and Implementation The sample project in C++ consists of a spin lock implementation considering the points stated above. It also has an implementation of Stack, Queue, and a thin Producer-Consumer class. I’ll only focus on then Spin Lock implementation here as the rest of it is easy to follow. The file SpinLock.h defines these constants: YIELD_ITERATION set to 30 - What this means is that the thread spinning will spin for 30 iterations waiting for the lock to acquire before it calls sleep(0) to give an opportunity to other threads to progress. MAX_SLEEP_ITERATION set to 40 - This means when the total iteration (or spin) count reaches 40, then it would force a context switch using the function SwitchToThread() in case another thread is in ready state. The struct tSpinLock acts as a lock object which is declared in the class whose objects are being synchronized. This object is then passed in the constructor to the object of tScopedLock which initializes (references) the lock object passed to it. The tScopedLock() constructor locks the object using the member function of the class tSpinWait. The destructor ~tScopedLock() releases the lock. The Lock() function in the class tSpinWait has got a nested while loop. This is done on purpose. So if a thread is spinning to acquire the lock, it wouldn’t call interlockedxxx() with every iteration, rather it would be looping in the inner while loop. This hack avoids the system memory bus being overly busy due to continuous calls to the interlockedxx function. 1234567891011121314151617// spin wait to acquirewhile(LockObj.dest != LockObj.compare) &#123; if(HasThreasholdReached()) &#123; if(m_iterations + YIELD_ITERATION &gt;= MAX_SLEEP_ITERATION) Sleep(0); if(m_iterations &gt;= YIELD_ITERATION &amp;&amp; m_iterations &lt; MAX_SLEEP_ITERATION) SwitchToThread(); &#125; // Yield processor on multi-processor but if on single processor // then give other thread the CPU m_iterations++; if(Helper::GetNumberOfProcessors() &gt; 1) &#123; YieldProcessor(/*no op*/); &#125; else &#123; SwitchToThread(); &#125;&#125; The inner while loop just compares the value of dest and compare and if they are not equal, then it tries to acquire them using interlockedxxx. Depending on the iteration count, the thread is either put to sleep or switched. When the application is running on a single CPU, then it always forces a context switch. Test Results I tested the performance of this Spin Lock implementation by inserting 10000 integers into a queue from multiple threads (each thread inserting 10000 integers into the queue). I then replaced SpinLock with a Critical Section synchronization primitive in the code and ran the same tests. I ran all the tests on an Intel Core DUO CPU T9600 @ 2.80 GHz. The x-axis is the number of threads and y-axis is the time taken in milliseconds. Both synchronization methods (spinlock and CS) showed close performance when the number of threads were 2 and 4. As the number of threads increased, critical section locking took more than double the time as compared to spin locks. Spin lock seemed to have scaled a lot better when contention increased due to the high number of threads. The time taken is calculated using QueryPerformanceCounter Win32 methods. However, I would suggest performing your own testing on the platform you intend to use. Here is the table with the results: | No. of Threads | Time taken (Spin lock) | Time taken (Critical Section) | | --- | --- | --- | | 2 | | 6.6 | 7.14 | | 4 | | 12.81 | 14.09 | | 6 | | 16.01 | 46.37 | | 8 | | 23.32 | 54.34 | | 10 | | 26.21 | 74.76 | | 15 | | 41.17 | 89.05 | | 20 | | 47.63 | 116.82 | | 25 | | 62.25 | 147.68 | | 30 | | 64.37 | 169.17 | | 35 | | 88.02 | 210.07 | | 40 | | 93.99 | 296.32 | Future Work Profiling the code on different platforms. Adding a couple more data structures to the project like associated arrays and hashtable. Conclusion This was an effort to develop a general purpose spin lock implementation. Pure spin locking isn’t a good option in all scenarios and therefore there is a need for an implementation which allows the spinning thread to be suspended by the kernel. History First draft. Revision 1 - Fixed a couple of typos. Revision 2 - Code is now re-entrant safe. Revision 3 - Lock release now uses interlockedexchange. Revision 4 - Added test results. 本文地址：http://xnerv.wang/spin-lock-in-cpp/ 转载自：Spin Lock in C++","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://xnerv.wang/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"C++","slug":"C","permalink":"https://xnerv.wang/tags/C/"},{"name":"Programing","slug":"Programing","permalink":"https://xnerv.wang/tags/Programing/"},{"name":"spinlock","slug":"spinlock","permalink":"https://xnerv.wang/tags/spinlock/"}]},{"title":"一个模块分配的内存由另外一个模块释放~引发的血案（转载）","slug":"release-memory-allocated-by-another-module","date":"2010-06-08T04:29:46.000Z","updated":"2023-08-21T02:24:20.367Z","comments":true,"path":"release-memory-allocated-by-another-module/","link":"","permalink":"https://xnerv.wang/release-memory-allocated-by-another-module/","excerpt":"exe调用了cglover.dll中的一个导出类成员函数，是一个读取lua配置文件类，其中一个函数是读取int数组，并通知参数std::vector传递。 结果一出调用函数作用域，就挂调用，挂在std::vector的析构释放内存上~ 郁闷找不到原因，往床上一躺，突然想起关于很早以前就在游戏编程精粹上看到过exe释放dll分配的内存会有问题，但一直没明白为什么，也没有深究，应该是这个问题了。于是上网查了下，然后用windbg跟了下，总算整明白了。","text":"exe调用了cglover.dll中的一个导出类成员函数，是一个读取lua配置文件类，其中一个函数是读取int数组，并通知参数std::vector传递。 结果一出调用函数作用域，就挂调用，挂在std::vector的析构释放内存上~ 郁闷找不到原因，往床上一躺，突然想起关于很早以前就在游戏编程精粹上看到过exe释放dll分配的内存会有问题，但一直没明白为什么，也没有深究，应该是这个问题了。于是上网查了下，然后用windbg跟了下，总算整明白了。 Malloc和Free由CRT提供，分别是 DEBUG版本： MSVCR71D.DLL (C运行时库) MSVCP71D.DLL (C++运行时库) RELEASE版本： MSVCR71.DLL (C运行时库) MSVCP71.DLL (C++运行时库) Malloc和Free内部实际是调用系统提供的HeapAlloc和HeapFree来实现的（Kernel32.dll），这两个API需要HeapCreate创建返回的内存堆HANDLE。 但却不是使用缺省的进程堆（GetProcessHeap）， 而是在XXXCRTStartUp入口函数中创建的一个全局句柄HANDLE _crtheap（入口函数中调用CreateHeap）； 其实也并不一定是入口函数中创建，这只是针对静态链接CRT库而已，对于动态链接CRT库的，在整个应用程序中，所有动态链接CRT库的使用同一个_crtheap（即CRT模块中的_crtheap）,它在ntdll!LdrpCallInitRoutine中调用，并且之后动态链接CRT的模块不再调用heap_init。 经过使用windbg测试，发现Exe或Dll使用静态链接CRT库那么就会在CRTStartup入口函数中静态链接_CRTDLL_INIT中调用的_heap_init函数（里面调用CreateHeap），但是所有使用动态链接CRT库的模块的就不会在CRTStartup中再调用了，整个应用程序装载的时候之后，确定有模块动态链接CRT库，那么就调用MSVCR71D!_CRTDLL_INIT，这时就设置了CRT库的全局变量堆句柄_crtheap，所有动态链接CRT的模块就用到它了。 总结一句，就是这个进程中，所有动态链接CRT库的模块公用CRT模块的heap，而所有静态链接CRT库的模块（包括进程本身）使用自身创建的heap。 动态链接 12345678910111213。。。call_dlld!mainCRTStartup+0x142kernel32!BaseProcessStart+0x23。。。。ntdll!RtlCreateHeap (FPO: [Non-Fpo])kernel32!HeapCreate+0x55 (FPO: [3,0,4])MSVCR71D!_heap_init+0x1a (FPO: [Non-Fpo]) (CONV: cdecl) [f:\\vs70builds\\3077\\vc\\crtbld\\crt\\src\\heapinit.c @ 173]MSVCR71D!_CRTDLL_INIT+0xab (FPO: [Non-Fpo]) (CONV: stdcall) [f:\\vs70builds\\3077\\vc\\crtbld\\crt\\src\\crtlib.c @ 212]ntdll!LdrpCallInitRoutine+0x14ntdll!LdrpRunInitializeRoutines+0x344 (FPO: [Non-Fpo])ntdll!LdrpInitializeProcess+0x1131 (FPO: [5,89,4])ntdll!_LdrpInitialize+0x183 (FPO: [Non-Fpo])ntdll!KiUserApcDispatcher+0x7 静态链接 1234kernel32!HeapCreate+0x55 (FPO: [3,0,4])call_dlld!_heap_init+0x1acall_dlld!mainCRTStartup+0xc1kernel32!BaseProcessStart+0x23 (FPO: [Non-Fpo]) 就是这样了！ 当Dll（分配内存）和Exe（释放内存）都使用静态链接CRT库，那么悲剧发生了：Dll分配使用的是Dll的_crtheap全局句柄分配堆，Exe释放使用Exe的_crtheap全局句柄分配堆。 在Debug模式下会有检查释放内存是否在自身的堆内存block链表中，但在Release模式下却是不可预知的崩溃了。 但是当二者都是使用动态链接CRT DLL，那么malloc和free对应使用的堆句柄就是CRT DLL中的_crtheap了。这依赖于各个模块的编译选项设置（都设置为动态链接CRT库） 所以最根本的错误在于：不同的模块使用了不同的堆。 所以必须保证使用同一个堆。 所以最根本的做法还是，哪个模块分配的，由那个模块来释放。 可选的方法有： 1。在DLL中输出一个函数给EXE调用，专门用来释放由DLL分配的内存； 2。用GlobalAlloc()代替new，用GlobalFree()代替delete； 3。使用单一的堆，分配内存使用HeapAlloc(GetProcessHeap(),0,size)，释放内存使用HeapFree(GetProcessHeap(),0,p)； 4。把dll和exe的Settings的C/C++选项卡的Code Generation的Use Run-time liberary改成Debug Multithreaded DLL，在Release版本中改成Multithreaded DLL；这样使用一个CRT了——MSVCRT.DLL。 （编者注：在不同版本的vc runtime之间也不能混用new和delete，即使用的都是CRT DLL。） 本文地址：http://xnerv.wang/release-memory-allocated-by-another-module/ 转载自：一个模块分配的内存由另外一个模块释放~引发的血案","categories":[{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/categories/Windows/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/tags/Windows/"},{"name":"Heap","slug":"Heap","permalink":"https://xnerv.wang/tags/Heap/"},{"name":"DLL","slug":"DLL","permalink":"https://xnerv.wang/tags/DLL/"},{"name":"DLL Hell","slug":"DLL-Hell","permalink":"https://xnerv.wang/tags/DLL-Hell/"}]},{"title":"(Stack Overflow) HeapCreate, HeapAlloc in Linux, private allocator for Linux","slug":"heapcreate-heapalloc-in-linux-private-allocator-for-linux","date":"2010-05-21T15:43:00.000Z","updated":"2023-08-21T02:24:19.488Z","comments":true,"path":"heapcreate-heapalloc-in-linux-private-allocator-for-linux/","link":"","permalink":"https://xnerv.wang/heapcreate-heapalloc-in-linux-private-allocator-for-linux/","excerpt":"","text":"Question In Windows, for very demanding applications, a programmer may use HeapCreate, HeapAlloc in order to better manage and control the allocation of memory- speed it up (aka private allocators). What is the equivalent in Linux c++ programming? Answer by psmears If you want to use your own private allocator, then use mmap() to map an amount of memory into your process, then you can use that memory as you like. Open a file descriptor to /dev/zero, and then use that as the ‘fildes’ parameter to mmap(). See man mmap for full details of the parameters to pass. In this respect mmap() plays the same role as HeapCreate(). Article link: http://xnerv.wang/heapcreate-heapalloc-in-linux-private-allocator-for-linux/ Reprinted from: (StackOverflow) HeapCreate, HeapAlloc in Linux, private allocator for Linux","categories":[{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/categories/Linux/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"OS","slug":"OS","permalink":"https://xnerv.wang/tags/OS/"},{"name":"Heap","slug":"Heap","permalink":"https://xnerv.wang/tags/Heap/"},{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/tags/Linux/"},{"name":"Stack Overflow","slug":"Stack-Overflow","permalink":"https://xnerv.wang/tags/Stack-Overflow/"},{"name":"Memory Management","slug":"Memory-Management","permalink":"https://xnerv.wang/tags/Memory-Management/"}]},{"title":"(Stack Overflow) What is private bytes, virtual bytes, working set?","slug":"what-is-private-bytes-virtual-bytes-working-set","date":"2009-12-31T14:16:00.000Z","updated":"2023-08-21T02:24:20.950Z","comments":true,"path":"what-is-private-bytes-virtual-bytes-working-set/","link":"","permalink":"https://xnerv.wang/what-is-private-bytes-virtual-bytes-working-set/","excerpt":"Question I am trying to use the perfmon windows utility to debug memory leaks in a process. This is how perfmon explains the terms: Working Set is the current size, in bytes, of the Working Set of this process. The Working Set is the set of memory pages touched recently by the threads in the process. If free memory in the computer is above a threshold, pages are left in the Working Set of a process even if they are not in use. When free memory falls below a threshold, pages are trimmed from Working Sets. If they are needed they will then be soft-faulted back into the Working Set before leaving main memory. Virtual Bytes is the current size, in bytes, of the virtual address space the process is using. Use of virtual address space does not necessarily imply corresponding use of either disk or main memory pages. Virtual space is finite, and the process can limit its ability to load libraries. Private Bytes is the current size, in bytes, of memory that this process has allocated that cannot be shared with other processes.","text":"Question I am trying to use the perfmon windows utility to debug memory leaks in a process. This is how perfmon explains the terms: Working Set is the current size, in bytes, of the Working Set of this process. The Working Set is the set of memory pages touched recently by the threads in the process. If free memory in the computer is above a threshold, pages are left in the Working Set of a process even if they are not in use. When free memory falls below a threshold, pages are trimmed from Working Sets. If they are needed they will then be soft-faulted back into the Working Set before leaving main memory. Virtual Bytes is the current size, in bytes, of the virtual address space the process is using. Use of virtual address space does not necessarily imply corresponding use of either disk or main memory pages. Virtual space is finite, and the process can limit its ability to load libraries. Private Bytes is the current size, in bytes, of memory that this process has allocated that cannot be shared with other processes. These are the questions I have: Is it the Private Bytes which I should measure to be sure if the process is having any leaks as it does not involve any shared libraries and any leaks, if happening, will come from the process itself? What is the total memory consumed by the process? Is it the Virtual Bytes or is it the sum of Virtual Bytes and Working Set? Is there any relation between Private Bytes, Working Set and Virtual Bytes? Are there any other tools that give a better idea of the memory usage? Answer by pankajt The short answer to this question is that none of these values are a reliable indicator of how much memory an executable is actually using, and none of them are really appropriate for debugging a memory leak. Private Bytes refer to the amount of memory that the process executable has asked for - not necessarily the amount it is actually using. They are “private” because they (usually) exclude memory-mapped files (i.e. shared DLLs). But - here’s the catch - they don’t necessarily exclude memory allocated by those files. There is no way to tell whether a change in private bytes was due to the executable itself, or due to a linked library. Private bytes are also not exclusively physical memory; they can be paged to disk or in the standby page list (i.e. no longer in use, but not paged yet either). Working Set refers to the total physical memory (RAM) used by the process. However, unlike private bytes, this also includes memory-mapped files and various other resources, so it’s an even less accurate measurement than the private bytes. This is the same value that gets reported in Task Manager’s “Mem Usage” and has been the source of endless amounts of confusion in recent years. Memory in the Working Set is “physical” in the sense that it can be addressed without a page fault; however, the standby page list is also still physically in memory but not reported in the Working Set, and this is why you might see the “Mem Usage” suddenly drop when you minimize an application. Virtual Bytes are the total virtual address space occupied by the entire process. This is like the working set, in the sense that it includes memory-mapped files (shared DLLs), but it also includes data in the standby list and data that has already been paged out and is sitting in a pagefile on disk somewhere. The total virtual bytes used by every process on a system under heavy load will add up to significantly more memory than the machine actually has. So the relationships are: Private Bytes are what your app has actually allocated, but include pagefile usage; Working Set is the non-paged Private Bytes plus memory-mapped files; Virtual Bytes are the Working Set plus paged Private Bytes and standby list. There’s another problem here; just as shared libraries can allocate memory inside your application module, leading to potential false positives reported in your app’s Private Bytes, your application may also end up allocating memory inside the shared modules, leading to false negatives. That means it’s actually possible for your application to have a memory leak that never manifests itself in the Private Bytes at all. Unlikely, but possible. Private Bytes are a reasonable approximation of the amount of memory your executable is using and can be used to help narrow down a list of potential candidates for a memory leak; if you see the number growing and growing constantly and endlessly, you would want to check that process for a leak. This cannot, however, prove that there is or is not a leak. One of the most effective tools for detecting/correcting memory leaks in Windows is actually Visual Studio (link goes to page on using VS for memory leaks, not the product page). Rational Purify is another possibility. Microsoft also has a more general best practices document on this subject. There are more tools listed in this previous question. I hope this clears a few things up! Tracking down memory leaks is one of the most difficult things to do in debugging. Good luck. Article link: http://xnerv.wang/what-is-private-bytes-virtual-bytes-working-set/ Reprinted from: (StackOverflow) What is private bytes, virtual bytes, working set?","categories":[{"name":"OS","slug":"OS","permalink":"https://xnerv.wang/categories/OS/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"OS","slug":"OS","permalink":"https://xnerv.wang/tags/OS/"},{"name":"Stack Overflow","slug":"Stack-Overflow","permalink":"https://xnerv.wang/tags/Stack-Overflow/"},{"name":"Memory Management","slug":"Memory-Management","permalink":"https://xnerv.wang/tags/Memory-Management/"},{"name":"Virtual Memory","slug":"Virtual-Memory","permalink":"https://xnerv.wang/tags/Virtual-Memory/"},{"name":"Private bytes","slug":"Private-bytes","permalink":"https://xnerv.wang/tags/Private-bytes/"},{"name":"Working Set","slug":"Working-Set","permalink":"https://xnerv.wang/tags/Working-Set/"}]},{"title":"(Stack Overflow) Why can't variables be declared in a switch statement?","slug":"why-cant-variables-be-declared-in-a-switch-statement","date":"2008-09-18T20:11:00.000Z","updated":"2023-08-21T02:24:21.568Z","comments":true,"path":"why-cant-variables-be-declared-in-a-switch-statement/","link":"","permalink":"https://xnerv.wang/why-cant-variables-be-declared-in-a-switch-statement/","excerpt":"Question I’ve always wondered this - why can’t you declare variables after a case label in a switch statement? In C++ you can declare variables pretty much anywhere (and declaring them close to first use is obviously a good thing) but the following still won’t work: 12345678910switch (val)&#123;case VAL: &#x2F;&#x2F; This won&#39;t work int newVal &#x3D; 42; break;case ANOTHER_VAL: ... break;&#125; The above gives me the following error (MSC): initialization of ‘newVal’ is skipped by ‘case’ label This seems to be a limitation in other languages too. Why is this such a problem?","text":"Question I’ve always wondered this - why can’t you declare variables after a case label in a switch statement? In C++ you can declare variables pretty much anywhere (and declaring them close to first use is obviously a good thing) but the following still won’t work: 12345678910switch (val)&#123;case VAL: &#x2F;&#x2F; This won&#39;t work int newVal &#x3D; 42; break;case ANOTHER_VAL: ... break;&#125; The above gives me the following error (MSC): initialization of ‘newVal’ is skipped by ‘case’ label This seems to be a limitation in other languages too. Why is this such a problem? Answer by Rob This question is tagged as [C] and [C++] at the same time. The original code is indeed invalid in both C and C++, but for completely different unrelated reasons. I believe this important detail was missed (or obfuscated) by the existing answers. In C++ this code is invalid because the case ANOTHER_VAL: label jumps into the scope of variable newVal bypassing its initialization. Jumps that bypass initialization of local objects are illegal in C++. This side of the issue is correctly addressed by most answers. However, in C language bypassing variable initialization is not an error. Jumping into the scope of a variable over it initialization is legal in C. It simply means that the variable is left uninitialized. The original code does not compile in C for a completely different reason. Label case VAL: in the original code is attached to the declaration of variable newVal. In C language declarations are not statements. They cannot be labeled. And this is what causes the error when this code is interpreted as C code. 123456789switch (val)&#123;case VAL: /* &lt;- C error is here */ int newVal = 42; break;case ANOTHER_VAL: /* &lt;- C++ error is here */ ... break;&#125; Adding an extra &#123;&#125; block fixes both C++ and C problems, even though these problems happen to be very different. On the C++ side it restricts the scope of newVal, making sure that case ANOTHER_VAL: no longer jumps into that scope, which eliminates the C++ issue. On the C side that extra &#123;&#125; introduces a compound statement, thus making the case VAL: label to apply to a statement, which eliminates the C issue. In C case the problem can be easily solved without the &#123;&#125;. Just add an empty statement after the case VAL: label and the code will become valid 123456789switch (val)&#123;case VAL:; /* Now it works in C! */ int newVal = 42; break;case ANOTHER_VAL: ... break;&#125; Note that even though it is now valid from C point of view, it remains invalid from C++ point of view. Symmetrically, in C++ case the the problem can be easily solved without the &#123;&#125;. Just remove the initializer from variable declaration and the code will become valid 12345678910switch (val)&#123;case VAL: int newVal; newVal = 42; break;case ANOTHER_VAL: /* Now it works in C++! */ ... break;&#125; Note that even though it is now valid from C++ point of view, it remains invalid from C point of view. Article link: http://xnerv.wang/why-cant-variables-be-declared-in-a-switch-statement/ Reprinted from: (StackOverflow) Why can’t variables be declared in a switch statement? (answered by AnT)","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://xnerv.wang/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"C++","slug":"C","permalink":"https://xnerv.wang/tags/C/"},{"name":"Stack Overflow","slug":"Stack-Overflow","permalink":"https://xnerv.wang/tags/Stack-Overflow/"},{"name":"Programing","slug":"Programing","permalink":"https://xnerv.wang/tags/Programing/"}]},{"title":"(Stack Overflow) mmap() vs. reading blocks","slug":"mmap-vs-reading-blocks","date":"2008-09-05T21:52:00.000Z","updated":"2023-08-21T02:24:19.511Z","comments":true,"path":"mmap-vs-reading-blocks/","link":"","permalink":"https://xnerv.wang/mmap-vs-reading-blocks/","excerpt":"Question I’m working on a program that will be processing files that could potentially be 100GB or more in size. The files contain sets of variable length records. I’ve got a first implementation up and running and am now looking towards improving performance, particularly at doing I/O more efficiently since the input file gets scanned many times. Is there a rule of thumb for using mmap() versus reading in blocks via C++'s fstream library? What I’d like to do is read large blocks from disk into a buffer, process complete records from the buffer, and then read more. The mmap() code could potentially get very messy since mmap’d blocks need to lie on page sized boundaries (my understanding) and records could potentially like across page boundaries. With fstreams, I can just seek to the start of a record and begin reading again, since we’re not limited to reading blocks that lie on page sized boundaries. How can I decide between these two options without actually writing up a complete implementation first? Any rules of thumb (e.g., mmap() is 2x faster) or simple tests?","text":"Question I’m working on a program that will be processing files that could potentially be 100GB or more in size. The files contain sets of variable length records. I’ve got a first implementation up and running and am now looking towards improving performance, particularly at doing I/O more efficiently since the input file gets scanned many times. Is there a rule of thumb for using mmap() versus reading in blocks via C++'s fstream library? What I’d like to do is read large blocks from disk into a buffer, process complete records from the buffer, and then read more. The mmap() code could potentially get very messy since mmap’d blocks need to lie on page sized boundaries (my understanding) and records could potentially like across page boundaries. With fstreams, I can just seek to the start of a record and begin reading again, since we’re not limited to reading blocks that lie on page sized boundaries. How can I decide between these two options without actually writing up a complete implementation first? Any rules of thumb (e.g., mmap() is 2x faster) or simple tests? Answer by Dietrich Epp I was trying to find the final word on mmap / read performance on Linux and I came across a nice post (link) on the Linux kernel mailing list. It’s from 2000, so there have been many improvements to IO and virtual memory in the kernel since then, but it nicely explains the reason why mmap or read might be faster or slower. A call to mmap has more overhead than read (just like epoll has more overhead than poll, which has more overhead than read). Changing virtual memory mappings is a quite expensive operation on some processors for the same reasons that switching between different processes is expensive. The IO system can already use the disk cache, so if you read a file, you’ll hit the cache or miss it no matter what method you use. However, Memory maps are generally faster for random access, especially if your access patterns are sparse and unpredictable. Memory maps allow you to keep using pages from the cache until you are done. This means that if you use a file heavily for a long period of time, then close it and reopen it, the pages will still be cached. With read, your file may have been flushed from the cache ages ago. This does not apply if you use a file and immediately discard it. (If you try to mlock pages just to keep them in cache, you are trying to outsmart the disk cache and this kind of foolery rarely helps system performance). Reading a file directly is very simple and fast. The discussion of mmap/read reminds me of two other performance discussions: Some Java programmers were shocked to discover that nonblocking I/O is often slower than blocking I/O, which made perfect sense if you know that nonblocking I/O requires making more syscalls. Some other network programmers were shocked to learn that epoll is often slower than poll, which makes perfect sense if you know that managing epoll requires making more syscalls. Conclusion: Use memory maps if you access data randomly, keep it around for a long time, or if you know you can share it with other processes (MAP_SHARED isn’t very interesting if there is no actual sharing). Read files normally if you access data sequentially or discard it after reading. And if either method makes your program less complex, do that. For many real world cases there’s no sure way to show one is faster without testing your actual application and NOT a benchmark. (Sorry for necro’ing this question, but I was looking for an answer and this question kept coming up at the top of Google results.) Article link: http://xnerv.wang/mmap-vs-reading-blocks/ Reprinted from: (Stack Overflow) mmap() vs. reading blocks","categories":[{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/categories/Linux/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"OS","slug":"OS","permalink":"https://xnerv.wang/tags/OS/"},{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/tags/Linux/"},{"name":"Stack Overflow","slug":"Stack-Overflow","permalink":"https://xnerv.wang/tags/Stack-Overflow/"},{"name":"Memory Management","slug":"Memory-Management","permalink":"https://xnerv.wang/tags/Memory-Management/"},{"name":"mmap","slug":"mmap","permalink":"https://xnerv.wang/tags/mmap/"}]},{"title":"我理解的逻辑地址、线性地址、物理地址和虚拟地址（转载）","slug":"logical-address-linear-address-physical-address-and-virtual-address","date":"2008-01-16T00:32:00.000Z","updated":"2023-08-21T02:24:21.110Z","comments":true,"path":"logical-address-linear-address-physical-address-and-virtual-address/","link":"","permalink":"https://xnerv.wang/logical-address-linear-address-physical-address-and-virtual-address/","excerpt":"本贴涉及的硬件平台是X86，如果是其它平台，嘻嘻，不保证能一一对号入座，但是举一反三，我想是完全可行的。 概念 物理地址(physical address) 用于内存芯片级的单元寻址，与处理器和CPU连接的地址总线相对应。 ——这个概念应该是这几个概念中最好理解的一个，但是值得一提的是，虽然可以直接把物理地址理解成插在机器上那根内存本身，把内存看成一个从0字节一直到最大空量逐字节的编号的大数组，然后把这个数组叫做物理地址，但是事实上，这只是一个硬件提供给软件的抽像，内存的寻址方式并不是这样。所以，说它是“与地址总线相对应”，是更贴切一些，不过抛开对物理内存寻址方式的考虑，直接把物理地址与物理的内存一一对应，也是可以接受的。也许错误的理解更利于形而上的抽像。","text":"本贴涉及的硬件平台是X86，如果是其它平台，嘻嘻，不保证能一一对号入座，但是举一反三，我想是完全可行的。 概念 物理地址(physical address) 用于内存芯片级的单元寻址，与处理器和CPU连接的地址总线相对应。 ——这个概念应该是这几个概念中最好理解的一个，但是值得一提的是，虽然可以直接把物理地址理解成插在机器上那根内存本身，把内存看成一个从0字节一直到最大空量逐字节的编号的大数组，然后把这个数组叫做物理地址，但是事实上，这只是一个硬件提供给软件的抽像，内存的寻址方式并不是这样。所以，说它是“与地址总线相对应”，是更贴切一些，不过抛开对物理内存寻址方式的考虑，直接把物理地址与物理的内存一一对应，也是可以接受的。也许错误的理解更利于形而上的抽像。 虚拟内存(virtual memory) 这是对整个内存（不要与机器上插那条对上号）的抽像描述。它是相对于物理内存来讲的，可以直接理解成“不直实的”，“假的”内存，例如，一个0x08000000内存地址，它并不对就物理地址上那个大数组中0x08000000 - 1那个地址元素； 之所以是这样，是因为现代操作系统都提供了一种内存管理的抽像，即虚拟内存（virtual memory）。进程使用虚拟内存中的地址，由操作系统协助相关硬件，把它“转换”成真正的物理地址。这个“转换”，是所有问题讨论的关键。 有了这样的抽像，一个程序，就可以使用比真实物理地址大得多的地址空间。（拆东墙，补西墙，银行也是这样子做的），甚至多个进程可以使用相同的地址。不奇怪，因为转换后的物理地址并非相同的。 ——可以把连接后的程序反编译看一下，发现连接器已经为程序分配了一个地址，例如，要调用某个函数A，代码不是call A，而是call 0x0811111111 ，也就是说，函数A的地址已经被定下来了。没有这样的“转换”，没有虚拟地址的概念，这样做是根本行不通的。 打住了，这个问题再说下去，就收不住了。 逻辑地址(logical address) Intel为了兼容，将远古时代的段式内存管理方式保留了下来。逻辑地址指的是机器语言指令中，用来指定一个操作数或者是一条指令的地址。以上例，我们说的连接器为A分配的0x08111111这个地址就是逻辑地址。 ——不过不好意思，这样说，好像又违背了Intel中段式管理中，对逻辑地址要求，“一个逻辑地址，是由一个段标识符加上一个指定段内相对地址的偏移量，表示为 [段标识符：段内偏移量]，也就是说，上例中那个0x08111111，应该表示为[A的代码段标识符: 0x08111111]，这样，才完整一些”。 线性地址(linear address)或也叫虚拟地址(virtual address) 跟逻辑地址类似，它也是一个不真实的地址，如果逻辑地址是对应的硬件平台段式管理转换前地址的话，那么线性地址则对应了硬件页式内存的转换前地址。 （编者注：有些文章也将段内偏移量描述为虚拟地址，从下文可以看出，在Linux中，逻辑地址/虚拟地址/线性地址 三者相等。） CPU将一个虚拟内存空间中的地址转换为物理地址，需要进行两步：首先将给定一个逻辑地址（其实是段内偏移量，这个一定要理解！！！），CPU要利用其段式内存管理单元，先将为个逻辑地址转换成一个线程地址，再利用其页式内存管理单元，转换为最终物理地址。 这样做两次转换，的确是非常麻烦而且没有必要的，因为直接可以把线性地址抽像给进程。之所以这样冗余，Intel完全是为了兼容而已。 CPU段式内存管理，逻辑地址如何转换为线性地址 一个逻辑地址由两部份组成，段标识符: 段内偏移量。段标识符是由一个16位长的字段组成，称为段选择符。其中前13位是一个索引号。后面3位包含一些硬件细节，如图： 最后两位涉及权限检查，本贴中不包含。 索引号，或者直接理解成数组下标——那它总要对应一个数组吧，它又是什么东东的索引呢？这个东东就是“段描述符(segment descriptor)”，呵呵，段描述符具体地址描述了一个段（对于“段”这个字眼的理解，我是把它想像成，拿了一把刀，把虚拟内存，砍成若干的截——段）。这样，很多个段描述符，就组了一个数组，叫“段描述符表”，这样，可以通过段标识符的前13位，直接在段描述符表中找到一个具体的段描述符，这个描述符就描述了一个段，我刚才对段的抽像不太准确，因为看看描述符里面究竟有什么东东——也就是它究竟是如何描述的，就理解段究竟有什么东东了，每一个段描述符由8个字节组成，如下图： 这些东东很复杂，虽然可以利用一个数据结构来定义它，不过，我这里只关心一样，就是Base字段，它描述了一个段的开始位置的线性地址。 Intel设计的本意是，一些全局的段描述符，就放在“全局段描述符表(GDT)”中，一些局部的，例如每个进程自己的，就放在所谓的“局部段描述符表(LDT)”中。那究竟什么时候该用GDT，什么时候该用LDT呢？这是由段选择符中的T1字段表示的，=0，表示用GDT，=1表示用LDT。 GDT在内存中的地址和大小存放在CPU的gdtr控制寄存器中，而LDT则在ldtr寄存器中。 好多概念，像绕口令一样。这张图看起来要直观些： 首先，给定一个完整的逻辑地址[段选择符：段内偏移地址]， 看段选择符的T1=0还是1，知道当前要转换是GDT中的段，还是LDT中的段，再根据相应寄存器，得到其地址和大小。我们就有了一个数组了。 拿出段选择符中前13位，可以在这个数组中，查找到对应的段描述符，这样，它了Base，即基地址就知道了。 把Base + offset，就是要转换的线性地址了。 还是挺简单的，对于软件来讲，原则上就需要把硬件转换所需的信息准备好，就可以让硬件来完成这个转换了。OK，来看看Linux怎么做的。 Linux的段式管理 Intel要求两次转换，这样虽说是兼容了，但是却是很冗余，呵呵，没办法，硬件要求这样做了，软件就只能照办，怎么着也得形式主义一样。 另一方面，其它某些硬件平台，没有二次转换的概念，Linux也需要提供一个高层抽像，来提供一个统一的界面。所以，Linux的段式管理，事实上只是“哄骗”了一下硬件而已。 按照Intel的本意，全局的用GDT，每个进程自己的用LDT——不过Linux则对所有的进程都使用了相同的段来对指令和数据寻址。即用户数据段，用户代码段，对应的，内核中的是内核数据段和内核代码段。这样做没有什么奇怪的，本来就是走形式嘛，像我们写年终总结一样。 123456789101112131415include/asm-i386/segment.h#define GDT_ENTRY_DEFAULT_USER_CS 14#define __USER_CS (GDT_ENTRY_DEFAULT_USER_CS * 8 + 3)#define GDT_ENTRY_DEFAULT_USER_DS 15#define __USER_DS (GDT_ENTRY_DEFAULT_USER_DS * 8 + 3)#define GDT_ENTRY_KERNEL_BASE 12#define GDT_ENTRY_KERNEL_CS (GDT_ENTRY_KERNEL_BASE + 0)#define __KERNEL_CS (GDT_ENTRY_KERNEL_CS * 8)#define GDT_ENTRY_KERNEL_DS (GDT_ENTRY_KERNEL_BASE + 1)#define __KERNEL_DS (GDT_ENTRY_KERNEL_DS * 8) 把其中的宏替换成数值，则为： 1234#define __USER_CS 115 [00000000 1110 0 11]#define __USER_DS 123 [00000000 1111 0 11]#define __KERNEL_CS 96 [00000000 1100 0 00]#define __KERNEL_DS 104 [00000000 1101 0 00] 方括号后是这四个段选择符的16位二制表示，它们的索引号和T1字段值也可以算出来了： 1234__USER_CS index= 14 T1=0__USER_DS index= 15 T1=0__KERNEL_CS index= 12 T1=0__KERNEL_DS index= 13 T1=0 T1均为0，则表示都使用了GDT，再来看初始化GDT的内容中相应的12-15项(arch/i386/head.S)： 1234.quad 0x00cf9a000000ffff /* 0x60 kernel 4GB code at 0x00000000 */.quad 0x00cf92000000ffff /* 0x68 kernel 4GB data at 0x00000000 */.quad 0x00cffa000000ffff /* 0x73 user 4GB code at 0x00000000 */.quad 0x00cff2000000ffff /* 0x7b user 4GB data at 0x00000000 */ 按照前面段描述符表中的描述，可以把它们展开，发现其16-31位全为0，即四个段的基地址全为0。 这样，给定一个段内偏移地址，按照前面转换公式，0 + 段内偏移，转换为线性地址，可以得出重要的结论，“在Linux下，逻辑地址与线性地址总是一致（是一致，不是有些人说的相同）的，即逻辑地址的偏移量字段的值与线性地址的值总是相同的。！！！” 忽略了太多的细节，例如段的权限检查。呵呵。 Linux中，绝大部份进程并不例用LDT，除非使用Wine ，仿真Windows程序的时候。 CPU的页式内存管理 CPU的页式内存管理单元，负责把一个线性地址，最终翻译为一个物理地址。从管理和效率的角度出发，线性地址被分为以固定长度为单位的组，称为页(page)，例如一个32位的机器，线性地址最大可为4G，可以用4KB为一个页来划分，这页，整个线性地址就被划分为一个tatol_page[2^20]的大数组，共有2的20个次方个页。这个大数组我们称之为页目录。目录中的每一个目录项，就是一个地址——对应的页的地址。 另一类“页”，我们称之为物理页，或者是页框、页桢的。是分页单元把所有的物理内存也划分为固定长度的管理单位，它的长度一般与内存页是一一对应的。 这里注意到，这个total_page数组有2^20个成员，每个成员是一个地址（32位机，一个地址也就是4字节），那么要单单要表示这么一个数组，就要占去4MB的内存空间。为了节省空间，引入了一个二级管理模式的机器来组织分页单元。文字描述太累，看图直观一些： 如上图， 分页单元中，页目录是唯一的，它的地址放在CPU的cr3寄存器中，是进行地址转换的开始点。万里长征就从此长始了。 每一个活动的进程，因为都有其独立的对应的虚似内存（页目录也是唯一的），那么它也对应了一个独立的页目录地址。——运行一个进程，需要将它的页目录地址放到cr3寄存器中，将别个的保存下来。 每一个32位的线性地址被划分为三部份，面目录索引(10位)：页表索引(10位)：偏移(12位) 依据以下步骤进行转换： 从cr3中取出进程的页目录地址（操作系统负责在调度进程的时候，把这个地址装入对应寄存器）； 根据线性地址前十位，在数组中，找到对应的索引项，因为引入了二级管理模式，页目录中的项，不再是页的地址，而是一个页表的地址。（又引入了一个数组），页的地址被放到页表中去了。 根据线性地址的中间十位，在页表（也是数组）中找到页的起始地址； 将页的起始地址与线性地址中最后12位相加，得到最终我们想要的葫芦； 这个转换过程，应该说还是非常简单地。全部由硬件完成，虽然多了一道手续，但是节约了大量的内存，还是值得的。那么再简单地验证一下： 这样的二级模式是否仍能够表示4G的地址； 页目录共有：2^10项，也就是说有这么多个页表 每个目表对应了：2^10页； 每个页中可寻址：2^12个字节。 还是2^32 = 4GB 这样的二级模式是否真的节约了空间； 也就是算一下页目录项和页表项共占空间 (2^10 * 4 + 2 ^10 *4) = 8KB。哎，……怎么说呢！！！ 红色错误，标注一下，后文贴中有此讨论。。。。。。 按&lt;深入理解计算机系统&gt;中的解释,二级模式空间的节约是从两个方面实现的: A、如果一级页表中的一个页表条目为空，那么那所指的二级页表就根本不会存在。这表现出一种巨大的潜在节约，因为对于一个典型的程序，4GB虚拟地址空间的大部份都会是未分配的； B、只有一级页表才需要总是在主存中。虚拟存储器系统可以在需要时创建，并页面调入或调出二级页表，这就减少了主存的压力。只有最经常使用的二级页表才需要缓存在主存中。——不过Linux并没有完全享受这种福利，它的页表目录和与已分配页面相关的页表都是常驻内存的。 值得一提的是，虽然页目录和页表中的项，都是4个字节，32位，但是它们都只用高20位，低12位屏蔽为0——把页表的低12屏蔽为0，是很好理解的，因为这样，它刚好和一个页面大小对应起来，大家都成整数增加。计算起来就方便多了。但是，为什么同时也要把页目录低12位屏蔽掉呢？因为按同样的道理，只要屏蔽其低10位就可以了，不过我想，因为12&gt;10，这样，可以让页目录和页表使用相同的数据结构，方便。 本贴只介绍一般性转换的原理，扩展分页、页的保护机制、PAE模式的分页这些麻烦点的东东就不啰嗦了……可以参考其它专业书籍。 Linux的页式内存管理 原理上来讲，Linux只需要为每个进程分配好所需数据结构，放到内存中，然后在调度进程的时候，切换寄存器cr3，剩下的就交给硬件来完成了（呵呵，事实上要复杂得多，不过偶只分析最基本的流程）。 前面说了i386的二级页管理架构，不过有些CPU，还有三级，甚至四级架构，Linux为了在更高层次提供抽像，为每个CPU提供统一的界面。提供了一个四层页管理架构，来兼容这些二级、三级、四级管理架构的CPU。这四级分别为： 页全局目录PGD（对应刚才的页目录） 页上级目录PUD（新引进的） 页中间目录PMD（也就新引进的） 页表PT（对应刚才的页表）。 整个转换依据硬件转换原理，只是多了二次数组的索引罢了，如下图： 那么，对于使用二级管理架构32位的硬件，现在又是四级转换了，它们怎么能够协调地工作起来呢？嗯，来看这种情况下，怎么来划分线性地址吧！ 从硬件的角度，32位地址被分成了三部份——也就是说，不管理软件怎么做，最终落实到硬件，也只认识这三位老大。 从软件的角度，由于多引入了两部份，，也就是说，共有五部份。——要让二层架构的硬件认识五部份也很容易，在地址划分的时候，将页上级目录和页中间目录的长度设置为0就可以了。 这样，操作系统见到的是五部份，硬件还是按它死板的三部份划分，也不会出错，也就是说大家共建了和谐计算机系统。 这样，虽说是多此一举，但是考虑到64位地址，使用四层转换架构的CPU，我们就不再把中间两个设为0了，这样，软件与硬件再次和谐——抽像就是强大呀！！！ 例如，一个逻辑地址已经被转换成了线性地址，0x08147258，换成二制进，也就是： 10000100000 0101000111 001001011000 内核对这个地址进行划分 12345PGD &#x3D; 0000100000PUD &#x3D; 0PMD &#x3D; 0PT &#x3D; 0101000111offset &#x3D; 001001011000 现在来理解Linux针对硬件的花招，因为硬件根本看不到所谓PUD,PMD，所以，本质上要求PGD索引，直接就对应了PT的地址。而不是再到PUD和PMD中去查数组（虽然它们两个在线性地址中，长度为0，2^0 =1，也就是说，它们都是有一个数组元素的数组），那么，内核如何合理安排地址呢？ 从软件的角度上来讲，因为它的项只有一个，32位，刚好可以存放与PGD中长度一样的地址指针。那么所谓先到PUD，到到PMD中做映射转换，就变成了保持原值不变，一一转手就可以了。这样，就实现了“逻辑上指向一个PUD，再指向一个PDM，但在物理上是直接指向相应的PT的这个抽像，因为硬件根本不知道有PUD、PMD这个东西”。 然后交给硬件，硬件对这个地址进行划分，看到的是： 123页目录 &#x3D; 0000100000PT &#x3D; 0101000111offset &#x3D; 001001011000 嗯，先根据0000100000(32)，在页目录数组中索引，找到其元素中的地址，取其高20位，找到页表的地址，页表的地址是由内核动态分配的，接着，再加一个offset，就是最终的物理地址了。 本文地址：http://xnerv.wang/logical-address-linear-address-physical-address-and-virtual-address/ 转载自：我理解的逻辑地址、线性地址、物理地址和虚拟地址","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://xnerv.wang/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"操作系统","slug":"操作系统","permalink":"https://xnerv.wang/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"内存管理","slug":"内存管理","permalink":"https://xnerv.wang/tags/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"},{"name":"虚拟内存","slug":"虚拟内存","permalink":"https://xnerv.wang/tags/%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98/"},{"name":"页表","slug":"页表","permalink":"https://xnerv.wang/tags/%E9%A1%B5%E8%A1%A8/"}]}],"categories":[{"name":"程序员生活","slug":"程序员生活","permalink":"https://xnerv.wang/categories/%E7%A8%8B%E5%BA%8F%E5%91%98%E7%94%9F%E6%B4%BB/"},{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/categories/Linux/"},{"name":"树莓派","slug":"树莓派","permalink":"https://xnerv.wang/categories/%E6%A0%91%E8%8E%93%E6%B4%BE/"},{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/categories/DBMS/"},{"name":"计算机硬件","slug":"计算机硬件","permalink":"https://xnerv.wang/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A1%AC%E4%BB%B6/"},{"name":"编译器","slug":"编译器","permalink":"https://xnerv.wang/categories/%E7%BC%96%E8%AF%91%E5%99%A8/"},{"name":"分布式及存储","slug":"分布式及存储","permalink":"https://xnerv.wang/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8F%8A%E5%AD%98%E5%82%A8/"},{"name":"网络协议","slug":"网络协议","permalink":"https://xnerv.wang/categories/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"},{"name":"操作系统","slug":"操作系统","permalink":"https://xnerv.wang/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"WinDbg","slug":"WinDbg","permalink":"https://xnerv.wang/categories/WinDbg/"},{"name":"区块链与比特币","slug":"区块链与比特币","permalink":"https://xnerv.wang/categories/%E5%8C%BA%E5%9D%97%E9%93%BE%E4%B8%8E%E6%AF%94%E7%89%B9%E5%B8%81/"},{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://xnerv.wang/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"},{"name":"编程语言","slug":"编程语言","permalink":"https://xnerv.wang/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"},{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/categories/Windows/"},{"name":"设计模式","slug":"设计模式","permalink":"https://xnerv.wang/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"Protobuf","slug":"Protobuf","permalink":"https://xnerv.wang/categories/Protobuf/"},{"name":"OS","slug":"OS","permalink":"https://xnerv.wang/categories/OS/"},{"name":"搜索引擎","slug":"搜索引擎","permalink":"https://xnerv.wang/categories/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/"}],"tags":[{"name":"程序员生活","slug":"程序员生活","permalink":"https://xnerv.wang/tags/%E7%A8%8B%E5%BA%8F%E5%91%98%E7%94%9F%E6%B4%BB/"},{"name":"VSCode","slug":"VSCode","permalink":"https://xnerv.wang/tags/VSCode/"},{"name":"Atom","slug":"Atom","permalink":"https://xnerv.wang/tags/Atom/"},{"name":"Markdown","slug":"Markdown","permalink":"https://xnerv.wang/tags/Markdown/"},{"name":"原创","slug":"原创","permalink":"https://xnerv.wang/tags/%E5%8E%9F%E5%88%9B/"},{"name":"Linux","slug":"Linux","permalink":"https://xnerv.wang/tags/Linux/"},{"name":"图形界面","slug":"图形界面","permalink":"https://xnerv.wang/tags/%E5%9B%BE%E5%BD%A2%E7%95%8C%E9%9D%A2/"},{"name":"X-Window","slug":"X-Window","permalink":"https://xnerv.wang/tags/X-Window/"},{"name":"X协议","slug":"X协议","permalink":"https://xnerv.wang/tags/X%E5%8D%8F%E8%AE%AE/"},{"name":"远程桌面","slug":"远程桌面","permalink":"https://xnerv.wang/tags/%E8%BF%9C%E7%A8%8B%E6%A1%8C%E9%9D%A2/"},{"name":"树莓派","slug":"树莓派","permalink":"https://xnerv.wang/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"},{"name":"Raspberry","slug":"Raspberry","permalink":"https://xnerv.wang/tags/Raspberry/"},{"name":"DBMS","slug":"DBMS","permalink":"https://xnerv.wang/tags/DBMS/"},{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://xnerv.wang/tags/PostgreSQL/"},{"name":"PG","slug":"PG","permalink":"https://xnerv.wang/tags/PG/"},{"name":"转载","slug":"转载","permalink":"https://xnerv.wang/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"阿里月饼","slug":"阿里月饼","permalink":"https://xnerv.wang/tags/%E9%98%BF%E9%87%8C%E6%9C%88%E9%A5%BC/"},{"name":"性能瓶颈","slug":"性能瓶颈","permalink":"https://xnerv.wang/tags/%E6%80%A7%E8%83%BD%E7%93%B6%E9%A2%88/"},{"name":"CPU","slug":"CPU","permalink":"https://xnerv.wang/tags/CPU/"},{"name":"内存访问","slug":"内存访问","permalink":"https://xnerv.wang/tags/%E5%86%85%E5%AD%98%E8%AE%BF%E9%97%AE/"},{"name":"网络传输","slug":"网络传输","permalink":"https://xnerv.wang/tags/%E7%BD%91%E7%BB%9C%E4%BC%A0%E8%BE%93/"},{"name":"分支预测","slug":"分支预测","permalink":"https://xnerv.wang/tags/%E5%88%86%E6%94%AF%E9%A2%84%E6%B5%8B/"},{"name":"硬盘","slug":"硬盘","permalink":"https://xnerv.wang/tags/%E7%A1%AC%E7%9B%98/"},{"name":"SSD","slug":"SSD","permalink":"https://xnerv.wang/tags/SSD/"},{"name":"Docker","slug":"Docker","permalink":"https://xnerv.wang/tags/Docker/"},{"name":"MinGW","slug":"MinGW","permalink":"https://xnerv.wang/tags/MinGW/"},{"name":"编译器","slug":"编译器","permalink":"https://xnerv.wang/tags/%E7%BC%96%E8%AF%91%E5%99%A8/"},{"name":"Cygwin","slug":"Cygwin","permalink":"https://xnerv.wang/tags/Cygwin/"},{"name":"Windows","slug":"Windows","permalink":"https://xnerv.wang/tags/Windows/"},{"name":"分布式及存储","slug":"分布式及存储","permalink":"https://xnerv.wang/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8F%8A%E5%AD%98%E5%82%A8/"},{"name":"Seafile","slug":"Seafile","permalink":"https://xnerv.wang/tags/Seafile/"},{"name":"网络协议","slug":"网络协议","permalink":"https://xnerv.wang/tags/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"},{"name":"TCP","slug":"TCP","permalink":"https://xnerv.wang/tags/TCP/"},{"name":"IP","slug":"IP","permalink":"https://xnerv.wang/tags/IP/"},{"name":"UDP","slug":"UDP","permalink":"https://xnerv.wang/tags/UDP/"},{"name":"操作系统","slug":"操作系统","permalink":"https://xnerv.wang/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"线程同步","slug":"线程同步","permalink":"https://xnerv.wang/tags/%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5/"},{"name":"原子操作","slug":"原子操作","permalink":"https://xnerv.wang/tags/%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/"},{"name":"volatile","slug":"volatile","permalink":"https://xnerv.wang/tags/volatile/"},{"name":"MySQL","slug":"MySQL","permalink":"https://xnerv.wang/tags/MySQL/"},{"name":"MDL","slug":"MDL","permalink":"https://xnerv.wang/tags/MDL/"},{"name":"gdb","slug":"gdb","permalink":"https://xnerv.wang/tags/gdb/"},{"name":"InnoDB","slug":"InnoDB","permalink":"https://xnerv.wang/tags/InnoDB/"},{"name":"MTR","slug":"MTR","permalink":"https://xnerv.wang/tags/MTR/"},{"name":"Signal","slug":"Signal","permalink":"https://xnerv.wang/tags/Signal/"},{"name":"Adaptive hash index","slug":"Adaptive-hash-index","permalink":"https://xnerv.wang/tags/Adaptive-hash-index/"},{"name":"Hash","slug":"Hash","permalink":"https://xnerv.wang/tags/Hash/"},{"name":"MySQL charset","slug":"MySQL-charset","permalink":"https://xnerv.wang/tags/MySQL-charset/"},{"name":"Index Condition Pushdown","slug":"Index-Condition-Pushdown","permalink":"https://xnerv.wang/tags/Index-Condition-Pushdown/"},{"name":"Transaction","slug":"Transaction","permalink":"https://xnerv.wang/tags/Transaction/"},{"name":"Lock","slug":"Lock","permalink":"https://xnerv.wang/tags/Lock/"},{"name":"Binlog Group Commit","slug":"Binlog-Group-Commit","permalink":"https://xnerv.wang/tags/Binlog-Group-Commit/"},{"name":"TokuDB","slug":"TokuDB","permalink":"https://xnerv.wang/tags/TokuDB/"},{"name":"RocksDB","slug":"RocksDB","permalink":"https://xnerv.wang/tags/RocksDB/"},{"name":"TiDB","slug":"TiDB","permalink":"https://xnerv.wang/tags/TiDB/"},{"name":"CockroachDB","slug":"CockroachDB","permalink":"https://xnerv.wang/tags/CockroachDB/"},{"name":"Dynamo","slug":"Dynamo","permalink":"https://xnerv.wang/tags/Dynamo/"},{"name":"Bigtable","slug":"Bigtable","permalink":"https://xnerv.wang/tags/Bigtable/"},{"name":"LevelDB","slug":"LevelDB","permalink":"https://xnerv.wang/tags/LevelDB/"},{"name":"SQL Server","slug":"SQL-Server","permalink":"https://xnerv.wang/tags/SQL-Server/"},{"name":"SIX Lock","slug":"SIX-Lock","permalink":"https://xnerv.wang/tags/SIX-Lock/"},{"name":"Buffer Pool","slug":"Buffer-Pool","permalink":"https://xnerv.wang/tags/Buffer-Pool/"},{"name":"Lazy Writing","slug":"Lazy-Writing","permalink":"https://xnerv.wang/tags/Lazy-Writing/"},{"name":"Eager Writing","slug":"Eager-Writing","permalink":"https://xnerv.wang/tags/Eager-Writing/"},{"name":"Checkpoint","slug":"Checkpoint","permalink":"https://xnerv.wang/tags/Checkpoint/"},{"name":"Read Ahead","slug":"Read-Ahead","permalink":"https://xnerv.wang/tags/Read-Ahead/"},{"name":"Memory Clerk","slug":"Memory-Clerk","permalink":"https://xnerv.wang/tags/Memory-Clerk/"},{"name":"Row Lock","slug":"Row-Lock","permalink":"https://xnerv.wang/tags/Row-Lock/"},{"name":"Page Lock","slug":"Page-Lock","permalink":"https://xnerv.wang/tags/Page-Lock/"},{"name":"FTP","slug":"FTP","permalink":"https://xnerv.wang/tags/FTP/"},{"name":"Active FTP","slug":"Active-FTP","permalink":"https://xnerv.wang/tags/Active-FTP/"},{"name":"Passive FTP","slug":"Passive-FTP","permalink":"https://xnerv.wang/tags/Passive-FTP/"},{"name":"InnoDB Log","slug":"InnoDB-Log","permalink":"https://xnerv.wang/tags/InnoDB-Log/"},{"name":"Row Format","slug":"Row-Format","permalink":"https://xnerv.wang/tags/Row-Format/"},{"name":"Isolation Level","slug":"Isolation-Level","permalink":"https://xnerv.wang/tags/Isolation-Level/"},{"name":"WinDbg","slug":"WinDbg","permalink":"https://xnerv.wang/tags/WinDbg/"},{"name":"Lock Escalations","slug":"Lock-Escalations","permalink":"https://xnerv.wang/tags/Lock-Escalations/"},{"name":"Intent Lock","slug":"Intent-Lock","permalink":"https://xnerv.wang/tags/Intent-Lock/"},{"name":"Update Lock","slug":"Update-Lock","permalink":"https://xnerv.wang/tags/Update-Lock/"},{"name":"MVCC","slug":"MVCC","permalink":"https://xnerv.wang/tags/MVCC/"},{"name":"区块链与比特币","slug":"区块链与比特币","permalink":"https://xnerv.wang/tags/%E5%8C%BA%E5%9D%97%E9%93%BE%E4%B8%8E%E6%AF%94%E7%89%B9%E5%B8%81/"},{"name":"区块链","slug":"区块链","permalink":"https://xnerv.wang/tags/%E5%8C%BA%E5%9D%97%E9%93%BE/"},{"name":"比特币","slug":"比特币","permalink":"https://xnerv.wang/tags/%E6%AF%94%E7%89%B9%E5%B8%81/"},{"name":"SPV","slug":"SPV","permalink":"https://xnerv.wang/tags/SPV/"},{"name":"Heap","slug":"Heap","permalink":"https://xnerv.wang/tags/Heap/"},{"name":"MSDN","slug":"MSDN","permalink":"https://xnerv.wang/tags/MSDN/"},{"name":"2PC","slug":"2PC","permalink":"https://xnerv.wang/tags/2PC/"},{"name":"3PC","slug":"3PC","permalink":"https://xnerv.wang/tags/3PC/"},{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://xnerv.wang/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"},{"name":"排序算法","slug":"排序算法","permalink":"https://xnerv.wang/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"name":"查找算法","slug":"查找算法","permalink":"https://xnerv.wang/tags/%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%95/"},{"name":"路由协议","slug":"路由协议","permalink":"https://xnerv.wang/tags/%E8%B7%AF%E7%94%B1%E5%8D%8F%E8%AE%AE/"},{"name":"C++","slug":"C","permalink":"https://xnerv.wang/tags/C/"},{"name":"Programing","slug":"Programing","permalink":"https://xnerv.wang/tags/Programing/"},{"name":"Memory Management","slug":"Memory-Management","permalink":"https://xnerv.wang/tags/Memory-Management/"},{"name":"Virtual Memory","slug":"Virtual-Memory","permalink":"https://xnerv.wang/tags/Virtual-Memory/"},{"name":"COFF","slug":"COFF","permalink":"https://xnerv.wang/tags/COFF/"},{"name":"linker","slug":"linker","permalink":"https://xnerv.wang/tags/linker/"},{"name":"编程语言","slug":"编程语言","permalink":"https://xnerv.wang/tags/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"},{"name":"Python","slug":"Python","permalink":"https://xnerv.wang/tags/Python/"},{"name":"未完成","slug":"未完成","permalink":"https://xnerv.wang/tags/%E6%9C%AA%E5%AE%8C%E6%88%90/"},{"name":"设计模式","slug":"设计模式","permalink":"https://xnerv.wang/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"Java","slug":"Java","permalink":"https://xnerv.wang/tags/Java/"},{"name":"信号","slug":"信号","permalink":"https://xnerv.wang/tags/%E4%BF%A1%E5%8F%B7/"},{"name":"中断","slug":"中断","permalink":"https://xnerv.wang/tags/%E4%B8%AD%E6%96%AD/"},{"name":"内核态与用户态","slug":"内核态与用户态","permalink":"https://xnerv.wang/tags/%E5%86%85%E6%A0%B8%E6%80%81%E4%B8%8E%E7%94%A8%E6%88%B7%E6%80%81/"},{"name":"文件系统","slug":"文件系统","permalink":"https://xnerv.wang/tags/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"},{"name":"Google","slug":"Google","permalink":"https://xnerv.wang/tags/Google/"},{"name":"Protobuf","slug":"Protobuf","permalink":"https://xnerv.wang/tags/Protobuf/"},{"name":"Google Developer","slug":"Google-Developer","permalink":"https://xnerv.wang/tags/Google-Developer/"},{"name":"C++异常","slug":"C-异常","permalink":"https://xnerv.wang/tags/C-%E5%BC%82%E5%B8%B8/"},{"name":"内存屏障","slug":"内存屏障","permalink":"https://xnerv.wang/tags/%E5%86%85%E5%AD%98%E5%B1%8F%E9%9A%9C/"},{"name":"DLL","slug":"DLL","permalink":"https://xnerv.wang/tags/DLL/"},{"name":"DLL Hell","slug":"DLL-Hell","permalink":"https://xnerv.wang/tags/DLL-Hell/"},{"name":"译文","slug":"译文","permalink":"https://xnerv.wang/tags/%E8%AF%91%E6%96%87/"},{"name":"Windows Internals","slug":"Windows-Internals","permalink":"https://xnerv.wang/tags/Windows-Internals/"},{"name":"IOCP","slug":"IOCP","permalink":"https://xnerv.wang/tags/IOCP/"},{"name":"Shell","slug":"Shell","permalink":"https://xnerv.wang/tags/Shell/"},{"name":"Session","slug":"Session","permalink":"https://xnerv.wang/tags/Session/"},{"name":"TTY","slug":"TTY","permalink":"https://xnerv.wang/tags/TTY/"},{"name":"PTS","slug":"PTS","permalink":"https://xnerv.wang/tags/PTS/"},{"name":"Oracle","slug":"Oracle","permalink":"https://xnerv.wang/tags/Oracle/"},{"name":"缓存","slug":"缓存","permalink":"https://xnerv.wang/tags/%E7%BC%93%E5%AD%98/"},{"name":"内存管理","slug":"内存管理","permalink":"https://xnerv.wang/tags/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"},{"name":"进程","slug":"进程","permalink":"https://xnerv.wang/tags/%E8%BF%9B%E7%A8%8B/"},{"name":"Shared Library","slug":"Shared-Library","permalink":"https://xnerv.wang/tags/Shared-Library/"},{"name":"Multi-Range Read","slug":"Multi-Range-Read","permalink":"https://xnerv.wang/tags/Multi-Range-Read/"},{"name":"GFS","slug":"GFS","permalink":"https://xnerv.wang/tags/GFS/"},{"name":"Buddy System","slug":"Buddy-System","permalink":"https://xnerv.wang/tags/Buddy-System/"},{"name":"Slab Allocator","slug":"Slab-Allocator","permalink":"https://xnerv.wang/tags/Slab-Allocator/"},{"name":"reprint","slug":"reprint","permalink":"https://xnerv.wang/tags/reprint/"},{"name":"协程","slug":"协程","permalink":"https://xnerv.wang/tags/%E5%8D%8F%E7%A8%8B/"},{"name":"线程","slug":"线程","permalink":"https://xnerv.wang/tags/%E7%BA%BF%E7%A8%8B/"},{"name":"C","slug":"C","permalink":"https://xnerv.wang/tags/C/"},{"name":"C内存管理","slug":"C内存管理","permalink":"https://xnerv.wang/tags/C%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"},{"name":"Consistent Hashing","slug":"Consistent-Hashing","permalink":"https://xnerv.wang/tags/Consistent-Hashing/"},{"name":"一致性哈希","slug":"一致性哈希","permalink":"https://xnerv.wang/tags/%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C/"},{"name":"分布式事务","slug":"分布式事务","permalink":"https://xnerv.wang/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"name":"Paxos","slug":"Paxos","permalink":"https://xnerv.wang/tags/Paxos/"},{"name":"Linux API","slug":"Linux-API","permalink":"https://xnerv.wang/tags/Linux-API/"},{"name":"SELinux","slug":"SELinux","permalink":"https://xnerv.wang/tags/SELinux/"},{"name":"进程管理","slug":"进程管理","permalink":"https://xnerv.wang/tags/%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/"},{"name":"孤儿进程","slug":"孤儿进程","permalink":"https://xnerv.wang/tags/%E5%AD%A4%E5%84%BF%E8%BF%9B%E7%A8%8B/"},{"name":"僵尸进程","slug":"僵尸进程","permalink":"https://xnerv.wang/tags/%E5%83%B5%E5%B0%B8%E8%BF%9B%E7%A8%8B/"},{"name":"OS","slug":"OS","permalink":"https://xnerv.wang/tags/OS/"},{"name":"Stack Overflow","slug":"Stack-Overflow","permalink":"https://xnerv.wang/tags/Stack-Overflow/"},{"name":"Working Set","slug":"Working-Set","permalink":"https://xnerv.wang/tags/Working-Set/"},{"name":"虚表","slug":"虚表","permalink":"https://xnerv.wang/tags/%E8%99%9A%E8%A1%A8/"},{"name":"多态","slug":"多态","permalink":"https://xnerv.wang/tags/%E5%A4%9A%E6%80%81/"},{"name":"Memory Barrier","slug":"Memory-Barrier","permalink":"https://xnerv.wang/tags/Memory-Barrier/"},{"name":"无锁队列","slug":"无锁队列","permalink":"https://xnerv.wang/tags/%E6%97%A0%E9%94%81%E9%98%9F%E5%88%97/"},{"name":"yield","slug":"yield","permalink":"https://xnerv.wang/tags/yield/"},{"name":"rsync","slug":"rsync","permalink":"https://xnerv.wang/tags/rsync/"},{"name":"BigTable","slug":"BigTable","permalink":"https://xnerv.wang/tags/BigTable/"},{"name":"MapReduce","slug":"MapReduce","permalink":"https://xnerv.wang/tags/MapReduce/"},{"name":"Savepoint","slug":"Savepoint","permalink":"https://xnerv.wang/tags/Savepoint/"},{"name":"Mutex","slug":"Mutex","permalink":"https://xnerv.wang/tags/Mutex/"},{"name":"Multi Threads","slug":"Multi-Threads","permalink":"https://xnerv.wang/tags/Multi-Threads/"},{"name":"Bash","slug":"Bash","permalink":"https://xnerv.wang/tags/Bash/"},{"name":"File System","slug":"File-System","permalink":"https://xnerv.wang/tags/File-System/"},{"name":"搜索引擎","slug":"搜索引擎","permalink":"https://xnerv.wang/tags/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/"},{"name":"PageRank","slug":"PageRank","permalink":"https://xnerv.wang/tags/PageRank/"},{"name":"spinlock","slug":"spinlock","permalink":"https://xnerv.wang/tags/spinlock/"},{"name":"Private bytes","slug":"Private-bytes","permalink":"https://xnerv.wang/tags/Private-bytes/"},{"name":"mmap","slug":"mmap","permalink":"https://xnerv.wang/tags/mmap/"},{"name":"虚拟内存","slug":"虚拟内存","permalink":"https://xnerv.wang/tags/%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98/"},{"name":"页表","slug":"页表","permalink":"https://xnerv.wang/tags/%E9%A1%B5%E8%A1%A8/"}]}